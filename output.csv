post_id,title,sub_title,post_date,blog_text
184371353.from-dba-to-data-everything,From DBA to Data Everything,Lessons From a Career in Data,2026-01-14 06:51:03.796-06,"In this episode of the Data Engineering Central Podcast, I interview a Data OG, someone who‚Äôs been around the data space forever, and we talked about all things data, past, present, and future.
I‚Äôm joined by
Thomas Horton
a longtime friend and one of the most well-rounded data professionals I know. Over the course of his career, Tom has worn just about every hat in data: developer, DBA, analyst, and everything in between. He‚Äôs lived through the era of on-prem databases, the rise of analytics, and the constant reinvention that defines modern data engineering today.
We talk about what‚Äôs changed, what hasn‚Äôt, and why many of the ‚Äúnew‚Äù problems in data feel oddly familiar. We also dig into lessons learned the hard way, lessons that are just as relevant for early-career data engineers as they are for seasoned practitioners navigating today‚Äôs ever-expanding stacks.
Subscribe now
On a personal note, a huge portion of what I know about relational databases and analytics can be traced back to Tom. This conversation is part reflection, part history lesson, and part reality check on where the data industry is headed next.
If you‚Äôre interested in the past, present, and future of data‚Äîand what really matters beneath all the tooling, this is an episode you won‚Äôt want to miss.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
183177983.apache-flink-for-dummies,Apache Flink for Dummies,... like me,2026-01-12 07:49:27.399-06,"So you don‚Äôt want to be a streaming dummy, eh? Who wants to be a dummy? Not me, not you. We live in a strange AI-generated world, maybe the writing on the wall says SkyNet is going to write all the code from now on.
You suck down your
Big Glup
and eat Doritos while you order your agents around.
Who would have guessed the future would taste so sweet?
I‚Äôm here to give you ye‚Äô old kick in the pants, smack on the back of the head, throw that dodgeball right where it hurts. It‚Äôs like we‚Äôre back in middle school, fighting for our lives in gym class.
While the world of programming burns around us, we will ignore it with sweet indifference and continue to push ourselves to learn new things, to poke under rocks,
and to grow by learning
.
Today, you and I are going to move from zero to
Apache Flink hero.
Streaming data, here we come.
A gentle introduction to Apache Flink
Ok, we have to start somewhere, and I‚Äôm not going to assume you have experience working with ‚Äústreaming.‚Äù If we head over to the
GitHub page for Flink
, we can see that it‚Äôs mostly written in Java.
That language no one talks about, but everyone uses.
What is it?
A data processing framework that is focused on streaming, but supports batch processing. How does Flink describe itself? This can sometimes be helpful.
If you went over to
the official Flink website
and started poking around the
docs and quickstarts
, all your Apache Spark users f‚Äô fingers would start tingling.
It has that feel to it.
How so? You would see things like using a Bash file to start a cluster, command line arguments to submit JAR job files ‚Ä¶ yeah ‚Ä¶ like I said ‚Ä¶ Spark.
Before you get your panties all in a bunch, I just said it
feels
like Spark, calm down.
Even some of the core concepts are reminiscent of other distributed systems you are used to, as they would be. Although one could argue that the backend is more complex, as streaming would be.
The truth is that we live in an abstracted world; this isn‚Äôt 2018,
where you have to build out your own Spark/HDFS clusters by hand, as I did.
Databricks, Snowflake, BigQuery, Redshift, the four horses of the apocalypse made sure that era was dead.
We live in the SaaS age, where every cloud vendor has done away with the need to manually set up your own clusters, configurations, and hire legions of Platform Engineers to build and maintain your own distributed systems.
I understand there are some corners of the interwebs where that still happens, but that is the exception, not the rule.
Today, the leading SaaS vendor offering for
Flink is Confluent
.
Heck, even
AWS is in the business of offering managed Flink.
All this to say that we may touch on the fundamentals of Flink, but more importantly, we will focus on how to use it and the APIs and SDKs for interacting with it. How to think about Apache Flink as a user of Flink, not a builder of Flink clusters.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Interacting with Apache Flink
How does one go about eating an elephant? One bite at a time. So
let‚Äôs start with the Flink APIs
, aka our gateway into using code to do things in Flink.
Stateful Stream Processing
Low-level and the most minute control.
DataStream API
Primary streaming usage API.
Expresses computations as a series of
transformations
on streams:
map
,
filter
,
keyBy
,
window
,
aggregate
, etc.
Table API
Structured streaming / Table mindset
Allows relational operations like:
select
,
filter
,
join
,
groupBy
,
aggregate
SQL API
Same as Table API, but with an SQL interface.
It‚Äôs also essential to see
which Connectors are offered out of the box for Flink to ingest or push to.
You will notice many other streaming tools listed, such as Kafka.
Wait, isn‚Äôt Flink a competitor Kafka?
No
.
Apache Flink is a stream processing framework, whereas Kafka is mainly used to move data.
Flink has the ability to work on State.
We can think about working on events in a stream individually, which is fine. Still, the power of Flink comes into play when you consider the concept of State and how to handle multiple events that occur in sequence and relate to each other.
Doesn‚Äôt this make the APIs make more sense?
Being able to deal with things aggregate, groupBY, or SQL interface.
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                  Apache Flink Stream Job               ‚îÇ
     ‚îÇ                                                        ‚îÇ
     ‚îÇ   [Source: events coming in continuously]              ‚îÇ
     ‚îÇ            ‚îÇ                                           ‚îÇ
     ‚îÇ            ‚ñº                                           ‚îÇ
     ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   Watermarks indicate event time   ‚îÇ
     ‚îÇ     ‚îÇ  KeyBy(user)‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
     ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ                            ‚îÇ
     ‚îÇ            ‚îÇ             ‚ñº                             ‚îÇ
     ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
     ‚îÇ     ‚îÇ  Window     ‚îÇ   ‚îÇ    State    ‚îÇ‚Üê‚îÄ(state          ‚îÇ
     ‚îÇ     ‚îÇ  (1 min)    ‚îÇ   ‚îÇ  Store:     ‚îÇ   updates)       ‚îÇ
     ‚îÇ     ‚îÇ  (tumbling) ‚îÇ   ‚îÇ  counts,    ‚îÇ                  ‚îÇ
     ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  sums,      ‚îÇ                  ‚îÇ
     ‚îÇ            ‚îÇ          ‚îÇ  last event ‚îÇ                  ‚îÇ
     ‚îÇ            ‚ñº          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
     ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄoutput‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂            ‚îÇ
     ‚îÇ      ‚îÇ     (aggregates/emitted results)                ‚îÇ
     ‚îÇ      ‚ñº                                                 ‚îÇ
     ‚îÇ   [Sink: external DB / Kafka / dashboard]              ‚îÇ
     ‚îÇ                                                        ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
For example, if we have streaming events for a user, we could aggregate them for that user. This is much more powerful than, say, simply consuming a stream of events directly into a final data store and then running batch jobs later to tell us what happened to that user over time.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Apache Flink with Docker and Python + SQL
If you want a headache, go ahead and read the page for Flink with Docker, it‚Äôs enough to put you in an early grave. I will save you the time.
Save this code as a
docker-compose.yml
file and run
docker compose up
.
This will give you a local, long-running Flink cluster with a UI and endpoints to submit test jobs. After you set up and run the Docker Compose file, you can go to
localhost:8081
to access the UI.
There will be two Docker containers running ‚Ä¶
CONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS                  PORTS                              NAMES
6c83d7b93697   flink:latest   ""/docker-entrypoint.‚Ä¶""   2 minutes ago   Up 2 minutes            6123/tcp, 8081/tcp                 flink-taskmanager-1
37b243e28fd7   flink:latest   ""/docker-entrypoint.‚Ä¶""   2 minutes ago   Up 2 minutes            6123/tcp, 0.0.0.0:8081->8081/tcp   flink-jobmanager-1
We can jump into the JobManager container.
$ docker exec -it $(docker ps --filter name=jobmanager --format={{.ID}}) /bin/sh
Now that we are there,
we can install pyflink
and vim inside the Docker image, doesn‚Äôt come with it.
>> apt-get update
>> apt-get install vim
>> apt-get install python3
>> apt-get install python3-pip
>> apt-get update && apt-get install -y --no-install-recommends \
  openjdk-17-jdk \
  build-essential \
  python3-dev
>> ln -s /usr/bin/python3 /usr/bin/python
>> python3 -m pip install apache-flink
>> export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-arm64
>> export PATH=""$JAVA_HOME/bin:$PATH""
Let‚Äôs create a sample CSV file inside the Docker image. (the Task image)
>> vim input.csv
user_id,amount
alice,10
bob,5
alice,7
bob,3
And some code to do an aggregate using the Table API.
And to run the code ‚Ä¶
>> ./bin/flink run --python job.py
Job has been submitted with JobID 6d7674483eb77b475e6ad6c2e9224ada
We can see the job running in the UI. If we check the container, we should see the aggregate results. Indeed, we do.
docker exec -it flink-taskmanager-1 bash -lc ""head -n 50 /opt/flink/out/output_dir/.staging_*/task-0-attempt-0/part-*""

==> /opt/flink/out/output_dir/.staging_1767457913209_5944528a-8f7e-4827-9f3b-b6f2812df288/task-0-attempt-0/part-b7ff1b6d-6ad5-42b6-bc2f-31dee18ac149-task-0-file-0 <==
alice,17
bob,8
user_id,

==> /opt/flink/out/output_dir/.staging_1767458019128_1464bc42-6c54-4a86-a74f-0d859de11104/task-0-attempt-0/part-98e3c995-d54e-4f11-8739-f272b02961b7-task-0-file-0 <==
alice,17
bob,8
user_id,
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Stating the obvious.
Well, anyone who‚Äôs spent some time around Spark, at a low level, is going to be sweating in their seats. The entire thing is basically a clone of Spark when it comes to setting up a cluster and dealing with distributed system ‚Äúthings.‚Äù
Lot‚Äôs of messing around with Java, getting the right JARs in place, the UI, submitting jobs, etc.
You could, like me, use Flink to do some batch processing, although I‚Äôm not sure why you would do that unless it‚Äôs pre-existing in your stack already. Also, just like Spark these days, who would want to set this up and manage a Flink cluster themselves? Not me, not even on the old Kube.
So much work to get to point A to start a project. Best to use a managed version from someone else at the end of the day, save the trouble.
Also, clearly, Flink was made to work on data streams.
I should attempt a local version of that, but I‚Äôm outta steam for the moment.
It really made me think about
Spark Streaming vs Flink
, why a person would choose one over the other.
The
has a great
article comparing Spark Streaming vs Flink.
As far as I can tell, Spark Streaming has made many improvements and is getting closer to being as good as Flink at streaming. Still, Flink is designed to be streaming-first, and as such, is the first choice for hardcore streaming platforms, especially when working with Kafka.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Did you learn something?
Well, at least I did. I‚Äôve heard a lot about Flink over the years, but never took the time to play with it. Even just playing around on Docker with our Flink setup, writing a simple batch pipeline, this sort of thing teaches me something about a tool.
I know more about Flink now than I did when I started. That‚Äôs the point."
183096607.apache-arrow-for-the-database,Apache Arrow for the Database,ADBC Drivers,2026-01-16 09:50:52.979-06,"Apache Arrow has been slowly sticking its fingers into every corner of the data world for the last few years. Today, we examine one of those dusty corners, namely,
database drivers built with Apache Arrow.
I suppose this makes sense, with Arrow being used up and down the stack for in-memory data, a
s the foundational building block for a myriad of tools.
Yeah, we live in a Lake House world, but relational databases are still the bread and butter of many a Data Platform, or at least a portion of the data storage and movement. Postgres ain‚Äôt going nowhere.
Heck, with the advent of Lakebase
,
its never-ending reign is pretty much set in stone now.
If I were to play the devil‚Äôs advocate for a minute, why would one even care about using Apache Arrow as a database driver?
Many tools already use Arrow; adding it at the database layer reduces friction and data copies/conversions.
Arrow is fast as crap
SQL support
It seems beneficial only if one is already using an Arrow-based tool, but Arrow
database
drivers may be that much faster, making them worth using anyway.
Let‚Äôs look for the answer to this question by simply playing around with an Arrow database driver, pushing a bunch of data back and forth (
to and from the database
), and seeing what it‚Äôs like to use Arrow as a database driver in general.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Python Arrow database driver with Postgres.
The easiest way to test this out is going to be using Python + Postgres to see not only if
the ADBC driver option
is fast, faster than other options, but generally speaking,
if it‚Äôs a pleasure to use.
I‚Äôm only mildly interested in performance; I mean, fast is always lovely, but the aesthetics and developer ‚Äúfriendliness‚Äù of a tool are almost more important.
How does it install?
What tools does it integrate with?
What are the ergonomics?
It‚Äôs a little strange talking about an ADBC database driver in this context (if you‚Äôve been around for the ODBC/JDBC nightmares), but nonetheless, onward.
So, let‚Äôs get to it. Data first.
Let‚Äôs gather
some 2025 data from the Divvy Bike Trips open source dataset.
Very standard data, and we shall do a very standard thing with these files. Read them all and shove them into a Postgres database. This isn‚Äôt going to be a well-thought-out or bulletproof test, but more of an exploration.
We will compare Arrow‚Äôs ADBC driver with Python and DuckDB (non-Arrow) to just get a baseline.
psycopg2
Python ADBC driver
DuckDB
More than anything, I want to get a sense of what it‚Äôs like to use each tool to shovel data, generally what performance is like as well.
It‚Äôs ‚Äúeach to his own,‚Äù you may do as you please, but it‚Äôs better to know our options rather than be stuck in the same old stuff.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The Code
Let‚Äôs dive into using Apache Arrow ADBC drivers to shove data into Postgres. We will use a local Docker instance for the database. Also, we will use PyArrow to read and prepare the data for insertion, and we will add timers only for the INSERT part.
All the code is available on GitHub, along with a README if you want to follow along.
We will only be inserting
4,758,124, so not much, but it will do.
>> docker-compose up -d
>> uv run pyarrow_adbc_driver.py
Connecting to Postgres at localhost:5432/postgres...
Testing connection to Postgres...
‚úì Connected successfully! Postgres version: PostgreSQL 16.11 (Debian 16.11-1.pgdg13+1) on aarch64-unknown-linux-gnu
Reading all CSV files from data/uncompressed...
Read 4,758,124 total rows from dataset
Creating table: CREATE TABLE ""divvy_tripdata"" (""ride_id"" TEXT, ""rideable_type"" TEXT, ""started_at"" TIMESTAMP, ""ended_at"" TIMESTAMP, ""start_station_name"" TEXT, ""start_station_id"" TEXT, ""end_station_name"" TEXT, ""end_station_id"" TEXT, ""start_lat"" DOUBLE PRECISION, ""start_lng"" DOUBLE PRECISION, ""end_lat"" DOUBLE PRECISION, ""end_lng"" DOUBLE PRECISION, ""member_casual"" TEXT)
Inserting 4758124 rows into divvy_tripdata...
Insert completed successfully in 17.33 seconds (274,633 rows/sec)
Verification: 4758124 rows in divvy_tripdata
Of course, we aren‚Äôt sending anything over the wire, but still, over four million records in 17 seconds is very fast indeed.
Remember, you can look more closely at this code on GitHub.
Really, if you get rid of all the fluff, the magic is in the simplicity of Arrow itself end to end, and also the ease of using the ADBC driver for Postgres.
Doesn‚Äôt get cleaner than that, *
I think.
Let‚Äôs try
psycopg2 without any Arrow, this probably isn‚Äôt the fairest test, but whatever.
You can see the full code here
, but here is the core of it.
Inserting 4758124 rows into divvy_tripdata_psycopg2_noarrow...
Insert completed successfully in 60.23 seconds (79,003 rows/sec)
If you are interested in reading more about
psycopg2 inserts, and performance, check out the below post.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The previous 60 seconds were super slow, so let‚Äôs at least try to speed them up to appear non-biased in this review. Most people know that using COPY statements is a good way to get the bytes screaming.
Inserting 4,758,124 rows into divvy_tripdata_psycopg2_noarrow...
Insert completed successfully in 24.48 seconds (194,375 rows/sec)
Dang, down to 24 seconds. Still not the 17 seconds of the ADBC driver,
and note the difference in code complexity between the two!
Never underestimate code complexity or the importance of simple solutions.
Now out of curiosity we must use DuckDB to get a good baseline on how vanilla Python and ABDC Arrow drivers compare to that GOAT of Data Engineering tools, DuckDB.
Good Lord.
Insert completed successfully in 4.12 seconds (1,155,454 rows/sec)
Total rows inserted: 4,758,124
Holy Batman, that‚Äôs fast.
While we are at it, did you know that you can use Polars + the ADBC driver?
Check the GitHub repo for the full code
‚Ä¶ basically, it uses ADBC in Polars.
Inserting 4,758,124 rows into divvy_tripdata_polars...
Insert completed successfully in 22.18 seconds (214,534 rows/sec)
Not bad. Slightly faster than vanilla Python.
I mean, if we pretend DuckDB doesn‚Äôt exist, ADBC Arrow drivers are the best option, and don‚Äôt forget that the code was extremely simple.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Thoughts
I think that Arrow, on the whole, is probably the future of a lot of Data Engineering; it already is at this point. The only reason it isn‚Äôt a ‚Äúmainstream‚Äù conversation is that Arrow is used to build many tools underneath the hood, so it isn‚Äôt always obvious to someone that they are using Arrow.
I was excited to hear about the ADBC Arrow drivers for the databases.
Pushing data to and from databases has always been a bottleneck in many data pipelines and processes. The idea of using Arrow in memory with certain tools, then pushing data back and forth across time-sensitive processes, is a great option.
In my play benchmark, ADBC drivers with Python beat out vanilla Python easily enough, but didn‚Äôt come close to touching DuckDB. Competition and options are good things in life, it will only push the data community farther, together, for the good of all.
Subscribe now"
182889224.data-engineering-central-podcast-f35,Data Engineering Central Podcast - 10,... back again,2026-01-02 07:03:33.782-06,"Back with another podcast episode, we talk the whole gamut this time.
The cost of AI Agents, Toon, tokens, and hosting models vs. paying per token.
The ease of building agents
Data Mesh, dead or alive?
Are Data Catalogs the future
Apache Arrow is eating the world
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
182652308.databricksspark-excel-data-source,Databricks/Spark Excel Data Source,... do I really have to say it?,2026-01-05 08:02:47.674-06,"Well, don‚Äôt pretend like y‚Äôall didn‚Äôt see it coming. If I close my eyes and listen to the winter wind blowing through the maple trees, I can see
Josue Bogran
dancing in the moonlight with joy. There is truth in what he says, ‚Äú
All roads lead to Excel
.
‚Äù
It does depend on the size and nature of the business in which a data person finds themselves. Still, at the end of the day, if you have any resemblance to different business groups to deal with, like Accounting, Marketing, Product, Ops, etc, then Excel
drives
and
contains
ALOT of business context.
Truth be told, business users can use Excel, and it works.
It is no surprise at all that Databricks released support for Excel as both a data
source
and
sink
.
Actually, it's surprising it took this long.
Take a moment to check out my
YouTube channel
, if you please, kind sir, or ‚Äòmam.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Databricks now supports Excel.
Today, I want to take an honest look at Databricks' new support for Excel as a read-and-write via Spark and SQL. We should be honest with each other about the need for such a feature and examine both the upsides and downsides of using Excel within the Lake House architecture.
There was a time in my life when I would foam and spit down my face like a madman as I fought tooth and nail against the inclusion of Excel in any part of a Data Platform.
But, time and experience have taught me moderation in my views, and most importantly, that code and perfection are less critical than enabling the business to succeed and meeting them where they are.
Save all your doom-and-gloom comments for later; we will get to them eventually.
Let‚Äôs poke at Excel in Databricks, see what happens and how it works, then we will get to talking about whether you should or not.
Working with Excel in Databricks Spark
So, let‚Äôs play around with this new feature in Databricks, see how it reacts to different types of Excel files, the way they are formatted, and get the basics figured out. After that, I would like to move on and Steelman the case for and against Excel in Data Engineering.
Basics ‚Ä¶
Databricks Runtime 17.1 or above.
First, we should get a ‚Äúnormal‚Äù Excel file, something that comes from the real world and hasn't been massaged to work. I have run another blog, www.confessionsofadataguy.com, for many moons. We will download the December numbers from Google Analytics from that website.
Raw, just as it comes in the Excel file download.
Export to Google Sheets, download as Excel.
Let‚Äôs crack open a Databricks Serverless attached Notebook. I went ahead and uploaded the Excel file to my workspace, copied the full path by right-clicking the file, and tried to open it.
Immediate error.
Path does not exist is a strange error.
This is how the
docs tell me to read Excel
files,
not sure if it simply doesn‚Äôt/can‚Äôt read it from a Workspace location??
Maybe I need to upload the Excel files like they mention here.
Let‚Äôs put the Excel file in a Volume.
And try again.
That seemed to do the trick, now we can see and read the Excel file easily with Spark.
Of course, this points out the obvious problem with Excel and Spark. Most Excel files used by the business will have multiple tabs/sheets, weird formatting, and the like.
All future Data Engineers who ingest Excel files in Databricks Spark will have to become experts in either ‚Ä¶
massaging Excel files
using options to read the needed data
Otherwise, Excel files are going to fit into Dataframes like square peg in a round hole.
Databricks provides several Excel parsing options ‚Ä¶
dataAddress
the address of the cell and/or sheet range
headerRows
duh
operation
little strange, but either read a sheet or list the sheets
timestampNTZFormat
dateFormat
These options are helpful. As you can see in our example Excel file, we could try some of them to get a clean Dataframe.
Our ‚Äú
dataAddress
‚Äù is
A10:H385
, with the first row of that range as the header.
So ‚Ä¶
Perfection. Of course the 90% of engineers who are hooked on SQL like crack, there is that option as well.
I am going to skip the other options Databricks has added for Excel, like AutoLoader, COPY INTO, blah, blah.
Feel free to go dig through that crap if you want.
Oh, one other thing I should mention, that is quite possibly the BEST Excel feature that Databricks added, is the ability to WRITE a Dataframe to Excel format.
We just ordered our original Excel read Dataframe by views, limited it to the top ten, and wrote it back out as another Excel file. The file appears, although in the UI it shows as a folder.
Hilariously, as per normal Spark, you get the classic multile file commit, including a part-blah-blah.xlsx file.
I mean, that defeats the purpose, doesn‚Äôt it? Why even support writing to Excel if you‚Äôre going to do it like that?
Bunch of ninnies.
Better than nothing maybe, or you just use Pandas or something to get an Excel file.
Ok, now that we have the lay of the land, let‚Äôs talk about Excel in Databricks, what they have done right and wrong.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Brass Tacks
I‚Äôm sure there are three groups of Data Engineers when it comes to the Excel discussion.
Don‚Äôt care.
Hate it.
Love it.
Is there really any other option? Excel has a storied history in the data community; it‚Äôs often seen, for good reason, as the bane of data existence, creating all sorts of problems. Yet, there is probably no other tool as critical to the modern corporation as Excel.
That‚Äôs the world we live in.
Let‚Äôs just be brutally honest with each other.
Is this a good thing that Databricks added Excel support?
Yes for some, no for others.
Excel is not going anywhere, does it have a place in the Modern Data Stack as a source? Well, that time has come now that Databricks has added support; other vendors will follow suit.
Just as well get used to it.
What will go wrong? Everything, of course.
No control over formatting
Things will change without notice
No versioning
Data pipelines will break
Guess what? You never had control over all those flat files (CSV/TXT) you were getting anyway. Those changed, headers changed, formats changed, pipelines broke.
No.
Data sources, regardless of format, break pipelines every hour, every day, 365 days a year, around the world. Excel, as a data source, will not change that; it might just make it a little worse.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The gaps are closing
All this indicates is the same thing that has been happening over the last few decades, and is now accelerating with AI. The gap between business and data is closing quickly.
The era of true self-service Analytics/BI and simply self-service data has already arrived.
Databricks One solved
that age-old problem.
Just because you‚Äôre not using it doesn‚Äôt make it untrue. People will catch on.
Databricks Spark having support for Excel will no doubt make life easier for a myriad of use cases and people, and for others, it will mark the beginning of many travails.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What Databricks got wrong with Excel.
As far as how Databricks integrated Excel into Spark, and therefore into data pipelines, it is almost flawless, but there was indeed some central oversight.
The basic read features and option
s are perfect and easy to use, as you saw.
But, just as important to the business, the writing of Excel files was buggered up, and should have been given the SAME attention as the READ.
If you were to ask me what is actually used more in real life in Excel for data pipelines, I would say the ability to
WRITE an XLSX file is at the top of the list.
Why even bother adding
spark.write.excel
if it was going to turn out like this. Better to not even support write.
Duh. I know how distirbuted Spark works you milk toast programmer, but the fact that it is supported to write to Excel and comes up like this is alomst laughable.
Back to Pandas for ye‚Äô old Excel file write I guess.
Anywho, what can a fella say?
Excel is here in Databricks Spark, some will rue the day, others will celebrate with laughter and joy.
What say you?
Leave a comment
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
182238422.1tb-of-parquets-single-node-benchmark,1TB of Parquet files. Single Node Benchmark. (DuckDB style),Haters gonna hate.,2025-12-28 15:55:49.719-06,"I give the people what they want; I‚Äôm a slave to clicks and the buzz of internet anger and mayhem. Not so long ago, I tried, with limited success,
to convince a legion of engineers who were raised on the Snowflake and Databricks teat
, that salvation lay right at their feet; all they must do is lean down and
put their hand to the plow.
When you‚Äôve been doing what I‚Äôve been doing (
writing whatever you damn well please
) for over a decade, the list of haters grows ever longer and longer.
Get in line pickles.
A little stone tossed into the pond, where do the ripples go? Apparently,
my first article on the matter
stirred the monsters in the deep. My mamma always told me,
haters gonna hate
.
I mean, writing such things is the teaching of heresy in the inner circles of the
distributed devils
and
warmongers
, whose purse strings are tied
inextricably
to the masses of data engineering
peons
passing on their
tithe in compute
to the¬†inner sanctum of the data illuminati bent on bringing me to account for my many sins.
I‚Äôve made many a power enemy for you, the 99%, yet here I am, still trudging along in the trenches to bring the good word to waiting converts.
Midwest boys raised on the river and in the woods
don‚Äôt bow the knee very easily.
The Single Node Rebellion awaits you.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Generating 1TB of Parquet files with Rust.
I‚Äôve got nothing much else today on this wonderful holiday break besides oiling my muzzleloader and waiting for deer season to start, just as well
break out cargo and spin up some Rust.
Head over to the
GitHub repo
and see for yourself.
I went ahead and generated 1 TB of data and put it into an S3 bucket using the above Rust code.
And the schema generated is straightforward.
transaction_id
datetime
customer_id
order_qty
order_amount
We should be able to run a straightforward SQL query to piddle with this
1TB of data
and see what gremlins, or not, we can sus out.
Many sorry saps, complainers, anons, milk toast programmers, and the like, where complaining in my last benchmark that I didn‚Äôt
FORCE
the tools to use every single column and gulp the entire dataset ‚Ä¶ (
why wouldn‚Äôt you, that doesn‚Äôt happen in production
), but to shut the dirty mouths of all those goblins,
our query will include every single column of the dataset.
Using
Linode
, we will spin up LittleStinker (
named affectionately after all my haters
)
Hold your breath: a gigantic
16 CPUs and 64 GB of RAM
. We are going to light ‚Äòer up, make ‚Äòer burn red hot.
>> curl -LsSf https://astral.sh/uv/install.sh | sh
>> uv init stinkers
>> cd stinkers
>> uv add duckdb pyarrow boto3
>> nohup uv run main.py > script.log 2>&1 &
Let‚Äôs get to the simple code.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
DuckDB gobbles 1TB of Parquets in S3.
If anyone can do this deed, it‚Äôs
DuckDB
, our feathered and weathered friend of SQL. Why pick
DuckDB
? Well, we need something that won‚Äôt blow up memory, of course, and DuckDB allows us to do two things ‚Ä¶
spill to disk as needed
set MEMORY limits
There is nothing really complicated about this code at all, simply normal DuckDB with the added local disk spill crud and setting the memory limit at 50GB.
This is part of the attraction. Before we get to results, the more important part that many engineers seem to miss ‚Ä¶ is the simplicity of this architecture.
No Spark clusters, no complicated installs, dependencies, or JARs, no backflips to use Apache Commet with some JAR to speed up Spark, no Docker images needed.
There is not only real cost savings here in terms of compute ... a single 64GB node vs some oversized cluster at a very high-priced charge. There is real savings in complexity and architecture.
It‚Äôs hard to imagine anything simpler than a single EC2 instance, or whatever compute, with two extra requirements ‚Ä¶
uv or pip
DuckDB
There is no free lunch. I say this as someone who relishes using big-data platforms on Spark that gobble up whatever you throw at them. But it comes at a cost, both real and otherwise.
The hidden cost of using distributed systems for everything is more than just the large compute bill at the end of the month. Complexity, complexity, complexity, it spills out into every part of the data stack, from top to bottom.
Everything becomes complicated ‚Äúwith scale,‚Äù no matter how hard they try to hide the pea under the mattress. The truth is that
less code == fewer bugs
(generally). The truth is that
simple
infrastructure == fewer problems
.
I‚Äôm not saying you CAN‚ÄôT use those platforms.
I‚Äôm saying some of you DONT HAVE TO
.
There are simple options, such as DuckDB and a reasonably sized Linux instance. Literally, a 3- or 4-line install later, there is nothing left to do but work on the data.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Results
DuckDB does just what we tell it to do with that 1TB of parquet data in S3;
it makes that Linode 64GB instance sing a song of joy‚Äîsitting around 48GB of memory used while in the thick of it.
Heck, we aren‚Äôt really tuning anything either. I‚Äôm sure there‚Äôs work to be done, but it‚Äôs the holidays, and we're lazy.
Here is what you‚Äôve been waiting for, probably, ignoring all my comments about the existential questions of overall data platform design with a focus on simplicity.
A little less than 20 minutes.
Of course, it wrote out the results to the CSV file as well.
I don‚Äôt see why we couldn‚Äôt keep cranking up the data sizes as well. I‚Äôm sure nothing would change but the runtimes. Most of ‚Äòyall ain‚Äôt crunching a literal TB in a single query anyway.
What do you think, DuckDB is the only tool that can do such black magic? Not so.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Daft gobbling 1TB of Parquet files in S3.
The truth is, there are plenty of tools that can do this (
besides Polars ‚Ä¶ more to come on that later; it failed this task
). We live in a C++ and Rust world where single-node data frameworks are the new law of the land, many of which are more than capable of crunching large datasets.
Daft is one of those Rust-based tools
that doesn‚Äôt get enough love. It‚Äôs always fast and straightforward.
If you are interested in both the DuckDB and Daft code (
I‚Äôd be curious to know if you can make it run faster
),
you can find it on GitHub.
Daft appears to be much slower than DuckDB, but it's probably my fault; who knows?
Under 30 minutes for 1TB on a single node, no complaints at the end of the day.
What‚Äôs the problem, bruv?
I still don‚Äôt get it; it must be some distributed brain rot that is connected to money, fame, fortune, who knows ‚Ä¶ that causes people to fight against this so hard.
Maybe it‚Äôs just the folks who have Panda‚Äôs brain or something.
Ignoring the fact that we are living in the age of swift and capable single-node processing frameworks is like burying one‚Äôs head in the sand. Polars, DuckDB, Daft ‚Ä¶ these tools are going nowhere and will continue to be widely adopted as time goes by.
Maybe old habits are hard to break, new patterns of thought and solutions can be complicated to adopt once you‚Äôve tread the same path year after year.
A little open-mindedness to consider alternative approaches and solutions is what drives innovation and success.
It‚Äôs clear these tools and scale way beyond what we did today. I‚Äôm trying to make a conceptual point that apparently is hard to swallow for some.
Either way, haters gonna hate.
You, friend, be a healthy learner:
always consider all options, push things to the edge, and never take no for an answer. The future is bright and full of interesting things; never forget to turn over the rocks and see what‚Äôs underneath.
Ignore the smart, pretentious, loud, doubters, haters who lurk in the shadows and suck all joy and life out of all that is good in this data life we live.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
181261013.scott-haines-on-the-future-of-data,Scott Haines on the Future of Data Engineering,"OSS, Lakehouses & Real-World War Stories",2025-12-17 07:44:29.054-06,"In this episode, I sit down with
Scott Haines
‚Äî O‚ÄôReilly author, Databricks MVP, and veteran of Yahoo, Nike, and Twilio
‚Äî for a wide-ranging conversation on the
real
state of modern data engineering.
We dig into open-source ecosystems, Lakehouse architectures, the evolution of Spark, streaming, what‚Äôs broken and what‚Äôs working in today‚Äôs data tooling,
and the lessons Scott has learned scaling platforms at some of the biggest companies in the world.
If you care about data engineering, architecture, OSS, or the future of the modern data stack, you‚Äôll love this one.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Make sure to
follow Scott here on Substack
, and
over on GitHub."
180926701.llms-for-pdf-data-pipelines,LLMs for {PDF} Data Pipelines,a Brave New World,2025-12-09 09:16:21.81-06,"I was recently in conversation with some people about AI, its use cases, the good and the bad, where it fits, and where it doesn‚Äôt. It was brought up in the context of reading random PDFs and generating JSON files from their contents.
This got me thinking.
Can you, should you, and what will happen, if we use LLMs not just to puke out the next code snippet ‚Ä¶ but what if we use an LLM mixed with a little Agentic AI, to actually to
BE the data pipeline????
It is a strange thought. I‚Äôm sure people are doing it. Maybe. It‚Äôs one thing for an LLM to spit out code for a Data Pipeline; it's another for an LLM to be the data pipeline, or at least part of it.
I want to try it.
What say you?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Inserting LLMs into Data Pipelines
(
all code on GitHub for this
)
I‚Äôve built my fair share of RAGs, vector stores, Chat this Chat that bots ‚Ä¶ whatever. I also use Cursor on a semi-regular basis, maybe a few times a week, either as something to bounce ideas off of or to generate some mindless code.
One thing I‚Äôve never done is try to use an LLM in the loop or stage of a data pipeline.
This will be new for me, as it is for you, and I am going to list out loud some of the questions I have at large about doing this sort of work. At this point, I have no idea how it will work out.
Some questions I‚Äôm asking myself.
I think it‚Äôs one thing for me to play around and force an LLM to do a thing by using coercion, glue, and string to make something happen, vs. an actual workflow that could be used in production.
Here‚Äôs what‚Äôs on my mind.
A small local LLM or a remote API-based one?
Can we force an LLM model to do what we need, or do we need an Agent?
To actually write/place (JSON) files on disk
Are orchestration tools like Airflow starting to provide, say, OpenAI operators?
LLM output is non-deterministic; it can hallucinate at any time.
How do you rely on the output?
How fast or, most likely, slow is it going to be?
That‚Äôs enough worries for now, I think. Let‚Äôs get out some of that Elmer‚Äôs glue and a shoelace and see if we can start making something happen.
Finding an LLM model to use.
Ok, so our options for LLM models. I couldn't care less about flavors, only hobbits worry about that stuff.
Hosted LLM
OpenAI
Databricks Model Endpoint
Small enough local model to run on a machine.
I have had this happen a few times: Google something and get my own website(s) showing up in the results‚Äîstrange feeling. Anywho, why not go with a local model that fits in the memory of commodity hardware instead of paying a bunch of money for hosting and endpoints, or forking over pennies to Sam Altman? I‚Äôm sure performance will be terrible with a local mode, but whatever.
Luckily, or unluckily, about two months ago,
I wrote a post about running Llama 3.1.8 B locally
, meaning it can fit and run in memory. No giant GPU cluster needed.
So, we will have the code to download and run this model locally. Let‚Äôs move on from that part and talk about the file generation problem we want this LLM to solve.
How about we have ChatGPT generate us a few PDFs of what would be insurance policy documents? This is what the job we will give the LLM:
‚ÄúTake these policy documents and extract these data points as a JSON file.
‚Äù
This seems reasonable and actually helpful in the real world.
I had ChatGPT generate three different full-length, realistic insurance policy documents for me based on fictitious people.
Auto Insurance Policy ‚Äì Robert L. Kendrick
‚ù§Ô∏è
Life Insurance Policy ‚Äì Melissa A. Davenport
üè°
Homeowners Insurance Policy ‚Äì Whitaker Residence
More or less, this is what they all look like.
I think, because I have no imagination, I'll have the LLM try to extract basic info from these PDFs ‚Ä¶
What kind of policy document
Who it is for
How much is the coverage
Should be interesting.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Writing our AI Agent for PDF ‚Üí JSON extraction.
This process is going to have to be an AI Agent approach, simply because calling an LLM back and forth is an exercise in tokens. Llama doesn‚Äôt know what a PDF file is; it only understands strings of tokens.
Oh, but don‚Äôt worry, AI will take our jobs.
Anywho, into the breach, my friends, let‚Äôs get coding. If you‚Äôre not using
UV for Python projects
, I‚Äôm going to call your mom and tell her.
And to get ollama locally, read on.
Blah, now to write code. At a high level, what do we need to do?
Iterate through the folder of PDF documents
Extract the text from the PDF
Send text to LLM with prompt
Get results
Write results to JSON file
Sounds like just another data pipeline. Maybe it is, and this is all for naught. Sounds boring.
Let‚Äôs try to make this as normal of a ‚Äú
data pipeline
‚Äù as possible. Here is our main entry point, and a nice obvious list of functions to call that will get us from a folder of PDFs, to another of JSON files.
Told you this would be boring, iterate files ‚Ä¶ yay. Let‚Äôs crack open
process_pdf()
to see the what‚Äôs cracking. This is the part of the ‚ÄúAgent‚Äù (
I don‚Äôt think I would call this an agent
) that would be very specific to PDF processing.
The theory is, if this code as a whole worked well, we could obvuscate some of the functionality, and simply put it behind an API or Service where we could call it and say ‚Äúhere is a PDF, pull x, y, data as a JSON file.‚Äù
The extract text from PDF is just boring Python.
The next part is where we actually incoporate the LLM/AI into the mix of this data pipeline. Interesting indeed.
Strange mix of prompting the LLM, and then pulling what we need from that response, as JSON of course. The rest of the code, as per above, is just writing that JSON out to a file.
It‚Äôs slow, but works.
I mean, not sure what to say, seems to work fine enough. Of course this is a little play example, who knows how it would act in a more complicated situation, with more complicated documents, are more difficult things to find.
Running local LLMs is always slow, this was slow, but whatever.
But, at the end of the day, we did write a data pipeline incoporating AI into our workflow to do an important and helpful task.
If you want to play around with this,
it‚Äôs all on GitHub.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Musing on AI/LLMs as PART of a data pipeline flow.
This was not what I expected at all, to be honest. I‚Äôve a love hate relationship with AI, but it mostly as to do with finding
GOOD
use cases that are useful for whatever problem.
This went way better than I expected in the beginning, although I‚Äôm not sure why I expected to to be ‚Äúbad"".‚Äù
Call me whatever you want, this seems to me, to prove that AI and LLMs have a TON of room to grow in regards to sovling everyday, boring, systems automation and even as a way to reimagine workflows and data pipelines.
Of course there are all sorts of ‚Äúproblems,‚Äù or maybe issues that need to be overcome.
How do we keep the hallucination and quality control in check for this data pipeline?
When we write ‚Äúnormal code‚Äù it is very structured and determinisic
LLMs seem to be very undeterminstic and ‚Äúuncontrolable.‚Äù
But, if we don‚Äôt care if some of the PDFs go wierd, does it matter? Depends on the business context, if the LLM does great 99% of the time, the business might not care about the 1%.
It also gets me thinking more. What other kinds of things could we use LLMs for in a data context, besides just generating code. I mean this is the whole rise of AI Agents for everything under the sun. Just seems to me, many times, the use cases are sorta unhelpful and toyish,
more than real value.
Processing millions of PDF documents to pull semi-structured data out with just a prompt.
Well ‚Ä¶ that is going to be valuable to someone, that‚Äôs for sure.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Let me know what you think about this, AI in general, how you are using LLMs, found any fun use cases? Have you incorporated AI/LLMs into more workflows than just ‚Äúgive me code?‚Äù
Drop a comment below.
Leave a comment"
180747165.building-agentic-ai-fancy,Building Agentic AI ... Fancy.,how about a simple SQL Agent?,2025-12-04 18:48:15.144-06,"I‚Äôm getting to the point where I‚Äôm a little burnt out on hearing about AI this and AI that. Work, LinkedIn, YouTube, Substack ‚Ä¶ it‚Äôs a never-ending glut of AI-generated content talking about AI. I get the sense that the closest most developers get to AI is asking Cusor to puke out the next code chunk.
Classic.
I‚Äôve never been one to make a mountain out of a mole hill. I find myself adrift in the middle ground of the AI purgatory we find ourselves in as writers of code.
There seem to be two deeply entrenched cabals of software engineers.
AI is for the weak; humans forever.
Vibe code your way to glory.
The truth is that the middle is most likely slanted towards the AI is the future group. You cannot turn back the clock once you‚Äôve opened Pandora‚Äôs box. Will there always be a bright future for intelligent and industrious writers of code and systems?
Absolutely.
Is AI here to stay and an extreme force multiplier in the creation and execution of software of all kinds?
Absolutely.
Old dogs die hard, and the old guard is having a hard time swallowing the new reality.
Let‚Äôs bring it all back down to earth.
I am no AI expert, but I‚Äôve probably done more than most, including fine-tuning my own LLM. I don‚Äôt get overly excited about building AI,
insofar as it‚Äôs actually helpful and interesting problems to solve.
I would guess that there is a fair number of Data Engineers and others who may be overwhelmed by the fast pace of development in AI and LLMs, and feel a little left behind and lost.
I assure you, there is hope.
Just remember, all those liars on LinkedIn and Reddit trying to be AI smart, they breathe the same air and drink the same water you and I do.
I‚Äôm going to prove that to you by showing how easy it is for you to build an AI SQL Agent.
Forget all the too-bright suckers, confusing acronyms; you, my friend, are capable of building ‚ÄúAgentic AI.‚Äù Don‚Äôt listen to the haters. Let‚Äôs do this.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Agentic AI
Let‚Äôs start with a simple concept: what is Agentic AI, and those Agents you hear about regarding LLMs? Let‚Äôs do what any normal person would do, and ask AI (ChatGPT) what it thinks Agentic AI is.
This is a decent response, I think.
Since I am a
Grug-brained developer,
I would summarize it like a good old Midwest-born, half Bigfoot, rascal that I am.
LLM ‚Üí knows a little about everything (question -? answer)
Agent ‚Üí takes it a step further and can DO A THING
An AI Agent will do something, something that a LLM (
a binary model blob in memory somewhere
) needs help doing.
It‚Äôs about system interaction
, not just answering a question.
Maybe an AI Agent searches the internet for recent blog posts, polls an API for a result, or creates a file for you like a cursor. Generally speaking, an Agent is going to be good at
DOING
a particular thing.
One of those things ChatGPT will NOT do for you would be to access, query, and summarize the results of some data sitting in a database ‚Ä¶ not just a database ‚Ä¶ YOUR database.
Let‚Äôs build an AI SQL Agent.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
How Agentic AI can be helpful.
Imagine you are a CTO somewhere, let‚Äôs say SanFran, that beacon of light ‚Ä¶ oh wait ‚Ä¶ I mean darkness ‚Ä¶ where the Illuminati tech bros plan the demise of humankind. I digress.
Let‚Äôs pretend you‚Äôre a CTO and you have a bunch of people in the C-suite and elsewhere constantly asking about sales numbers.
How much have we sold? Who‚Äôs sold the most? What were the numbers last month? How are we doing on sales so far this month?
No one uses the crusty Dashboards that were set up; one, they don‚Äôt know how to use them, and they are half-broken and slow much of the time. Then you get a brilliant idea. All that sales data is sitting in a database.
Why not connect a SQL Agent to it, adopt Agentic AI, and have people interact with an LLM to get the answers they want?
You fall asleep that night dreaming of being paraded around the company parking lot riding a white stallion, crowned with laurels, like a new Caesar.
Building a SQL Agent as an introduction to Agentic AI.
Ok, so hopefully after that pretend discussion above, you can see the use of what we are trying to do (
As a side note, an Agent is only as helpful as the problem it solves for you ‚Ä¶ aka ‚Ä¶ what it does
).
In our case, it seems helpful because we don‚Äôt have direct access to a Dashboard; in fact, we can delete the Dashboard, the person who made it, the pipeline that filled it, and the compute that ran it. Delete. Delete. Delete.
We can replace it with a
Sales SQL Agent
ready 24/7 to answer whatever you need about sales numbers.
Let‚Äôs talk about it at a high level: which components are needed?
- Some UI / App (probably web based)
- Web Server to serve that web app
- LLM (let's just someone else's like OpenAI)
- SQL Database with our data
- Code to glue it together (let's use LangChain)
It‚Äôs really not that complicated, and that‚Äôs the point I‚Äôm trying to make. Pretty much every single thing on the list above is a solved problem.
Tools like
LangChain
make building Chat workflows super easy.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I think what throws most people off at first about building AI Agents, and was annoying for me as well ‚Ä¶ was that it includes the Full Stack. Have you ever wanted to be a full-stack engineer? Well, this is your chance.
I have all the code available on GitHub
; it‚Äôs wrapped in a Docker Compose command so that anyone can run it. You just need an OpenAI API key.
Here is the result first.
Look, it ain‚Äôt rocket science, friend. My guess is that, as far as SQL Agents go (
putting an LLM on top of your database(s), it would not be much more complicated than this
.)
You might have too ‚Ä¶
Provide more context with DDL and JOINS to the LLM
Provide specific instructions about the data in the tables, etc.
But you will be surprised how well general LLMs are writing SQL queries against any data and answering most questions with ease.
To show you that there is no magic here, let‚Äôs talk more about this ‚Äúarchitecture,‚Äù aka the different technology pieces involved here. Remember, this is what high-paid consultants call ‚ÄúAgentic AI‚Äù
and charge obscene amounts of money to build.
Let‚Äôs look at our Docker Compose file for this project, remember,
it‚Äôs all on GitHub
, and see at a high level the different components. You will see two things you‚Äôve seen a thousand times.
Backend
Frontend
Who would have thought?
Let‚Äôs also look at two Dockerfiles: one for the backend and one for the frontend. We could call these ‚Äúservices‚Äù ehh???
You are clever folk; there are about a thousand ways to deploy a service, a million SaaS options under the sun, or you could wing it and just run docker-compose up on a bare-metal instance.
I mean, if you‚Äôre looking for rocket science, you ain‚Äôt going to find it here, my friend.
Can I be honest with you?
There are some positives to the AI arms race for Developers. So many companies and frameworks are vying for the hearts of the programmers and AI developers that they have made it SUPER easy to build Agents and every kind of LLM workflow possible.
This means less work for us. I mean, look at our SQL Agent code. Flipping langchain comes with a SQLDatabaseToolKit for crying out loud.
I mean, good Lord, could it be any easier to make an SQL Agent???
Look, all I‚Äôm trying to do here is make a point. So keep your spicy comments to yourself. If you spend all day on YouTube and LinkedIn, you might conclude that you‚Äôve been left in the AI Race dust and have no hope for the future.
You see job postings for ‚ÄúAI Engineering,‚Äù building ‚ÄúChatBots,‚Äù and ‚ÄúAI Agents,‚Äù and your eyes glaze over.
I‚Äôm not trying to downplay good AI Engineers and the difficult task of putting an Agentic Agent(s) into production. But I do want to let you in on a secret.
The complex parts aren‚Äôt what you think.
How to integrate Agent LLMs into more ‚Äúcomplicated‚Äù tools
say some internal specialized API or systems calls chained together
How to set up the infrastructure
backend
frontend
How to keep costs down
How to keep latency down
How to apply governance and security
How to find impactful product-driven use cases
Fast iteration and development
It isn‚Äôt actually hard to use tools like LangChain to do anything LLM-related. Backends have been around forever, services, web apps, front ends ‚Ä¶ all that stuff is old hat.
What is new? RAGs, unstructured documents, tying various datasets together, governance of LLMs (input and output), managing AI costs, tightly coupled infrastructure, and deploying multiple Agents.
These are simply classic Software problems.
Remember, all this code is on GitHub
. I encourage you to clone it and play around with it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
180551944.building-a-ducklake-open-source-style,Building a DuckLake ... Open Source Style.,behold,2025-12-22 07:26:23.471-06,"I am sort of an addict for a good Lake House; it‚Äôs just an incredible architecture that is a joy to build and use daily. No more SQL Server or Oracle, you know what I‚Äôm saying if you were born in the right decade.
Truth be told, at this point in the Modern Data Stack lifecycle, you pretty much have two major Lake House storage layers that dominate the landscape: Iceberg and Delta.
I was surprised when DuckDB/MotherDuck threw their hat in the ring with DuckLake. Didn‚Äôt see that coming at the time.
Like anything done by DuckDB, it‚Äôs always top-notch and for a good reason. They have a knack for doing things right. Ever since I wrote that first article,
I‚Äôve been meaning to return to DuckLake and try it out in a more open-source setting again.
What I mean by that is can I use maybe a RDS Postgres instance and back DuckLake on AWS S3.
I mean, if someone is actually looking to use DuckLake in the open as a viable alternative to Iceberg and Delta Lake, then that is the game.
Checkout today‚Äôs sponsor
Carolina Cloud
One-click notebooks, genomics tools, and Ubuntu VMs at 1/3 the cost of AWS with no egress fees
If you enjoy Data Engineering Central, please check out our
sponsor's website above
. It helps us a lot!
Also, since it‚Äôs been a while since I‚Äôve looked at DuckLake, one telling note we will look for is: has there been any adoption of frameworks in the broader data community? Aka, can I read DuckLake with Polars, Daft, Airflow, whatever?
Simple Review of DuckLake
Again, there are better places than here to l
earn about DuckLake in depth
. That‚Äôs not my focus today, as usual, I want to simply throw a problem at the DuckLake architecture, turn over a few rocks, and see what crawls out.
DuckLake is built on simplicity, of course, and that appears to be one of its main features.
Built on top of parquet
SQL database for catalog/metadata
Of course, they will give you a myriad of other reasons to use DuckLake, but at the end of the day ‚Ä¶ simplicity.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
As I wrote before,
this seems to be one of the main differences between say DuckLake and Apache Iceberg. Below is the concept visually, no more hosting a complicated Catalog.
DuckLake lets you use pretty much any SQL database as the metadata and catalog. Easy. That‚Äôs the point. No spinning up an instance to host some heavy catalog, or worse yet, having to pay for SaaS Catalogs.
This lowers the barrier to entry.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Setting up DuckLake on AWS (S3 + RDS Postgres)
There is no time like the present, so let‚Äôs dive in and see if we can get DuckLake up and running on an AWS setup, far away from that feathery MotherDuck,
to see if indeed DuckLake is open-source-worthy.
Either it will be easy and boring, or it will be bumps in the night.
We need two things to make this work,
in true open source style.
RDS Postgres instance.
S3 bucket.
We can use my gigantic and massive RDS instance, which I use for various sundry projects, a mighty
db.t4g.micro
. That little blighter costs me like
$.50
per day to run.
Also, we will attempt to use my S3 bucket,
s3://confessions-of-data-guy,
as the data path for DuckLake to store the parquets it generates.
In theory, this DuckDB/DuckLake command will look something like this.
Of course, I will have to ensure that AWS credentials are available and that Postgres is available for DuckDB. Hold onto your seatbelt, Rita, here we go.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
A lot of code, but it appears to run fine with UV.
Well, that was dissapointly easy. I know you all come here from the mess and mayhem. Not today, at least not yet. Let‚Äôs get some of that
Divvy Bike trip data from the ole‚Äô iterwebs
, and see if can shove that data in there.
After munging the code a bit, this seemed to work like a charm.
Heck, to make sure I wasn‚Äôt hallucinating, I checked S3 and can see the files written there for our DuckLake.
They are there, and the results of those sample queries appear to be running fine.
Please leave it to DuckDB and MotherDuck to make this a boring article, those little rats. They could have made me try a little bit harder, you know? Indeed, DuckLake is a valid Lake House option in the open source world if you‚Äôre looking for a lightweight option.
Don‚Äôt get no lighter than that. No Big A$$ catalog needed here.
If you want to do this yourself,
all the code is available on GitHub.
More thoughts on DuckLake
That went better than I expected, although I should have learned my lesson; DuckDB rarely gets anything wrong, and the same applies to DuckLake. You never know how a tool will react to trustworthy open source.
I enjoyed the fact that I could use an RDS Postgres database for the metadata and a random S3 location for the data storage.
What remains to be seen is whether MotherDuck and DuckDB will push for first-class support in other popular tools, such as Polars and Daft.
It looks like some of this is already underway, but time will tell whether it keeps chugging forward.
It‚Äôs hard to imagine anyone giving Delta Lake and Apache Iceberg a run for this money. Still, with Data Catalogs now and in the future heavily integrated into those tools, there might come a day when people have had enough and look for an easier option, DuckLake being one, as long as it has broad tooling adoption.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
180334664.apache-arrow-is-eating-the-world,Apache Arrow is Eating the World.,... the data world,2025-12-15 07:23:30.047-06,"About three years ago,
over on my other long-running blog, I wrote about Apache Arrow
, that silent but mighty tool that is swallowing up the Data Landscape one bite at a time.
I figured it‚Äôs due time I again tip my hat to that great Oz hidden behind the digital curtain.
It‚Äôs hard to even know where to start with
Arrow
; it‚Äôs so ubiquitous and has infiltrated just about every Data Engineering framework built in the last 5 years+.
I very much doubt I can do
Arrow
any real justice, but I will give it a try. Come along for the ride, my friend.
My goals are simple,
I hope you come away with ‚Ä¶
a new respect for Apache Arrow
knowledge of where all
Arrow
is used
a curiosity to use
Apache Arrow
yourself
This is where the road lies before us, with lots of twists and turns. I don‚Äôt know where exactly we are going or how we will get there, but we can trust it will all work itself out in the end. Let‚Äôs get to it shall we?
How shall we tackle such a GOAT of Data Engineering and give it the respect it deserves?
What is Apache Arrow?
Brief history of Apache Arrow.
Where is Apache Arrow used?
Using Apache Arrow (code)
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
üß† What is Apache Arrow?
Ok, let‚Äôs start from the top for all you newish data hobbits who‚Äôve yet to crack the lid on the underpinnings that hold up much of the work you do on a daily basis.
What is Apache Arrow?
Apache Arrow defines a language-independent columnar memory format
Arrow‚Äôs libraries implement the format and provide building blocks for a range of
use cases
Libraries are available for
C
,
C++
,
.NET
,
Go
,
Java
,
JavaScript
,
Julia
,
MATLAB
,
Python
,
R
,
Ruby
,
Rust
, and
Swift
I know it sounds simple on the surface, but this is why Apache Arrow has become the Atlas of Data world, showing up inside many of the frameworks we use on a daily basis.
Beyond that, it‚Äôs gotten to the point where it‚Äôs actually being used directly to do daily data work.
At its core, Arrow set itself up to be used to build other data tools.
Apache Arrow is a universal language for data.
Think of all your data tools: Spark, Pandas, DuckDB, Polars, blah, blah. They all speak different native formats internally (row-based, column-based, custom memory layouts).
Arrow provides a
shared-memory format
so they can all exchange data
without copying, conversion, or serialization
.
‚û°Ô∏è That means
faster data processing
,
lower memory use
, and tools that can finally ‚Äúplug into‚Äù each other.
If you think about it, how data is stored in memory, copied, transformed, moved, whatever ‚Ä¶ has a huge impact on performance, cost, speed, and complexity of our data pipelines and tools that do the work.
Apache Arrow says, Hey, use me as a format to do all that work.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
üèõÔ∏è Brief history of Apache Arrow.
Well, maybe I made the muddy waters of what Apache Arrow is a little clearer, or maybe not. If you are still struggling to understand Apache Arrow, I get it. For the average Data Engineer who simply uses a tool¬†or calls
to_arrow()
at random, consider Arrow the format of choice for moving data to and fro.
Just think, you use Parquet, CSV, Avro, etc for on-disk file formats.
Apache Arrow should be your go-to in-memory data format.
Not sure if we will get anything out of this, but lets just do a brief history lesson on Arrow.
- In the year of our Lord, 2016, Arrow started as an Apache project.

- Inital codebase was Java

- Inital came from Apache Drill

- Circa 2016-2017 artilces start to appear in the tech landscape for ""Apache Arrow taking on In-Memory for big data.""

- Circa 2016-2017 both Dremio and Cloudera talk about Apache Arrow (it's inception and shared columnar in-memory concepts cleary had been worked on internally)

- Circa 2016, Wes McKinney of Python fame starts to talk about Apache Arrow
If you want to do some extracurricular reading, here you go.
https://wesmckinney.com/blog/pandas-and-apache-arrow/
https://www.cloudera.com/blog/technical/introducing-apache-arrow-a-fast-interoperable-in-memory-columnar-data-structure-standard.html
https://www.dremio.com/press-releases/introducing-apache-arrow-columnar-in-memory-analytics/
Basically, from there, Apache Arrow has spread like fire ever since. Backed by and talked about by big names, becoming a top-level Apache project, everything came together at the right time and in the right places. It was off to the races.
üìö Where is Apache Arrow used?
This might be the most interesting part of digging into Apache Arrow. We now know what it is and where it came from,
but where has it spread to?
Apache Datafusion
uses Apache Arrow
many, many projects are built on Datafusion
Polars the new popular Dataframe tool uses Arrow
DuckDB closely integrates with Arrow.
PySpark integrates with PyArrow
Pandas integrates with PyArrow
ClickHouse supports Arrow
Dask has integrations with Arrow
delta-rs integrates with Arrow
Ray also uses Arrow
PyArrow has become a framework in its own right
Of course, I have skipped many others, but you get the point. You would be hard-pressed to find another Apache project that has had as significant an impact on the Data Engineering world as Apache Parquet.
When the data community at large starts to see things like
to_arrow()
showing up everywhere, that‚Äôs when you know a tool has become mainstream.
I think that, because of its in-memory nature rather than file storage, is the only reason Apache Arrow doesn‚Äôt have name recognition like Parquet ‚Ä¶ it does the invisible work.
Also, it should be evident that Arrow is being used to build other amazing tools and frameworks we all use; even if we don‚Äôt see it right in front of our eyes, it‚Äôs still there in the background, doing the Lord‚Äôs work.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Trying out Arrow.
Trying out Arrow is a little strange because, if you remember our introduction, it‚Äôs language agnostic, and each programming language has its own implementation of Arrow.
But, none the less, it‚Äôs worth giving it a poke in a few different languages just to see some of the benefits of Arrow as tool and what different implementations might have to offer.
Let‚Äôs start with Python.
PyArrow
is probably the lowest bar of entry to get some hands-on experience with Arrow.
As you can see above, there are two main ways to ‚Äúinteract‚Äù with data in the Arrow format ‚Ä¶
- Array
- Table
- Dataset
You can ‚Äúact‚Äù on Arrow data, generally speaking, with features like ‚Ä¶
- Grouped Aggregations (group_by, sum, count)
   - performing these functions is ‚Äúodd‚Äù
- Joins
- Filters using Expressions
- UDFs
You can do IO for the Arrow interface with other formats.
- File IO (read, write)
   - Parquet
   - CSV
   - ORC
   - JSON
   - FileSystems (S3, local file system etc.)
Of course, I‚Äôm skipping and glossing over many things, but I‚Äôm trying to make this relevant for
the Average Engineer
.
In theory, you could swap in PyArrow for basic use cases with DuckDB, Polars, Pandas, etc., but I think this isn‚Äôt often needed. Once you use PyArrow, you will catch on; it‚Äôs very clumsy compared to DuckDB and Polars, for example, and best suited for specialized use cases.
Case in point: I‚Äôve used PyArrow in several open-source packages as a sort of ‚Äú
data interface or interchange
‚Äù between formats. Say between some data and Delta Lake, for example.
In the past, if I‚Äôve had ‚Äúproblems‚Äù integrating DuckDB or Polars into a specific data source, for whatever reason, many times I could swap in PyArrow to read/write that data source, and easily translate that PyArrow dataset directly into, say, DuckDB or Polars to do the heavy lifting.
Again, as we‚Äôve said 50 times, Arrow is a de facto standard for representing data, excellent for fast data interchange, and even for working with datasets for other tooling.
Can you use PyArrow for data engineering? Why yes, you can. Here is an example of reading a
Divvy Bike Trip CSV data file
and doing a simple groupby aggregation.
danielbeach@Daniels-MacBook-Pro-2 testpyarrow % uv run main.py                          
0:00:01.999884
I mean, it‚Äôs fast, it works.
But it‚Äôs awkward to write, I see no reason to use PyArrow over DuckDB or Polars, unless you have a specific reason to do so.
Not so long ago, in another space-time dimension,
I wrote about using PyArrow for processing large datasets
, using
RecordBatches
.
If you want the best deep dive into Arrow for Data Engineers, you must read
VuTrinh, the GOAT of Substack technical data writing.
I guess we could keep playing with Arrow in other languages, C++ or Rust, but the truth is it‚Äôs the same deal everywhere you go. Arrow is best used to build other tooling for sure.
If you are doing custom stuff, have a special use case, need data interchange, building a framework or CLI to be used by others, by all means, reach for Arrow of some sort. If your day to day is simply building pipelines using everyday frameworks, there is no need to for Arrow. You might be already using it, and just don‚Äôt know it.
Another interesting use case I wanted to mention is
Arrow Flight SQL.
Arrow Flight SQL is a protocol for interacting with SQL databases using the Arrow in-memory format and the
Flight RPC
framework. - docs
What is amazing about Arrow is that its popularity and performance is leading it into other places like SQL, and below
even database drivers.
A set of abstract APIs in different languages (C/C++, Go, and Java, with more on the way) for working with databases and Arrow data. - docs
For example, result sets of queries in ADBC are all returned as streams of Arrow data, not row-by-row. - docs
What we have here, clearly, is an entire ecosystem of data tools being brought forth by the almighty Arrow. Now maybe you understand why I wrote this title ‚Äú
Apache Arrow is eating the (data) World.
‚Äù
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
180113969.data-engineering-and-modern-data,Data Engineering & Modern Data Platform Consulting,,2025-11-27 09:51:48.46-06,"Guidance, architecture, and hands-on support for teams that need to move fast and build real systems that work.
I‚Äôm Daniel Beach‚ÄîSenior/Lead Data Engineer, architect, author, and creator of the Data Engineering Central Substack, followed by 22,000+ data professionals.
I‚Äôve spent the last decade building and scaling data platforms across fintech, fraud, retail, agritech, SaaS, and high-volume behavioral datasets‚Äîeverything from Spark, Delta Lake, Iceberg, Databricks, Polars, DuckDB and AWS-native stacks, to ML pipelines, governance, cost optimization, and production-grade architecture.
Now, I‚Äôm opening up a limited number of
consulting and advisory slots
for companies that want help without hiring another full-time engineer.
How I Can Help
Data Engineering & Architecture
Build a foundation that doesn‚Äôt collapse under its own weight.
End-to-end platform architecture
Data modeling & schema strategy
ETL/ELT design (batch, micro-batch, streaming)
Modern ingestion patterns (APIs, CDC, event-driven, SaaS connectors)
Pipeline modernization or re-architecture
If you‚Äôre rewriting legacy pipelines, untangling a spaghetti platform, or preparing for scale, I can help you build something durable.
Data Platform Migrations
Move your platform without burning it down.
On-prem ‚Üí Cloud migrations
Hadoop/Spark/EMR ‚Üí Databricks/Unity Catalog
Delta Lake ‚Üî Iceberg migrations
Warehouse ‚Üí Lakehouse transitions
Cross-cloud migrations (AWS, GCP, Azure)
I‚Äôve led multi-year migration programs‚Äîeverything from storage and compute to catalogs, governance, and access patterns, to cost reduction and efficiency.
Lakehouse Design & Implementation
Making the single biggest change in your data platform‚Ä¶ safely.
Delta Lake & Apache Iceberg architecture
Table layouts, partitioning, clustering, compaction
Catalog choices (Unity Catalog, Polaris, Boring Catalog, Lakekeeper, R2)
Storage, retention, data-quality & lineage strategy
Performance & cost optimization
If you‚Äôre moving toward a Lakehouse or cleaning up an existing one, I‚Äôll help you design it right the first time.
Machine Learning & AI Integration
Modern data platforms don‚Äôt end with storage‚Äîthey end with value.
ML feature pipelines & training data design
ML data versioning, lineage & reproducibility
Integrating DuckDB, Polars, Iceberg/Delta for ML workflows
Generative AI, RAG, vector search
Embedding models & low-latency inference pipelines
Whether you‚Äôre modernizing ML workflows or starting fresh, I help you build systems that are reliable, scalable, and cost-conscious.
Data Teams, Processes & Leadership
Because the tech only works if the team works.
Team structure, hiring & seniority leveling
Process design (DevOps, CI/CD for data, pull-request workflows)
Engineering culture, mentorship & upskilling
Roadmap planning & platform vision
If you‚Äôre growing a data or ML engineering team‚Äîor trying to fix communication gaps‚ÄîI help you build clarity, direction, and momentum."
180073012.data-mesh-theology-dead-or-alive,Data Mesh Theology. Dead or Alive?,dying on the vine?,2025-12-02 14:52:11.664-06,"Meh.
That's how I feel about the languishing and short-lived ideology called ‚Äú
Data Mesh.
‚Äù The Modern Data Stack has seen its fair share of prophets seeking the loving embrace of the community, a fickle thing, only to fall out of favor all too soon.
Data Mesh
appears to be one of those ideas.
The thing is, I keep my trained ear close to the ground, listening to the comings and goings of various and sundry ideas. Watching what becomes popular, what dies on the vine.
But I won‚Äôt be the judge of that in the end. Never say never. Let‚Äôs just lay it all out.
Try to understand what a Data Mesh architecture is
How long has it been around
Pros and cons
Is there any hope left for mainstream adoption?
This is just as much for me as for you. I have my preconceived notions, of course, but we will try to give Data Mesh a fair shake and see where it's come from and where it might be going.
You know one thing is true ‚Ä¶ those cloud compute bills are getting out of hand.
Check out
https://carolinacloud.io/
for
High-Performance CPUs, GPUs, Notebooks,
and
Bare-Metal Servers.
If you want to save serious money on compute, head over to
Carolina Cloud.
What
is
Data Mesh? (In Plain English)
Data Mesh is an architectural approach that aims to address the problems of large, centralized data teams by
decentralizing
data ownership. Instead of one monolithic team owning all datasets, each domain (fraud, marketing, product, etc.) owns
its
data as a product
.
This can best be explained visually, contrasted with the most common approach of gathering all data into a single spot to be shared from there.
Let me give you a few bullet points to further drive home what a Data Mesh Architecture is.
decentralized data architecture,
domain-oriented
self-serve design
‚ÄúResponsibility for analytical data is shifted from the central data team to the domain teams.‚Äù
This is in direct opposition to the classic approach of a single Data Platform team handling ALL data for an org, ingesting all data, doing their work, and becoming the central point of contact and distribution.
Data Mesh is the reverse of that approach. It treats each dataset as a product that is handled, produced, and served ‚Ä¶
by the data producer.
Where did Data Mesh come from?
I think that it can be helpful to understand a concept by examining where it came from, how it‚Äôs morphed over time, and so on. Seeing what the people who made a thing are up to can give us insight into where something might be going.
So, let‚Äôs look at where Data Mesh came from.
‚ÄúThe term
data mesh
was first defined by Zhamak Dehghani in 2019
[7]
while she was working as a principal consultant at the technology company
Thoughtworks
.
[8]
[9]
Dehghani introduced the term in 2019 and then provided greater detail on its principles and logical architecture throughout 2020. The process was predicted to be a ‚Äúbig contender‚Äù for companies in 2022.
[10]
[11]
Data meshes have been implemented by companies such as
Zalando
,
[12]
Netflix
,
[13]
Intuit
,
[14]
VistaPrint
,
PayPal
[15]
and others.
In 2022, Dehghani left
Thoughtworks
to found Nextdata Technologies to focus on decentralized data‚Äù
You can find
Zhamak
today working on
NextData
.
NextData OS is apparently the final regurgitation of the Data Mesh theology.
‚ÄúNextdata OS - Autonomous data product platform for agents, machine learning and analytics. Turn any source into data products that run and govern themselves autonomously: composable, AI-ready, can ingest any format, run on any stack and serve any application.‚Äù -
NextData
‚Äú‚Ä¶ make an observable mesh of data products across your enterprise.‚Äù
Reading the NextData website, you will find a few mentions of Data Mesh, but it definitely is not the main thrust per se. I‚Äôm not sure what to think about that ‚Ä¶ well, I do know what to think, I will leave the rest to your imagination.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Pros and Cons of a Data Mesh architecture.
I think we can all agree, at a high level, that there are many benefits to the Data Mesh philosophy. On the surface, it‚Äôs not that hard to see the good in it.
Pros

- Very large organzations could benefit from forcing data ownership
- The bottle neck of a single Lake House for ALL data could be removed
- Data sits and resides in the domain that knows it best
- Forces self serve Data Platform/Analytics
vs
Cons

- Cannot be resonable adopted at even medium sized companies
- Domains don‚Äôt want to own their data
- Domains don‚Äôt have know-how to manage their data
- Governance becomes harder, not easier when everything is decentralized
- Tooling never matched vision
I mean, back in
2023 places like PayPal mentioned Data Mesh as their architecture of choice
, so you know it has real value as an idea within a large and complex organization.
I polled my
LinkedIn
and X folk about Data Mesh to see if it was still alive and well in the wild, and got a mixed bag of responses. A whole lotta nothing on LinkedIn, but some positive feedback on X that it is indeed a thing in some circles yet.
Yet others are clearly dissatisfied with Data Mesh and migrating away due to ‚Äúhigh costs‚Äù and ‚Äúdata silos.‚Äù
Heck,
even Gartner created some buzz by saying that Data Mesh was going to die out.
Depends on how seriously you take them.
Clearly, there are some strong headwinds against Data Mesh, which is struggling, real or not. The general perception, aside from a few oddities, is that Data Mesh was good in theory but lacked seriousness in practice.
So, if big tech has always been a North Star for many techies,
why has Data Mesh been so silent lately?
Why has it never been talked about in
r/dataengineering except for people asking if it's still alive?
The truth is that only seriously technical teams and companies can implement a Data Mesh in a way that overcomes the downsides of that architecture.
Clearly, orgs like Netflix are capable and willing to do it, and do it well.
But, at the end of the day, if only a few select orgs have done it, and no one in the community at large even mentions the words Data Mesh for a few years straight, it‚Äôs going to be hard to make a real argument that Data Mesh is anything but a fringe architecture that seems to be dying on the vine.
Clearly, this could just be one person‚Äôs take, but I think it‚Äôs more than that. If we put our fingers and noses to the wind, it‚Äôs fairly easy to make reasonable judgement calls using the obvious writing on the wall in front of us.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The Real Take
Data Mesh wasn‚Äôt
wrong
‚Äîit was just
misaligned with how organizations actually operate
. I also want to be clear, we are talking about the 90% of organizations that simply don‚Äôt have the size to pull off a Data Mesh architecture.
Most companies are not of size.
Most companies need:
strong centralized platforms
collaborative data modeling
clear ownership
and consistency
‚Ä¶ not each domain reinventing its own data stack. It takes serious chops to have experts in data scattered across teams, doing whatever they want with technology and approach, and defining business terms.
It‚Äôs hard enough on a single Data Platform.
Do you force everyone to use the same technology? How are they trained? Do you have a centralized Data Executive?
But, on the flip side you can find a few examples of sucessful Data Mesh implementations at large compaines, that seems to have a good experience.
This talk on Wayfairs multi-year migration to Data Mesh is interesting indeed.
I mean, you can find some vendor blogs,
like Snowflake
, talking about how to do a Data Mesh with ‚Äútheir tool,‚Äù but as far as general industry knowledge, writing, talks, and tools specific to Data Mesh, there just isn‚Äôt a lot going on.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
It‚Äôs a problem of semantics and tools
With all the time I‚Äôve spent researching Data Mesh and its shaky history of starts and stops, I just can‚Äôt help thinking that Data Mesh was never actually alive, in any real sense.
Could the future prove me wrong? Maybe, it‚Äôs hard to know the fickle flow of the data commuity at large, and where the tides will go in the future.
At the end of the data, a few things are clear to me.
Data Mesh is an idea, not a technology
Data Mesh is for ‚Äúlarge‚Äù companies
It was only ever sparsely adopted
It lacks any consistent serious development (unlike Lake House architecture)
It works great for some people
Others only have horror stories
Based on comments from all around the interent, most of the experiences are not good, but it appears to be highly dependant on the technical chops and the commitment of said organzation.
That has more to do with the business, and less of a judgement on Data Mesh as an architectual design.
Let‚Äôs be truthful, in the world of cost-cutting, layoffs, and the like, if a Data Executive looks at both the traditional Data Platform approach and the Data Mesh approach, what are they going to pick??
You need at least a Data Engineer and a Data Analyst for each ‚ÄúData Product,‚Äù and most likely a Data Scientist to boot. Where do the middle management and the Data Leaders sit? At each product, nowhere, everywhere??
If you argue that you can have a single Data Platform, aka say Databricks, yet apply Data Mesh architecture and give the power to each Data Product/Business Unit line ‚Ä¶ then ‚Ä¶
Who runs maintenance and oversees the Data Platform as a whole?
Who makes the executive-level decisions on data strategy?
How does alignment happen?
Who‚Äôs in charge of bringing all together for Analytics and DS use cases?
If your answer to any of those questions is some set of person(s) or department that makes over-arching decisions ‚Ä¶ then that flies in the face of one of the MAIN arguments for Data Mesh architecture.
To get away from those single bottlenecks.
I haven‚Äôt been unclear, have I? I hope not. Personally, I think Data Mesh is a bad idea for most orgs, though I have to tip my hat to the
few large and brave who have pulled it off and like it.
You can‚Äôt argue with results.
If someone says they love Data Mesh, it solved many of their problems, what‚Äôs a brother to say? Nothing. Just tip of the hat to ya.
We live in a large world with lots of people who don‚Äôt do all sorts of things. Heck, we got people running Oracle Data Warehouses to their heart‚Äôs content, ignoring the words ‚ÄúLake House.‚Äù
Anything and everything is possible. But not everything is advisable.
The glass-half-empty dude in my says that after decades of watching single Data Teams fight and claw to provide valuable and correct insights, while not killing each other, it is hard enough. Let alone 10 different teams trying to align on business values, concepts, technology, and definitions.
Seems like a Pipe Dream, but apparently even Pipe Dreams can come true every once in a while.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
You know one thing is true ‚Ä¶ those cloud compute bills are getting out of hand.
Check out
https://carolinacloud.io/
for
High-Performance CPUs, GPUs, Notebooks,
and
Bare-Metal Servers.
If you want to save serious money on compute, head over to
Carolina Cloud."
179162052.what-is-toon,What is TOON?,too scared to ask?,2025-11-24 07:39:38.613-06,"TOON.
I was having a happy Friday, just bumbling along, dreaming about turkey hunting and a weekend free of tech issues. I made the mistake of getting on that now infamous LinkedIn. There it was. Everywhere. A few things go from zero to everywhere overnight.
Somehow
TOON
did it.
Angry people. Happy people. Confused people.
I do pride myself on keeping somewhat up to date on the comings and goings of all things data/ml/ai-related.
I mean, some of you who aren‚Äôt
cheap little hobbits
actually pay me to do this for you.
TOON
done slapped me in the face. Never heard of the word until now.
This little dive into TOON is just as much for me as it is for you. I have no idea what to expect, or what it really is, other than ‚Äú
replace JSON for LLM applications with TOON.
‚Äù So, forget the screaming heads on LinkedIn; they haven‚Äôt done what you and I are about to do. Just empty heads regurgitating empty phrases to get more clicks.
üîé What is TOON?
What some of the ninnies on LinkedIn were regurgitating immediately, like little parrots, was that TOON was/is a replacement for JSON/CSV carte blanche.
This is simply not true.
Context is key. The TOON context is LLM tokens;
replace JSON in the LLM token context.
If you‚Äôre eyes were glazing over with the above, let me make it simple for you.
TOON? ‚Üí ‚Äú‚Ä¶ data model for LLM prompts.‚Äù
That‚Äôs all you need to know, and honestly, it's useless for 90% of the people reading this. Heck, I‚Äôve spent enough time in the salt mines of LLMs, playing with
vector databases
,
RAGs
,
Agents
, and even
fine-tuning LLMs
, and I barely care about TOON.
This does bring up an obvious question. Should you care? Probably not, move on. But if you want to play along with me and learn more about TOON because you‚Äôre already here.
Read on.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
So why is TOON a thing then?
Well, I‚Äôm glad you asked. The same answer for everything these days. Money.
Tokens ‚Üí Money in the AI race to the bottom.
What, you think I lie?? Clearly, more efficient tokens directly impact the bottom line.
One thing is true: it‚Äôs expensive to run LLMs from both sides, to host them, and to use them, especially when our primary interaction with LLMs is tokens. So, is it that much of a surprise that someone is trying to upset the apple cart of tokens? Not at all.
The numbers are indeed impressive if they are true.
‚Äú
Token-efficient:
typically 30-60% fewer tokens on large uniform arrays vs formatted JSON
1
‚Äù - TOON Docs
It‚Äôs also important to note that
TOON should NOT be used
in some applications.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Playing with TOON.
I‚Äôm not sure how far we will go with this, probably until I lose interest, which will most likely be very quick. I am a fickle reviewer and will abandon on a whim.
It‚Äôs pretty obvious to me that the Python-centric AI and ML world will probably be angry at being told to use Node (npm or npx) to install and use TOON.
Aka, they simply won‚Äôt do it.
If you want to be taken seriously in the LLM space, like it or not, Python is the language of choice.
Anywho, let‚Äôs keep going. Let‚Äôs take a look at a concrete example of some JSON format that might be sent to an LLM.
What does this look like in TOON?
Note the
--stats
flag I passed in, which gives you an estimate of the number of tokens saved.
TOON is for Prompt Engineers.
So, this is where things get a little sticky, and may or may not make sense to you based on how much LLM/AI buggery you‚Äôve been doing behind the scenes.
Take what I say with a grain of salt. I‚Äôm a player with AI, a self-proclaimed script kitty, no expert.
Go read this ‚Äú
LLM/TOON integration guide.
‚Äù
What will you find?
There is no direct integration with LLMs for TOON. As far as I can tell, you can‚Äôt directly interact with APIs, for example, OpenAI, with TOON as a direct supported argument/format.
Not the case.
You actually have to include prompt instructions about TOON in the text of your request to get the LLM to do/use/understand TOON stuff. Think about it, these LLMs, their APIS, LangChain, etc. They‚Äôve been using JSON and JSONL-type formats for a long time.
Not TOON.
It‚Äôs early days, and there is no direct support.
Basically, you can send TOON data in your prompts on the backend, but you are going to need to give the model context about TOON, how it is structured, and how to interpret it.
The TOON GitHub for the ‚ÄúDevelopment‚Äù Python version explicitly says and shows this approach.
So, basically, you can use TOON in your LLM prompts if you are in a situation where you are Token/Cost-bound and need to squeeze out every last drop of savings.
But you should test accordingly.
Notice there are many caveats.
To me, it only makes sense if you have built a weird sort of LLM chat/prompt/agent thing where you are taking large chunks of uniform data and passing along that data into a prompt on a hosted/charge per token LLM.
Who the crap is doing that and why?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
In the end.
So, as I get more and more bored with TOON after discovering how useless it is, I think my journey with it ends here. I have no use case for such tom-foolery. My guess is that it‚Äôs still early days for TOON and that it will take some time for the real applications to be built on.
Now, the day LangChain adds direct support for TOON, I might take notice. Until then,
just a big nothing burger."
178823340.work-with-me,Work With Me,,2025-11-13 13:50:17.03-06,"Sponsor Data Engineering Central
Data Engineering Central
is one of the fastest-growing data engineering newsletters on Substack ‚Äî read by
22,000+ data engineers, architects, ML engineers, and technical founders
.
If you want to reach the people who
build modern data platforms
, this is where they are.
üì£
Audience Snapshot
22,000+ subscribers
(growing ~20/day)
Highly technical, senior audience, including roles like:
Data Engineers
Data Architects
Senior ICs & Tech Leads
Platform Engineers
ML/AI Engineers
CTOs & Startup Founders
Readers work at companies across fintech, SaaS, AI/ML, cloud, and enterprise tech.
My content focuses on:
Lakehouse architecture (Delta, Iceberg, DuckDB, Polars)
ETL/ELT pipelines
Real-world data engineering
ML systems in production
Fraud detection data stacks
Data tooling deep dives
Performance & cost optimization
This is a
high-intent
,
high-signal
technical audience.
üíº
Sponsorship Options
üîπ Standard Newsletter Placement ‚Äî $400
Includes:
Featured sponsor block in one newsletter
Your logo, description, and strong CTA
Included in email + web version
Me writing the ad copy for higher conversions
üöÄ
Optional Add-Ons
(Available only with newsletter placements)
üî∏ LinkedIn shoutout
Your product featured in a dedicated or blended post to my LinkedIn audience (5‚Äì10k impressions typical).
üî∏ Twitter/X shoutout
One post with your tool highlighted, targeted for engagement.
üî∏ Podcast Sponsorship
Your tool is mentioned in the
Data Engineering Central Podcast
(which includes distribution on Apple + Spotify).
‚Äì Pre-roll or mid-roll shoutout
‚Äì Mention in episode notes
‚Äì Link + CTA
Bundle discounts available when combining newsletter + podcast + social.
üì¶
Packages
Single Placement
$400 per slot
Bundle (4 slots over 1‚Äì2 months)
$1,400
(save $200)
Newsletter + Social Bundle
$550
‚Äì 1 newsletter placement
‚Äì LinkedIn post
‚Äì Twitter post
Podcast Add-On
+$150
(pre-roll)
+$250
(mid-roll)
üéØ Why Sponsor?
Hyper-targeted audience of practitioners and decision-makers
High trust environment ‚Äî readers actively look to me for tool recommendations
Strong engagement despite a large list size
Data engineering is a
premium B2B audience
Many sponsors see strong CTR and signups
üóìÔ∏è
Availability
I usually publish 4-6 times per month.
Sponsorship slots are limited and often fill up in advance.
üì¨
Book a Sponsorship
To reserve a slot or ask questions:
Email:
dancrystalbeach@gmail.com or message on
Linkedin"
178818802.data-engineering-central-podcast-db1,Data Engineering Central Podcast - 09,Cluster Fatigue and the Death of Open Source,2025-11-13 14:59:14.92-06,"Hello! A new episode of the Data Engineering Central Podcast is dropping today. We will be covering a few hot topics!
Cluster Fatigue
The Death of Open Source
Going to be a great show, come along for the ride!
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
178708690.all-you-can-do-before-airflow,All You Can Do Before Airflow: ,4 Orchestration Levels From Cron to Full Pipelines,2025-11-17 07:54:35.637-06,"Hello, this is Daniel! Today, we have another Guest Post from
Alejandro Aboy
. You can check out his Substack
The Pipe and the Line.
Alejandro Aboy is a data engineer at Workpath, building scalable pipelines with Airflow, dbt, and the modern data stack. Through
The Pipe & The Line
, he writes hands-on tutorials on topics like building dbt-style orchestrators with Python and DuckDB, and implementing RAG systems for data engineers
Let‚Äôs get started!
Most orchestration tutorials start with Airflow DAGs processing static CSV files.
You see the fancy UI, the complex graphs, the task dependencies, and think, ‚Äúthis is what real data engineering looks like.‚Äù
When I say Airflow I also mean Mage, Kestra, Dagster, Prefect or whatever many other options of orchestrator we have out there in the market right now.
I remember when I was starting out and first saw Airflow demos. Beautiful graphs, complex DAGs, Redis queues, Celery workers.
Data enthusiasts might be getting the wrong idea, bootcamps or courses out there place Airflow as the only option to make your workflows come to life.
The truth? There are levels to this. Your orchestration approach should match your project maturity, not your aspirations.
And this principle also gets dragged everywhere else, even to data teams making decisions based on FOMO and now on their own context.
The Modern Data Stack narrative usually pushes dbt, Airflow, Snowflake, and Fivetran as the default starting point.
Most teams don‚Äôt need the complete solution on day one, or they don‚Äôt even need it at all. There‚Äôs something in the
middle
that offers many alternatives.
Orchestration Fundamentals: Beyond Tool Names
Before diving into tools, let‚Äôs talk about what orchestration actually means.
This helped a lot when I was getting started: understanding that knowledge can be transferred and that you can break down the orchestration approach into layers.
Note that this won‚Äôt deep dive into exclusive concepts, its just a baseline for the most common patterns.
A schedule
is when something runs. Daily at 2 AM. Every 15 minutes. Universal across all approaches, from cron to EventBridge.
A task
is a discrete unit of work. Extract from API. Transform DataFrame. Load to S3. Building blocks of any workflow.
A workflow
is a set of tasks with an execution order.
Dependencies
define what runs when. Task B needs Task A‚Äôs output. Task C runs in parallel with Task B.
Triggers
determine when workflows start beyond scheduled times. File lands in S3, run workflow. Workflow completes, start another. Webhook arrives, process it.
The tools are just different interfaces to these concepts. Airflow calls it DAG, Prefect a workflow, Mage a block, that‚Äôs not the point here.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Different Levels of Orchestration For Different Stages
GitHub Repository:
Check out the full implementation at
Github - Orchestration Levels
I built a project to show what this journey actually looks like.
It‚Äôs a real ETL workflow that extracts data from Piwik PRO (web analytics) that calls multiple endpoints, and stores results in S3.
The business logic stays identical across all four orchestration levels. Only the scheduling mechanism changes.
Note that the code won‚Äôt change across different stages because that‚Äôs not what we are pursuing. What changes is the ecosystem around it based on the use cases to orchestrate it.
Let‚Äôs go over the different levels.
Level 1: Local Cron
If you just want to see how this behaves, with a simple POC and without rushing into more complex orchestration setups, start here.
Start here. Cron has been around forever, and it can handle more than you think. You don‚Äôt have to worry about setting up anything else for the moment, and it can keep going.
Ask yourself: Can this script finish before the next run?
Level 2: GitHub Actions + Docker
When CRON isn't enough and you need version control and secret management, but still don‚Äôt want to worry about infrastructure headaches, GitHub Actions is the right choice.
By default, you are generating logs, monitoring, and manual triggers.
You keep using CRON to set up the schedule. In this particular case, I am using Docker, but it‚Äôs not mandatory.
You can see a successful run
here
.
Level 3: ECR Registry + Lambda
Now we are getting into a more Cloud-based context. But we haven‚Äôt got to the orchestrator stage yet.
In this case, I added CI/CD to the
mix
to deploy the Docker Image to AWS ECR so it‚Äôs available for building the Lambda.
There are some extra steps on IAM you need to take care of, not to mention that if you need your script to work locally, on Github Actions and on Lambdas, you will need to adapt it so it can double check in which environment its working.
In this¬†case,
AWS
EventBridge
triggers on schedule, but the trigger could be a file landing in S3 or an SNS topic receiving a message.
You get all the logs from CloudWatch metrics, so that part is already covered.
Lambda‚Äôs 15-minute limit gives you an idea on what‚Äôs your thresold to consider other options. If your batch job needs more, rethink your approach.
Level 4: Full Orchestration (e.g. Prefect)
You realise that the previous 3 levels are not enough, you need an orchestrator.
Maybe you can get away with chaining lambdas, I‚Äôve been seen saavy data engineers do it and I would not prefer to follow that path.
I decided to use Prefect instead of Airflow because we mentioned knowledge can be transferred, so here‚Äôs a good example to prove it.
The principles are quite similar to Airflow.
You use decorators @task, same as Airflow does:
Then can be referenced in @flow, what Airflow calls @dag:
There are many opportunities, such as parallelization, reusable outputs across tasks, custom retry definitions, rich observability, and more.
As you can see, you should only need this when you have these problems and are asking for it
How to Actually Choose
Here‚Äôs a decision tree that can help:
Level 1: Local Cron
- Use when: POC, single machine, development testing

- Perfect for: Scripts that run on schedule and finish before the next run
Level 2: GitHub Actions + Docker
- Use when: Need version control for schedules, container parity, no server maintenance

- Perfect for: Small daily batches, simple CI/CD workflows (most teams should stop here)
Level 3: ECR Registry + Lambda
- Use when: Deep AWS integration, jobs under 15 minutes, serverless preferred

- Perfect for: Cloud native teams already invested in AWS infrastructure
Level 4: Full Orchestration (Prefect/Airflow)
- Use when: Complex task dependencies, managing dozens of workflows, parallel execution needs

- Perfect for: Multi-step pipelines with genuine orchestration requirements
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
üìù TL;DR
Start simple, feel the pain, then add complexity. At the end of the day, orchestration is about reliability, not sophistication.
Typical orchestration tutorials jump to Airflow with static CSVs, making it seem binary when there are actually four maturity levels
Start simple (cron) and add complexity only when you feel actual pain: version control needs ‚Üí GitHub Actions, AWS integration ‚Üí Lambda, task dependencies ‚Üí Prefect
Your business logic should stay identical across all orchestration levels; only the scheduling mechanism changes
Most teams should stop at GitHub Actions; full orchestrators are justified only with genuine task dependencies and multiple workflows
Orchestration thinking is asking ‚Äúwhat‚Äôs the simplest thing that works?‚Äù not ‚Äúwhat tool do senior engineers use?‚Äù"
178468478.one-big-table-data-modeling,One Big Table Data Modeling,was it all for naught?,2025-11-20 11:37:48.28-06,"Oh boy, have I been lied to my entire life? Should I finally give up and move to that cabin in the woods? The life of a Data Engineer, one thing to the next.
I was born again as a Kimball monk, baptised into Medallion Architecture, and will be burned as a heretic for One Big Table (OBT) theology?
What's new?
I, for one, have mostly ignored the rumblings of OBT design from various corners of the internet. But with the death of data modeling and the rise of the Lake House, it seems that previous prejudices and technological barriers have fallen away, opening the door for OBT.
üèÅ
1. Introduction ‚Äì The Temptation of One Big Table
Welp, based on how old this question is, we can probably answer it now. (
OBT always seems to be a fringe discussion
)
Why not? Just do it, bro. I mean, maybe we should just follow our gut, throw Kimball into the trash heap, and move on with life.
üî• We live in the age of distributed systems, like it or not, so what could be better than simply getting rid of the need for all those joins? That's the point, isn't it?
Opps. Maybe we should explain what OBT data modeling is.
üìä Think of it as:
‚ÄúEverything you need for the query, all in one place.‚Äù
I mean, when we talk about it like that, it sounds perfect, nice, and you would be crazy not to do it.
Right? Right???
Well, I'm not here to tell you what you should or should not do in your situation. Data Modeling, especially, is highly dependent on the data AND the use cases.
But remember, simplicity now can and will turn into chaos later.
Trade-offs between
speed vs. structure
,
simplicity vs. scalability
.
üí¨ ‚Äú
Every data engineer starts with one big table ‚Äî until that one big table starts fighting back.
‚Äù
Why would someone even choose OBT data modeling in the first place?
simplicity
less setup up front,
no distributed joins,
‚Äúfaster‚Äù
Before we throw the OBT fanboys into the heretic fire and burn them to a crisp, we should at least give some credence to what is being argued.
It could be argued that some data teams already have ‚Äúloose‚Äù data modeling that isn't enforced; maybe they don't need a rock-solid single source of truth for customer or product data.
If that is the case, why normalize the data and split tables out? It‚Äôs just extra work that will not be used by anyone downstream, adds complexity and compute, and is only nice to look at, with no technical benefit.
This seems unlikely, but it is a real possibility.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
üß©
2. What Is ‚ÄúOne Big Table‚Äù Modeling?
I feel as if calling One Big Table's data modeling an oxymoron is an oxymoron. The entire point of OBT is that you don‚Äôt have to model the data. It all goes in a single spot. In the table. Duh.
This isn‚Äôt data modeling.
It‚Äôs more of a casting off of the shackles of needing to do any data modeling at all. It‚Äôs telling Spark, for example, that you‚Äôre just going to need to crunch and aggregate on this one table; no need to JOIN different datasets and shuffle a bunch of crap.
Indeed, when using OBT, the idea is to address messy data issues and establish a single source of truth for customers, products, and other dimensions in a different way. We are saying that using stuff isn‚Äôt as important as making queries simple and aggregations straightforward.
It‚Äôs saying ‚Äú
we are always going to be doing the SUPER simple analytics, there isn‚Äôt really a need or want for advanced or complex analytics.‚Äù
I‚Äôm not saying you can't do advanced OBT analytics, I‚Äôm saying it‚Äôs harder
OBT is built for straightforward queries, GROUP BY ‚Üí AGGREGATE
üí° BigQuery, Redshift, and Spark users often do this for convenience.
This becomes painfully obvious at scale, say, in PB+ data systems, where it‚Äôs extremely time-consuming and expensive to split out massive datasets into more normalized views.
üìä This focuses on simple analytics and straightforward queries.
‚öôÔ∏è
3. Why People Love It (At First)
üöÄ
Speed to deliver:
Analysts can query quickly ‚Äî no joins, no complexity.
üí∞
Simple billing:
Fewer queries = fewer joins = cheaper compute (in theory).
üß†
Easier onboarding:
New team members get data fast.
‚ÄúIt‚Äôs not elegant, but it works ‚Äî until it doesn‚Äôt.‚Äù
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
üß®
4. The Dark Side of One Big Table
‚ùå
Duplication Hell:
Data copied across rows endlessly.
üìâ
Inconsistent updates:
One column change = full rewrite.
üßÆ
Performance drag:
Compression fails, scanning costs rise.
üß©
Loss of semantics:
Relationships vanish, lineage breaks.
OBTs are amazing as a
consumer
experience‚Ä¶ but a
maintenance nightmare
behind the scenes. OBTs are usually
materialized
rather than computed on the fly.
Which means every time
any
upstream logic changes:
A new column is added
A definition changes (‚Äúmerchant_type‚Äù ‚Üí new categories)
A dimension is updated
A bug is discovered
This makes dealing with ‚ÄúOh, we changed this product ID or description‚Äù a serious problem. There is no choice but to layer on logic after logic, layer upon layer over time, to deal with these inconsistencies. Often, downstream consumers are forced to deal with this.
They will not be happy.
OBTs almost always require:
Updates
Deletes
Backfills
Column rewrites
Patches
But large lakehouse tables
hate
mutations.
Especially when you‚Äôre using:
Deletion vectors
Z-order clustering
Iceberg position deletes
Object-level storage (S3/GCS/Azure Blob)
This leads to:
File fragmentation
Gigantic metadata
Compaction hell
Long vacuum cycles
Broken partitions
Query performance tanking
It‚Äôs not uncommon to see
500+ columns
.
Problems caused:
Downstream consumers break when columns reorder
VAST Parquet files (sometimes >1,000 cols)
Hard to compress
Hard to predicate prune
Hard to read efficiently in Spark/Trino/DuckDB
Wide column sets =
slow scans
,
high I/O
,
lots of memory pressure
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
If it sounds too good to be true ‚Ä¶. it is.
To be honest with you, sometimes one simply has to listen to what your grandmother told you as a 5-year-old sitting next to the armchair, stealing Andes Mints. If something is sound to good to be true, it probably is. Plain and simple.
Sure, you might find the need, because of data size or lax business requirements, to use OBT. But this is not the norm.
Data is gold to the business; we need it to make decisions, support Machine Learning and AI, and produce consumable analytics. Most of the time, this requires good data modeling to help ensure data quality and to conceptually enforce logic on the data, which in turn can yield insights.
OBT is messy by nature.
It lets the data be free, free to morph, change, unfettered; it will grow into a pile of spaghetti that won‚Äôt be undone. Eventually, the Data Platform and downstream consumers will break under the pressure of layers of logic to ‚Äúfix‚Äù and ‚Äúupdate‚Äù things.
You will be asked to normalize the OBT at some point."
178241656.databricks-compute,Databricks Compute.,Examining the Inscrutable,2025-11-09 20:38:14.72-06,"I am certainly no master of the minutiae of Databricks compute, able to unwind the wound-up ball of string that leads us to where we are today.
Yet DBUs matter in the world of the Lake House
. It‚Äôs the difference between an angry CTO and a happy one.
For a more accurate, academic approach to Databricks compute, I recommend you follow and read
Josue Bogran
‚Äî the true master.
I will give you the 5-cent tour of Databricks compute, from the vantage point of someone who only half pays attention to the
comings and goings of drop-down selections in the UI
. Like Robin Hood, maybe we can dream of swooping in to steal some money back for ourselves.
Databricks Compute for the Uninitiated
For certain, there is a whole mad cadre of consultants who‚Äôve built careers and large bank accounts simply by advising Databricks customers when to use what compute for what ‚Äúthing.‚Äù
Makes a guy wonder. Where did I go wrong in life?
I run most of my Databricks Compute life with a strict set of simple principles that have served me well.
Never use All-Purpose clusters.
Job Clusters for all.
Fleet Clusters at large.
Serverless for quick hits.
I‚Äôve yet to be fired, but you never know, never say never.
My hero is Grug
, and I follow those principles.
Finding the Amusing.
In a
pragmatic
sort of way, I‚Äôm always looking for a reason to laugh, you know, not take life and tech too seriously. When it comes to Databricks compute,
you don‚Äôt have to go far to find a smile.
Say you are new to the whole gambit and simply Google ‚ÄúDatabricks compute,‚Äù then click the first link that pops up. You will come straight to the
main ‚ÄúCompute‚Äù page on Databricks.
Behold, what are the first things your eyes will feast on, and what will automatically get embedded into your subconscious brain?
Serverless
.
I would like to pontificate upon this more, but for lack of time, and fear of the
ever-watchful Eye of Sauron
watching over my every word, we will return to Serverless later.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Databricks Compute Options.
The academics among you may cringe and give some boring and bland list of compute options that will probably change next week, but I‚Äôm going to take a different tack. As always.
On a regular basis,
I must remind people that not everything is a technical problem to solve, but more of a thought-provoking conundrum of choices.
Let‚Äôs actually review Databricks Compute by the options that show up in the UI!
Wow, who would have thought to do that? The UI is where most Databricks newbies get their digital feet wet. Below are the options when we click on the Compute button in Databricks.
First and foremost, you will actually see the most used options today, regardless of what the talking heads say. Funny enough, starting with the most expensive.
- All-purpose compute
- Job compute
- SQL Warehouse
The Big Three. Ignore the other stuff.
This is where the psychological warfare comes into play. The Orwellian marketing gambit.
The Spice of the Intergalactic Cloud compute bills.
What don‚Äôt you see here? Serverless. Doesn‚Äôt show up here. Strange.
The other thing you will note is that Databricks is changing the wording of how compute is generally refered too, if you‚Äôve been around more than 2 years, you will know this.
I‚Äôm not here to sit in judgment on the inner workings of the demi-god like Illuminati who run that company and pull the strings.
I wouldn‚Äôt dare.
You can make up your own mind.
Let me sum up the current state of affairs for you.
I ain‚Äôt making this stuff up, I‚Äôm just a corn-fed boy raised on a river. Databricks compute documentation refers to two overarching types of compute.
Classic Compute
Serverless
Basically, anything that isn‚Äôt a new-fangled serverless is Classic. Old. Mundane. Manual. Buggery.
More or less, Classic Compute options give you the ball ‚Äî put me in, coach. You get to set the instance size, blah, blah, blah. You‚Äôre the captain of your own ship, either to crash on the shoals or smoothly sail to
wherever those elves went in the west.
More funny.
If you simply Google, yet again, ‚Äúall-purpose compute Databricks,‚Äù it seems the internet, or Databricks docs, have been scrubbed of an actual page(s) devoted specifically to this very expensive compute type.
The ball of string has been tangled and woven into an intricate web of inscrutability.
If we go to the pricing page
, if you know anything about Serverless vs Jobs vs All Purpose, you can start to smell out the difference in pricing.
‚ÄúInteractive workloads‚Äù here would be where that All Purpose compute that
SHOWS UP IN THE UI AS THE FIRST OPTION
, falls. Sometimes the left hand has a hard time keeping up with the right hand, if you catch my drift.
What is even funnier is that if you click on the ‚Äústarting at
$.40 per DBU
‚Äù option, the next page shows our word we‚Äôve been looking for: ‚ÄúClassic All-Purpose Clusters‚Äù at
$
.65 per DBU.
Generally, this means avoiding All Purpose Compute.
<<
insert angry comment from a consultant who says having a single All Purpose Cluster used by multiple people is cheaper than a bunch of Jobs running helter-skelter.
>>
Job Compute.
I mean, good Lord,
if you go to the official Job Compute page
on Databricks, you will find a list of Tasks and their corresponding recommended Compute type.
Let me help you read the table,
Rita
. It says
SERVERLESS
for everything.
Well, if we scroll to the bottom, we can select Classic Jobs. Which is the bread and butter of a lot of compute today.
Good ole‚Äô Spark Submit.
There you are my old friend. You‚Äôve been relegated to the past, shuffled off to the care home ‚Ä¶ classic ‚Ä¶ pun intended.
So strange, Serverless Jobs are recommended for pretty much everything on the list. Hey, I‚Äôve got a bad memory. What was the cost of Classic Job compute vs Serverless Job compute pricing?
Serverless Jobs are twice as expensive.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
They better be twice as fast for twice the cost eh.
Dang bruv. We ain‚Äôt even got to that SQL Warehouse compute we have in our original UI compute list from the Databricks UI. If you are now very curious, welcome to the club. What should you choose?
Jobs over All-Purpose
Use Serverless for ‚Äúquick‚Äù or ‚Äúshort‚Äù compute needs
That‚Äôs all this Grug can give you. Other than that, you will simply have to test your workloads and figure it out yourself.
I suggest, again,
you read and watch Josue talking more about Serverless Compute and the problems it solves.
Now that we‚Äôve beaten the dead horse to death, let‚Äôs move on.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
SQL Warehouse
If you remember from the previous screenshots, SQL Warehouse compute appears in the middle of the cost list.
Again, within that selection,
you get three different options with very large swings in pricing
.
From $.22 - $.70 DBU.
What‚Äôs that you say? Huh? You want to know WHAT is a SQL Warehouse as compared to All-Purpose, Jobs, and the like?
Oh, I‚Äôm sure Databricks Docs will tell us what a SQL Warehouse is.
Ok, I lied. They literally will
NOT
tell you what SQL Warehouse is, other than that you should use the Serverless option!!! LOL!!!!
Dang, I‚Äôm sorry they wouldn‚Äôt tell you what it is. Let me try.
If you were to ask Grug, he would simply say that a SQL Warehouse is, or was in the past, a long-running compute resource for SQL workloads that could sit out there in a nebulous manner and run as needed.
In the past I‚Äôve seen BI tools like Tableau hooked to SQL Warehouses as a sort of endpoint. In this case Serverless although expensive, is probably better for not used much Warehouses, wereas classic might still be ok for something that gets hammered all day.
The Serverless Question.
I mean, how in the good Lord have I managed to only mention Serverless compute this whole time when it‚Äôs literally being shoved down your unwilling throat at every turn?
Clearly, the writing is on the wall.
Serverless is the desired future of Databricks compute. And, contrary to what you may think, I believe Serverless options are indeed a wise choice in many instances.
Serverless is the natural extension of human laziness and annoyance at having only one option to do something.
Spark resources take a while to spin up
People don‚Äôt want to fine-tune resources to workloads
People want simplicity more than low costs
Want to pay for what you use? Want simplicity of use and configuration? Don‚Äôt want to spend hours trying to figure out what size Cluster to set up for your Job or another random task?
Serverless is what the masses want, and that‚Äôs what they got.
Point and click, baby.
Does it come with a price? Well, if you have been paying attention to the numbers, it does, of course. Does that mean it‚Äôs a bad idea? Not at all.
Depends.
It could very well save you a bunch of time, it could very well increase your costs. Will it reduce overhead, complexity, and the like? Yes indeed it will.
This is the part where you get to do what you‚Äôre paid to do: be an Engineer, think, and figure out what‚Äôs best for your Data Platform, your workloads, and your organization.
Sorry, not sorry.
Hey, I told you from the start I‚Äôm not a Databricks Compute savant. I just like talking theoretically about how Databricks approaches these things and the changes I‚Äôve seen over the years.
Here are some good resources to further your Databricks Compute journey that I think you will find interesting.
https://www.reddit.com/r/dataengineering/comments/1gpxcfm/serverless_costs_for_databricks/
https://www.reddit.com/r/databricks/comments/1jnefyv/how_do_you_guys_think_about_costs/
https://docs.databricks.com/aws/en/lakehouse-architecture/cost-optimization/best-practices
https://www.linkedin.com/posts/josuebogran_databricks-compute-101-classic-compute-the-activity-7298759550524968961-pX28/
https://www.databricks.com/product/pricing/product-pricing/instance-types
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
178136533._internaldeltaprotocolerror,_internal.DeltaProtocolError:,The Databricks and Delta Lake lie (the mighty have fallen),2025-11-06 17:37:17.615-06,"First, a note from me before we chomp on them meat and taters.
Today I want to bring on along for ride down the long winding lane of an average day in the life of just another average Data Engineer. No glorious writing about the wonders of AI and how digital rainbows and unicorns leap from the ends of my fingers while building Data Platforms that never break and cost nothing.
Not today.
If that‚Äôs where you find yourself, you‚Äôre not trying hard enough. Interesting work and life is found in turning over rocks and looking to see what‚Äôs underneath. Stubbing the ole‚Äô toe.
Today?
We are going to stub our toe on some Databricks + Delta Lake + Polars + DeltaLake (python) + Daft.
The best laid plans of men and mice you know.
I‚Äôm going to give you my list of frustrations out the gate. So if you want some LinkedIn fodder, I will make it easy for you.
Nothing is ever as it seems
I smell the beginnings of troubles that aren‚Äôt easy to reverse
Your tech stack isn‚Äôt as open as you think
Devils in the details
üîÑ Off the beaten path.
For most of my life I‚Äôve found the most interesting path to walk is not the well trodden one. Of course I know it‚Äôs important to have balance in all things Data Platform. We want ‚Ä¶
reliability
large community of support
stability
‚Ä¶ etc
Yet, in an effort to bring value and push the boundaries of what is possible, we must at times stray from the safe path. Of course, the minute we stray from the well trodden way, we can expect trouble around every corner.
I for one, think that
difficulty in all parts of life tends to harden and make one more resilient.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Ok, so let‚Äôs just walk through the problem I recently faced.
Turns out neither Polars, PyArrow, DeltaLake (python), or Daft could do the job.
I mean we should just shake hands all around, pat ourselves on the back ‚Ä¶ what a wonderful idea!
Save money, it‚Äôs fun, simple.
If we were going to count our chickens before the eggs hatch, this would be the time to do it.
Heck, who can‚Äôt appreciate the simple approach of reading even a
Unity Catalog Delta table with Polars
.
Hot dang. It‚Äôs even lazy in nature so not as to blow up memory. At this point we start to whistle a little tune. So wonderful, we can just imagine the cost savings and praise we will receive.
Who would have thought that going against the grain would be so easy??!
I mean blimey, the write to Postgres is even better.
What could possibly go wrong?
So, off we go to our development environment. Time to test in Airflow with some real live dev tables in Databricks Unity Catalog. Then it happens.
What we all knew was coming.
We stare at our screen for awhile, rerun things a few times, of course the error never changes.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
üè¢ Debugging time.
At this point we get a little annoyed. Never run into these problems with PySpark on Databricks with too many Delta Tables to count, created over many versions of DBRs, different versions of this and that.
It just works.
Polars? Nope.
Well ok, let‚Äôs just do what the error tells us to do. Upgrade the Delta Table reader version to 3. Of course that will work right?
No go. Same error. What??!! I updated the flipping table. What gives?
Well, we could
poke around docs for minute I suppose.
Read read. I did notice something about required upgrade required for certain features, is it a coincidence that our
timestamp-ntz
shows up in the list?
Probably not. What does it mean?
How should I know??
What, you think I have unlimited in time both professionally and personally to pick and prod at the minutia of Delta Lake versions and the underpinnings of Polars Delta Lake readers?
Give me a break. After poking around the matrix for a bit, it seems the consensus might be this?
üß¨ Lordy, they got a way of keeping a
guy nervous about this stuff
. Honestly, sometimes I would rather not know. It‚Äôs one thing to just fix something that breaks randomly. It‚Äôs another thing to be sitting on the edge of your seat waiting for the ball to drop.
Buggers.
Well, what‚Äôs for it? Let‚Äôs just re-write the table with the new version reader and see if Polars can chomp happily on the Delta Table like that.
Look ma! A new error!!!
Well ain‚Äôt this just all sparkles and butterflies?
Par for the course I think.
The reality of day to day data engineering in real life, when you stray off the well worn path that is.
Then, my friend, the saga got worse.
One would think re-creating the table with ‚Ä¶
enableDeletionVectors‚Äô = ‚Äòfalse‚Äô
Would do the trick.
To no avail.
It seems just those magic words DeletionVectors borks Polars, true or false. I suppose in theory this makes sense.
üß∞ DeletionVectors are newerish. I had been running these commands on DBR 17 something, what if we wind back the DBR clock to something like DBR 13?
The truth is though, winding back DBR versions, creating tables with maybe ReaderVersion 1 ‚Ä¶
this isn‚Äôt really a true fix.
We need our Delta Lake tables to keep up with the times, to grow and mature.
You can‚Äôt wind back the clock to move ahead.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
A strong tool needs an ecosystem of frameworks that all work together in harmony. That‚Äôs what Delta Lake used to be back in the day. No more.
Sadly, these other non-Spark tools are lagging behind, these breaking changes makes this difficult, and now we‚Äôve painted ourselves into the proverbial corner.
To make matters even worse, or funnier, depending on your point of view , I thought I would try using
daft
from a Notebook inside Databricks.
To read a Delta Lake table
. (
the pip install instructions are incorrect in the their docs
) I‚Äôve done this many times in the past, but, things change. If you catch my drift.
Databricks is saying ‚Ä¶ not going to happen Sunny Jim, get your dirty little hands off.
üóÇÔ∏è And yes, external data access on the metastore is turned on.
So, what has happened? Quietly, on purpose, or not, if you‚Äôre using Delta Lake tables on Databricks environment with any semblance of recent tooling (DBR, etc).
You are getting locked in. You‚Äôre Delta Table and DBUs ain‚Äôt going anywhere you bugger, lock in for the ride.
Who in their right mind is going to start manually upgrading and downgrading reader and writer versions depending on what tool you want to use??!!
I thought to myself, heck why don‚Äôt I use the
deltalake
Python package from the horses mouth (Databricks) so to speak. THAT, of all tools must work to read me a Delta Lake table these days.
Oh how the mighty have fallen.
Number one, the docs have an incorrect example that might through you off until you go through the verbiage closely. You need a
uc://
in front of your full `
uc_full_url
`. At which point, it breaks again.
This by no means works, and pukes right out.
This is quite the change from even a year or two ago if you ask me. This new state reminds me of Apache Iceberg. Fragmented, many moving parts, nothing works besides the DBU money accumulating version with Spark.
If Apache Iceberg ever had a change to resurrect itself as a good open-source option that can be relied upon and actually works with many frameworks in various situations,
this is it.
Databricks and Delta Lake are no longer the innocent bringers of Lake House truth to a broken world.
Corrupted and stumbling over dollar signs and corporate mishap.
I have to admit, in my heart of hearts I knew it was coming. But being slowly boiled like a frog in a pot feels good at first. No one thinks it will reach the point where you literally are locked into your Data Platform without zero wiggle room without major antics.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
‚öôÔ∏è The future.
The endless cycle of heartache where the digital rubber meets the road. Nothing is clear and clean cut, decisions are messy and full of nuance. Tools change fast, we change fast, and before we know it, we are in a spot we didn‚Äôt expect.
Listen, I don‚Äôt claim to be a Databricks or Delta Lake super savant. I‚Äôve been using the stuff for years, but what does that mean? Nothing. So have a lot of other people.
Sure, smarter people than I could tweak and turn knobs and dials and throw salt over their left shoulder at midnight. I work and operate in the real world. I don‚Äôt have ‚Ä¶
infinite amounts of time to force an issue
simplicity and reduction of complexity is key
things happen in a larger context
I do things I don‚Äôt like in the name of ‚Äúbusiness‚Äù
If you think this polemic against Databricks and Delta Lake is uncalled for, take a hike. I‚Äôve written a plethora of wonderful content lauding many of their features, and will continue to do so when warranted.
I also have, and will, continue to preach the truth about the everyday average engineer down in the trenches mucking about in the mud.
I will voice concerns and annoying things when I find them.
Many times it‚Äôs those on the bottom of the ladder, (
I am one of those I assure you
), pay the price for the inattention and lack of care from those at the top of digital food chain.
These buggy nuances of versions of this and that, mismatched frameworks not keeping up, they tell a large story. One that is not easily reversed and backed out of. The truth is, the future is probably not brighter but filled with more pain in this regard."
177575691.gzip-csv-python-s3-polars-vs-duckdb,Gzip. CSV. Python. S3. (Polars vs DuckDB),headaches ya' know?,2025-11-03 07:47:01.126-06,"In my never ending quest to replace every bit of Spark possible with something ‚Ä¶ else ‚Ä¶ you do run into problems that make you realize why PySpark is the go-to tool for the Modern Data Engineer.
It‚Äôs hard letting those old loves go, like that first date in high school ‚Ä¶ they keep their claws in ya.
It‚Äôs the small things in life that add up to big headaches. Don‚Äôt we all strive for simplicity? I do. Every single line of code is a liability. A sure sign of someone who‚Äôs been writing code for more than a decade.
Some of these nuances become clear when doing seemingly insignificant data engineering ‚Äúthings.‚Äù
I‚Äôm not saying these things are deal breakers, but you do notice them when moving from a well matured Big Data tooling, to the new kids on the block. Things I will be thinking about ‚Ä¶
what nuances are worth the tradeoff?
compression matters in a S3 bucket world (cost).
do newish frameworks work well with compression?
complexity vs simplicity tradeoffs.
Doing stuff.
This article is going to be more of a simple musing style, I have no plan for where it‚Äôs going, just examining a seemingly simple workflow, and talking about complexity, tooling, and the maturing of products and frameworks.
This is a common data ingestion pattern is it not? Life‚Äôs never perfect and neither is data.
we get data that MUST be pre-processed before ingestion
cost matters, storage is cheap, but not that cheap
we don‚Äôt want write complex code to do simple things
That‚Äôs just life ain‚Äôt it? Let‚Äôs say we have a Lake House running on Databricks and PySpark ‚Ä¶ that the workhorse of our Data Platform. But, there is just dirty data and some things Spark isn‚Äôt good at.
We have to do pre-processing of files.
Some X and Y transformations
Compress the files to .gz
These are very real and very common problems. S3 cost can add up, storage cost does matter, it‚Äôs money off the bottom line. Why store CSV files when we can store
.csv.gz
?
Many tools, including Spark, can deal with
.gz
in a transparent manner.
Let‚Äôs do this with Polars, an obvious option for CSV files.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Transform and compress CSV files with Polars.
So, let‚Äôs just reach for one of the obvious choices to do some simple CSV processing, like Polars. Why not. In theory maybe this code is running inside an AWS Lambda that is triggered of another S3 bucket where the raw CSV files land.
One would assume that ‚Ä¶
reading and writing S3 CSV files should be easy.
compressing CSV files seamlessly should be easy?
I mean every time we add another import to Python code, adding more lines, etc, is just more bugs and things that have to be maintained.
Does Polars support CSV with file compression?
Yes. And No.
Simple to test eh?
Now, can we read with Polars? Well this is nice, we can read
.gz CSV
files seamlessly with Polars, this is a nice feature.
But, it‚Äôs only half the battle right?
We need both read and write compression options.
Polars does not offer this.
So, IF want to use Polars, we will have to start adding more code to make this happen, maybe add some vanilla Python code to make Polars write a
.csv.gz
file.
I guess this isn‚Äôt horrible. Just two more packages. The gzip package itself doesn‚Äôt support s3 paths, so we had to add
s3fs/fsspec
so we are able to get access into s3 for the write.
Of course in real life we are doing some transformation in between reading and writing, maybe removing headers and footers,
or forward filling NULL values
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Hold the phone. What about DuckDB?
Ok, so one could argue that the Polars + Python code isn‚Äôt really that bad. Depends on your OCD and lines you draw in the sand. Are you pragmatic or hardcore?
What about DuckDB?
DuckDB has long impressed me with their ability to drive features that top-notch, and they are seemingly obsessed with providing the best developer experience to the most mundane or minute features.
(DuckDB has never paid me a dime, I simply say it how I see it).
They are not going to disappoint us this time either.
Look at that would you. DuckDB provides compression as a CSV option.
@#!$@# they good.
Those little buggers think of everything.
Much more concise with the DuckDB, which we can all appreciate.
Less things to break eh?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Musings on simplicity and every day pipelines.
You know, when you spend your days thinking and planning about big projects, making sure the wheels don‚Äôt fall off this or that, it can be a breath of fresh air to poke and feel around some day to day data pipelines.
What we talked about today is what you can start to loose track of if you don‚Äôt keep your fingers in the mix, and try new things, and think.
Thinking ‚Ä¶ something that is is every so foreign now in the day of AI and Cursor.
‚ÄúBut, you say, why does it matter? It‚Äôs code, just a little here and a little there. We can get the job done one way or another. What‚Äôs the big deal?‚Äù
Maybe there is no deal to worry about. Either way, the day to day building and low level operations of a Data Platform do add up to something don‚Äôt they?
Decisions at a high, architectural level of ‚Äú
We will build with this tool
‚Äù seem to trickle down to the mundane pipeline of the day. How the code looks, how much code there is, how many hoops have to be jumped through, developer happiness.
Everyday around this world, similar decisions are being made by engineers.
The code we write matters in the end, all those data pipelines add up and work together to produce an experience and a way of development and day to day processing.
Yes, there are times to get the job done, it depends on the context.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
177097189.lance-file-format-parquet-killer,Lance file format. Parquet killer?,time will tell,2025-10-27 07:47:41.322-05,"Who doesn‚Äôt love to play with something new? At least new to us. The Lance file format has been around since ‚Ä¶ well, I‚Äôm not sure, but the PRs in the GitHub repo start around 2022. Young in digital years. Just a little ‚Äòfella.
There isn‚Äôt really anything to do but jump in the deep end when it comes to file formats. It‚Äôs hard to imagine anything unseating Parquet file, the proverbial Altas carrying Iceberg and Delta Lake on its shoulders. But, parquet files started from nothing too.
I‚Äôm not going to waste my time with nonsensical deeply technical questions about how this Lance file format works.
No one cares.
What do we care about?
What big name Data Engineering frameworks support Lance?
How well does it work with S3?
Is the performance at least on par with Parquet?
So today I hope we can leave with a little bit of knowledge about the Lance file format, its use cases, and if it‚Äôs worth your trouble to play around with it.
Start from the top.
Well, let‚Äôs just start at the beginning and see where the road takes us. I have no set plans for Lance, at this moment I know nothing more than you do about it. Which is I‚Äôve heard the name mentioned a handful of times. That‚Äôs it.
Here we go.
What is Lance?
Something tells me in 2022 they weren‚Äôt lauding it as an AI file format, but who knows.
I‚Äôm just an online grifter with mid engineering levels.
What else do they have to say?
‚Äú
Lance
is designed to be used with images, videos, 3D point clouds, audio and tabular data. It supports any POSIX file systems, and cloud storage like AWS S3 and Google Cloud Storage.‚Äù - docs
Well, maybe I am wrong, appears like on the surface, at least from what they are saying, it is a very ML heavy file format if folks are using it for images, videos, and the like.
The key features list
from the website seems to make it more obvious as well.
They did mention tabular data at the top, but don‚Äôt seem to say anything about it really in the features, seems like the focus on more random access type usage.
Off the bat, it does make you wonder how it holds up against parquet for just Analytics type query support which is probably the 80% usage case in the data world today,
like it or not.
As a side note, of course, it appears 80% of the GitHub of Lance is written in Rust.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Straight to the code.
I mean file format is a file format, but what matters is where the rubber meets the road. A file is only as good as the tools that support it, that‚Äôs just being honest.
In the Data Engineering space we only really are going to use things that have first class support in the frameworks that are running on our Data Platforms ‚Ä¶ otherwise what‚Äôs the point.
So what tools have 90% of the minds, hearts, and pipelines today?
Polars
DuckDB
Arrow-based tools
{python} ‚Üê like it or not (
various ML libraries
)
Let‚Äôs just see what happens,
we will start with vanilla Python and then move onto tools like DuckDB.
So, how do we even go about writing and read Lance files? Let‚Äôs just start with some CSV files in S3, because that is the most classic Engineering dataset on the planet.
Luckily, I keep some
Backblaze hard drive
CSV datasets available in S3 just for moments like these.
Let‚Äôs try reading this data with PyArrow and then writing / converting it into a Lance dataset.
Well, the Python code is simple enough. Simple enough to throw errors.
Sigh. You know what, let‚Äôs just give Lance the job or writing a single CSV file, I don‚Äôt feel like fighting with it this early on. Do you?
Well, a single CSV file gives the same error, simply doesn‚Äôt like the CSV file. Jeez, why‚Äôs everything gotta be so hard?
Let‚Äôs switch to a more simple CSV dataset,
the Divvy Bike Trip dataset
. Just a single file. I have a feeling it‚Äôs just pyarrow not sniffing the files deep enough, but whatever, still annoying.
Well, that worked.
We now have some Lance files in S3. You will not that it appears more like something similar to Delta Lake or Apache Iceberg, the directory we wrote to has folders like versions and transactions. Interesting ‚Ä¶ but not surprising.
The for profit company who controls Lance, what your surprised? This is the new norm. Calls themselves
LanceDB
.
Make you jumpy? Well it should, but that‚Äôs a topic for a different day.
Actually, what we should do now is write that same dataset to Parquet, and then put DuckDB on top of both datasets, and run a few queries, see if Lance vs Parquet makes any difference at all.
Ok, so now the DuckDB part. Let‚Äôs read and do a simple aggregation with DuckDB on top of Lance and Parquet ‚Ä¶ see what happens.
BTW, the single parquet file is 32.7MB in size.
The Lance file is 105.8 MB. That is just out of the box using both these tools.
I would hope that means Lance makes things way faster, because that is some serious size difference. If you‚Äôre storing large datasets, that would add up over time.
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ member_casual ‚îÇ rides  ‚îÇ
‚îÇ    varchar    ‚îÇ int64  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ casual        ‚îÇ 265461 ‚îÇ
‚îÇ member        ‚îÇ 449298 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
I added some timers to the code, and took the average of 3 runs.
Total runtime: 10.43 seconds
Total runtime: 11.48 seconds
Total runtime: 14.41 seconds
Now for parquet. Here is the code.
Here are the results.
Total runtime: 4.70 seconds
Total runtime: 4.83 seconds
Total runtime: 4.16 seconds
Well, now we can safely assume that Lance files are NOT made for analytics ‚Ä¶ the majority of what Data Engineers do. This is clearly a more specialized file for ML workloads ‚Ä¶ maybe I should have listed to their docs.
It‚Äôs extremely slow at supporting analytical workloads. Not really any point trying it on Spark or Polars.
I guess we could try it at random access, just pick a record in the middle and have DuckDB and Lance find it. Run it three times.
Total runtime: 15.80 seconds
Total runtime: 13.23 seconds
Total runtime: 13.21 seconds
Oh boy, let‚Äôs see with Parquet.
Total runtime: 11.51 seconds
Total runtime: 6.60 seconds
Total runtime: 6.13 seconds
Look, I don‚Äôt know what to say, what do I know about LanceDB? Ok, so you just want to blame it on DuckDB or something? Well, I don‚Äôt think so.
You interact with file formats via tools that support it. Duh. It is what it is. If Lance is slower than Parquet or 99% of what most Data Engineers do with data, what‚Äôs the point?
It‚Äôs never going to be a Parquet killer, EVER!
Let‚Äôs try to be nice.
At this point I can probably just say I took the wrong approach with Lance as a file format. Clearly it‚Äôs a very specialized tool that probably does great for ML things like images and videos maybe.
Maybe.
They have nice documentation, they say a lot of nice things. Maybe some are true, maybe some are not.
They do seem to have broad support since tools like Spark, DuckDB, and Polars can mess with the files. I‚Äôm sure that development was pushed by LanceDB the for profit company.
The for profit LanceDB calls Lance ‚Äúmultimodal lakehouse‚Äù whatever that is. They do call out Analytics as something they do.
‚ÄúActing as a vector database, LanceDB natively stores vectors alongside multiple data modalities (text, images, video, audio), serving as a unified data store that eliminates the need for separate databases to manage source data.‚Äù -
LanceDB
Again, this looks just more specialized, this Lance format appears to be made to do non-traditional data for sure.
That‚Äôs ok, now we know right? We learned something.
That‚Äôs how we move forward and keep on learning, we can file Lance in the back of our minds for what it is, and where is most likely shines. Which is not Analytics and normal Data Engineering workloads.
It‚Äôs a specialized format that won‚Äôt shine in that area.
Not a parquet killer, but maybe a great tool to store non-traditional datasets.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
176872884.650gb-of-data-delta-lake-on-s3-polars,650GB of Data (Delta Lake on S3). Polars vs DuckDB vs Daft vs Spark.,cluster fatigue,2025-11-12 10:38:14.594-06,"I recently tried to light the tinder for what I hoped would be a revolt ‚Äî the
Single Node Rebellion ‚Äî but
, of course, it sputtered out immediately. Truth be told,
it was one of the most popular articles I‚Äôve written about in some time
, purely based on the stats.
The fact that I even sold t-shirts
, tells me I have born a few acolytes into this troubled Lake House world.
Without rehashing the entire article, it‚Äôs clear that there is what I would call ‚Äú
cluster fatigue.
‚Äù We all know it, but never talk about it ‚Ä¶ much ‚Ä¶ running SaaS Lake Houses is expensive emotionally and financially. All well and good during the peak Covid days when we had our mini dot-com bubble, but the air has gone out of that one.
Not only is it not cheap to crunch 650 GB of data on a Spark cluster ‚Äîpiling up DBUs, truth be told ‚Äî but it‚Äôs not complicated either; they‚Äôve made it easy to spend money. Especially when you simply don‚Äôt need a cluster anymore for *most datasets and workloads.
Sure, in the days of Pandas when that was our only non-Spark option, we didn‚Äôt have a choice, but DuckDB, Polars, and Daft (
also known as D.P.D.   because why not
) ‚Ä¶
have laid that argument to rest in a shallow grave.
Cluster fatigue is real
D.P.D. can work on LTM (
larger than memory
) datasets
D.P.D. is extremely fast.
Sometimes I feel like I must overcome skepticism with a little bit of show-and-tell,
proof is in the pudding,
as they say. If you want proof, I will provide it.
Look, it ain‚Äôt always easy, but always rewarding.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Choose, we must.
We have two options on the table. Like Neo, you have to choose which pill to take. Ok, maybe you can take both pills, but whatever.
Distributed
Not-Distributed
Our minds have been overrun by so much marketing hype pumping into our brains, we are like Neo stuck in The Matrix. We just need some help to escape.
I‚Äôm going to shove that red pill down your throat.
Open up, buttercup.
Into the breach, my friends, let‚Äôs get to it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Setting up the test.
Ok, so to at least simulate what would be a production-like environment, with data that is still small-ish, but approaching reality, let‚Äôs get our test setup to see if we can choke Polars and DuckDB (
and Daft to keep ‚Äòem honest
), because we know Spark would have zero problems with data this size.
The steps will be simple.
Create a Delta Lake table in S3.
Fill the table with 650GB of data. (I was going to 1TB but got tired of waiting)
On a small but reasonably sized EC2 Instance run ‚Ä¶
DuckDB
Polars
Daft
Compare all these to Spark
Next, we must somehow generate 650GB of data. What we will do is just mock up some data that could be described as social media posts, make a dict with Python, and convert it to a
Daft Dataframe that can be written to a parquet file.
Once we have a Dataframe in Daft, we can just pump it to S3.
Now, we just gotta do this like a million times, waiting for 650GB of data to accumulate.
Basically, at this point, I left my laptop to run all night and went to bed.
To sleep troubled dreams of AWS bills.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Converting to Delta Lake table.
Next, we need to convert these Parquet files into a Delta Lake table. Easy enough on Databricks.
Note: I partitioned the data by year and month. We can see here we have about
650GB of data, excluding the Delta Logs.
Stating the obvious.
The problem is that we are/or will be on a single-node architecture with
only 32GB of memory available for 650GB of data
, so we need
a streaming option
when running queries to see if DuckDB, Polars, and Daft can handle the load.
In the real world of Lake Houses, where we could use either Delta Lake or Iceberg, if we wanted to do this in production, we would want tools that do not OOM and can work on datasets larger than memory.

Again, this is what we are trying to answer, is it even remotely possible?
Apparently, this is not an uncommon use case and a problem that needs to be solved. See here and below for an open Polars issue where
someone has this exact problem and wants an out-of-the-box way to ‚Äústream writes to Iceberg.‚Äù
The point I‚Äôm trying to make is that we need all these new-fangled frameworks, like Polars, DuckDB, etc,
to have out-of-the-box support for reading and writing to Lake House formats in a streaming manner, reducing memory pressure.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
650GB Lake House (Delta) with DuckDB, Polars, and Daft.
Here we have a 32GB, 16 CPU EC2 instance on AWS. This is a fairly normal size and would be considered commodity-sized hardware. Many Spark clusters are composed of these node sizes.
Once the node is running, we will use uv to set up and install the needed tooling.
Not sure if you noticed it above, but I‚Äôm using a fake social media posts dataset. What we need now is a query that reads the entire dataset and performs some work, like aggregation.
That simple query should suffice to push these single-node frameworks to the limit with 650GB and an EC2 with 32GB of RAM available.
They're going to have to eat that whole dataset.
We will also run this on PySpark Databricks Single Node Cluster to get an idea of how these tools each stack against the GOAT.
You can see below that I had to downgrade to an old DBR Version when generating the Delta Table, so no Deletion Vectors would be used. DuckDB is the only one able to handle deletion vectors.
A serious flaw and fragmentation in Polars.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
DuckDB goes first. üê§
We are going to start with that little quacker DuckDB. You know, this tool has grown on me more and more as I've used it. I just had to replace Polars in a production Databricks environment because DuckDB was the only tool that could handle Deletion Vectors.
Everything MotherDuck touches turns to gold.
Code is simple, clean. Can DuckDB crunch 650GB of S3 Lake House data on a 32GB commodity Linux machine and come out the other side with all feathers intact?
Well, I‚Äôll be. It worked.
16 minutes. Heck, it seems like a single node can handle 650GB of data. Didn‚Äôt even play with any settings. Indeed, using vim, I can see the local file was written out with the results.
Heck, nothing for it, onto the next!!! Single Node Rebellion is alive and well.
Polars goes second. üêª‚Äç‚ùÑÔ∏è
Well, here we go. I‚Äôve got a bad taste in my mouth about no Deletion Vector support, which makes Polars useless in a new Lake House environment. Stinkers.
But, I will hold my disdain, and let‚Äôs see our Polars code. Nice and clean, good-looking stuff.
Remember, with Polars, you must use the Lazy API‚Äôs to get the job done ‚Ä¶ aka ‚Ä¶
scan
and
sink
. If you don‚Äôt, she‚Äôll blow a gasket.
There we go, no problems, 12 minutes on the Rust GOAT. Beating out DuckDB by a few minutes. But we are all friends here, we are just proving a point, and it‚Äôs going well.
These single-node engines are chomping the 650GBs no problem.
(I checked the results of the local file, all was well)
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Daft goes third.
One of my personal favorites is Daft, Rust-based, and it screams whenever you put it to work. Smooth and fun to use. I was recently working on some Iceberg stuff, and Daft was about the only thing that worked.
What a beaut.
Not much difference between Daft and Polars code-wise, both Rust-based. Egads, that was slow. Done put a dagger in my heart.
50 minutes is better than nothing
, I guess. I‚Äôm no expert in Daft; I just did what I did ‚Äî probably something wrong.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
PySpark for last. üî•
Ok, so this one is really our anchor point: a single-node 32GB with 4 CPUs, which matches up to our EC2 pretty well, or close enough. Aren‚Äôt you curious to see how it stacks up to the single-node buggers?
I don‚Äôt really care if it‚Äôs that much faster; one would expect it to be. The main point is: could we migrate many expensive DBUs and other distributed compute engines to the Single Node Rebellion?
Dang, over an hour.
Of course,
PySpark is most apt to have troubles without tuning
, but we will let the ragers rage. I mean, we all know that the
spark.conf.set(‚Äùspark.sql.shuffle.partitions‚Äù, ‚Äú16‚Äù)
should be that instead of 200. But whatever.
We are simply making a point that single-node glory can do the job just as well.
What‚Äôs the takeaway?
Of course, this was not some scientific TCP benchmark; it‚Äôs not like those are fair either half the time. We were not so much concerned about who‚Äôs the fastest as much as
whether these single-node tools can handle large Lake House datasets on small memory footprints without giving up the ghost.
That we proved.
Single-node frameworks can handle large datasets
Single-node frameworks can integrate into the Lake House
They can give reasonable runtimes on cheap hardware
The code is easy and uncomplicated
Truly, we have not been thinking outside the box with the Modern Lake House architecture. Just because Pandas failed us doesn‚Äôt mean distributed computing is our only option.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
176665833.aws-outage-pipeline-problems-what,AWS Outage. Pipeline Problems? What next?,do nothing?,2025-10-20 19:09:58.081-05,"Phew, what a fun Monday to come back to after a weekend on the open prairie full of peace and tranquility. To be honest, when the internet goes down, my favorite part is the memes. This used to be something in the years past that seemed to be infrequent, now, maybe it‚Äôs just a feeling, it seems to be a normal part of tech life.
There is always a bright side on a dark day.
So, pipeline not running, SaaS down? Chaos unfolds on a Monday morning and all the sudden the C-suite becomes familiar with non-existent (
or non-working
) Disaster Recovery (
DR
) plans.
That does bring up conversations that don‚Äôt happen very often at most orgs. Disaster Recovery (
DR
) just isn‚Äôt that sexy until things blow up.
Who wants to invest the time, resources, blood, sweat, and tears into DR when things are just pumping along as expected.
Out of sight out of mind.
I actually searched my archive of blog posts on this Substack as well as
www.confessionsofadataguy.com
(going back to 2018) and found big
ZERO
. I should put myself in timeout for ignoring the topic.
Disaster Recovery is an interesting topic ‚Ä¶ especially in the context of Data Teams, mid-sized to small especially, and I suppose at the lazy large ones too.
Disaster Recovery for Data Teams
Look, I know you want some smoke and mirrors Disaster Recovery plan that is ‚Ä¶
free
requires no effort
always works
never needs to be tested
is easy to use and deploy
Give me a break and grow up. Ain‚Äôt going to happen Sunny Jim. It‚Äôs one thing to have multi-az setups for all your tools, finely tuned, tested and true, and when !@#$ hits the fan it doesn‚Äôt touch you.
But let‚Äôs be honest, MOST orgs simply don‚Äôt have the appetite for that stuff.
I don't think that the answer to DR on Data Teams and Platforms is that complicated of a question, it's just uncomfortable to the business.
Your Data Team leader should sit down with the ‚ÄúBusiness‚Äù and ask the following questions.
What is the SLA (service level agreement) for the data products we produce?
What data products are customer faxing
Are you willing and able to foot the bill for full blown DR?
Are you willing to prioritize the time to build and test DR?
What are the downtimes (in hours or days) we can eat before people start screaming?
It really isn't any harder than that. Yet, few people have these conversations out loud.
Good disaster recovery is not easy to build and put in place, no matter what the SaaS hobbits tell you ‚Ä¶ or that Platform engineer lurking in the basement.
Ask the question first. How often?
Truth be told, for many small and medium sized data teams, they neither the bandwidth, desire, or expertise required the build out a ‚Äúreal‚Äù DR system that works.
I think this is OK, and normal for the most part.
Let say, like pretty much everyone, your running the majority of your Data Platform on AWS.
How often do they go *poof? What does the DATA say??!!
So, if the internet is correct about these AWS apocalypses that happen periodically, they are very, very rare.
Even rarer would be one lasting for more than 24 hours. Look me in the eye. Do it. Look deep into my Midwest hazel eyes. You wouldn't lie to me would you?
How many Data orgs is this totally acceptable to carry the risk of a 9 hour outage once a year?
Yeah. Pretty much all of them.
If it wasn't, they would have already built and put DR into place. That's the truth, like it or not.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Finding middle ground.
Look, I don't want to get in your way of building a robust DR plan they can be executed to bring production back the moment you smell smoke from AWS. Go for it, or build a middle ground.
Truth is that many Data Platforms are not even in a state which COULD be DR‚Äôd even if they wanted too.
Is your Data Platform hobbled together with twine?
Zero IaC (infrastructure as code)?
No CI/CD or deployments they don't work?
Hard coded stuff everywhere in code base?
Than pack up and go home üè°. You simply can't go from zero to DR heaven overnight simply because AWS pooped üí©.
But, you have Cloud Formation Stacks stored away somewhere? Got some blasted Terraform in a repo spelling things out? Codebase that is tight and tested, all packaged up in git with deployment at the drop of a hat?
Then you are already on the cusp or DR nirvana.
Most of the work in putting a DR plan in place that works, besides testing, is simply bringing the stack up to date with best practices you SHOULD have been doing already.
If you have IaC than it isn't rocket üöÄ science to simply point that into a new region.
Got money üí∞ and time ‚è∞ coming out your ears? That just build a clone of your stack in another AZ and let it sit ü™ë. Poke with a stick once a year to make sure it's alive. Doubt that happens much.
Being honest here, based on the data, if you can't recover and run your DR plan within say 3 hours or less, most likely the systems will be back online before you are finished cobbling some half baked solution in another spot manually.
I guess what I'm trying to say is build a Data Platform that is honed to the point where it COULD be DR‚Äôd, which your should be doing anyway, then go ask the business the right questions and figure something out if you should and can do it.
Aka. Do nothing.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
176065322.fivetran-dbt-labs-merger-what-does,Fivetran + dbt Labs merger. What does it mean?,anything at all?,2025-10-13 17:27:24.569-05,"Nothing like a little excitement in the data community to get that talking heads talking and the rumor mills churning. Hey, I like a little excitement on a Monday morning after a long weekend.
That came in the form of
Fivetran
and
dbt Labs
combining forces. Guess what, I will do you the favor of giving the WHY and WHY CARE right out the gate, so you don‚Äôt have to read the rest.
Data vendors are reading the writing on the wall; they are learning the lesson that Databricks is spanking them and eating the world with its
one-stop-shop Data Platform.
This is what you call scrambling to catch up.
What the announcement says
Fivetran and dbt Labs are combining via an
all-stock merger
, forming a unified company with projected annual recurring revenue of ~$600M.
Business Wire
Under the deal,
George Fraser
(
currently CEO at Fivetran
) will lead the merged entity, and
Tristan Handy
(
dbt Labs CEO
) will become co-founder & President.
Business Wire
The combined company is positioning itself as an
‚Äúopen data infrastructure‚Äù
platform ‚Äî combining data movement (ingestion), transformation, metadata, and activation ‚Äî while preserving flexibility around compute, catalog, BI, and AI layers.
Business Wire
Importantly, they commit to maintaining
dbt Core
as open source under its current license and to continuing support for its community.
Business Wire
Until regulatory and other closing conditions are met, Fivetran and dbt Labs will continue operating independently.
Business Wire
TDLR:
The data infrastructure layer is maturing and consolidating, especially as AI workloads impose stricter demands (latency, scale, data quality). This merger could accelerate the shift toward more unified, opinionated ‚Äúdata platforms,‚Äù while also reviving debates about openness, flexibility, and vendor choice.
You can see the mad
r/dataengineering Reddit mob
, for once, can sniff out why this is happening.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What else is there to say?
I mean, for those of us without a horse in the race, it does signify an interesting shift we‚Äôve been seeing in the open-source space ‚Ä¶ namely, the line between open-source and vendor SaaS has been blurring.
There is nothing open-source about Fivetran. There was/is a large open-source community around dbt, but they had already started complaining before this merger even happened.
If you doubt me, just Google around about it.
The anger is real when
someone‚Äôs open source baby gets murdered by a SaaS vendor.
One has to imagine, good or bad, that this merger is just another step in that direction. The blurring of lines when it comes to open-source tooling and vendor-provided platforms.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Any good news?
Well, that depends on your viewpoint. I, for one, am an avid Databricks user and self-proclaimed evangelist. Open source projects come and go, fall in and out of popularity; what‚Äôs a guy to do?
The best you can do is find a SaaS vendor, like Databricks, where you can pretend you‚Äôre not vendor locked in (
we all know we all are anyway
), and just sleep soundly telling ourselves it‚Äôs ok because we use Iceberg, Delta Lake, and Spark. All of which are ‚Äúopen source.‚Äù
Maybe this Fivetran + dbt Labs thing will turn into the third horseman of the apocalypse. Competition is never bad in the SaaS field; it drives innovation and benefits end users.
The truth is, we live in the age of CTOs that are seeking REAL payback from expensive Data Platforms. They want to pay one cloud bill to a vendor that gives you an end-to-end Data Experience.
This merger is just another outworking of that; the market listened and watched ‚Ä¶ and did what the market wanted. Data Platforms are shifting and changing. We don‚Äôt want to go to different places for ‚Ä¶
data storage and modeling
data compute
governance and permissions
analytics and dashboards
etc, etc, etc
The truth is that a single source of Data Platform truth provided by Databricks/Snowflake, and the rest ‚Ä¶
makes it easier to provide business value and outcomes quickly and seamlessly.
Sure, I‚Äôm a good basement engineer like you; we all love to simply write code and build things because we can ‚Ä¶ not because we should, but because we can.
Single entry points for a Data Platform are the future.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What does the future hold? I don‚Äôt know. The SaaS demi-gods will continue to battle it out; sometimes, the open source community will take a few battle scars.
You'd probably be better off ignoring the happenings in the greater SaaS world for the most part. Focus on building reliable, scalable, and cost-effective Data Platforms that deliver the value the business is seeking."
175990081.drainage-the-missing-piece-in-your,Drainage: The Missing Piece in Your Lake House Health Strategy,How a Rust-powered Python library is revolutionizing data lake monitoring and optimization,2025-10-16 07:15:02.473-05,"The Hidden Cost of Unhealthy Data Lakes
Picture this:
You‚Äôve built a beautiful data lake with
Delta Lake
and
Apache Iceberg tables
. Your data engineers are happy, your analysts are getting insights, and everything seems to be running smoothly.
But beneath the surface, your data lake is quietly hemorrhaging money and performance.
Unreferenced files are accumulating like digital dust. Small files are creating query performance bottlenecks. Partition strategies are creating data skew nightmares. And you have no idea any of this is happening until your monthly AWS bill arrives with a nasty surprise.
This is the reality for most organizations running data lakes today.
The tools to monitor and optimize data lake health simply didn‚Äôt exist
‚Äî
until now.
Enter
Drainage
: The Data Lake Health Analyzer
Drainage is a high-performance Rust library with Python bindings that brings comprehensive health monitoring to your data lakes. Built for the modern data stack, it supports both
Delta Lake and Apache Iceberg tables
stored in S3,
providing the visibility and optimization recommendations you need to keep your data lake healthy and cost-effective.
Why Rust + Python?
The choice of Rust for the core engine wasn‚Äôt arbitrary. Data lake analysis involves:
Massive I/O operations
(scanning thousands of S3 objects)
Complex metadata parsing
(Delta transaction logs, Iceberg manifests)
Heavy computational workloads
(calculating skew, analyzing file distributions)
Rust‚Äôs performance characteristics make it ideal for these workloads, while
PyO3 provides seamless Python integration
, ensuring data engineers can use it with their existing tooling.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The Comprehensive Health Check Your Data Lake Needs
To be honest, in the age of the Lake House we find ourselves in, everyone is obsessed with compute ‚Ä¶ Spark vs DuckDB vs Polars ‚Ä¶ and all the rest.
Everyone under the sun, from AWS (S3 Tables) to
Cloudflare is releasing managed versions of Apache Iceberg
, then of course you have the GOAT of Delta Lake pushed by Databricks. That‚Äôs all everyone talks about these days.
Yet, people obsess over the queries being run on PySpark or DuckDB, and how to make them scream and run like lightning. The Lake House tools, like Iceberg and Delta Lake, came on so fast and furious that no one has taken the time to answer the question ‚Ä¶
What makes a healthy Lake House?
Strange.
It‚Äôs a matter of life and death in the Lake House, built on files bumbling around on cloud storage. One mistake unseen can drag performance down the drain. W
here are all the tools to monitor and detect these problems??
Here is what you can expect from the Python tool drainage.
pip install drainage
1. File Health Analysis
Orphaned Files
:
Identifies files that exist in S3 but aren‚Äôt referenced in table metadata
File Size Distribution
:
Categorizes files as small (<16MB), medium (16-128MB), large (128MB-1GB), or very large (>1GB)
Storage Optimization
:
Recommends compaction strategies to improve query performance
2. Partition & Clustering Intelligence
Partition Analysis
:
Evaluates partition count, file distribution, and data balance
Clustering Support
:
Analyzes Delta Lake liquid clustering and Iceberg clustering strategies
Data Skew Detection
:
Identifies uneven data distribution that kills query performance
3. Advanced Metadata Monitoring
Schema Evolution Tracking
:
Monitors schema changes and identifies breaking changes
Deletion Vector Analysis
:
Tracks deletion vectors and their performance impact
Time Travel Storage Costs
:
Analyzes historical snapshot storage costs
Table Constraints
:
Evaluates data quality through constraint analysis
4. Performance Optimization
File Compaction Opportunities
:
Identifies when to run OPTIMIZE or rewrite_data_files
Z-Ordering Recommendations
:
Suggests optimal column ordering for query performance
Snapshot Retention
:
Recommends retention policies to balance performance and storage costs
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The Health Score: Your Data Lake‚Äôs Vital Signs
Drainage calculates a comprehensive health score (0.0 to 1.0) based on multiple factors:
Unreferenced Files
(-30%): Clean up orphaned data
Small Files
(-20%): Optimize file sizes for better performance
Data Skew
(-15-25%): Balance data distribution
Schema Instability
(-20%): Minimize breaking changes
Storage Costs
(-10%): Optimize time travel retention
Data Quality
(-15%): Improve constraint coverage
Real-World Impact: Before and After
The Problem
A typical enterprise data lake might have:
10,000+ files across hundreds of tables
15% of files are unreferenced (wasting storage)
30% of files are too small (hurting query performance)
Significant data skew in 40% of partitions
No visibility into schema evolution impact
The Solution
With Drainage, you get:
Immediate visibility into all health issues
Actionable recommendations for each problem
Quantified impact of each optimization
Automated monitoring across your entire data lake
Working on Databricks
The Business Case: ROI of Data Lake Health
Cost Savings
Storage Optimization: 15-30% reduction in S3 costs through orphaned file cleanup

Query Performance: 20-50% faster queries through file size optimization

Compute Savings: Reduced query costs through better data organization
Operational Efficiency
Proactive Monitoring: Catch issues before they impact users

Automated Recommendations: Reduce manual analysis time by 80%

Data Quality: Improve data reliability through constraint analysis
Developer Productivity
Clear Visibility: Understand table health at a glance

Actionable Insights: Know exactly what to optimize and when

Performance Predictions: Anticipate issues before they become problems
============================================================
Table Health Report: s3://my-bucket/my-delta-table
Type: delta
Analysis Time: 2025-01-27T10:30:00Z
============================================================

üü¢ Overall Health Score: 85.2%

üìä Key Metrics:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Total Files:         1,234
  Total Size:          2.45 GB
  Average File Size:   2.03 MB
  Partition Count:     12

üì¶ File Size Distribution:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Small (<16MB):         45 files ( 3.6%)
  Medium (16-128MB):   1,156 files (93.7%)
  Large (128MB-1GB):      33 files ( 2.7%)
  Very Large (>1GB):       0 files ( 0.0%)

üéØ Clustering Information:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Clustering Columns:  department, age
  Cluster Count:       12
  Avg Files/Cluster:   102.83
  Avg Cluster Size:    204.17 MB

üìä Data Skew Analysis:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Partition Skew Score: 0.23 (0=perfect, 1=highly skewed)
  File Size Skew:       0.15 (0=perfect, 1=highly skewed)
  Largest Partition:    245.67 MB
  Smallest Partition:   12.34 MB
  Avg Partition Size:   89.45 MB

üìã Metadata Health:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Metadata Files:       15
  Metadata Size:        2.34 MB
  Avg Metadata File:    0.16 MB

üì∏ Snapshot Health:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Snapshot Count:       15
  Retention Risk:       20.0%

üóëÔ∏è  Deletion Vector Analysis:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Deletion Vectors:     3
  Total DV Size:        1.2 MB
  Deleted Rows:         1,456
  Oldest DV Age:        5.2 days
  Impact Score:         0.15 (0=no impact, 1=high impact)

üìã Schema Evolution Analysis:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Total Changes:         8
  Breaking Changes:      1
  Non-Breaking Changes:  7
  Stability Score:       0.85 (0=unstable, 1=very stable)
  Days Since Last:       12.3 days
  Change Frequency:      0.15 changes/day
  Current Version:       8

‚è∞ Time Travel Analysis:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Total Snapshots:       45
  Oldest Snapshot:       15.2 days
  Newest Snapshot:       0.1 days
  Historical Size:       1.2 GB
  Storage Cost Impact:   0.25 (0=low cost, 1=high cost)
  Retention Efficiency:  0.85 (0=inefficient, 1=very efficient)
  Recommended Retention: 30 days

üîí Table Constraints Analysis:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Total Constraints:     12
  Check Constraints:     3
  NOT NULL Constraints:  8
  Unique Constraints:    1
  Foreign Key Constraints: 0
  Violation Risk:        0.15 (0=low risk, 1=high risk)
  Data Quality Score:    0.92 (0=poor quality, 1=excellent quality)
  Constraint Coverage:   0.75 (0=no coverage, 1=full coverage)

üì¶ File Compaction Analysis:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Compaction Opportunity: 0.85 (0=no opportunity, 1=high opportunity)
  Small Files Count:     23
  Small Files Size:      45.2 MB
  Potential Compaction:  23 files
  Estimated Savings:     12.8 MB
  Recommended Target:    128 MB
  Compaction Priority:   HIGH
  Z-Order Opportunity:   Yes
  Z-Order Columns:       department, age, created_date

‚ö†Ô∏è  Unreferenced Files:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Count:  5
  Wasted: 12.3 MB

  These files exist in S3 but are not referenced in the
  Delta transaction log. Consider cleaning them up.

üí° Recommendations:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1. Found 5 unreferenced files (12.3 MB). Consider cleaning up orphaned data files.
  2. High percentage of small files detected. Consider compacting to improve query performance.
  3. Old deletion vectors detected. Consider running VACUUM to clean up deletion vectors older than 30 days.
  4. Recent schema changes detected. Monitor query performance for potential issues.
  5. High file compaction opportunity detected. Consider running OPTIMIZE to improve performance.
  6. Z-ordering opportunity detected. Consider running OPTIMIZE ZORDER BY (department, age, created_date) to improve query performance.
  7. Significant compaction savings available: 12.8 MB. Consider running OPTIMIZE.

============================================================
The Future of Data Lake Management
Drainage represents a fundamental shift in how we think about data lake management. Instead of reactive troubleshooting, we now have proactive health monitoring. Instead of guesswork, we have data-driven optimization.
What‚Äôs Next?
The roadmap includes:
Automated Repair Actions
: Not just recommendations, but automated fixes
Query Performance Prediction
: Anticipate performance issues before they occur
Web Dashboard
: Visual monitoring across your entire data lake
Cloud Integration
: Native integration with CloudWatch, Datadog, and other monitoring tools
Why This Matters for Your Organization
In the age of data-driven decision making, your data lake is the foundation of your analytics strategy. An unhealthy data lake means:
Slower insights
for your business users
Higher costs
for storage and compute
Frustrated data engineers
spending time on maintenance instead of innovation
Missed opportunities
due to poor data quality
Drainage gives you the tools to build and maintain a world-class data lake that scales with your business needs.
Getting Started Today
Ready to transform your data lake health? Drainage is available now:
pip install drainage
Whether you‚Äôre running Delta Lake, Apache Iceberg, or both, Drainage provides the comprehensive health monitoring your data lake deserves.
The question isn‚Äôt whether you can afford to monitor your data lake health‚Äîit‚Äôs whether you can afford not to.
Drainage is
open source and available on GitHub and PyPI
. Start monitoring your data lake health today.
Please help and consider testing and contributing to
drainage
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
175555899.run-llama-31-8b-locally-with-langchain,Run Llama 3.1 8B Locally with LangChain and SQLite Memory,Learn AI by doing,2025-10-13 07:41:57.623-05,"I tell you what, times have changed since I first taught you bunch of hobbits
how to fine-tune an LLM
. Heck, it was only a year ago or something. We still have developers in two camps:
those who hate the AI and those who love it.
Look, you can do what you want; I don‚Äôt care.
What I want to teach you today is something a little different. I want to explore, at a high level some local LLMs. By building a thing with our own wizzened fingers, showing different moving pieces and parts to an AI Data Pipeline.
We don‚Äôt have to be AI or LLM experts to do this sort of thing. The tooling around LLMs is changing and becoming easier to use. While there are numerous complex topics surrounding AI/LLM, for Data Engineers and practitioners, we can learn a great deal about ‚Ä¶
tooling
frameworks
databases
models
data
‚Ä¶ and how they all work together to provide a reasonable experience when it comes to an LLM.
What makes the LLM/AI world special is that a good system contains a number of technical things that span both the frontend and backend, including Data Platforms (
if you‚Äôre talking about production stuff
).
Most AI tools are only as useful and ‚Äúgood‚Äù as the data that is available to them. This should come as no surprise at all. Data is at the core of these systems.
The ‚Äúinterface‚Äù or ‚Äúchat‚Äù between the user and the model + data is a side note. Leave that to the birds.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Understanding the pieces and parts of LLMs by building.
So, crack those fingers, take a seat, and get ready to be not amazed at all. That‚Äôs kinda the point, though, isn‚Äôt it? We like to peel back the layers and find out that there is no black magic behind it all.
Concepts. Let‚Äôs start with concepts first, then move on to the code.
Ok, so if we were tasked with building a simple LLM Chatbot as a POC (proof of concept), that we, you, me, ‚Ä¶ us ‚Ä¶ could run on our ‚Äúinfrastructure‚Äù and eventually make it do something interesting,
what would you do??
Also, keep your comments to yourself. If you want to go build LLMs for Meta, you probably shouldn‚Äôt be wasting your time reading this crap. Duh.
I mean, let‚Äôs be real, that‚Äôs all that there is to it for the most part, at the 10,000-foot level. Sure, we can tack on some RAG later, add some Agents to the mix.
That‚Äôs just another LLM box, you ninny.
Yes, you can build a ChatBot that runs on your laptop or a small, commodity Linux box in the cloud with 10GB of RAM,
so you don‚Äôt have to pay OpenAI for every token.
These things are possible.
And the best part is that we can learn while we do it. So what do you need??
model
some compute (
laptop or otherwise
)
database
framework
I don‚Äôt see any necromancers or wizards in that list, do you? No black, or good magic for that matter, to be found.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Don‚Äôt miss the Data Engineering lessons in here.
Again, I want to take a moment to step back and think about the BIG picture at a high level. Especially when we reach the point of incorporating RAG or additional data into this ‚ÄúLLM workflow.‚Äù
In the back of your mind, you should be thinking of these boxes as pieces and parts of a larger Platform that needs to be built, managed, and monitored.
This is no different from the data platform you‚Äôre working on today.
We would need somewhere to
store
and
serve
a model or endpoint
Some
compute
to hold the model in memory and run everything else
A
database
somewhere
A good
LLM framework
with wide usage and an active community for support
Clean and massaged
data
that can make our LLM ‚Äúbetter‚Äù or ‚Äúmore useful‚Äù
HEY! Wait a minute ‚Ä¶ this sounds oddly familiar.
Oh yeah, it‚Äôs what we do every day.
Enough chit chat, let‚Äôs build.
We are going to start from scratch. We will first need a model/framework/serving.
Ollama
is one of the best for local work, making it easy to run and obtain an LLM model.
Here we go, hold onto your panties.
Ollama
, here we go. (
install and pull llama3
)
curl -fsSL https://ollama.com/install.sh | sh
# OR
HOMEBREW_NO_AUTO_UPDATE=1 brew install ollama
>> brew services start ollama
>> ollama pull llama3.1:8b
Next.
>> uv init llmsaremidatbest
>> cd llmsaremidatbest
>> uv add langchain langchain-core langchain-community sqlalchemy ollama
I mean, to be honest, this next part is the most boring part.
LangChain
makes it super easy, combined with ollama, to build and host our model ‚Ä¶
it‚Äôs drop-dead simple.
Now, we can just twin the code together.
I mean,
if you look at the code
(Github), 90% of it is crap related to just building nuanced ways to deal with user input, building the chat, etc.
It‚Äôs for the birds.
We can run it via the command line like this.
uv run hello.py --session pickles
Chatting with Llama 3.1 (8B) ‚Äî session: ‚Äòpickles‚Äô
Type ‚Äòexit‚Äô to quit.

You> Do you think that data modeling is dead?

  lambda session_id: get_history(session_id),

Assistant> No, I don‚Äôt think data modeling is dead. While data warehousing and traditional ETL (Extract Transform Load) approaches have given way to more agile and flexible architectures like cloud-native data lakes and real-time processing pipelines, the principles of data modeling remain relevant.

Data modeling helps ensure that data is properly organized, normalized, and consistent across different systems. It also facilitates communication between stakeholders and developers, reduces data quality issues, and enables easier maintenance and evolution of data systems.

In fact, with the rise of modern data architectures, data modeling has become more crucial than ever to ensure proper data governance, compliance, and scalability.

You>
Well, I mean, it‚Äôs something.
Of course, this is a toy, running locally, but we can draw some lines and thoughts when we start thinking about ‚Äú
how to bring something to production.
‚Äù
Things like maybe ‚Ä¶
putting the Chat behind a REST API
wrapping up dependencies into a container
how we would deploy changes
how to test and think about performance
needing a more stable and production-ready ‚ÄúPostgres‚Äù database
blah, blah, blah
It helps us start to paint the picture of what we need to consider as we build, perhaps an internal knowledge base, Chat, or something similar. Just saying.
You can check out this
repo on GitHub and play with it.
AI and LLMs are here to stay for Data Professionals, in one way or another, for the foreseeable future. They will not be swept under the rug. Who knows in what form, but here to stay.
Sure, you don‚Äôt have to build models yourself or understand everything. Most Data Engineers will simply be consumers of Cursor or another tool that spews code for them.
You can easily take it one step further and actually play around with these LLMs locally.
It might not seem like much, but it really does start to teach you what is going on behind the curtain of these AI tools. You start to think about sending requests to an LLM behind an endpoint, and you think about how they must be serving requests to that API.
Adding RAG to the LLM.
Another ‚Äúthing‚Äù you will hear a lot about when it comes to LLMs is making them smarter with context, many times in the form of proprietary documents and data.
One can imagine why companies might want to build Chats that have access to their own internal knowledge base, FAQs, etc.
Let‚Äôs improve our local LLM with a RAG setup, still using simple technology like
FAISS
.
Again, focus on the big picture of what we do to add this extra layer to our LLM system. We are doing it on a small scale, but you can imagine what would be required to do this at scale.
Of course, we would need to add our new tool.
uv add faiss-cpu langchain_ollama
Next, we would need a script that could ingest our ‚Äúdocuments.‚Äù
In our case, or my case, it could be all my previously written blog posts as TXT files, so I could ask my LLM if I‚Äôve written about a topic before, and what I said about XYZ.
Could be useful.
For example, I will drop in one of my blog posts about ‚ÄúHow to tune Spark Shuffle Partitions.‚Äù Let‚Äôs ingest it, than update our script to use the RAG approach, see what happens.
>> uv run build_index_faiss.py
Indexed 16 chunks ‚Üí /Users/danielbeach/code/llmsaremidatbest/faiss_index
Now, we can update our main script to use RAG approach. Basically we take our script from before and replace the
build_chain()
method with the below.
Ok, so let‚Äôs start a chat session and check with our little Bot and see if it knows if I wrote anything about Spark shuffle. Little blighter.
llmsaremidatbest) danielbeach@Daniels-MacBook-Pro-2 llmsaremidatbest % uv run hello.py

You> Did I write about How to tune Spark Shuffle Partitions.?

Assistant> You mentioned tuning Spark Shuffle Partitions as a relevant context. However, the text you provided doesn‚Äôt directly answer ‚ÄúHow to tune Spark Shuffle Partitions.‚Äù It seems more like an introduction to the topic and provides some background information on Spark tasks, partitioning, and shuffling.

If you‚Äôd like, I can provide guidance on how to actually tune Spark Shuffle Partitions based on best practices and configuration options.
Little bugger thinks I wrote a bad blog post on the topic. Stinker. Stupid AI. I‚Äôm going to delete em‚Äô and show em‚Äô who‚Äôs boss.
Big picture again.
Well, if we think about the big picture again, we can learn some stuff at a high level. If we want context for RAG or other applications, we have to ‚Ä¶
reterive it
ingest it
store it
make it available
update our and use new framework(s)
Anywho, you get the picture.
There is a lot to learn when piddling around with LLMs and data on your local machine. It opens your eyes to a host of new tools, tech, and framework used to bring everything to life.
Aways learning, always pushing forward!
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
175167628.the-single-node-rebellion,The Single Node Rebellion,a tale of revolt,2025-10-07 11:00:49.628-05,"The real question is, do you think Spark is the bloody British, the AWS EC2 instance is Boston, and DuckDB + Polars are the John Adams and George Washington, bent on freedom or death? You paid attention in history class, didn‚Äôt you?
I can see it.
The natural evolution of things, the rise and fall of nations, and the evolution of technologies. You will never get me to say that Spark is going anywhere, anytime soon. It‚Äôs not.
The Lake House, built on storage formats like Delta Lake and Iceberg, is the new Oracle and SQL Server; decades will pass before Spark and Lake House are gone.
But all rebellions start at the zenith of the great powers; overlooked users with less than a TB or two of data get upset with their cloud bills.
A tale as old as time.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Big, or not so big.
It‚Äôs something everyone knows but no one talks about. There are actually
not very many organizations that have and work on ‚ÄúBig Data.‚Äù
Read the DuckDB summary of the Redshift files, an interesting look into the size of datasets, and better yet, the size of data queried in individual pipelines.
The truth is, ‚ÄúBig Data‚Äù is in the eye of the beholder. We all think our data is big when, indeed, it is not. Yes, there is Big Data floating around out there, and some big queries need to crunch years‚Äô worth of data.
There are Google, Meta, and other out-of-the-ordinary datasets. But, technology has evolved, how we store and process data has changed too.
The days of needing to write some PySpark every time you need to run a pipeline are over; that truth simply hasn‚Äôt trickled down to the masses just yet, though. There are also some hurdles left to jump over for the Single Node Rebellion that are solvable but formidable.
Back in the day.
This has remained largely unchanged from the early days to the present. Proof can be seen as far back as
2015 in this study on GitHub.
Even in the MDS age when all you hear about is data growing exponentially, most datasets that exist, let alone queries on,
are not that big.
This ‚Äúolder‚Äù study
, along with the Redshift files, proves what we all already know and is whispered under the old oak tree at midnight for fear of being tarred and feathered ‚Ä¶
whisper
‚Ä¶
most data and queries work on small datasets.
Hence, the rise of Polars, DuckDB, Daft, and the like.
Why the backlash against clusters, cost only?
Maybe, and maybe not. Times and budgets have indeed tightened up since the days of Covid, when the tech money ran hot and strong. Wasn‚Äôt anybody complaining or asking about Cluster costs back then? Today is a little different.
Cloud computing has never been cheap,
at least not when you're buying from AWS.
The truth is, more memory equals more money. Remember, that doesn‚Äôt even include the cost
Databricks would layer on top of your compute, for example.
You start adding that stuff together, and things get out of hand really quickly.
Besides, how many Spark clusters do you know of that run a single node?
Sorta defeats the purpose of distributed compute.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
There is some sort of complexity that comes along with most distributed compute and clusters that come from places like Databricks. Yes, I know that has made it pretty easy, considering them
against something like that God-awful EMR.
However, the
rise of the Serverless approach
has attempted to sidestep the complexity problem.
Of course, this comes at a price, a literally big price. Like, I mean, expensive.
If you‚Äôre interested in the nuances of Serverless, in a Databricks context, when and where to use it, check out J
osue Bogran on LinkedIn
.
What does this all mean? Why did the ‚ÄúBig Saas Vendors‚Äù push so hard towards serverless, and why did the popularity of DuckDB and Polars explode so much?
Is it just a classic case of doing more with less? Complexity? A return to the draconian ways of the SQL Server past, where every GB of RAM was cared for and watched over like the Ring of Power by Golum?
There must be a business payback in some form for all this burned computing cost. New products, better insights, the ability to be more ‚Äúdata driven,‚Äù something.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Enter the world of ‚Äúfast‚Äù and ‚Äústreaming‚Äù computation.
My guess is that it was the perfect storm of data and global economics coming together at the right time. The renewed focus on actual compute and SaaS vendor spend, the need to reduce costs, and save money. The ‚Äúdiscovery‚Äù of fast (
Rust and C-based
) underpinnings that can drive (Python) top-level frameworks. Continued ability to reduce memory usage by streaming computations on OOM datasets.
Costs storm-crows coming home to roost.
Advanced low-level-ish fast-ish frameworks wrapped in Python
Working on larger-than-memory datasets is becoming mainstream.
Being reminded that your data isn‚Äôt that big.
To prove your fealty to the
Single Node Rebellion
, you can now visit my
new Merch page
for this Substack and purchase the shirt. Hey, I can be a grifter too. Who do you think is paying for all my Databricks credits??!! You are.
The lazy computation that saved our weary souls from Pandas and Spark, both. Hard to understand the importance.
Back in the day, we either Pandas OOM it, or Spark destroyed it and us. The truth is somewhere in the middle; both sides of the Single Node Rebellion have a point.
Databricks, Snowflake, and the like offer more than just the crunch of data bewteen the teeth of bits and bytes. They offer an end-to-end Data Platform that provides EVERYTHING needed to run a Lake House at scale.
You can‚Äôt simply turn a blind eye to what these platforms have done besides charging a lot of money. They have democratized data in the whole business.
AI, ML, Governance, Alerting, Data Storage, Data Compute, Dashboards, the list goes on.
Compute are only how they make money, not how they get their hooks in you.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Let‚Äôs prove a point.
The point is not whether you can do it, that is, join the Single Node Rebellion, but do you have the guts to do it in production on your data stack? It‚Äôs easy.
Let‚Äôs prove the point by using an 8GB
Linode instance
‚Ä¶
To process 50GB of CSV data.
In the bygone years, Pandas would have puked on something like this, and everyone reached for Spark or something else.
We can reach for Polars ‚Ä¶
We old dogs need to learn new tricks. Instead of reaching for a heavy hand with a Spark Cluster, we need to turn to commodity hardware on Linux with limited memory.
I mean, it takes less than 2 minutes to do such work.
root@localhost:~# python3 hello.py 
/root/hello.py:28: UserWarning: ‚Äò(default_)region‚Äô not set; polars will try to get it from bucket

Set the region manually to silence this warning.
  agg_lf.sink_parquet(
Sank Parquet to s3://confessions-of-a-data-guy/analytics/ride_counts_by_day.parquet
0:01:58.106536
The code is concise, the cost is minimal, and the runtimes are brief. Why aren‚Äôt more people doing this sort of thing?
I do not know.
In the past, it was more about infrastructure,
but now that Polars Cloud is here
‚Ä¶
You can use tools like Polars and DuckDB to handle Iceberg and Delta Lake, so the Lake House is also an option.
I do think that one of the biggest ‚Äúblockers‚Äù that keeps people from simply replacing a particular Databricks, Snowflake, or whatever pipeline with Polars or DuckDB ‚Ä¶ is infrastructure.
WHERE
are we going to run this compute? This isn‚Äôt that easy.
If you think about it, this is what the Databricks and Snowflake SaaS vendors provided, and this is why they are popular. It‚Äôs not just Spark and compute, it‚Äôs the ease of writing code and running the compute on the same platform without thinking about it.
It‚Äôs taken the DuckDBs and Polars demi-gods long enough to figure out people
want managed platforms and an end-to-end solution
. It takes time for things to catch up.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
There is complexity here, like it or not. It complicates pipelines, business logic, Data Platforms, and generally everything when you start to add
flexibility into systems.
Let‚Äôs say you ARE running a Databricks or Snowflake Data Platform for the majority of your workloads, but you know that at least 30% of your pipelines could easily move to DuckDB or Polars, no problem. What comes next?
code migration
integration questions as far as governance
integration into the overall platform
monitoring and alerting on new tool(s)
new testing infrastructure for new tools(s)
added complexity
That barely scratches the surface; you can‚Äôt just wave your hands in the air and say, ‚ÄúIt will be fine.‚Äù No, it needs to work in production.
Can you simply run your new DuckDB or Polars compute on Airflow workers? Will those workloads impact other processes or cause issues? Can you get the CTO's sign-off to have a MotherDuck account?
The path to Single Node Rebellion might not be easy.
Look, the path to all rebellions that are worth anything, or going to cost you something. There is no free lunch; nothing hard comes easy.
Sure, we can reduce costs with a single-node architecture, casting off the shackles of our Clusters that have plundered our pockets for many years. However, it will require some work and mindset shifts.
Into the breach, my friends!"
175142526.simplifying-cicd-with-databricks,Simplifying CI/CD with Databricks Asset Bundles (DABs),From Chaos to Control,2025-10-20 07:35:12.833-05,"Look,
I don‚Äôt want to be a YAML engineer
any more than you do. It‚Äôs one thing to have IAC (
Infrastructure as Code
), and another thing to have
Pipelines as Code
. Yikes. This old dog learns new tricks hard.
I‚Äôve started to fix my wicked ways and use that killer of souls and happiness, terraform. Better late than never.
Trust me, I get the north star. We want reproducible, source-controlled, and easy-to-deploy codebases. The reduction of human error and the reduction of complexity are what we all strive for.
The question is, how do we get there? One solution available for Databricks teams is
Asset Bundles, also known as DABs
.
Today, instead of talking about theory, we are going to crack open
Databricks Asset Bundles
and try them out ourselves from scratch.
What is the user experience like?
What is the development experience like?
What is the deployment experience like?
Can DABs really be the end-all, be-all for bringing some consistency to out-of-control Spark and Databricks codebases and teams? The talking heads like to talk,
be we like to DO
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Diving into DABs (Databricks Asset Bundles)
So, how does Databricks define a DAB?
üß∞ Install CLI ‚Üí üîë Auth ‚Üí üèóÔ∏è Init Bundle ‚Üí üì¶ Deploy ‚Üí ‚úÖ Test
‚ÄúBundle metadata is defined using YAML files that specify the artifacts, resources, and configuration of a Databricks project. The Databricks CLI can then be used to validate, deploy, and run bundles using these bundle YAML files. You can run bundle projects from IDEs, terminals, or within Databricks directly.‚Äù
           - Databricks DAB Docs
Ok, so let‚Äôs learn by doing, step by step, until we have a feel for DABs and what they are actually like to use. Reading docs is one thing, but you will never learn a thing until you use a thing. It‚Äôs the difference between good engineers and GREAT ones.
üß© Step 1 - Installation
The first step in using DABs is to install the
Databricks CLI
, of course if we are going to use CI/CD for deployment on some system, we will need the
CLI installed
there as well.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
‚öôÔ∏è Step 2 - Building a DAB
Ok, let‚Äôs create a brand new DAB bundle. The CLI gives us the option to use prebuilt default templates. Let‚Äôs use the Python one. They also provide the other
options below for default projects
.
To create a new DAB project from scratch ‚Ä¶ we run the
databricks bundle init
command.
Not sure what that was about at the end, it failed. I‚Äôm assuming int was looking for a PAT token, maybe? The docs and instructions on setting up a DAB bundle don‚Äôt mention that at all. Most people don‚Äôt use the Databricks CLI except in the context of DABs.
üí°
Pro Tip:
Always auth the CLI before running
databricks bundle init
.
üßæ - We need to make sure our Databricks CLI is authenticated.
This time, we get somewhere.
What is files make up a Databricks Asset Bundle?
Ok, so we have made our first DAB bundle, but what comes next and what is contained inside a DAB folder structure?
A bunch of folders and files. Below is generally what you would find created. üìÇ
[databricks.yml]
   ‚Üì includes
[resources/]
   ‚Üì references
[src/]
   ‚Üì used by
[Job Definition YAML]
Let‚Äôs go over what each of these files and folders are for.
Just what one would expect, a bunch of YML files spread around in different spots, holding different configurations and definitions of Databricks resources.
I suppose there is no black magic hidden in there; it seems fairly straightforward once you get the lay of the land and figure out what each location is/does.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Worth it?
Now one could stop here and ask the simple question ‚Ä¶ ‚ÄúIs it worth it?‚Äù
It probably depends on the context in which you are already creating and deploying your Databricks assets and pipelines.
Don‚Äôt forget, the main selling point of DABs according to Databricks themselves is ‚Äú
continuous integration and delivery (CI/CD)‚Ä¶
‚Äù
It‚Äôs a way to package a software project and its dependencies so they can be easily managed, developed, and deployed.
You might already be doing this without DABs.
That‚Äôs ok. You should ask yourself if there is enough juice to squeeze to move to Databricks Asset Bundles, if already doing those things a different way.
Which many Data Teams are. Including myself.
‚ÄúWorth it‚Äù is in the eye of the beholder. If you think your team could use a little extra ‚Ä¶
structure and direction
ease of CI/CD with the Databricks CLI
‚Ä¶
then by all means you should use DABs.
Getting into the meat and taters‚Äô
So if we open up our
DAB_example
project in a visual editor of our choice, we will be drawn to two main files that will probably take most of our time.
resources YAML
src Python code.
Of course, we do have things like
databricks.yml
where we can define the locations for Databricks production and development, locations etc. But after all, those things that probably won‚Äôt change much ‚Ä¶ the rest of a ‚Äúnormal development‚Äù lifecycle will probably be spent in defining
Databricks Jobs with YML
, and writing the
PySpark code in the src location
.
Example ‚Ä¶
For example, say we have this table,
confessions.default.hard_drive_failures,
that holds the model and serial number by day for hard drives, and whether they failed or not.
Maybe we want to create an analytics table, organized by month and year, showing the number of hard drive failures we‚Äôve had.
Of course, we would write all our PySpark code and stick it in our
src/main.py file
. If we had a big project, we could define our customer libraries and logic here, etc. (this file inside the DAB bundle will be reference by other YML definitions)
Then we would head over to our YML to define our job for this pipeline.
‚Äî>
`
resources/DAB_example_job.yml`
Note: if you look in the tasks, you will see our DAB_example and main
resource
noted ‚Ä¶ it‚Äôs our pipeline code.
None of this is too surprising or earth-shattering;
it‚Äôs just the Databricks Jobs that run every day, defined in a YAML file that points to a source containing our source code.
Again, that doesn‚Äôt seem to be the big reason why Databricks Asset Bundles (DABs) were designed in the first place.
We are locked into HOW we define our pipelines and Jobs.
We get consistency across projects and pipelines
We get easier CI/CD thanks to this + the Databricks CLI.
The real power of DABs appears to lie in testing and deploying our pipelines. This is where the combination of the Databricks CLI, the fixtures (
for testing
) we‚Äôve set up, and something like GitHub Workflows can create a bulletproof CI/CD pipeline.
In our example, it‚Äôs easy to create GitHub Workflows for both dev and prod deployments.
You can get all my code for this
example on GitHub
if you want to browse through it.
Again, there is no magic to DAB CI/CD. As you can see above, you're essentially telling GitHub:
hey, if a pull request is opened, run this test, or hey, if something is merged to main, do that.
Very standard CI/CD stuff you should already be doing.
Yeah, yeah, I know.
Look, I know we have barely scratched the surface of using Databricks Asset Bundles (DABs); they are as wide and deep as the big blue ocean.
We can with DABs ‚Ä¶
Modularize complex sets of code/pipelines/environments
Variable and Configuration injection
Dynamic and artifact versioning
Define Dashboards, Apps, Volumes, Models, Schemas, etc
Create Python whl files
, etc
As with life and technology, you can make DABs as complex and far-reaching as you want, or not. You can keep it simple, as in our example, using DABs to keep everyone on the same page when defining Jobs and storing Spark code.
You can use it as a way to run tests in an automated way, or just as the perfect tool to deploy your production code.
I think it‚Äôs the flexibility and combination of all of the above features that make DABs attractive.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Should you, I, we, use Databricks Asset Bundles?
That‚Äôs more of a question of the heart and mind, rather than a strictly technical answer. I can tell you today, on the Databricks Platform I built and run, that I do not use DABs to build, manage, test, or deploy the myriad of Databricks Jobs and ML pipelines that get run.
Yes, I‚Äôve thought about it.
It‚Äôs a great way to bring continuity to the chaos.
If you are already using other means to test, deploy, and manage your Databricks Pipelines, should you switch to DABs? Depends on how much time you have on your hands.
Assuming Databricks continues to invest time and effort into DABs, then yes, it might be good to do a migration project over the long winter. If you are happy with your current setup and don‚Äôt have any holes ‚Ä¶ then you probably won‚Äôt get any ‚Äúamazing‚Äù breakthroughs migrating to DABs.
Only you will know the answer to this question. Most likely, you will never regret migrating to or using DABs; there is no downside if used properly.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
174968230.what-is-a-semantic-layer,What is a Semantic Layer?,another Orwellian marketing speak?,2025-10-01 21:40:13.922-05,"Heck, as long as the data gods keep sending me special gifts, I‚Äôm going to keep opening them. I‚Äôm a sucker for unraveling some marketing speak. Nothing gets people hot under the collar like that.
Ok, I would be lying to you if I said over the last two years I‚Äôve never seen the words ‚Äú
Semantic Layer
‚Äù sprinkled through articles and
LinkedIn
. It‚Äôs getting pretty close to that point
where I‚Äôm too scared to ask
, which means it‚Äôs time to rip that bandaid off and see what purtrid mess we find inside.
If I had to guess, it was the
Illuminati
(
since they clearly own all the SaaS vendors
) that brought us the
Semantic Layer
, but we will find won‚Äôt we?
All unbelievers bow before almighty Semantic Layer
Ok, so how did we get here? Look, I‚Äôve been writing about data tom-foolery for nye on forever at this point. Part of that work requires me to dredge the endless depths of
r/dataengineering
where mortals dare not tread.
So, let‚Äôs just say I have my ole‚Äô ear to the ground and can hear things coming afar off.
This Semantic Layer just seems to have sunk up on me and gave me the ole‚Äô one-two punch when I wasn‚Äôt looking. How did it go from nothing to apparently everyone supposedly knowing exactly what it is overnight??!!
I worry that I‚Äôm slowing down in my old age.
What IS a Semantic Layer?
Ok, this is where I first smelled something funny. I was getting different answers when looking. But here is my best shot.
We had to provide two definitions because that is simply the state in which it currently resides. This smells to me of battling vendors already, you know?
Before we start ripping into things, let‚Äôs give everyone their fair chance to give two cents on the Semantic Layer. Starting with
Databricks
‚Ä¶
And
DuckDB
‚Ä¶
Interestingly, if you are an astute observer, you will notice that DuckDB and Databricks actually differ significantly in their definitions of a Semantic Layer.
Databricks says it‚Äôs
AFTER
Data Lakes, Data Marts, but
BEFORE
the BI tools.
DuckDB says it‚Äôs simply something
AFTER
the database and
BEFORE
the business user.
We should add
Snowflake
to the mix to ensure we capture all views.
My friends, this is where I start mumbling various and strange incantations under my breath and tell my wife I need to go for a walk in the woods.
At this point, my fingers are shaking, and the froth of emotions broiling and roiling inside me is threatening the stability of my keyboard.
A promise to use my newfound emotional intelligence training to hold that in, for now, and continue exploring the Semantic Layer, saving my polemic against such abuses for the end of the article.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
So, can we say what a Semantic Layer is?
Yes and no. Not really. Since none of the vendors agree on what a Semantic Layer is, there can be no clear definition; it will simply depend on who you‚Äôre talking to.
But, based on what we‚Äôve seen so far,
we know it‚Äôs something that sits between some data and the end user.
I mean, as far as I can tell, this is about all the different definitions agree upon, although upon looking more closely, I could be wrong about that; there appear to be other common points.
standardize metrics and calculations (definitions)
data governance and permissions
data transformation (
only SOME agree on this
)
‚ÄúHmmm ‚Ä¶ me thinks I‚Äôve heard something similar to this ‚Äúsingle source of truth‚Äô idea before. LOL!
What is spinning through my head right now is how the Semantic Layer differs from a Data Mart in a Data Warehouse, a
Gold Layer in the Lake House Medallion Architecture
??
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I don‚Äôt think it does, I think those things are ‚Äúpart‚Äù of a larger Semantic Layer if I‚Äôm following the rabbit trail correctly. I will tell you, dearest reader, what a Semantic Layer is beyond all doubt.
This also begs another question: which SaaS vendors are actually implementing ‚Äútools‚Äù or ‚Äúfeatures‚Äù that are directly tied to a semantic layer?
This one caught me by surprise, for once,
Snowflake beat everyone else to the punch
.
They released an actual ‚Äúthing‚Äù related to the Semantic Layer called a ‚Äúsemantic view.‚Äù
‚ÄúYou can store semantic business concepts directly in the database in a
semantic view
, which is a new schema-level object. You can define business metrics and model business entities and their relationships.‚Äù -
Snowflake
I mean, it is helpful to see, touch, and feel an actual implementation of something that, at the very least, a SaaS vendor considers a piece of the Semantic Layer.
For example, if you Google and research
Databricks Semantic Layer
, you will realize that ‚Ä¶
They consider their entire product offering in and of itself a semantic layer
You can use third-party Semantic Layers on top of Databricks.
The MDS (modern data stack) history of Semantic Layers.
From all I can tell, the Semantic Layer is mostly a way for data vendors to describe what they are selling, which is something very specific (
something that people have been selling for a LONG time
) ‚Ä¶ a single source of truth for all data, governance, and logic.
I thought it would be interesting to have AI go search the dregs of the internet and tell us more about how the phrase ‚Äúsemantic layer‚Äù came to be used in the context of the MDS (modern data stack).
Here is the history ‚Ä¶
2015 ‚Äî
AtScale
(BI on Hadoop ‚Üí early MDS)
AtScale‚Äôs
Series A press release (June 23, 2015)
explicitly touts ‚Äúa
unified semantic layer
‚Äù integrating with Excel/Tableau/Qlik‚Äîone of the first modern-era vendor mentions tied to cloud/‚Äòbig data‚Äô stacks.
AtScale
2019 ‚Äî Cube (open-source)
Cube states it
‚Äúwas originally founded in 2019 as an open source semantic layer project,‚Äù
marking one of the earliest open-source entrants using the exact term for MDS use cases.
cube.dev
2022 ‚Äî ‚ÄúSemantic layer‚Äù crosses from concept to product in dbt world
dbt Labs starts talking publicly about a
dbt Semantic Layer
(Aug 31, 2022 update; ‚ÄúNext layer of the modern data stack,‚Äù Feb 24, 2022). Partner blogs (Mode, Oct 18, 2022) adopt the phrasing.
dbt Developer Hub+2dbt Labs+2
2022 ‚Äî Community think pieces go mainstream
‚ÄúThe Rise of the Semantic Layer‚Äù (Sept 29, 2022) chronicles the new wave (Cube, MetricFlow, etc.), explicitly using the term in the MDS context.
Data Engineering Blog
2023‚Äì2024 ‚Äî Consolidation & GA
dbt
acquires Transform
(MetricFlow) and details
how the dbt Semantic Layer works
‚Äîcementing ‚Äúsemantic layer‚Äù as standard MDS vocabulary.
dbt Labs+1
What is the common denominator when we look at the history of ‚Äúsemantic layer‚Äù purveyors in the context of the MDS? It‚Äôs that they are selling a literal software ‚Äúlayer‚Äù on top of existing data and data services.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
To be honest, the semantic layer appears to be a good approach to data modeling, combined with common definitions. Nothing more. Consumable, governed, ‚Äúcorrect‚Äù data insights.
It appears we are still rabbits chasing the same carrot.
You know, it appears we data professionals, if we can be called that, are still chasing the same golden goose after all these decades.
We want that single source of truth ‚Ä¶
data truth
metrics and analytics truth
business definitions truth
calculations truth
governance truth
All in a single spot or tool, nonetheless. And, this simply tells us one thing. That if the semantic layer is becoming a popular topic,
it‚Äôs because organizations are still struggling with these very problems today, even with their fancy new Databricks and Snowflake tools.
Data Platforms and teams do want to go to a single spot to see the calculation of a ‚Äú
closed customer.
‚Äù The same calculation is performed in five spots and eight different Dashboards, yielding different results.
We know the pain.
We are good engineers; we could solve that pain if we wanted to. Without a so-called Semantic Layer to save us from our data sins. The problem is, the squeaky wheel gets the grease.
But, we don‚Äôt. We rush through the JIRA tasks and projects, producing what the business requires in the timeline we are given. Then later, when the data is dirty and things get out of hand, a Sales Engineer shows up with a new Semantic Layer tool to save the day.
You know, after reading and researching the almighty Semantic Layer, and seeing Snowflake‚Äôs semantic views up close. That ain‚Äôt a bad idea!
The problems it is trying to solve are real. Age-old problems.
Do I think it‚Äôs worth buying a whole new SaaS to layer on top of the SaaS you already have?
$@$@#$% No! Platforms like Databricks and Snowflake already provide you with first-class features to build the perfect Data Platform that can, and often do, run like a well-oiled machine.
Guess what?
They are only as good as the people building those components.
If those people don‚Äôt care about ‚Ä¶
defining calculations encapsulated and testable way
building data quality solutions
ensuring data governance is part of the design
providing analytics that the business actually needs
Then yeah ‚Ä¶ you do need a Semantic Layer to fix the problems
YOU
put in place. Surprise surprise.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
174645345.revisiting-data-quality,Revisiting Data Quality,options?,2025-11-25 07:19:13.945-06,"Oh, that age-old topic of Data Quality, often discussed but never implemented. Poor little blighter. Who knows what it‚Äôs like to be the old, beat-up rock that always gets kicked, gets no love or attention, just told about everything you do wrong (
try writing a Substack like me, you‚Äôll find out quick enough
).
That is Data Quality, isn't it?
The age-old question.
And the age-old answer.
Nothing could be truer and to the point.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Data Quality isn‚Äôt cool enough.
I‚Äôve written about Data Quality a few times over the years; it‚Äôs simply not a popular or sexy topic. Always mediocre reaction to mediocre tools that don‚Äôt work well.
Truth be told, Data Quality is something CTOs and Data Leaders love to talk about, but when you ask them to either pony up some money for a tool, or pay some engineers for work on this topic for a quarter or two, you get mumbling, muttering, and a general waving of hands. Then it‚Äôs back to business as normal.
It‚Äôs always sight out of mind, the old nagging suspicion in the back of the head. Random data problems that pop up and get fixed. Not enough pain to actually do a thing.
BTW, here are some past DQ articles.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Data Quality in the Lake House era
What is Data Quality in the Lake House era? It‚Äôs a mess, is what it is. We Modern Data Engineers were spewed forth from the mouth of the Data Lakes, jumbles of data piled into stacks, with rigidity to match that haphazard approach.
We landed in the detritus of Lake House architecture, made up of Iceberg and Delta Lake. Somehow reminding remenicent of the old SQL Server days, yet different in many ways. Less rigid, schema evolution is no problem.
Do what you want; the Lake House is a forgiving master.
It‚Äôs sorta like being your own boss. You‚Äôre nice to yourself. You say, ‚Äú
We don‚Äôt need any stinking constraints
,‚Äù or ‚Äú
Go ahead and add those columns, no problem.
‚Äù
loosey goosey approach to data modeling
constraints are for Postgres, not Iceberg/Delta
we add or change columns willy-nilly
We spend all our time optimizing pipelines to save money on that giant cloud bill. No time to work like a filthy peon and worry about DQ issues; we can solve them as they come.
In the end, we are all victims of our collective sins. We never think the piper will come to collect his pound of Data Quality flesh.
Anywho, enough pontification, let‚Äôs get to the matter at hand. Say we WANT to do a little extra in the field of Data Quality in our Lake House.
What are our options? What should we do?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Data Quality Baseline for the Modern Lake House
I have a feeling that I‚Äôm already going to sound like your Mother standing over you, shaking my finger at you, saying, ‚Äú
Clean your room, you filthy ingrate, who wants to live like this?
‚Äù
Yeah, well ‚Ä¶ it‚Äôs true.
Of course, when we think about Data Quality, our minds go straight to an open-source or SaaS product that promises to solve all our problems. This is maybe 50% true, or less.
We should start by introspecting our data. Data Quality starts right here at home.
Contrary to popular belief, Data Quality starts with simply having a ‚Äúhandle on your data.‚Äù What does that mean? Well, not everything is a STRING. Not everything can be NULL.
You can‚Äôt build (or shouldn‚Äôt) Data Quality with a shaky foundation. You need to understand and therefore control the incoming data.
Most Data Quality ‚Äúissues‚Äù come from a poor understanding and lax approach to data.
Start small.
Schema ‚Üê data
Constraint ‚Üê data
Testing ‚Üê business logic
Everyone is capable of doing this. You should use the schema of your data store to your advantage, squeeze every last drop out of it, Postgres to Delta Lake to Iceberg, you have a tool before you.
Schema
Let‚Äôs start with the schema, and make this very clear.
Heck, your Junior Engineer can even figure that one out if you show ‚Äòem. It seems painfully obvious, yet it is ignored for the sake of speed, laziness, or ‚Äúflexibility.‚Äù This will bite you more than anything else.
(
I know technically most people consider NULL or not a constraint, but it‚Äôs usually written as part of the DDL
).
So, you‚Äôre telling me you're willing to shell out serious  and time to put some SaaS Data Quality platform in place, but you‚Äôre not willing to decide which columns should be STRING vs INT, and what can be NULL vs NOT NULL?
I will come out of this screen and slap you in the face.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Constraint
After schema comes the much-maligned but probably the most useful free data quality check, no SaaS required. Nothing to install, no fancy UI, no alerts. Just constraints.
Again, I‚Äôm about to blow your DQ mind.
Wow, look at that, the exact same check as is probably written in about as many fancy DQ tools as you can imagine. Except here, we enforce Data Quality the minute it hits the Lake House. Imagine that.
The problem is that neither the schema nor the constraints for Data Quality are sexy. No one makes any money on this approach.
Testing (unit)
I bet you were not expecting this one, were you? Think about it for a minute. Many Data Quality checks are not only logical (
e.g., someone cannot be less than 0 years old
), but also business-specific.
This is sorta esoteric, but your Data Quality will never be any good if you don‚Äôt test your codebase. Many so-called DQ issues are directly related to incorrect or unexpected business logic
For example, is the amount column all in the same currency?
This sort of business logic is hard to capture in a schema/constraint, but easy to test for.
At the end of the day, with any approach, you are trying to get ahead of the ball, to force some rigidity and expect something from your data up front, not just shovel the slop forward and use expensive SaaS to solve your problems later.
The problem is, when it happens later, you‚Äôve already buggered the system and someone has to clean it up.
SaaS Data Quality
Well, this is the moment I‚Äôve been dreading all along. It‚Äôs been a while since I‚Äôve poked at the open-source and SaaS options for Data Quality frameworks. Now is my chance to see if anything new has entered the scene, or if it‚Äôs the same old.
I don‚Äôt know what else to do but simply dig around in the digital mud and see what‚Äôs popping.
I would like to break them into open source vs. SaaS, but that line has been so blurred; it‚Äôs hard. We will try.
Let me just list the options.
Monto Carlo
Open MetaData
Great Expectations
Soda
Elementary Data
DQX
deequ
Pandera
pydantic
Spare me your insolent comments about which tool I forgot. In all my googling and researching, there hasn‚Äôt been much change, though a few new tools have popped up since last time. To be honest, this is a good sign, for years, nothing has moved in this category.
First, let‚Äôs try to bucket these for our own sanity. Don‚Äôt quote me on this, it‚Äôs my best shot.
Looks like we have a mix of open source only, SaaS only, and a few handfuls in the middle. If you want my honest opinion, you should always pick something in the middle group.
We can go around in circles all day about semi-open source, rugs getting pulled, etc, but usually a SaaS with some open source on the side is a good sign, and that quality will probably be top-notch.
Also, not all these tools are made to do the same thing.
some are full-fledged observability + DQ
some are DQ only
some are Python-specific
some are for Spark
Truth is, you don‚Äôt have as many options as you think for your specific use case. For example, if you are running Snowflake, most of these are useless.
Again, here is my quick take on tooling support for these DQ frameworks.
Spare me the angry comments, I won‚Äôt read them.
As you can see, even a cursory glance over the the array of DQ options will point out that the tool you use is most likely highly dependant on
how that actual tool fits into your wider Data Platform framework.
These decisions are not made in a vacuum, you don‚Äôt simply pick one that looks cool and the force it onto your use case. It should be the opposite.
how complex you do you want your DQ to be?
what is your main datasource?
how large are the datasets needing DQ checks?
do you want to host the tool yourself?
These sorts of questions narrow down the focus and will mostly likely leave you two tools to choose from, maybe three.
Each Data Platform team has it‚Äôs own context in which they operate. Some perfer plug and play, others love drinking from the chalis of open-source.
Pick your poision.
I‚Äôm not going to spend anytime today going into the internals and ‚Äúhow to use‚Äù x, and y tool. If you‚Äôre interested in that sort of thing, let me know, drop a comment. I‚Äôm not opposed to it, I think generally implementation details in a post like this can simply be distracting and pulling you away from the high level decisions that are argubly more important.
Instead you should focus on ‚Ä¶
Data Quality at home first.
schema
constraints
unit tests
Understanding your data very well
combined with your data types and DQ needs
Armed with these points well thought out and decided, the rest is simply up to you. Pick a tool that appears to fit well into your culture, data, and business needs.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
174535187.clickhouse-a-super-fast-columnar,ClickHouse: A Super-Fast Columnar Database,Guest Post Series,2025-11-05 06:47:20.616-06,"Hello! This is Daniel. I‚Äôm excited to introduce a new guest post series that showcases other data professionals, giving them a space to teach us all new tech and ideas.
Today we are going to hear from
Ahmed Shaaban
, you can check out his work here on
Substack
and find him on
Linkedin
.
With that, meet our first guest, Ahmed.
As a passionate and results-driven Data Engineer, I specialize in designing and building robust data systems that support advanced analytics and decision-making.
With experience in developing and optimizing Data Lakes and Data Warehouses, I enable organizations to manage vast datasets seamlessly. My expertise extends to creating real-time Dashboards that provide actionable insights, empowering stakeholders to make informed decisions. Outside of work, I enjoy writing and sharing what I learn with the community.
When it comes to databases, there are two main types you‚Äôll hear about:
row-based
and
column-based
. That‚Äôs pretty much the big divide in the database world.
Column-based databases are the go-to choice for
analytics
. They‚Äôre great when you want to run large aggregations quickly‚Äîlike calculating totals, averages, or trends across millions or billions of rows.
That‚Äôs because they store records column-by-column, instead of the traditional row-by-row format. So whey you need to read all values for a column to do some sort of aggregation on it, you can read only that column directly from the disk. You don‚Äôt have to read other columns simply because they are stored together.
But if you‚Äôre looking to set a columnar database, you‚Äôll quickly notice there aren‚Äôt a ton of options. One of the most interesting (and powerful) ones out there is
ClickHouse
.
It‚Äôs an open-source
, columnar database built specifically for
real-time analytics
. It scales both
vertically
(
beefier servers
) and
horizontally
(more servers), and if you don‚Äôt want to manage infrastructure yourself, there‚Äôs also a
fully managed SaaS version
that handles setup and maintenance.
ClickHouse by the Numbers
ClickHouse makes some bold performance claims:
Up to
95% faster
than Google BigQuery
3‚Äì5 times cheaper
than Snowflake
Uses
half the storage
of PostgreSQL
Sure, some of these numbers might be a little hyped up by sales teams, but even if they‚Äôre only half true, that still says a lot. You‚Äôve got to be pretty confident to make claims like that, and in many real-world scenarios, ClickHouse does live up to the hype.
Why Is ClickHouse So Fast?
So, what‚Äôs the secret sauce behind ClickHouse‚Äôs speed? It all comes down to smart architectural choices and clever optimizations. Here‚Äôs a quick breakdown:
Columnar Format
ClickHouse stores data by column, not by row. This is perfect for analytical queries that need only a few columns but scan through many rows.
Less data read = faster results.
Row-oriented DBMS
Column-oriented DBMS
Source (
https://clickhouse.com/docs/intro#row-oriented-vs-column-oriented-storage
)
Efficient Compression
Similar values are stored next to each other in a columnar layout, which makes compression much more effective.
ClickHouse supports several compression algorithms, like
ZSTD
and
LZ4
(general-purpose), and more specialized ones like
FPC
for floating-point data or
Delta
for integers.
Concurrent Inserts with ‚ÄúParts‚Äù
Incoming data is written into separate chunks called
parts
. These parts are later merged in the background.
This means inserts are fast and isolated‚Äîyou‚Äôre not constantly locking or rewriting existing data.
Merge-Time Computation
Instead of doing expensive computations or checks when data is inserted, ClickHouse defers that work until the background merge phase.
This makes writing data fast and opens up room for smart transformations later‚Äîlike
replacing duplicates
,
aggregating rows
, or even
archiving old data
.
All of these architectural choices explain why ClickHouse is fast‚Äî
but performance isn‚Äôt everything
. What really makes ClickHouse stand out is how much flexibility and control it gives you when designing your data model and optimizing for different workloads.
Let‚Äôs take a look at some of the features that make it more than just a fast engine.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
ClickHouse Features That Stand Out
Now that we‚Äôve covered why ClickHouse is so quick, let‚Äôs talk about some of the
cool features
that make it even more powerful‚Äîand flexible.
Sparse Primary Indexes
In ClickHouse, indexes are
sparse
, which means they don‚Äôt store an entry for every single row. Instead, they keep track of just the
first row in each
granula
.
A
granule
is the smallest indivisible data set that is stored on disk and streamed into ClickHouse for data processing.
Since tables are sorted by the primary key, this is usually all you need to quickly find the data you‚Äôre looking for. And because the index is small, it often fits entirely in memory, making lookups super fast.
Data part (on disk)  
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  
‚îÇ Columns: (UserID)   (URL)   (EventTime)       ‚îÇ  
‚îÇ                                               ‚îÇ  
‚îÇSorted by primary key: (UserID, URL, EventTime)‚îÇ  
‚îÇ                                               ‚îÇ  
‚îÇPartitioned into ‚Äúgranules‚Äù (e.g. 8,192 rows)  ‚îÇ  
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚Ä¶       ‚îÇ  
‚îÇ ‚îÇ Granule 0    ‚îÇ   ‚îÇ Granule 1    ‚îÇ           ‚îÇ  
‚îÇ ‚îÇ rows 0‚Äì8191  ‚îÇ   ‚îÇ rows 8192‚Äì‚Ä¶  ‚îÇ           ‚îÇ  
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ  
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  

Sparse Primary Index (in memory)  
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  
‚îÇ Mark 0 ‚Üí (UserID‚ÇÄ, URL‚ÇÄ)   ‚îÇ ‚Üê first row of Granule 0  
‚îÇ Mark 1 ‚Üí (UserID‚ÇÅ, URL‚ÇÅ)   ‚îÇ ‚Üê first row of Granule 1  
‚îÇ ‚Ä¶                          ‚îÇ  
‚îÇ Mark N ‚Üí (UserID_N, URL_N) ‚îÇ ‚Üê first row of Granule N  
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Projections
Tables in ClickHouse are sorted based on their primary keys, so it‚Äôs smart to design those keys to match your most common queries. But what if you have
multiple types of queries
that need different sort orders?
That‚Äôs where
projections
come in. A projection is basically a different physical version of your table‚Äîone that‚Äôs optimized for a specific query pattern. It can have a different primary key, be filtered, or even pre-aggregated.
Multiple Storage Engines
ClickHouse lets you choose different storage engines depending on your use case, even within the same schema. The two main families are:
MergeTree-based engines
: These support background merges and are super flexible. Most production tables use one of these. Examples include:
MergeTree: The default, general-purpose engine.
ReplacingMergeTree: Automatically deduplicates rows based on a version column.
AggregatingMergeTree: Stores data in an intermediate aggregating state, which greatly improves aggregations
Log-based engines
: These are simpler and append-only. Good for smaller, quick jobs or temporary tables. Examples:
Log: Basic append-only engine.
TinyLog: Lightweight version for tiny datasets or testing.
Materialized Views
Materialized views in ClickHouse help you
shift processing from query time to write time
. They let you precompute results and store them as data comes in. There are two types:
Incremental materialized views
: Automatically update as new data is inserted.
Refreshable materialized views
: Can be recomputed manually or on a schedule.
While
projections
give you
multiple layouts
of the same data (ideal for speeding up queries),
materialized views
let you
transform the data
during insert time (great for things like aggregations, rollups, or schema simplification). Think of it this way:
Projections = multiple optimized
copies
Materialized views = preprocessed
summaries
ClickHouse Isn‚Äôt a One-Size-Fits-All Tool
As fast and powerful as it is, ClickHouse isn‚Äôt built to do
everything
. It‚Äôs amazing for real-time analytics and large-scale data crunching, but it‚Äôs
not a transactional database
. You won‚Äôt get full ACID guarantees, and it‚Äôs not meant to replace systems like MySQL or PostgreSQL for complex transactions.
Also, if your workload relies heavily on
frequent updates or deletes
, ClickHouse might give you some headaches.
While those operations are supported, they come with a
serious performance cost
and often require rethinking your data model to fit ClickHouse‚Äôs strengths (It‚Äôs worth noting here that ClickHouse has improved the updates statement‚Äôs performance by a lot recently, so this assumption may not be valid in the near future)
In the end, we can say that ClickHouse is a powerful option if you need
real-time analytics
,
fast inserts
, and
low-latency queries
over huge datasets. It‚Äôs used by companies across industries for observability, product analytics, monitoring, and time-series storage.
It‚Äôs not trying to be your all-in-one database. But if you understand its strengths‚Äîand design around them‚Äîit can be a game-changer.
So if you‚Äôre building a system that needs to move fast, crunch data hard, and scale effortlessly, ClickHouse is definitely worth a look.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
174368057.exclusive-discord-server,Exclusive Discord Server,top notch data professional discussions,2025-09-23 13:43:34.604-05,"Hello!
Excited to announce a new and exclusive Discord channel for Paid Subscribers only!
Below you will find a link for access to this unique community where we can trade ideas, solve problems, and interact with other serious data professionals without wading through the piles of detritus and marketing noise.
Please
click the link below to join the discussions on the Data Engineering Central Discord server
.
Tired of wading through the dregs of Reddit to find data advice and news. Want to meet and work with other real data professionals who are solving real problems every day!
Join now!
Who know‚Äôs the future of this Discord Server is, but I know with a group of exclusive serious, smart, and awesome data professionals, the value will be top notch!
Join now!!"
174209187.change-killing-comfortable,Change. Killing Comfortable.,path to a brighter future,2025-09-29 12:15:15.306-05,"Have you ever got up on a cool fall morning, going through the same morning routine you‚Äôve had for years? Take a shower (
or not
), make some coffee, feed the kids, and get them to school. Do a few chores around the house and get them done just in time for the morning
Stand Up
?
You talk about the same old problems, open a few PRs for boring bugs, everything is running smoothly, status-quo in fact. Same ole‚Äô same ole‚Äô. Blah.
It almost feels too comfortable, doesn‚Äôt it? Something tickles in the back of your mind
‚Äú
Am I getting out of touch
?‚Äù
‚Äú
I see people doing other cool things around me
.‚Äù
‚Äú
Nothing seems new anymore.
‚Äù
‚Äú
Am I/we falling behind?
‚Äù
Look, I get it, in our jobs as builders and maintainers of Data Platforms, we strive and find our bonuses where consistency and reliability live.
But let‚Äôs not be coy.
We‚Äôve all lived and moved in those jobs and technical cultures where everyone is still using technology two decades old. We are dinosaurs working on fossilized patterns and tech. The world has moved on without us. Stuck in the past, scratching and clawing at the same old problems for 15 years.
How can we strike a balance? At some point, do you need to invest in new technologies and ideas? How do you avoid being left behind while simultaneously providing business value in a consistent and trustworthy manner?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
It‚Äôs a mindset shift more than a technology shift.
I have struggled myself between two seemingly conflicting ideas.
moving a Data Platform forward to deliver more value
keeping a Data Platform stable and running well
It‚Äôs hard to strike a balance where we aren‚Äôt leaving things on the table, and falling behind the general movement of data technology, being able to take advantage better tech and driving more value.
But, it ain‚Äôt no good if we are breaking things every day. We must adopt a serious attitude that new isn‚Äôt always good; chasing the pot of gold at the end of the rainbow doesn‚Äôt always lead to riches.
Stability over the long haul will shape how people perceive you and your ability to deliver consistent, trustworthy business value. If everything is always breaking and incorrect, no one will trust what you are doing.
That being said.
Okay, so we know we need to avoid breaking things and maintain a stable environment. Surprise uh? That is obvious. But what is something else that is obvious in pretty much every Data Team?
People don‚Äôt like change.
People get comfortable.
People drag their feet
Don‚Äôt lie to me. I know you have your tool(s) that you just roll out of bed and want to use. They feel good, familiar, right; it‚Äôs what you‚Äôve spent a long time learning.
Then something happens.
New things emerge, or you've been using the same tools for the last five years, and someone suggests a new tool or platform to improve processes and business impact.
Now what do you do?
Some people will balk at and resist the change, while others will embrace it.
Selling change as a mindset shift.
Probably one of the easiest ways to encourage people to get on board with changes and not to fight every step of the way is to shift how you present these changes to a wider group.
It‚Äôs about communication.
You have two options when moving a data platform forward, and you face resistance from stakeholders who seem unhappy and scared about what is happening or what is to come.
Sell it as a pure technology change, and force the issue
Sell it as a first mindset shift, and secondly, as technology changes.
In truth, we are being more honest about an upcoming technology change if we discuss mindset shifts first.
Underneath all the needs to swap out parts and components, to do some ‚Äúupgrade‚Äù or ‚Äúchange,‚Äù lies a nugget of a
mindset shift
. We want to do something differently, or at least that‚Äôs what should be happening.
Changing technology just to make a change, with no direction, will make people nervous, which it should.
If we want to change from tool X to Z, we need to communicate first why we want to make such a change and what kind of mindset shifts we are looking for.
Example ‚Ä¶
Hello, Engineering team! We are transitioning from AWS EMR to Databricks. Please start planning for this transition.
This would likely cause widespread screaming, wailing, and moaning from everyone involved. ‚Äú
Why are we doing this?
‚Äù ‚Äú
This is going to take forever?
‚Äù ‚Äú
Who‚Äôs in charge?
‚Äù
‚ÄúWhat about x, y, and z technical pieces? They will break and never work again.
‚Äù
Sure, all those are valid questions and concerns, but mostly they come from a place of concern from those who are simply uncomfortable with change.
Instead ‚Ä¶
Hello, Engineering team! In an effort to modernize and centralize our Data Infrastructure and improve our AI and ML operations, we have decided to move forward with a migration towards Databricks.
This presents a whole different set of challenges and assumptions, in a positive way.
How we communicate what we are trying to do is extremely important.
We need to lead others down the road towards the north star we are heading towards, trying to bring the focus off specific tools and struggles (technical problems), towards the bigger goal.
‚Äú
We want to get to X, Y, and Z north star at the end of the day, how we get there, and with what tools is secondary to where we end up.
‚Äù
At the end of the day.
Look, when you are running a Data Platform at a high level, providing business value and stability on a day-to-day basis ‚Ä¶ there are going to be times when we have to make ‚Äúbig changes.‚Äù
Unless you are already dead and work in heaven, you ARE going to get pushback and foot-dragging by many people who are involved at different levels.
Stepping back from technical arguments and explanations will help. Instead, communicate at a high level,
WHERE
and
WHY
you are trying to go. Make the technical details a secondary and later conversation.
This sort of communication will reduce stress for all parties involved and actually bring people along for change, making them your champions. It‚Äôs nearly impossible to drag unwilling people into the future.
You must communicate your goals in a way that inspires others to own the future, change themselves, and take the road with you."
173580165.honest-review-of-motherduck,Honest review of MotherDuck,pluck em' feathers,2025-09-22 07:43:37.699-05,"Well, I enjoyed my recent review of Polars Cloud so much that I figured I would keep the streak going, you know, like your grandpa said, ‚ÄúStrike while the iron is hot.‚Äù With such a thing fresh in our mind and on our fingertips, I thought to myself, what about that DuckDB and MotherDuck, why not taste of that fruit as well.
So here we are again, today, not so much to review DuckDB, with which we are all familiar, but to pluck a few feathers from that MotherDuck and stick them into our hunting hat.
Similar to Polars Cloud, I‚Äôm really not interested in performance, but just generally, how good of a job has MotherDuck done providing DuckDB as a service (
DaaS)??
What am I looking for? I‚Äôm unsure, but at least ‚Ä¶
Onboarding experience
UI experience
Easy of development and deployment
Concepts
Documentation
Integrations, etc.
In short, in the Era of Databricks and Snowflake, what is MotherDuck offering as far as developer experience.
Attention!
Because you‚Äôre all cheapskates and I spend my hard earned money burning AWS and Databricks compute for you ungrateful hobbits,
I‚Äôm offering 50% OFF a Yearly Subscription
to you my most ungrateful subjects.
$27.50 for a year of my genius??!!
Back to MotherDuck
Ok, so after my un-shameful reminder to get %50 off, let‚Äôs start pulling the puffed plumage of MotherDuck and see if the care and beautify of the open-source DuckDB translates to its money hungry mother.
First, because we have no choice, let‚Äôs see what MotherDuck calls themselves. I always find this interesting, how does a SaaS vendor see themselves, what are the core tenants they sell you out of the gate.
The following is pulled straight from the
MotherDuck homepage
.
With MotherDuck and DuckDB you get:

- Ergonomic Querying: 
Autocorrect SQL Typos with FixIt and Take a Bird‚Äôs Eye View of Your Data with Column Explorer and Pivots

- Analytics: Share Data with your Team Simply and Securely

- Data Ingestion: 
Supports Parquet, CSV, JSON, Iceberg, Delta, and more

- Pricing: 
Billing at the CPU Minute and Automated Query Planning Across Local + Cloud with Dual Execution
‚ÄúFast, Scalable Analytics without the overhead‚Äù -
MotherDuck
So, that‚Äôs pretty much what I expected, not sure about you. What I do find interesting is that they aren‚Äôt exactly, or explicitly,
saying anything about Lake House or Data Warehouse.
I mean they sort of say it, don‚Äôt say it, when they say make big data feel small and run scalable analytics. But that could just be me reading between the lines.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Going through the onboarding workflow.
I‚Äôm not going to lie, after that janky nightmare of onboarding with Polars Cloud and that horrid UI, the MotherDuck signup flow is incredible.
It‚Äôs a beautiful UI, step by step, snappy. Everything you would expect from something built in this century, and bodes well for the things we have yet to do.
One of the first things you will notice is the nice clean layout, one that doesn‚Äôt leave you wondering where to click next or what to explore.
The first thing I‚Äôm going to do is connect my data in S3. So I will use their nice UI and add my AWS creds.
I also shouldn‚Äôt fail to mention their top-tier documentation, not surprising of course, but it‚Äôs well laid out, thoughtful, and you can find whatever you need right away.
But, before I start playing with my data in S3, I probably got ahead of myself, let‚Äôs take a moment to in a more academic manner,
understand some core concepts
, at least from my 10,000 foot view and understanding as a new user.
I have little practical experience with DuckDB, other than what I‚Äôve done playing around with it on this blog, and zero experience with MotherDuck, so take this with a grain of salt.
If you made me try to summarize MotherDuck for you, from my perspective,
as to what it actually is
?
With the caveat that they did release
DuckLake
, so you are more than welcome to store your data with MotherDuck as well, if you want and end-to-end experience.
You can read my unkind and spicy take on DuckLake below. May the good Lord forgive me.
Anywho, they (MotherDuck) doesn‚Äôt push it down your throat, and this is probably why they will win.
The make a nice clean product by which you can easily move and compute data in and out of any Cloud Provider.
aka ‚Ä¶ no hard vendor lock-in
Ok, back to the matters at hand. What I really need to do (
whether I want to or not
), is to create an Airflow DAG that can do a ‚Äúdata thing‚Äù with MotherDuck.
When it comes down to brass-tacks, it‚Äôs of course easy enough to
oohh
and
ahhh
our way through a nice UI, but at the end of the day I should be able to setup a simple data pipeline with Apache Airflow for ANY data tool worth its salt.
If it works well, you have a good thing on your hand, if it struggles or is hard to do, than you have found out all you need to know.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Building a MotherDuck pipeline with Apache Airflow.
Well, first things first lets mosey over into the Notebook UI that MotherDuck stole from the GOAT Databricks.
I created a Database to hold my tables,
pickles
of course. But, if we are going to Airflow this @!@$# then let‚Äôs find where we can create a TOKEN for the authentication.
Easy enough once you find the settings page.
The Airflow Part.
Ok, let‚Äôs start writing the Airflow DAG that can do the MotherDuck work for us. Once we have that scaffolded we can write the DuckDB code itself.
Lucky for us, that much lauded
Astronomer has some examples we can peak at.
We will have to add our logic to connect to MotherDuck, but we can worry about that later.
Now for the DAG, maybe at the same time we will write the DuckDB/MotherDuck crap and talk about it all at once.
BTW, we are going to use 50GB of CSV hard drive data in s3 to let DuckDB crunch on. Can‚Äôt make it easy, you know what I‚Äôm saying??‚Äô
Anywho, Airflow DAG + MotherDuck + DuckDB logic.
Code on GitHub.
Well, I do have to admit ‚Ä¶ kinda looks nice don‚Äôt it?? Hold on, going to send a picture of this my mother, make her proud and all.
What a match made in heaven, that MotherDuck and Airflow, to think all the poor saps out there tripping over FiveTran when you can write this sort of thing.
Let‚Äôs be honest, connecting your code to MotherDuck vs running it locally is lit.
con = duckdb.connect(f""md:{MD_DB}?motherduck_token={MD_TOKEN}"")
I mean seriously, MotherDuck is a genius.
Ain‚Äôt nobody else by a mile who is making it this EASY to move from development code to production.
Also, you hobbits ain‚Äôt going to believe how fast that crap ran.
To top it all off, check the results in the UI was easier and smoother than your hair with all the gel at prom in 9th grade.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Sign me up buttercup.
This MotherDuck thingy is such a beauty it brings a tear to me old eye. You have no idea how many new tools (maybe you do) I agonize over while trying to bring you the good, the bad, and the ugly.
They are always ugly and take copious amounts of time to get working correctly.
What MotherDuck has done with with DuckDB in the cloud is insane, the entire experience end to end is bullet and idiot proof. As per usually everything DuckDB touches turns to gold."
173382292.8-data-engineering-principals,8 Data Engineering Principals,for the analytics expert,2025-12-26 12:22:05.755-06,"Analytics
. What, you think I should stick to my lane and keep with Data Engineering-specific content? Well, I bring you tidings of great joy, my friend.
Many moons ago, before the age of Databricks, when
Hadoop
clusters roamed the land and wizards who wrote
Pig
were worshiped as demigods, you could have found me creating reports in SAP and dashboards for a living.
A few long and arduous years later, if you peeked through my windows, you would have seen me studying Microsoft SQL Server exams in a bid to get certified, after which I had my green eye set on that elusive but popular ‚Äú
Business Intelligence Engineer
‚Äù title.
I did get that title, by the way. If you care,
you can read more about my journey here.
All that to say, I was bringing analytical insights to those
cranky business users while you were at your high school prom
, or playing beer pong in a dorm room.
The lines have become blurred.
We live in a world now where the lines have blurred, namely between ‚Äúclassic‚Äù Data Engineering and ‚Äúclassic‚Äù Analytics. Much of this blurring and mixing is due to the likes of Databricks, Snowflake, BigQuery, etc.
The era of the
Lake House
has brought analytics to the masses, with tools like
Databricks One
redefining what we think of when we think of Self-Service Analytics.
So, with all this being said, I‚Äôm writing to you, creator of analytics, dashboards, and insights. What‚Äôs that saying?
Don‚Äôt shoot the messenger.
We are just victims of the times we find ourselves in.
Since the lines are blurred, Data Engineers are expected to create dashboards and insights, and Analytics Engineers are expected to generate production pipelines ‚Ä¶ why don‚Äôt you settle in, grab a drink, and let me spin you a tale of 8 Data Engineering Principals ‚Ä¶
for the analytics expert
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
TDLR: let me cook, don‚Äôt run off yet.
The experts say we live in the generation of short attention spans, and I fear I‚Äôve been waxing poetic, so let me get straight to the point, and unpack later for those with time to spare.
Here are 8 Data Engineering principles you can embrace and imbibe as an analytics professional that will put you far and above your peers in terms of skill and acumen.
üîÑ Automation (CI/CD) is a must.
üóÇÔ∏è Source control is your friend.
üß™ Testing is your friend.
üìä Own data quality.
‚ö° Complexity is your enemy.
üêç Learn something beyond SQL.
üèõÔ∏è Have an architecture mindset.
üéØ Develop ruthless focus.
I would‚Äôt lie to you. If you simply take this list, write it down and stick it on your fridge, send it to your Mom, whatever. You will become the most value Analytics professional to touch finger to keyboard within a 200 mile radius of where you sit right now.
We live in the era of AI; everyone is expected
and needs
to step up their game to stay relevant. Going above and beyond has always been the quickest way to set yourself apart and rise through the ranks of squabbling keyboard punchers around you.
Let‚Äôs get in the dirt and roll around.
You will notice that nothing I listed above is really earth-shattering; all of it is common sense that has been around before you, and I took our first screaming breath on this rock.
That‚Äôs good and bad news.
Yeah, so this is going to be similar to exercise and health; there is no special sauce. It‚Äôs a slow and simple burn with consistency over long periods of time,
no shortcuts to the top.
That‚Äôs what makes it so hard and so unique.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Fancy can come later; master the basics first.
üîÑ Automation (CI/CD) is a must.
In analytics and life, you will never achieve
consistency
over time (
which is essential
) without automation via GitHub Actions, CircleCI, or, Lord help you, Jenkins.
How are your data assets, and whatever produces them, deployed to production?
Want to iterate quickly to produce results faster? Automation.
Do you want to spend time working on what matters? Automate the boring stuff.
üóÇÔ∏è Source control is your friend.
There should never be an instance in which you provide a stakeholder data, no matter the
urgency
, in a manner or with code that is not checked into source control.
You know the questions will come. How do you produce this?
You, or someone else,
will
make mistakes.
Don‚Äôt want to be the sole person always ‚Äúresponsible‚Äù for every critical component?
Want to take the guesswork out of the game? Source control.
üß™ Testing is your friend.
Bugs are a part of life; we are only human after all, at least for now.
Unit tests will squash 80%+ of your bugs today.
Ensuring a development environment is available will raise the bar.
This testing mindset raises the bar of overall expectations.
üìä Own data quality.
No one wants to own it, so you want to be an invaluable part of the business and provide real value? Own and implement minimal data quality on core dataset(s).
Everyone has dirty data and comes to expect it.
No one wants ownership; you can show initiative.
Want to become the go-to person?
‚ö° Complexity is your enemy.
Fancy CTEs and long queries are not a sign of senior-level problem-solving. Suppose you can‚Äôt explain your process for producing a dataset or dashboard in a few minutes without encountering problems.
Houston, we have a problem.
Less is more (
code and logic
).
Want to deal with fewer bugs?
Want simple debugging and problem-solving?
Want to reduce costs?
üêç Learn something beyond SQL.
SQL is indeed the fountain of youth that never dries up; it‚Äôs like the Force that binds us all together. But¬†being adaptable and versatile, say, in writing
PySpark,
will only make you better.
Want to have confidence that you can solve any problem?
Want to love what you do because you‚Äôre a learner?
Want to be known as an expert?
Want to set yourself apart in that next job hunt?
üèõÔ∏è Have an architecture mindset.
Moving from using a tool to produce a thing, to understanding how multiple tools can come together to provide a
Platform
on which to build greatness ‚Ä¶
now there is a skill.
Who doesn‚Äôt want to be in the top 1% of data professionals?
Want to understand what‚Äôs happening around you at a fundamental level?
Would you like to
BUILD
analytical platforms that run the business?
üéØ Develop ruthless focus.
Work with data, do you? Yeah, a never-ending stream of interruptions that leaves you with unfinished work and embarrassment at the end of the Sprint.
No more
. Ruthless focus helps you finish a task on time, and over the long run, produce 10x what your peers will.
Tired of falling behind?
Would you like to get rid of your work-related stress?
Want to be known as someone who gets the job done?
Want to be given the largest and most impactful projects?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
That is the end of the matter.
What, you thought I was going to tell you to meet me under the old oak tree up the hill at midnight, throw salt over your left shoulder, and say an incantation?
Sorry to disappoint.
I assure you, success as an analytics professional varies just slightly, or not at all, from what makes success work for anyone in tech. We should all look to the simple, mundane things that foster good habits and a solid foundation for our own Analytics City on a Hill.
Let‚Äôs leave chasing shiny objects to the birds; you and I, let's turn to the old, tried, and true ways of bringing some solid best practices to the never-ending chaos that unfolds around us every day.
The answer to these age-old problems lies not in a new tool, not even in a new process, but in recognizing that we need to shift the very low-level, often-ignored ground on which we stand‚Äîthose things I listed above.
Things that seem far away from that shiny dashboard shining in all its glory.
But, these ‚Äúboring things‚Äù as they appear, can build you an analytics castle with a foundation of rock, not sand.
Embrace the new, challenge yourself to accept change, give it a try! If you don‚Äôt like it, you can always go back to your old, evil ways.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
172819547.honest-review-of-polars-cloud,Honest Review of Polars Cloud,from honest Abe,2025-09-19 08:51:15.232-05,"Well, we all knew it was coming, don‚Äôt lie, it‚Äôs how the open source and SaaS (
can you tell the difference
) world works these days. Some hot new tool comes along, DuckDB, Polars, etc, then shortly thereafter some SaaS offering pops up and the drama begins.
But, here we are whether we like it or not. Nothing left for it, but to step into the breach.
Today, we are going to do an honest review of
Polars Cloud
. We are are familiar with that Rust based Polars, the self-proclaimed Pandas killer.
It may be exciting, it might be boring, it will be what it is. Let‚Äôs just poke at it and see what happens.
Polars Cloud
So, one could ask, what is Polars Cloud and what problems does it solve, why does it exist?
I mean, it clearly has something to do with Spark, (
it always does
), and the budgetary fatigue of the Mt. Olympus of data processing, namely, Databricks and Snowflake.
This is the way of the data world, it happened with SQL Server, and it‚Äôs happening right now with Databricks.
Things get expensive, a veritable data processing monopoly happens, then enterprising Startups envision the ability to hack off corners of the marketshare.
Lest you think I‚Äôm lying, here
from the front page of Polars Cloud
, is their comparison of speed and cost to Glue (Spark on AWS).
There argument is, ‚ÄúW
e can process data faster and cheaper than You Know Who
.‚Äù
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
What about pricing?
I mean it‚Äôs never all about the money, but it‚Äôs always about the money ain‚Äôt it? Making those greenbacks sing and yell. That‚Äôs what makes those CTOs happy.
Many times the ‚ÄúSave money by processing faster < insert SaaS > tool have a small but valid point. But, Polars seems to be doubling down not only on speed,
but on cheap price per compute hour.
Not bad eh? .05 cents per vCPU/h.
That is surely a strike at the achilles heal of Snowflake and Databricks.
I mean this is interesting when you think about it.
same price per CPU/h across whatever compute you choose. (
so choose wisely
)
I mean compare that to Databricks DBU per hour costs of $0.55 for All Purpose Compute to $0.15 for Job Compute. Of course, you are getting A LOT of bang for your buck with the Databricks platform. Way more than just fast compute.
Unity Catalog + a plethora of top tier features, clearly you pay a premium for those things.
You get what you pay for in a sense.
Let‚Äôs try out Polars Cloud.
I know you‚Äôre probably tired of me pontificating now, so let‚Äôs just get to it, we can‚Äôt know what we are dealing with until we crack it open and give it a try.
First,
let‚Äôs register for Polars Cloud
.
After I verified my email, logging in for the first time, I am asked for my Organization Name, I will try to keep it all above board, but you know it‚Äôs tempting.
This is where I ran into my first error. Makes, no sense to me.
This is not a good sign. Hey, I tried not to be silly with the Org name, but maybe I should try something like PickleBob since it isn‚Äôt planing nice.
All I ended up doing was entering ‚ÄúDaniel‚Äù and finally that worked? I have no idea what the deal and message is around this ‚ÄúOrganization‚Äù part of the setup is. It wouldn‚Äôt take my other names I input and made up. No idea if it had to do with spaces in the name or something too long. Zero idea.
Ok, at this point I am inside a Workspace, which I can see from the URL routing. Next, we are going to lick on
Begin Setup
and see what happens.
I entered a
Workspace
name and click
Deploy for AWS
.
At this point we are routed to AWS to login.
At this point we are prompted to create a new Stack (
Cloud Formation
) inside AWS for Polars.
Next, we just sit around waiting for that to be done. Once that create is complete, I went ahead and refreshed that strange login page about the Workspace in Polars Cloud, and acutally got something that looks legit.
(I have to be honest here, the whole onboarding process is strange and you‚Äôre not sure if it‚Äôs going to work, with strange errors included)
At this point, there is no popup telling me what to do next, no workflow or anything. So your guess is as good as mine.
Funny enough, if I click on Dashboard in the left, I get this same page. But if I click on the ellipses on the right, I get another dropdown that says Dashboard.
After that I get something that looks like a Notebook with code.
If I click on Workflows on the left, I get a blank page. Not sure what‚Äôs supposed to be there.
At this point, I wish the docs had a high level Concepts page or something to pull everything together, but no such luck. Y
ou can see the docs here.
This is what I‚Äôm most interested in from their docs.
""Any Environment: Start a remote query from a notebook on your machine, Airflow DAG, AWS Lambda, or your server. Get the flexibility to embed Polars Cloud in any environment.""
     - Polars
I mean, to be honest, I‚Äôm clicking around the documentation and would love to see an example of connecting Airflow to this environment so I can see a production example of running a workflow.
Not happening.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
We gotta get the concepts down.
At this point, I‚Äôm a little at a loss, because I don‚Äôt want to necessarily just run Polars Cloud code locally, which I will probably just end up doing. I think to best suit us, since they (Polars Cloud) don‚Äôt give us a straight forward Concepts page, we (meaning me) are going to have to spend some time reading the docs and finding the core concepts out on our own.
1. Unified API & Serverless Compute
- Same API for local and cloud: Write Polars DataFrame code once; run it locally or remotely via LazyFrame.remote().
- Serverless engine: Automatically scale compute on CPU (GPU support coming soon), no infrastructure setup required.

2. Compute Context + Remote Execution
- Use ComputeContext(workspace=..., cpus=..., memory=...) to define compute environment.
- Chain .remote(context=ctx) to your LazyFrame to execute in cloud, then .sink_parquet(...) to store output.

3. Cloud Accessibility: ""Run Anywhere""
- Execute from notebooks, Airflow DAGs, AWS Lambda, or your server‚Äîflexible integration options.

4. Scalability, Resilience & Monitoring
- Distributed engine supports horizontal scaling for heavy workloads, with built-in fault tolerance.
- Provides monitoring and analytics tools to assess query performance and resource utilization.

5. Authentication Methods
- Supports short-lived access tokens, environment variables, service account credentials.

Workflow:
- authenticate() attempts cached or env var tokens (or browser login if needed).
- login() always opens browser for fresh login.
- For automation, use service accounts.

6. Organizational Features & Billing
- Multi-user org support: manage members, set up trial, billing, workspaces, teams.
- Flexible ‚Äúpay-as-you-go‚Äù pricing model: pay per vCPU per hour plus AWS infra costs; scales to zero when idle.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Dang, that‚Äôs a lot to take in.
Well, there‚Äôs nothing we can do about that, when we get a new SaaS tool, there‚Äôs always going to be some learning curve and confusion as we relate to other tools.
If you click on Settings on the left menu, we get a few good options that were mentioned nowhere else.
default cluster timeout (
set to 1 hour by default
)
ability to create
service accounts
set default
compute configuration
I‚Äôm going to lower the default cluster timeout to 15 minutes, set a small cluster as default, and create a new service account and get the creds from that setup.
Let‚Äôs make a pipeline.
There‚Äôs nothing for it now but to try and make a pipeline and see what happens. Hopefully nothing strange.
We will use
Divvy Bike Trip‚Äôs open source dataset, getting all of 2025 data
.
Also, let‚Äôs use the
astro CLI
to get a local Airflow environment for which we can write a small DAG, so we can at least get an idea of generally what a Production Ready setup might look like.
So, running some Astro CLI init commands, adding polars and polars-cloud the requirements.txt ‚Ä¶ blah, blah, blah. I came up with this first past DAG and Python code.
Not pretty, I tried to follow their example they give in the docs.
Honestly, the Polars code is pretty low-key, and the Polars Cloud part is only a few extra additions.
Well, all we need to do is run the DAG now, but honestly, we can get a good idea now of what we are dealing with when it comes to Polars Cloud, albeit a quick look.
Except it didnt‚Äô work. Also, when I checked the Workflows UI in Polars Cloud, I got nervous.
Someone tell me how a 3 retry DAG + Polars Cloud caused this many Clusters to be started.
Good Lord, watch your wallet.
Either way, it failed, and here is the errors.
On the positive side, we got it work pretty easy. Anywho, looking at the code, it‚Äôs obvious what I did wrong. Updated this line to be more accurate.
.sink_parquet(uri=""s3://confessions-of-a-data-guy/results"")
This time the DAG finished, but way too quick.
Looking at S3, clearly the results did not write, but there were no errors with Task. Also, the compute in the Workflows tab of the Polars Cloud UI shows what queries ran on that compute.
The only thing it dhows is ‚Äú
Starting planning query.
‚Äù
No idea what happened, what‚Äôs a guy to do? No errors from the polars-cloud Python package. The UI in Polars Cloud appears to have no logs that tell me anything.
I mean as far as I can tell, the calls are making it to Polars Cloud to run the code, since this is showing up in the UI, but as to what is going wrong? No idea.
No logs or errors for me to look at mean I can‚Äôt debug anything.
It says ‚Äú
Query received and queued
‚Äù, but nothing more.
The only thing left for me to try is to get rid of the ‚Äúdistributed‚Äù mode and simply try the single node processing remotely on Polars Cloud.
I can do this by simply removing the
distributed()
call.
in_progress \
            .remote(ctx) \
            #.distributed() \ <-- remove this
            .sink_parquet(uri=""s3://confessions-of-a-data-guy/results"")
Again, the DAG runs, but no results. This time I get two lines indicating query received and planning.
Part of me is wondering if it is because I‚Äôm not calling
.execute(),
I sort of assumed that calling a
.sink_parquet()
would force the computation.
in_progress \
            .remote(ctx) \
            .distributed() \
            .execute() \
            .sink_parquet(uri=""s3://confessions-of-a-data-guy/results"")
This is a no go.
I mean the
Polars docs shows two examples
, both of which I‚Äôm using verbatim.
result = (
      lf.remote()
      .distributed()
      .execute()
)

result = (
    query.remote(pc.ComputeContext(cpus=16, memory=64, cluster_size=32))
    .distributed()
    .sink_parquet(""s3://output/result.parquet"")
)
Ok, I‚Äôm ready to just move on.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Thinking out loud about Polars Cloud
Ok, that was alot to go through, but there really is no other way to actually kick the tires on something without going through the motions of trying something out.
It‚Äôs one thing to read the docs and get an idea, it always looks like sparkles and rainbows on the surface, until you try to do a thing.
Overall, I think I was expecting a little more out of Polars Cloud, but I‚Äôm not sure why I say that. Probably as someone who‚Äôs used to using Databricks and UI‚Äôs like Airflow, you come to simply expect a baseline level interacting with things.
My Pros and Cons.
Here goes nothing. Let the complainers complain.
Pros.
 - Easy to setup
 - Easy to integrate with AWS
 - Code is fairly straight forward
 - Simple, not a lot of features to confuse you

Cons.
 - Janky workflow when setting up
 - UI is very underwhelming
 - Documentation needs better ""Concepts"" section
 - Couldn't make it work or fail (no logs, errors etc)
That‚Äôs pretty much my takeaway after playing with it for a few hours.
The truth of the matter is that in the age we live in, we‚Äôve been piddling around with Airflow, Databricks, and Snowflake UI for many a year now. We‚Äôve come to expect a certain thing.
My personal opinion is that, although I might not care about a fiddly underwhelming UI, a lot of data people will. Data people are visual by nature.
If you decide to provide a lack luster UI, you‚Äôre going to pay the price in the eyes of the average data user. They are going to assume that sort of fly-by-night approach applies to the rest of the project.
And, in defense of this standpoint, as you saw above, I SORTA got the tool to work. What I mean by that is that I got everything wired up and set to work, but it didn‚Äôt actually work. Everything said it ran, but no errors or logs for me to review.
Sure, I‚Äôm new to it, but so is every single other person, and
first impressions matter.
Hey, before you yell at me, I‚Äôm a Polars user. I brought Polars into
PRODUCTION
in my current workplace years ago. It‚Äôs a great tool. I use it a lot. So, I‚Äôm writing this as a fan of Polars.
They need to work it, from both a UI and Developer experience. How can they teach someone new like me, to use the
remote()
and
distributed()
modes of Polars Cloud
NOT IN A HELLO WORLD MANNER!
The remote() and distributed() nature of this tool is supposed to be the whole selling point. It should work in a way that is bullet proof, and dummy proof for dummies like me.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
172289348.dataframe-to-postgres-fastest-option,DataFrame to Postgres. Fastest option?,into the Thunderdome,2025-09-15 07:36:00.411-05,"You know, if I think back all those many years ago when I first started playing around with PHP, Perl, MySQL and the like, some things have stayed amazing consistent.
Like very consistent.
Even when I graduated to VBA and C# (
heaven protect me
), found Python and decided to stick to that, one thing remained the same. I‚Äôve literally being doing this over and over gain. What?
Pushing
data/files/dataframes
into Postgres or MySQL. Even today, these decades later here I sit infront of Databricks (
Yes, I know about
LakeBase
), tell Spark to ‚Ä¶
Attention!
Because you‚Äôre all cheapskates and I spend my hard earned money burning AWS and Databricks compute for you ungrateful hobbits,
I‚Äôm offering 50% OFF a Yearly Subscription
to you my most ungrateful subjects.
$27.50 for a year of my genius??!!
Imagine that, still playing with JDBC after all these years, you would have thought the world could have moved on.
While I can appreciate a SIMPLE way to solve a problem, it is annoying when that simplicity leads to runtimes in the hours for datasets that are not very large.
It‚Äôs amazing what we put up with when we don‚Äôt know any better.
We do what we do until it becomes a problem.
There‚Äôs nothing inherently wrong with that.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
When things get slow.
So, I‚Äôve used the above PySpark / Spark option on a DataFrame using JDBC to push data into a relational database, usually Postgres, for years.
It‚Äôs simple, it works. I can appreciate a simple design. Less code is more, ya know? You better believe I‚Äôm gonna take a short gut when I can find one like
DataFrame.jdbc
Problem is, it‚Äôs slow as crap. Most of the time I just put up with it, what else is a guy gonna do? I got a lot of things to work on, and never time to work on nagging problems like this.
So sue me.
Problem was I was working with a very wide table with about 22 million records, not very many, but enough to slow the old DataFrame write in Postgres to a snails pace.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thinking through options.
The first thing we should do when, or I do, when coming up on a new problem that doesn‚Äôt necessarily seem novel (inserting data into Postgres), but the next step isn‚Äôt obvious, is to step away and consider all options.
new problem -> don't assume the answer -> consider all options -> test a few -> pick one
My first bent was to simply throw Spark out the window at this point, and reach for some vanilla Python. Maybe ‚Ä¶
dump the Dataframe to CSV/Parquet etc.
use Polars/Daft etc to dump to database.
I could have simply walked down this road, and it probably would have out performed Spark (who knows), but I decided to muddle on it for a little bit. See if anything interesting popped into the old noggin.
Then it hit me, a novel, to me, approach.
Don‚Äôt throw something away, use it to your advantage.
What if instead of throwing Spark out the window, or simple using only Python (not known for its quickness), I use them BOTH? Yes, what a novel idea, use them both instead of one or the other.
I didn‚Äôt feel like fighting Spark to make it work with JDBC, that‚Äôs been slow since I was just a whipper-snapper. I‚Äôve done plenty of INSERTs with Python,
many years ago I wrote this piece
.
Told you I‚Äôve been working on the same problems for awhile.
What do we know about the nature of Spark?
Well, we know Spark is a distributed system capable of running many processes at once. That in fact, is what makes Spark ‚Ä¶ Spark. How can we take advantage of this multi-processing nature?
If you think about our DataFrame made up of many partitions and chunks ‚Ä¶ why can we not MAKE Spark be creative and use this super power to get these records pumped in Postgres.
Something tickled the edge of my mind. What about
foreachPartition()
?
I don‚Äôt care in what order or how these records from a Spark DataFrame make it into Postgres, I just want them there as fast as possible so I can move on with life.
But what next?
Well, I do have plenty of experience writing Python methods to do just this, insert records from a thingy into the database. Why not combine that with the parallel processing of
foreachPartition()
?
Enter the code.
There is nothing amazing going on here.
read my Spark DataFrame (actually a Delta Lake table).
decide on a CHUNK size and how many WRITERS I want.
Repartition the DataFrame to match the number of WRITERS.
Write up the good ole‚Äô
psycopg
function to push a batch into the database.
Ensure the usage of
COPY from Postgres
.
way faster than INSERT
The secret sauce to this code is the combination of
COPY from Postgres
and
foreachPartition()
These my friend, are like a match made in heaven.
COPY
moves data between PostgreSQL tables and standard file-system files.
COPY TO
copies the contents of a table to a file, while
COPY FROM
copies data from a file to a table (appending the data to whatever is in the table already).
COPY TO
can also copy the results of a
SELECT
query.
‚Äì
Postgres
Here are some of the highlights.
COPY
vs
INSERT
(JDBC)
JDBC
.write
uses prepared INSERTs in batches. Every batch incurs parse/plan/execution overhead, per-row type binding, many round-trips, and transaction bookkeeping.
COPY
is a specialized bulk path inside Postgres. It parses a stream and stuffs pages as fast as storage and WAL allow‚Äîdramatically fewer syscalls and protocol messages than INSERT batches.
Parallel writers from Spark
df.repartition(WRITERS).foreachPartition(copy_partition)
gives you
WRITERS
concurrent
COPY
streams (one connection per partition). That lets you saturate CPU, I/O, and WAL bandwidth on the Postgres box‚Äîsomething a single JDBC connection can‚Äôt do.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What do I know? I just solve problems.
In my case the import time of the 22 million records (wide table), went from too long to under 14 minutes. That‚Äôs not bad ehh?
Nothing beats a little
parallel execution
and a
COPY
statement. Just the good old basics win the day every time. No rocket science required.
I left the well worn path of trying to be the smartest person in the room a long time ago. You get to a point where you realize this was never case, won‚Äôt be the case, and that‚Äôs all right. Your best life lays ahead of you, you are provide value with ‚Ä¶
experience
tenacity
delivering value on-time
being a team player
having a good attitude
caring about what you do
These seemingly simple things will will put you ahead of everyone else. If you are in the grinding Leetcode stage for glory, that‚Äôs ok, it will make a better programmer in the long run.
How do you get your data into Postgres or another relational database? What‚Äôs your favorite tool or method? Slow or fast?
Leave a comment"
172081123.medallion-architecture-truth-or-fiction,Medallion Architecture. Truth or Fiction?,let's rattle the cage,2025-09-08 08:03:44.657-05,"‚Äú
What is truth?
‚Äù Famous words said by an infamous man. When I have a
hot take
the first thing I do is to pray that the data gods will have mercy, and that I will not regret it, and if I do, may I be living in the woods when they come looking for me.
I've got an axe to grind today, you dear reader, come along with me and together we will pull down false idols from the high places. Today, the so called
Medallion Architecture
is in the crosshairs.
Before we get into the weeds,
whacking and cutting
, let me see the record straight.
If you run dbt and Airflow, read this before your next deploy. Cosmos is the OSS bridge between them. With 20M+ monthly downloads, data teams everywhere are turning dbt projects into Airflow DAGs for controllability, observability, and scale.
Join Astronomer‚Äôs webinar on September 25 to learn:
How Airflow orchestration unlocks control and visibility for your dbt models
How Cosmos runs dbt Core or dbt Cloud/Fusion as DAGs and Task Groups with just a few lines of code
Best practices and performance tuning for Cosmos at scale
Register now
Data Modeling
Data modeling and architecture find their value, worth, and beauty in the eye of beholder. Developers are amazing people, able to spin a web of code to accomplish literally anything.
All that to say,
you can run a successful Lake House platform with and without Medallion architecture
.
But, a recent
r/dataengineering
post brought to bare before my angry eyes, the utter barren wasteland such marketing speak can have on a fresh field of engineers who don't know any better.
We live in an age where the line between
honest
and
bonafide
technical reality
, is blurred and mixed up with marketing and sales engineering content.
To the point the numb masses drink from the fountain of poison and become victims of every snake oil hawked on their unsuspecting souls
.
Medallion architecture is bordering on the edge of pure malarkey, and proof I will deliver to you.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What is Medallion Architecture and from whence does it come?
The
Medallion Architecture
is a data design pattern popularized by
Databricks
. It organizes data into multiple layers (or ‚Äúmedallions‚Äù) to improve
data quality, performance, and usability
as it moves through the platform.
The layers are typically:
Bronze (Raw Layer)
Stores raw, unprocessed data.
Ingested directly from source systems (databases, logs, IoT streams, APIs, etc.).
Often includes duplicates, schema drift, and poor quality.
Purpose:
Immutable record of truth
‚Äì never delete, only append.
Silver (Cleansed Layer)
Data is cleaned, filtered, de-duplicated, and enriched.
Joins from multiple bronze sources may occur.
Business logic starts here (e.g., data quality rules, standard formats).
Purpose:
Curated, analytics-ready tables
.
Gold (Business Layer)
Data is aggregated and optimized for analytics, BI dashboards, and ML.
Domain-specific data marts (e.g., sales KPIs, fraud detection aggregates).
Purpose:
High-performance, business-consumable datasets
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Origins of the Medallion Architecture
Databricks coined the term
in the context of
Lakehouse architecture
(Delta Lake + Spark).
It evolved from
traditional data warehouse layering
(staging ‚Üí cleansed ‚Üí presentation).
The medallion metaphor (bronze ‚Üí silver ‚Üí gold) emphasizes
progressive refinement
and the
value add
at each stage.
While originally tied to
Delta Lake
, the concept is widely adopted across modern data LakeHouses (Iceberg, Hudi, etc.).
Visual of Medallion Architecture
Here‚Äôs a simple conceptual diagram:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ          GOLD              ‚îÇ
          ‚îÇ  - Aggregated tables       ‚îÇ
          ‚îÇ  - KPIs, BI, ML features   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ          SILVER            ‚îÇ
          ‚îÇ  - Cleansed data           ‚îÇ
          ‚îÇ  - Standardized formats    ‚îÇ
          ‚îÇ  - Joined across sources   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ          BRONZE            ‚îÇ
          ‚îÇ  - Raw ingested data       ‚îÇ
          ‚îÇ  - Unvalidated, append-only‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What else do you notice about the Medallion Architecture?
You know what else strikes me about Medallion Architecture, at least compared to other popular data modeling approaches like ‚Ä¶
Kimball
Inmon
Star and Snowflake schemas
There is never a discussion about the more technical details of the bronze, sliver, gold layers other than that ‚Ä¶ ‚Äú
This is raw, this is transformed can cleaned.
‚Äù
The fine details are essential just ignored, arguments are made for existence, not its implementation.
Let‚Äôs get grumpy.
Call me and old curmudgeon, whatever, it‚Äôs very possible I‚Äôm an old dog that simply can‚Äôt learn tricks, if that‚Äôs the case, bury me in the prairie.
But, one must look the the past, examine what has been accepted and used for decades, things that have worked in Production since before you were born.
This must be balanced with a healthy dose of ‚Ä¶ new things CAN be good and an improvement.
I shall propose to you that this so called ‚ÄúMedallion Architecture‚Äù is NOTHING more than Marketing Speak.
The earliest public mention I can find from Databricks is
June 2, 2020
, in a blog post on monitoring audit logs that references ‚Äú
our medallion architecture
.‚Äù
Databricks
Shortly after, other Databricks posts in
June and September 2020
also used the term and diagrammed the Bronze/Silver/Gold flow, confirming the concept was already in use by then.
Databricks+1
So: first public reference ‚Üí
June 2, 2020
(Databricks blog).
Let‚Äôs be clear about something, Databricks says ‚ÄúOUR
medallion architecture.‚Äù
Keyword our.
Drawing Lines.
That‚Äôs what it really comes down to from my perspective. Databricks is the GOAT at building new things, they also have excelled are marketing and communication.
When Databricks brought forth the Lake House, they WANTED an entirely new and catchy set of ideals and concepts to go along with it. Something to sink the hooks into their future Lake House users.
What better than to rename same already accepted Data Warehouse ETL patterns as
Medallion Architecture.
Can you blame? Maybe, maybe not. But it also has caused a whole host of downstream problems, people confusing things and getting the point where NO ONE even knows what a proper
Medallion Architecture
is.
All you have to do is read
this Reddit post and comments.
People are conflating
Data Contracts
with Medallion Architecture. Yes.
Data Contracts
.
And it continues, more confusion on what to do with this Medallion Architecture.
I mean, the confusion is obvious. People still read Kimball‚Äôs Data Warehouse Toolkit, and for the most part, model and talk about data using this tried and true principals.
The minute you decide to adopt the half-baked concepts from a SaaS company, yeah, things are going to get weird.
It‚Äôs all about Data Modeling.
The long and short of it is that
medallion architecture IS an approach to Data Modeling.
From this respect, maybe I should just leave Databricks alone, since Data Modeling is in the eye of the beholder. My problem with the whole thing is that we have lived with the Staging / Fact / Dimension / Data Mart nomenclature since the beginning of the Data Warehouse era.
It was, and is ‚Ä¶ really a TWO step program, sometimes THREE when called far, but %90 two stepper.
RAW -> FACT/DIM
You load raw data ‚Ä¶ you transform that raw data into its final state and then write to fact or dimension.
Done.
Oh, you want a Gold layer? It‚Äôs called Data Mart to store specific analytics used to feed Dashboards, etc. When it comes to the classic type of Data Modeling, there is all sorts of talk about ‚Ä¶
primary and foreign keys
data normalization and do-normalization
the grain of the data
etc.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
ETL Concepts in Classic Data Warehousing (Kimball Methodology)
Kimball‚Äôs approach to data warehousing emphasizes
ETL pipelines
that transform raw operational data into
star schema
or
dimensional models
optimized for analytics.
1.
Extract
Pull data from source systems (ERP, CRM, flat files, logs, APIs).
Capture both historical (full load) and incremental (change data capture).
Sources are diverse and often have inconsistent formats.
2.
Transform
Cleanse, standardize, and integrate data.
Apply
business rules
and
conform dimensions
(so Customer means the same across systems).
Handle deduplication, null handling, type conversions.
Create
surrogate keys
and maintain slowly changing dimensions (SCD Type 1, 2, 3, etc.).
Build fact tables with measures (sales amount, order count, etc.).
Classic Warehouse Layers (Kimball vs Medallion)
Staging Layer
‚Üí Similar to
Bronze
(raw ingestion).
Cleansed/Integration Layer
‚Üí Similar to
Silver
(clean + conformed).
Presentation Layer (Star Schema, Data Marts)
‚Üí Similar to
Gold
(business-ready).
Why Kimball‚Äôs Approach Mattered
Performance
: Optimized schemas for BI tools.
Usability
: Business-friendly star schemas (easy for analysts).
Consistency
: Conformed dimensions enforce enterprise-wide definitions.
Auditability
: Historical changes preserved with SCDs.
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ     Presentation Layer     ‚îÇ
        ‚îÇ  - Star Schemas (Facts +   ‚îÇ
        ‚îÇ    Dimensions)             ‚îÇ
        ‚îÇ  - Data Marts              ‚îÇ
        ‚îÇ  - BI / OLAP Reports       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Cleansed / Integration    ‚îÇ
        ‚îÇ  - Standardized Data       ‚îÇ
        ‚îÇ  - Conformed Dimensions    ‚îÇ
        ‚îÇ  - Business Rules Applied  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ        Staging Layer       ‚îÇ
        ‚îÇ  - Raw Extracted Data      ‚îÇ
        ‚îÇ  - Minimal Processing      ‚îÇ
        ‚îÇ  - Audit Trail             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ       Source Systems       ‚îÇ
        ‚îÇ  - ERP, CRM, Flat Files    ‚îÇ
        ‚îÇ  - APIs, Logs              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Proof people do whatever they want and call it what they want.
In truth, what Databricks was calling the Gold layer was really a ‚ÄúData Mart,‚Äù which was a further aggregation of analytics based on a Fact table. However, in the real world, you would have numerous fact tables that feed all the major reports and downstream consumers, with only a few consumers requiring further aggregation in the form of a Data Mart.
What happened was that the writers of
medallion architecture
at Databricks simply didn‚Äôt have the proper background and experience to understand these nuances. They sold the idea that you HAD to have three layers, Bronze, Silver, Gold for every dataset (
You do realize that this required more storage and compute, which goes directly into the pockets of Databricks, right??
)
You should model your data in your Lake House the same way it was modeled in the Data Warehouse. It works quite well and is not confusing, and based on marketing material, about about three decades of proven use.
load raw data
transform raw data into Fact or Dimension,
create Data Marts of aggregation where needed.
What, you think this IS
medallion architecture?
Well, you hobbit, which came first, Kimball or Databricks? All Databricks managed to do was mislead a generation of developers and confuse them, which is front and center in this
Reddit post
.
I still can‚Äôt get over the fact that people are now conflating Data Contracts with the medallion architecture (data modeling). This is hilarious. They deserve each other and the misery each of them bring to the other.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Look, beauty is in the eye of beholder.
At the end of the day people are going to model data however they want, mostly using what they find on the internet, YouTube, etc. It‚Äôs not like they teach you much data modeling in school.
If you‚Äôre lucky someone will teach you normalization, but that‚Äôs about it.
What, you think Databricks was the first SaaS company to co-opt some well accepted and popular concept, twist it to fight a narrative, and move on with life, be-damned the consequences?
In the Modern Data Stack world, this is an every day occurrence. I think the real lesson here is to tread lightly and carefully when following lemmings off a cliff.
You might end up like a Redditor thinking that Medallion Architecture is about Data Contracts.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
171417184.behold-shortcircuitoperator-apache,Behold. ShortCircuitOperator. Apache Airflow.,I was today years old when ...,2025-09-01 07:12:21.567-05,"It‚Äôs challenging to keep up with the rapidly changing landscape of tooling, let alone the evolving features and updates across the broad spectrum of Data Engineering tooling.
Apache Airflow is no different.
We use it. We love it. Once every year or two, we might upgrade a version if we are feeling spicy.
Truth is, just like SQL, Spark, or whatever ‚Ä¶ we fall into, or learn to write DAGs in a certain way and simply move on with life and Data Engineering, we work on more interesting things.
This sums up my relationship with Apache Airflow. Use it, abuse it, and mostly ignore it,
writing my DAGs in the same way
I learned to do so¬†many years ago.
This is good and bad. Good in the sense that a tool like Apache Airflow is so mature and well-oiled that you don‚Äôt have to worry much about breaking changes; you can learn the fundamentals and move on.
Subscribe now
The downside of this approach is that, although you may become proficient in a tool like Apache Airflow, you will not be an expert. As a result, you will likely overlook many valuable features, leaving them unused.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Does it matter how we solve a problem?
As one grows older and perhaps a little wiser in the way of Software magic, you tend to stop worshiping at the altar of complexity and code itself. Don‚Äôt get me wrong, we do what we do because we appreciate a beautiful and elegant solution when we see one.
But, we aren‚Äôt, or shouldn‚Äôt be, so niaeve to belief that we ourselves are a fount of knowledge and wisdom, the only one‚Äôs able to design a solution.
There are a few hallmarks of a junior engineer so blaringly obvious
as the person who spends all their time and energy obsessing over obscure technical details related to something ‚Äúthing.‚Äù
The business rarely cares how, only results.
Real rubber meets the world's software solutions, and people recognize the following universal truths.
It‚Äôs typically a waste of time to argue semantics
Software is an iterative process
There is no perfect solution ever
Tradeoffs must be made
Ignoring the business context of what you're doing is stupid
All this to say, I typically don‚Äôt spend a lot of time (unless the nature of the problem is critical) worrying about the HOW a problem is solved. One must consider the larger context in which a change is happening.
I won‚Äôt argue about the symatics of HOW you do a thing, just that you do it, do it well, in a timely fashion that fits into the overall picture.
Let‚Äôs talk about examples with Apache Airflow.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Solving an Apache Airflow problem.
The problem is simple: you have an Apache Airflow DAG with two related tasks, where the second is dependent on the first.
task_1 (daily) >> task_2 (monthly) >> done
But,
task_1
runs on a daily schedule,
task_2
runs on the 1st of every month.
task_2 is always dependent on task_1 being complete
task_2 runs monthly while task_1 is daily
I mean, is this not the classic problem that comes up every single day in various forms and various ways? There are a million ways to solve a problem;
this is no different
.
However, how we choose to solve the problem and what we consider as options is highly dependent on ‚Ä¶
How familiar are we with Airflow
Our past experiences
The timeline of the larger project in which this question arises
Is there a right answer, is there a perfect answer, should we even care??
Maybe we are familiar with and decide to use the Branch Operator.
def is_first_day_of_month(execution_date, **kwargs):
    return ""task_2"" if execution_date.day == 1 else ""skip_task_2""

branch = BranchPythonOperator(
    task_id=""branch_task"",
    python_callable=is_first_day_of_month,
    provide_context=True
)

skip_task = DummyOperator(task_id=""skip_task_2"")
done = DummyOperator(task_id=""done"")

task_1 >> branch
branch >> task_2 >> done
branch >> skip_task >> done
Maybe we should consider separating them and using two DAGs instead.
Daily DAG (task_1 only)
@dag(schedule_interval='@daily', ...)
def daily_task_1_dag():
    task_1 = ...
Monthly DAG (task_2 waits for task_1)
ExternalTaskSensor(
    task_id='wait_for_task_1',
    external_dag_id='daily_task_1_dag',
    external_task_id='task_1',
    execution_delta=timedelta(days=1),  # wait for previous day's success
    mode='poke',
    timeout=600
)

task_2 = ...
wait_for_task_1 >> task_2
Or, maybe we could use TaskFlow with Conditional Logic
@task
def maybe_run_task_2(execution_date=None):
    if execution_date.day == 1:
        run_task_2()
    else:
        print(""Skip task_2"")

task_1 >> maybe_run_task_2()
I don‚Äôt know, they all felt a little dirty to me, but that‚Äôs just my personality, sometimes things don‚Äôt just feel right. The problem is, I‚Äôm an old school Aiflow user of many years, I‚Äôve settled into my ways, and I simply don‚Äôt pay attention to ‚Äú
what is happening with Airflow.
‚Äù
The truth is, sometimes I simply don‚Äôt know what I don‚Äôt know, but that‚Äôs ok. Can a person keep up with the best practices of everything? Not someone my age. I stopped caring about
appearing as the smartest person
in the room, a sure sign of mediocre skills.
What did I end up doing?
I wanted the
cleanest and most Airflow-native solution
:
Keep all tasks in the same DAG
Avoid multiple DAGs or external sensors
Avoid weird scheduling conflicts
I wanted something obvious and not obtuse.
Then I realized my lack of caring about Apache Airflow for a few years had caught up with me, and it was time to do some reading. Enter the
ShortCircuitOperator
.
‚úÖ How
ShortCircuitOperator
Works
It
evaluates a condition
and:
If
True
: downstream tasks
run
If
False
: downstream tasks are
skipped
üëá How to Use It in Our Case
You want
task_1
to run
daily
, and
task_2
+
done
to only run
on the 1st of each month
,
but still depend on task_1 being successful
.
Here‚Äôs how to implement that:
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from datetime import datetime

def is_first_of_month(execution_date, **kwargs):
    return execution_date.day == 1

with DAG(
    dag_id=""daily_task1_monthly_task2"",
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
) as dag:

    task_1 = PythonOperator(
        task_id=""task_1"",
        python_callable=lambda: print(""Doing task 1""),
    )

    check_if_first = ShortCircuitOperator(
        task_id=""is_first_day"",
        python_callable=is_first_of_month,
        provide_context=True,
    )

    task_2 = PythonOperator(
        task_id=""task_2"",
        python_callable=lambda: print(""Doing task 2 (monthly)""),
    )

    done = DummyOperator(task_id=""done"")

    task_1 >> check_if_first >> task_2 >> done
üîç What This Does
‚úÖ Pros
All logic is in one DAG
No DAG scheduling conflict
Easy to test and reason about
Keeps
task_2
correctly dependent on
task_1
And there you have it. Who‚Äôs to say any of the other options, or options I have no idea exist, because I‚Äôm simply not an Airflow savant, are not acceptable?
They are. Just depends on the context.
What makes one solution better than another in most cases is simply in the eye of the beholder.
Should we argue about how we get from Point A to B unless we are sending rockets to the moon? Probably not.
Software becomes a liability the moment it‚Äôs pushed to production.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
More musings on life and software.
It‚Äôs somewhat amusing to sit back and think about my younger self, the one who cared only about writing the perfect code, learning obscure things, and checking behind every rock. You know what?
I have a feeling that sort of thing is a natural part of ‚Äúgrowing up‚Äù in your software writing career.
My children have to fail and learn hard lessons; it‚Äôs an important part of life. I spent years obsessing over writing perfect code, defending my pet ideals, and thinking that code was the end-all, be-all.
It was natural for me to do so, and natural for others to do so.
Now I‚Äôm more pragmatic. The nature of my job requires me to be more aloof to such trivial arguments. I have to worry about delivering to the business a product that is stable, works, is on time, and meets the requirements.
That sort of shift requires a person to simply look with disdain upon the zealots that rant and rage about the semantics of solving a problem a particular way.
I don‚Äôt care about your perfection; I care about what matters in the end. Results.
Get there as you like.
The truth is that whatever you're working on right now could be worked on by 1000 other engineers who would solve it in a different manner, yet deliver the same result;
you should take that to heart
.
You are not, and never will be, the smartest and best writer of code. If you
care too much
about the technical details, the perfect answer, and doing things ‚Äúyour way,‚Äù then AI will indeed swallow you up whole, and you deserve your fate.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
171280287.polars-gpu-execution-70-speed-up,Polars GPU Execution. (70% speed up),to the moon,2025-08-25 09:33:17.787-05,": TDLR
Just when you thought that Rust-based GOAT data processing tool known as Polars could not get any faster ‚Ä¶ they go and add GPU to the mix.
‚ÄúOne of the
core philosophies of Polars
is to utilize all available cores on your machine. One type of computation core that makes up a lot of the compute power is in Graphics Processing Units, or GPU.‚Äù
- Polars
So, this either means everything or nothing. Not sure. It‚Äôs hard to know when new stuff like this comes up if the masses are the target (
the everyday Polars users
),
or some massive customers running AI and ML GPU workloads inside data centers.
The Polars GPU engine now supports datasets larger than GPU VRAM seamlessly using Unified Virtual Memory, thanks to smart memory management via RAPIDS‚Äô RMM. You can process massive datasets that exceed VRAM, at only a moderate performance trade-off‚Äîespecially when using pooling and prefetching. And it's all configurable with minimal code changes.
I mostly care about
PERFORMANCE
in the context of a normal Polars user like me.
Can this GPU execution help workloads in general? Does it speed things up? Does it slow things down??
Join Beyond Analytics
, a virtual DataOps conference presented by Astronomer, on September 16 for a half-day of virtual sessions where data leaders share how modern DataOps and orchestration are powering AI, ML, and production-grade data products beyond analytics.
What to expect
:
Apache Airflow 3 Crash Course with Marc Lamberti to prep for the certification exam (includes live Q&A + $150 free exam code)
Hear from data leaders at Ramp, Laurel AI, Airbrx, and more on how orchestration is driving AI adoption, powering GenAI pipelines, and connecting data strategy directly to business outcomes
Register now
GPU processing with Polars?
This is going to be a little weird to test, and I‚Äôm no expert, just a boy in a cornfield playing around. I have an M4 MacBook that we can use for the first test. Depending on how that goes, we can also try spinning up some GPU resources on
Linode
and give it a test.
So, this whole GPU processing thing appears to be super straightforward with Polars, as far as I can tell. Well, simply until you start looking at the
Benchmarking code on GitHub
.
But at least the benchmarking code gives us somewhere to start. Let‚Äôs just start with a benchmark query in Polars we can run a few times and get a baseline of performance.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The Baseline.
I want to talk as much variability out of my homegrown test as possible, so let‚Äôs download a bunch of data locally, and skip s3 to take network IO out the picture.
Let‚Äôs use
Divvy bike trip data, from 2025 so far
.
Also, in case you are not already, you should, like me, be using UV to make Python development suck less.
uv init polarsGPU
cd polarsGPU
uv add polars
uv venv
source .venv/bin/activate
Next, let‚Äôs just count the number of records we are dealing with, how many rides have happened on Divvy in 2025 so far?
2,904,857
That ain‚Äôt going to cut it Sunny Jim, let‚Äôs pivot towards the
Backblaze Harddrive dataset, it‚Äôs free also
, and much larger. Appears we can look at all of 2024 and get about 40GBs on disk to play with.
That should be enough to maybe tease out some Polars GPU vs Non performance if we are lucky. Time will tell.
Cracks in Polars already.
Ok, so we have 4 subfolders (one for each quarter) with 90+ CSV files in each other them. 40GBs in total.
What do you think the chances are that the schema over 400ish files match across all those CSVs? About zero.
So what happens polars when you do a simple read?
Yeah, well, of course they are different you hobbit. Apparently, excuse my lack of knowledge of Polars, you can‚Äôt simply MERGE schema mismatches.
You can see the maturity (lack thereof) of the product here.
Well this is dumb. What other option do I have than simply create my own schema and pass it in?
This sucks and I would prefer not do this, but this is the schema.
But, if we look through the Polars code on GitHub for these methods for reading CSV files, there is no options for merging schemas that I can see.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Digging Deeper
After digging around a little harder on Google, I found this
little open Issue on the Polars GitHub
, that seems to be similar, with a nasty ole‚Äô solution.
Here is that modified code for my use case.
No go, still fails.
polars.exceptions.ComputeError: schema lengths differ

Resolved plan until failure:

	---> FAILED HERE RESOLVING 'sink' <---
Csv SCAN [data/data_Q1_2024/2024-01-22.csv]
So, back to the drawing board. I used a little munging around to figure out which files in which folders had the ‚Äúextra‚Äù columns. Adding this ‚Äúextra‚Äù columns into the original schema and code will hopefully let the code run.
Opp. OOM errors, of course, I‚Äôm sure the
.collect()
tries to materialize the entire 40GBs into memory. The scan_csv() we are using is
Lazy
in nature.
Let‚Äôs just select a single column, the first one,
collect()
that and
count()
.
And results!!!
Hello from polarsGPU
Found 366 CSV files
Testing schema with first file...
Testing with: data/data_Q4_2024/2024-11-02.csv
Now scanning all files...
Total records across all files: 105,379,761
Jezz, took a while to get there. Let‚Äôs do a simple aggregation and get a baseline for Polars‚Äô performance.
Polars aggregation performance baseline.
So, we will write a simple aggregation for Polars that we can run and get a good baseline of performance, so when we get to GPU execution, we can see if any real difference is made.
To avoid any local cache of data that the operating system will do, I will run the code twice, discard those results, then run it three time again and log the performance metrics.
There you go, 40ish seconds for vanilla Polars doing 40GBs worth of aggregation.
GPU exceleration with Polars.
Ok, so here comes the hard part, can we get this to work, or has this all been for naught. That‚Äôs nothing new to me, that‚Äôs the price you pay when you poke at new stuff.
uv add ""polars[gpu]""
And our first error.
Read, read, read ‚Ä¶
uv pip install ""polars[gpu]"" --extra-index-url=https://pypi.nvidia.com
Success.
So how do we use the power of the GPU with Polars.
‚ÄúDesigned to make processing 10-100+ GBs of data feel interactive with just a single GPU, this new engine is built directly into the Polars Lazy API ‚Äì just pass
engine=""gpu""
to the
collect
operation.‚Äù
-
Polars Notebook
I am skeptical that this will will either work, or do anything ‚Ä¶ but if anyone can pull this off ‚Ä¶ it‚Äôs the Polars team.
Note:
They said we have to use Lazy Polars API, which we were already doing.
This is the only thing I changed in the code. Could this work??
You thought it was just going to be like that? Sucker. The data god‚Äôs require more blood than that.
polarsGPU) danielbeach@Daniels-MacBook-Pro-2 polarsGPU % uv pip install cudf-polars-cu11==25.06 
  √ó No solution found when resolving dependencies:
  ‚ï∞‚îÄ‚ñ∂ Because the current Python version (3.9.21) does not satisfy Python>=3.10 and cudf-polars-cu11==25.6.0 depends on
      Python>=3.10, we can conclude that cudf-polars-cu11==25.6.0 cannot be used.
      And because you require cudf-polars-cu11==25.6, we can conclude that your requirements are unsatisfiable.
Ok, I noticed we are running Python 3.9X
Because the current Python version (3.9.21) does not satisfy Python>=3.10 and cudf-polars-cu11>=24.10.1 depends on
      Python>=3.10, we can conclude that cudf-polars-cu11>=24.10.1 cannot be used.
Let‚Äôs get Python 3.10+ in our UV environment.
uv python install 3.12
uv python pin 3.12
All this seems to have NOT solved the
cudf-polars
issues with versions, it still pukes.
uv pip install --extra-index-url https://pypi.nvidia.com cudf-cu12
hint: `cudf-cu12` was found on https://pypi.nvidia.com/, but not at the requested version (all versions of cudf-cu12). A compatible
      version may be available on a subsequent index (e.g., https://pypi.org/simple). By default, uv will only consider versions that are
      published on the first index that contains a given package, to avoid dependency confusion attacks. If all indexes are equally trusted, use
      `--index-strategy unsafe-best-match` to consider all versions from all indexes, regardless of the order in which they were defined.

      hint: Wheels are available for `cudf-cu12` (v25.8.0) on the following platforms: `manylinux_2_24_aarch64`, `manylinux_2_24_x86_64`,
      `manylinux_2_28_aarch64`, `manylinux_2_28_x86_64`
Is there anything worse than fighting with Python dependencies?
I should have done more reading at the start. Sounds like CUDA stuff simply isn‚Äôt available on MacOS Hardware, probably obvious enough if you are paying attention more than me.
Back to good ole‚Äô
Linode
.
I had better work fast at the price of .50 cents per hour.
We are using an RTX4000 Ada x1 GPU-accelerated machine. T
he things I do for you.
Well, all the installs went fine with Python 3.12 on this GPU Linux box.
(polarsGPU) root@localhost:~/polarsGPU# uv pip install ""polars[gpu]"" --extra-index-url=https://pypi.nvidia.com
Resolved 16 packages in 246ms
nvtx                 ------------------------------ 476.46 KiB/532.80 KiB
rmm-cu12             ------------------------------ 276.78 KiB/1.08 MiB
Installed 16 packages in 34ms
 + cuda-bindings==12.9.2
 + cuda-pathfinder==1.1.0
 + cuda-python==12.9.2
 + cudf-polars-cu12==25.8.0
 + libcudf-cu12==25.8.0
 + libkvikio-cu12==25.8.0
 + librmm-cu12==25.8.0
 + numpy==2.3.2
 + nvidia-ml-py==13.580.65
 + nvtx==0.2.13
 + packaging==25.0
 + polars==1.31.0
 + pylibcudf-cu12==25.8.0
 + rapids-logger==0.1.1
 + rmm-cu12==25.8.0
 + typing-extensions==4.14.1
Now we have to wait while I scp all the data (
40GBs of CSV
) to that machine. Check back in an hour when the almost 400 files are uploaded.
scp -r data root@172.234.198.146:/root/polarsGPU/data
Forget that, let‚Äôs get the zip files and put those up there, then extract them.
wget https://f001.backblaze2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2024.zip
... etc, etc
Once we got 'em all, extract 'em‚Ä¶ etc.
for f in data_Q3_2024.zip; do
    folder=""${f%.zip}""
    unzip -d ""$folder"" ""$f"" && rm ""$f""
done
HEY!
You bored yet??!! I‚Äôm just showing you what it takes to give you the real juice in these articles. You want fluff, go read someone else‚Äôs article doing ‚Äú
hello world
,‚Äù
this is real life.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Round Two.
Ok, so now we have the files on our GPU-accelerated Linux box, let‚Äôs re-run the baseline tests and see what happens. (A
gain, I ran the command a few times to get data caching over with, and recorded the last 3 runs
)
Basically, the times didn‚Äôt change at all vs my Mac.
Ok, how about adding in our GPU acceleration, now that we are using a NIVIDA machine??!!
You knew that was coming, didn‚Äôt you? I almost thought it was going to work, you know? Almost forgot we can run this ‚Ä¶
pip install --extra-index-url=https://pypi.nvidia.com cudf-cu12
No go, still fails with the same error.
Good Lord, here comes some mud being thrown at the wall, see what sticks.
sudo apt install nvidia-driver-535 nvidia-utils-535
Ok, so we have NVIDIA drivers showing now.
I do have a question for you, though. If you were Linode and provided someone with a GPU Nvidia-accelerated machine, would you ensure the drivers were installed on that machine?
Just saying.
Hobbits
.
Here goes nothing.
uv run main.py
BY THE HAMMER OF THOR!
Holy crap.
(polarsGPU) root@localhost:~/polarsGPU# uv run main.py 
Hello from polarsGPU
Now scanning all files...
shape: (366, 2)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ date       ‚îÜ failure_count ‚îÇ
‚îÇ ---        ‚îÜ ---           ‚îÇ
‚îÇ date       ‚îÜ i64           ‚îÇ
‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°
‚îÇ 2024-10-17 ‚îÜ 17            ‚îÇ
‚îÇ 2024-06-26 ‚îÜ 11            ‚îÇ
‚îÇ 2024-05-13 ‚îÜ 12            ‚îÇ
‚îÇ 2024-08-26 ‚îÜ 24            ‚îÇ
‚îÇ 2024-11-17 ‚îÜ 8             ‚îÇ
‚îÇ ‚Ä¶          ‚îÜ ‚Ä¶             ‚îÇ
‚îÇ 2024-10-06 ‚îÜ 8             ‚îÇ
‚îÇ 2024-01-18 ‚îÜ 11            ‚îÇ
‚îÇ 2024-06-25 ‚îÜ 15            ‚îÇ
‚îÇ 2024-09-12 ‚îÜ 9             ‚îÇ
‚îÇ 2024-03-04 ‚îÜ 14            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Time taken: 13.280710220336914 seconds
What the actual Black Magic?
That‚Äôs insane. That‚Äôs like a 70% performance increase using Polars GPU with a simple
collect(engine=‚Äùgpu‚Äù)
.
Not going to lie, wasn‚Äôt expecting that.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
So what‚Äôs the TDLR?
I'm not sure, but it‚Äôs hard for the excitement to wear off, leaving my brain clear enough to think straight and make more grounded judgments. I‚Äôm already wondering in my mind ‚Ä¶
‚ÄúWhat‚Äôs the cost look like if you dropped some Databricks compute and replaced it with some short-lived GPU compute to do some big data processing on GPU-accelerated Polars?‚Äù
No one would bother if the speed improvements were 10-20% even, you just stick with what you got. But 70% speed up?
It makes a Data Engineer wonder and stare into the sky.
GPUs are notoriously expensive to run, but you know, turn something on and off quick-like, do some crunching, how bad could it be?
I don‚Äôt know what the future holds, but it looks bright for Polars."
169531062.json-in-a-lake-house-world,JSON in a Lake House World,Delta Lake style,2025-12-01 07:21:36.746-06,"I often look back on the days of yore, long before we lived in Lake House Land and had Data Warehouses with JSON or JSONB columns that were just another column in the database.
Life has gotten more complicated since then.
If I think back far enough and hard enough, I can remember cutting my teeth on a ‚ÄúData Lake.‚Äù It was a beautiful and terrible thing, millions of JSON files deposited daily into S3 buckets. What more could a lonely Data Engineer ask for?
Spin up the Glue Crawler, kick off a Glue Job, un-nest that nasty old JSON, and pile it into Parquet files. Sprinkle a little Athena on top and wa-la, Dashboards ‚Ä¶ beautiful Dashboards.
Such were the days of old.
Is that all ye ole‚Äô JSON is good for these days too?! Simply a necessary communication protocol for all those silly APIs, only to be unwrapped and flattened like some nasty old frog that‚Äôs been cooking on the highway pavement for a month?
What do we do with our JSON in the Lake House world?
And then, in the midst of writing this, the
VARIANT data type
was released. Only to add to the confusion. Both Delta Lake and Iceberg now have VARIANT.
What do you think we should do with it? I have no idea, honestly.
I‚Äôve always been one of those wicked flatteners of JSON.
It dies an untimely death when it hits my desk.
It isn‚Äôt that I don‚Äôt
want
to store it, it‚Äôs just that I don‚Äôt want to extract it later once it‚Äôs been written.
Deal with the pain up front, hard things first my mother always said.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Business and Engineering Context of JSON
I just want to make a point at the start of this, maybe it will save me angry comments, doubtful, but hope springs eternal.
Dealing with JSON in the Lake House and Analytics world is in the eye of the beholder.
The pea doesn‚Äôt go away; you simply must decide where to hide the (
complexity
) pea.
The complexity of JSON must be dealt with somewhere
JSON data doesn‚Äôt ‚Äúmesh‚Äù well in the tabular world
JSON is very common
Decide what to do based on your context
At the end of the day you either deal with JSON upfront, in the middle, or at the end. Somewhere, someone is going to have to do something with it. So just pick the unlucky person or spot.
You can deal with JSON at three points.
upon
ingestion
upon
transform/storage
upon
consumption
The JSON piper will come a calling, and someone must open the door and let 'em in. All of them are probably equally valid depending on the tools and use case.
Best to consider things upfront, though, rather than wait till it‚Äôs too late.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
JSON in a Lake House context.
This is something I haven‚Äôt had the opportunity to deal with much in my Lake House life. Only once have I stored a ‚ÄúJSON‚Äù dataset in my Delta Lake table ‚Ä¶ and guess what I used as the data type?
That‚Äôs write. Say it. Say it louder.
Say it!!!
STRING
Deal with it.
But the better question is, what are our options, and what do we do with the JSON data that we are storing in the first place?
What JSON options are available in Delta Lake and Iceberg?
Why are we storing JSON in the first place?
I don't know the answer to the first question, so we'll learn together. I do know the answer to the second question and have strong opinions.
Surprise, surprise.
Delta Lake (for JSON)
I have no idea, up front, how or whether we should store JSON data structures in Delta Lake or, for that matter, Apache Iceberg.
Part of me thinks it has a lot to do with the tools and frameworks that consume and use that data.
My personal opinion would be simply flatten it straight out, but let‚Äôs pretend we can‚Äôt.
Oh, and FYI, there is a
WHOLE lotta stuff we CANT do with a VARIANT data type in Delta Lake
.
- You cannot use variant columns to partition a table.
- A variant column cannot be a clustering key for a table.
- You cannot use variant columns with GROUP BY or ORDER BY clauses.
- You cannot call DISTINCT on a variant column.
- You cannot use SQL set operators (INTERSECT, UNION, EXCEPT).
- You cannot use column generation to create a variant column.
- Delta does not collect minValues or maxValues statistics.
- A variant column cannot contain a value larger than 128 MiB
Jezz, that‚Äôs a lot of cants, sounds like my wife when I want to head to the woods when the weekend comes. Can‚Äôt, can‚Äôt, no, no. Let‚Äôs get one with it and create a table.
Here is a Delta Table with a defined
VARIANT
, and here is an example of inserting JSON data into that table.
Hot dang, that was easy.
One could see how that would be useful in storing ragged data structures. Turns out we have a few options to query that JSON/
VARIANT
data that are super easy to use, and make sense (
the important part
).
:
to select top-level fields.
.
or
[<key>]
for nested fields.
[<index>]
to select values from arrays.
So let‚Äôs give it a try.
Well, that seems obvious and easy to do; you have a few options in how you want to query the JSON, pick your poison, I guess. No strange explode code or whatever.
Herein lies the problem:
according to the docs, you can‚Äôt really use the VARIANT column for anything useful. Say we were storing our customer address information as such, we can‚Äôt even use it in a GROUP BY.
So what use is it? Very little.
Yes, the
VARIANT
data type is slicker than a whistle for dealing with JSON, but what‚Äôs the point if you can‚Äôt use it for anything???
What did I tell you about simply storing JSON as a STRING??!! You probably laughed at me. Spit at me. Threw rocks at me. Look ye‚Äô here, you little pickle.
Just a STRING with an identifying schema, a little GROUP BY. There you have it.
Who‚Äôs laughing now??
That also made me wonder, what if we store as VARIANT, cast to STRING, and GROUP BY to our hearts‚Äô delight? Bypassing our ever-watching overlords.
Well, that works. Jumping hoops, I guess.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
More thoughts on JSON in the Lake House.
I still am not sure if I‚Äôm a fan of this whole JSON thing; maybe it‚Äôs just my scars from the past years of pain and suffering. The thing is, I‚Äôm sticking to my guns; you can‚Äôt hide the pea forever. Eventually, someone is going to feel the bump and strangeness of working with JSON in a Lake House world of tabular data.
Methinks the Data Engineers who are ingesting the data upfront should be the ones to deal JSON a death blow from the start. I get that VARIANT is sweet and nice, I have to admit it, but you can‚Äôt even GROUP BY one of those data points. It‚Äôs essentially useless.
But that‚Äôs just my dogma, I wouldn‚Äôt complain that much if you are a lover of JSON and think that workarounds to use columns like VARIANT are worth it, just shove that JSON down Delta Lake‚Äôs throat and move on.
Let someone else deal with the consequences."
169491609.data-deduplication-for-dummies,Data Deduplication for Dummies,you dummy,2025-08-04 07:32:19.715-05,"Hey dummy! Why did you get duplicates, you dummy?! What‚Äôs the matter with you??
You know, after literally multiple decades in the data space, writing code and SQL, at some point along that arduous journey, one might think this problem would be solved by me, or the tooling ... yet alas,
not to be.
Regardless of the industry or tools used, such as Pandas, Spark, or Postgres, duplicates are a common issue in pipelines, and SQL remains the most classic and iconic problem. Things just never change, and humans never learn their lessons, at least I don't.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Solving the duplicate record problem.
Honestly, there is a never ending list of vendors who would love to sell you some fancy
Data Quality and Monitoring
tool that will solve every problem, including duplicates, except it wont. Duplicates have been a problem since the Data Warehouse days of SQL Server and SSIS. Duplicates are still a problem with a Modern Data Lake House architecture using
Delta Lake
/
Apache Iceberg and Spark.
Technology can rarely solve our data problems like we want. Duplicates will be with us forever.
Best get used to it.
Instead of making it complicated, which we can, I think there are some straight forward ways to deduplicate data, any data set, in any system, that are accessible to everyone.
Yes, each tool we might use probably has built in deduplication methods which can be helpful, but it's also help to take our duplicate problems back all the way into the dataset, instead of making the data processing tools do all the hard work.
Here is how you should solve your data duplicated.
every dataset should have a primary key
every dataset when processed should drop duplicates based on the primary key
Yes, there are some nuances hidden in here, but this applies to 95% of datasets and tools, regardless if you are on Snowflake, Databricks, Pandas, Polars, or DuckDB.
Primary Keys
Ok, the age old solution that no one uses, especially in the Lake House world. Pretty much everyone who works in data is familiar with primary keys, at least in concept, yet they are not widely used outside of RDBMS databases, for whatever reason. There are two types of primary keys you will run into.
enforced primary keys in things like Postgres
simple unenforced logical primary keys in things like Delta Lake
It honestly doesn't matter if a primary key is enforced or not, it's the process of creating primary keys that will, help with the deduplication and processing of duplicates in data. Every single dataset you come into contact with has a primary key, and you should know it, and calculate it.
You will have one of the two following primary keys.
composite primary keys
user generated primary keys
A composite key is basically the combination of column(s) that make a record unique. For example ...
The above composite primary key is made up of two columns. This SQL above is what you would run into for a RDBMS database like Postgres or MySQL.
So, on the opposite end of the spectrum in a Lake House we might have a logical primary key that is not enforced AND user generated in Delta Lake.
Honestly, the primary key kinda just depends on the context and what tooling, but it's all the same in the end.
You need to define what makes each record unique, create a primary key for that record, and use that primary key for all downstream data de-duplication.
Post processing duplicates and deduplication.
So, now that you have your primary key, the uniqueness of your dataset defined, the last step is to simply use whatever tool is doing the data processing and ETL of your data, DataFrame, SQL, whatever ... to deduplicate your data in your pipeline. You can't just assume a dataset will never have duplicates, most likely it will at some point.
If you don't design for it, it will break.
Examples of deduplication using a primary key column with PySpark, Polars, Pandas, and DuckDB.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
In PySpark,
dropDuplicates()
is the go-to method:
docs here.
Polars provides
unique()
:
docs here.
Pandas use
drop_duplicates()
:
In DuckDB, a
SELECT DISTINCT ON
(Postgres-style) or
ROW_NUMBER()
works:
That‚Äôs pretty much the end of the matter.
Not much rocket science going on.
That's pretty much what you need to deal with duplicates in your data.
I know, nothing fancy, sorry.
It's best just to go straight to the data and deal with by adding some sort of primary key, logical or enforced, doesn't really matter.
Putting the work in up front to know what is a unique record takes an extra five minutes, and will pay dividends later.
Once you have that primary key, it's a one-liner insert into most of your data pipelines to ensure know duplicates are making their way downstream to blow up a Dashboard or MERGE statement.
That's always a good thing.
Some folk might complain that their 15 PB's of data is too large to deduplicate. Fair enough, but if you are processing your data in smallish chunks, it's a good idea to give it a try ... see if you can get away with the extra compute and processing.
Same for streaming datasets, at least get a primary key defined, and maybe in downstream pipelines and analytics you can deduplicate when looking at slices of the data."
169406866.full-vs-incremental-pipelines,Full vs Incremental Pipelines,"full, partial, or both?",2025-08-11 08:23:32.584-05,"This is another classic, isn't it? Some data questions have had folk spitting and fighting for nearly a few decades now. Funny, how nothing really changes, even in the age of AI.
Technology has undergone significant shifts from the days of SSIS to DBT. But, I still see the exact same topics being discussed and poked at all these years later.
And I mean literally the same topics.
Today, we shall peel back the rotting old onion üßÖ and talk about
Full
vs
Incremental
data loads.
This topic can come in various forms, often under the guise of full historical loads vs something else, such as smaller incremental loads.
TDLR
:
Full historical loads (done well) can be the norm under medium-sized data sizes. At scale, separate pipelines will be required.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Concepts of Full vs Incremental Loads.
I know there are probably some avocado toast-eating Gen-Z suckers out there who are Data Engineering script kiddies, so we should probably go back to kindergarten and learn the basics.
When writing data pipelines that contain data loads of some sort, you really have two options, whether you know it or not.
- Full Historical loads 

- Incremental (recent) loads
(
This begs the question of what is CDC (change data capture) ‚Ä¶ full or incremental, or does this depend on the implementation??
)
There really is no in-between. Sure, you might use a tool, a SaaS thingy-ma-bob to obfuscate the complexity
. But that doesn't change the fact that you're doing one of the two things above.
Consider this:
it's a very annoying problem to deal with because the proper answer is highly dependent on the business and other outside factors.
How do you process only new records?

How far do you ‚Äúlook back?‚Äù

Can you afford the do full historical loads?

What are the performance implications?

The cost?

How does the data source act?
These are questions and answers that change with every single data pipeline.
We have data sources and data sinks. This never changes. What changed is HOW we sync those two separate data sets.
Is the data ‚Äúfact‚Äù like in nature? Does it ever change, or is it set in stone? Perhaps the data is ‚Äúdimensional‚Äù in nature and changes slowly or quickly.
These different use cases require totally different approaches, and sometimes different tools. CDC vendors have been hawking their wares in the open air for a few decades now, yet we still write data pipelines manually to push and pull data between systems.
Why is that?
Clearly, the topic is more complex than simply ‚Äúplug in this connector,‚Äù and the data will not magically sync perfectly between the systems without further manipulation.
If it were that easy, Data Engineers wouldn‚Äôt exist.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Example. Concepts in action.
While this topic is fresh on the mind, let's just do a simple example of a full and incremental pipeline, so we can explore these ideas more.
Here‚Äôs a straightforward PySpark + Delta Lake example that pulls only the last 2 days of updated orders from Postgres and upserts them into a Delta table with MERGE
I would call this a very standard practice for an incremental load, which probably has been writing a million times over the last 10 years by a million different people.
Yet, clearly, for those in the know, what looks on the surface to be totally normal has a wide variety of gotcha‚Äôs and holes.
What if the pipeline itself doesn‚Äôt run (
for whatever reason
) for a few days?
What if someone asks for a historical load?
What if the source blips without us knowing because of an upgrade or problem and we aren‚Äôt notified?
What if the source gets a dump of data?
For the 95% of the time, this query and pipeline would probably do the job just fine, it‚Äôs the other 5% of the time that causes the problems and wreaks havoc on data teams and downstream consumers that depend on this information.
Whats the answer?
We can‚Äôt always simply default to full historical loads every single time.
Or can we?
I mean in theory, no matter the technology, it is easy enough in most cases to simply gulp down all the data and do a pump and dump from the source into the target.
Of course, depending on situation, the pipeline will slow to a crawl and the costs will skyrocket.
If we are running Spark and the data is less than a TB, it probably wouldn‚Äôt make much difference, but it does feel kinda icky doesn‚Äôt it?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I mean if our boss got mad at us and told us to stop doing that, only load the new records and stop spending all that money, we could do something equally as acky, but would do what they asked.
Simply get ALL the primary keys from each table and compare, pulling only new records in.
You tell me, are we just picking between a few bad options, maybe there is no good options. We should just buy some SaaS data connector that obfuscates the entire thing for us, black magic could be the path to walk down after all.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Could we find what works best for us?
Maybe, the best answer is to just to understand our data sources better, and strike a balance where our pipeline doesn‚Äôt incremental loads, but gives us some configurational option to pull pass amounts of data as needed.
It could be the best option, no?
Herein lays the question that has plagued us for decades, and indicates why Data Engineers still write pipelines for syncing data between source and target, after all these decades and promises of magic SaaS that will sync all the data for us.
I guess we could update our original 2 day pull with the following configuration and move on with life. Maybe we go talk the business or data source owner and ask the worst case scenario they‚Äôve had for late data arrival or something along those lines.
I wish the answers were simple, but we live in a messy data world and it just never works that way.
Some datasets maybe are small enough we do historical pulls every day.
Some datasets are too large and get a 7 day pull at most
Some are massive and only get a day or two
I‚Äôve even seen circumstances and large datasets (50 million records a day), where maybe a 3-6 day pull is done to ensure no missing records, and a separate once a week pipeline runs to do a 30 day comparison of primary keys to pull in any missing records.
It probably depends on a number of factors.
The business appetite for completeness
The business appetite for spending money on compute
How reliable the data source is
How much the pipelines break and go stale
Should we give in to the SaaS?
Heck, maybe we should stop fighting and kicking against the goad like Paul. Databricks gives us ‚Äúauto cdc‚Äù with tools like DLT.
I mean, who is going to argue with that little bit of code?? The longer one writes code, the more one can appreciate tools that make a complicated thing very easy.
There is a beauty in less code.
But, then reality strikes and we know there is no free lunch. Everything comes at a price. Literally, or figuratively.
What do you do when it comes to full vs incremental loads, historical loads and daily loads in the same script?? Leave a comment and let us know!
Leave a comment"
169334829.databricks-workflows-vs-apache-airflow,Databricks Workflows vs Apache Airflow,... or both?,2025-07-28 10:28:38.44-05,"I‚Äôm such a cheerful guy, I figured I would try to write a positive and inspiring post while comparing two tools.
Just kidding
. I guess we will see how it goes. Who knows, we'll just see where it leads us, I suppose.
There are very few areas where technology battles are still being played out, such as orchestration tools. With the recent infamous Astronomer happenings, Apache Airflow has been back in the conversation again.
As recently as this spring,
I wrote an honest review of the data orchestration landscape.
What sparked this post comparing Apache Airflow and Databricks Workflows was something my Databricks Account Manager told my boss. Something to the effect of ‚Äú
You should replace your Apache Airflow with Workflows, trust me, it will be great.
‚Äù
There have been a few times I wanted to reach across the digital landscape and throw a bit or byte into someone‚Äôs eye, but that was one of them.
Instead of making you read to the end to give you the TDLR, I‚Äôm going to say it up front.
I use BOTH Apache Airflow and Databricks Workflow in production today. These tools complement each other and are not mutually exclusive. Picking one over the other is a bad decision; using them both, in combination, gives you the best of both worlds.
There you have it.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Apache Airflow vs Databricks Workflows.
Before we dive too deeply into these orchestration tools, let‚Äôs take a look at what I think is an obvious aspect of data orchestration: the
what
and
why
.
If we were to consider the top reasons for using an orchestration tool, or, in other words, what features do we look for in those tools, this might help us evaluate both Apache Airflow and Databricks Workflows.
data flow visualization
complex task dependency support
basic retry support
top-notch error monitoring and handling
top-tier and broad connections and integrations
straightforward development
We could go on to list 1,000 things, but let‚Äôs try to keep it straightforward and simple. If a person can check off the above items, regardless of the tool, it‚Äôs most likely that things will work out well.
Data Flow Visualization
I‚Äôm not sure we really need to discuss data visualization with Apache Airflow, but, for the sake of argument and in the off-chance we have some small, little data hobbits, let‚Äôs just go through the motions. This will make it easier to compare to Databricks Workflows anyway.
setting things up.
To get started, and if you'd like to follow along, I suggest installing the
Astro CLI.
This little beauty makes creating and running Airflow locally a breeze.
Run these few commands, and you are on your way to glory.
Once you have done that, you will get a localhost Airflow that should pop up automatically in your browser; otherwise, localhost:8080 should do the job.
So, at this point, let‚Äôs just write an example DAG and workflow that we will end up creating in Databricks Workflows as well. What the pipeline does itself isn‚Äôt that important, as much as what it is like to write it, and how it looks.
Here is our example Airflow DAG.
So, here is a summary of what is happening in this example workflow.
Start
- Dummy entry point

Data Ingestion (ingest_data Task Group)
- ingest_from_s3: Pull from S3
- ingest_from_api: Fetch from API
- ingest_from_db: Extract from database

External Dependency
- wait_for_partner_pipeline: Waits on external DAG/task

Conditional Transformation
- branch_decision: Randomly selects:
- transform_heavy_task: Intensive processing
- transform_light_task: Lightweight processing
- join_transform: Merges transformation paths

ML Workflow (ml_pipeline Task Group)
- train_model: Train model
- score_model: Generate predictions
- validate_model: Evaluate model performance

Downstream Trigger
- trigger_reporting_dag: Kicks off another DAG

End
- Dummy terminal task
For what we are discussing now, we need to look at in the UI to see visually how it shows up. So in our command line, inside our astro projects, we would `
cd dags
`, and place this example DAG there.
If you have problems with the DAG showing up in the UI, run the following ‚Ä¶
astro dev restart
Below we can see our pipeline in the Airflow UI.
And of course, Airflow provides a plethora of other information readily about runs,  failed or not. In other words, Apache Airflow has tier-1 level support for visualization.
Sure, some people might argue that tier-1 level, but whatever. The truth is a lot of people use Apache Airflow every day, and while sometimes it leaves some things to be desired, it‚Äôs easy to move around the UI, click on a pipeline, see what has failed or not, and visually inspect WHAT a pipeline does.
Of course, some of this is dependent on the person writing the DAG not being a ninny.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Databricks Workflows visualization.
Ok, so while everything is fresh from our Airflow DAG and what this pipeline looks like. Let‚Äôs build on in Databricks Workflows and see what gives.
Now, I‚Äôm not going to lie, better hold your breath for this one. Look, I can say confidently I‚Äôm a bigger Databricks fan than you, big time, but this kinda stuff makes my toes curl.
I mean visually, there is really not that much difference between the two. Other than the fact that Apache Airflow has been around WAY longer than Databricks Workflows, and therefore the fine details are always going tip in the favor of Airflow.
I do want to make some other comments while these code examples, and visuals are fresh in our minds.
First, it might be my opinion, but Databricks Workflows kinda suck to write. Not the worst, but not the best. They simply don‚Äôt flow off the finger tips like an Airflow DAG does.
Heck, even looking at the JSON used to define them visually just looks like a giant jumble of ‚Ä¶ JSON.
Second, beauty is in the eye of beholder, that‚Äôs why your significant other is with you right? I mean just like us, Databricks Workflows and Apache Airflow are going to have their visual UI ‚Äúthings‚Äù that either attract or repel people.
That‚Äôs ok.
I mean look at the home UI for both Airflow and Workflows. They are similar, and again, besides little things, who‚Äôs to say one is better than the other. It‚Äôs preference.
Anywho, let‚Äôs move on.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Complex Task Dependency Support.
I‚Äôm just going to get straight to the point here. Most common Production like scenarios and pipelines in real life will have a number of oddities and complexities that won‚Äôt fit the easy and perfect mold all the time.
When it comes to choosing an orchestration tool, we don‚Äôt want to have to fight it when it comes to defining things that are simple like conditional logic and branching.
The truth is that even in the simple example I used, I had to move from `if-else` nested logic to Notebooks ‚Ä¶ simply because Databricks Workflows fall apart a little when things get tricky.
Before you send me angry emails ‚Ä¶ I get it. Yes, anyone given enough time and enough grit can easily ‚Äúmake it work,‚Äù find workarounds and solutions to any problem. Sure, you can do complex things in Databricks Workflows, but it simply won‚Äôt be as straight forward as doing that thing with Apache Airflow. Get over it.
1. Task Limits per Job
- A single Databricks job supports up to 1,000 tasks only 
- For highly intricate pipelines, you may need to chain multiple jobs.

2. Simplified Conditional Logic
- Native branching (if/else) is available, but each condition task only   handles a binary branch (‚Äútrue‚Äù vs ‚Äúfalse‚Äù) via the visual UI, and lacks support for more than two outcomes without nesting multiple tasks 

- There's no built-in ‚Äúrouter‚Äù task for multi-way branching‚Äîso tasks like A / B / C would require separate condition tasks or logic embedded in notebooks 

3. Limited ‚ÄúRun If‚Äù Conditions
- Databricks offers limited run‚Äëif options like ALL_SUCCEEDED, AT_LEAST_ONE_SUCCEEDED, CUSTOM, etc., but you can‚Äôt combine or nest them flexibly like Airflow‚Äôs branching or trigger rules 

- Merging branches often requires setting run_if: ONE_SUCCESS, which can lead to skewed behavior‚Äîtasks marked ‚Äúskipped‚Äù still count as success, affecting downstream flow.

4. Scheduling Constraints in Continuous Mode
- Workflows using continuous schedule mode (polling, streaming) cannot include task dependencies within the same job, unlike Airflow. Many users report frustration that you can‚Äôt build dependency chains in those cases 

5. No Looping or Dynamic Task Generation via API
- While Databricks UI supports a For Each loop over static arrays, programmatic dynamic task creation or looping via REST API isn‚Äôt supported. Complex dependency graphs may require orchestrating by spawning jobs via notebook logic or API externally 

6. Orchestration Granularity
- For deeply nested DAGs or highly conditional logic (many forks), you often need to break the pipeline across multiple Jobs, connecting them via trigger tasks (run_job), impacting transparency and manageability.
I know Databricks Workflows is changing fast, Databricks is the GOAT and half of what I say might be already out of date, or soon be.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Retry Support with Airflow and Databricks Workflows
People never think about this, or give it the time of day it deserves, the super tier-1 support for various and complex retry support at all levels of a data pipeline mean the difference between a janky tool and one that beats your pipelines into submission.
Pipelines fail. They always will.
üîÑ Apache Airflow: Rich and Flexible Retry Support
Per-task retry configuration
:
Each task can define its own
retries
,
retry_delay
,
retry_exponential_backoff
, and
max_retry_delay
‚Äîor you can set defaults via
default_args
in the DAG context.
Exponential backoff
:
Supported by enabling
retry_exponential_backoff=True
, with optional bounding via
max_retry_delay
.
Execution and Sensor timeouts
:
You can set
execution_timeout
, and for sensors in reschedule mode, a
timeout
. Retries happen on recoverable problems but don‚Äôt reset timeout windows.
Custom retry logic
:
Use callbacks like
on_retry_callback
, or raise
AirflowSkipException
/
AirflowFailException
to override retry behavior programmatically.
Manual and UI rerun
:
The Airflow UI and CLI support clearing failed tasks to rerun them manually or backfill past DAG runs.
üîÅ Databricks Workflows: Basic Retry Support Per Task
Retry policy configuration via UI/API
:
For each task in a Lakeflow job, you can specify a fixed number of retries and a retry interval. By default, tasks
do not retry
unless configured.
Continuous job mode
:
In continuous trigger mode, Databricks applies
automatic exponential-backoff
retries to tasks by default.
Single retry config per task
:
There‚Äôs no support for per-attempt custom delays, exponential backoff toggles, or different retry behavior for different types of failures.
No programmatic retry logic
:
Unlike Airflow, you cannot dynamically alter retry behavior using callbacks or exception handling within notebook code.
No manual rerun to resume
:
While you can re-trigger a whole job run, there is no granular UI support to resume failed tasks in-place‚ÄîCLI/API reruns start entire job runs.
Again, take what I say with a grain of salt. You spend your weekend trying to figure this all out. Good luck you hobbit.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Top-tier and Broad Connections and Integrations
This is another one where I will be honest with you, Databricks Workflows is going to fall into pieces and Airflow is going to step into the light shining down from heaven.
The obvious answer is that Databricks Workflows were made for and have first support for all things Databricks. Anything else? Not so much.
External System Triggers (Limited)
Webhook Triggering
:
You can trigger a Databricks Workflow
via an external system
(CI/CD, Airflow, GitHub Actions, etc.) using the Jobs API.
Job-to-Job Triggering
:
A task in one Databricks job can
trigger another Databricks job
using the
run_job
task type.
This enables chaining, but only between Databricks jobs.
On the other hand,
Apache Airflow‚Äôs ecosystem was made to bring everyone else to their knees
. It‚Äôs simply unparalleled and not even comparable to Workflows.
I mean sure, you could write all the custom Python code you want and to trigger this and that, but that‚Äôs a giant waste of engineering time and is a sure sign of poor architecture and decisions on someone‚Äôs part.
Airflow is the clear winner.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Development with Apache Airflow vs Databricks Workflows
This is one area with Databricks is going to win hands down. Surprised? You shouldn‚Äôt be. Know one can lay a hand on Databricks when it comes to developer experience and the options they give you for developing things like Workflows, or anything else for that matter.
Databricks Asset Bundles (DABs) ‚Äì CLI‚ÄëFriendly, CI/CD‚ÄëReady
Smart packaging
:
Define notebooks, jobs, cluster configs, and parameters all in declarative YAML‚Äîbundled as a versioned unit for promotion across environments
CLI support
:
Use
databricks bundle init
,
bundle validate
,
bundle deploy
, and
bundle run
via Databricks CLI to iterate locally and promote between dev, staging, and prod
GitOps-style workflows
:
Combine with GitHub Actions or other CI/CD tooling to manage deployments and workflows in a structured, observable manner
üß∞ Other Tooling Options
Databricks CLI
:
Manage workspaces, jobs, clusters, secrets, and more programmatically.
Databricks REST API
:
Create/update jobs, trigger runs, fetch metadata‚Äîall consumable via SDKs (Python, Java) or HTTP.
SDKs and Terraform
:
Use official SDKs or Terraform providers for infrastructure-as-code deployments.
dbx
(now deprecated):
A CLI tool for local project builds and deployment; Databricks now recommends transitioning to DABs
The truth is that Airflow does support some CLI stuff and does have a REST, but no one uses them and they suck for the most part. It‚Äôs simply NOT how most people use Airflow.
Databricks on the other hand has first class support for all these things AND they are widely used.
People write Python DAGs with Airflow, that‚Äôs 99% of people, like it or not.
TL;DR: Apache Airflow vs. Databricks Workflows
Use both
‚Äî don‚Äôt pick one over the other. They
complement
each other in real-world production data platforms.
I mean, I use both Databricks Workflows and Apache Airflow DAGs in the same DAGs a lot. They are the perfect pairing, you can use Databricks Workflows to easily wrap together Databricks compute tasks, and then using other Airflow Operators, tie in numerous other OUTSIDE Databricks Tasks.
Do you really want to pay Databricks compute costs to run some Python requests hitting a REST API? That‚Äôs what you call a bad idea and waste of money."
168317439.agentic-ai-for-dummies,Agentic AI for Dummies,AI within AI,2025-07-20 16:27:10.919-05,"One of the most wonderful parts of my job, when presenting my dear readers with a new topic, is to cut through the layers of obscurity and useless nonsense
the madding crowds add to specific issues.
Today, we shall whet our blades in the sticky blood of the AI gods and their magic-weaving marketing prophets who have tried to blind our eyes with various wizardry, if they are able, to convince us that this AI stuff is far beyond us.
We will grasp ‚Äú
Agentic AI
‚Äù and pull it down from its towering throne, dragging it through the dust and grime in which we daily find ourselves wading.
I‚Äôm sure you‚Äôve heard these words, Agentic AI, thrown around by your CTO, boss, or LinkedIn peddler over the last few months. It‚Äôs hot, you know ‚Ä¶ all the cool kids are doing it.
Let‚Äôs do the unthinkable and ask AI what it thinks Agentic AI is.
""""""
Agentic AI refers to artificial-intelligence systems built as autonomous ‚Äúagents‚Äù that can decide, plan, and act in pursuit of a goal with only limited human supervision. In practice, that means the software can:

- Form or accept a high-level objective, break it into sub-tasks, and sequence those tasks (iterative planning and reasoning). NVIDIA Blog

- Take real actions‚Äîcalling APIs, writing code, moving money, booking tickets, composing e-mails‚Äîrather than just returning text. UiPath

- Monitor results, learn, and adapt its plan on the fly, often coordinating multiple specialized agents in a larger workflow.
""""""

- ChatGPT
The Background of Agentic AI
Truthfully, there is no magic when it comes to ‚Äú
what is Agentic AI.
‚Äù It‚Äôs simply a regurgitation and re-imagination of the classic workflow where someone interacts with a ‚Äúsingle‚Äù LLM.
Generally speaking, Agentic AI is the approach of developing multiple ‚Äú
task-specific
‚Äù Agents that can be combined together to provide an overall better experience for specialized tasks.
A visual of this conceptual ‚ÄúAgentic AI‚Äù workflow can be seen below.
You can think of it as RAG 2.0. (
although RAG might now just be one of many agents added to an Agentic AI workflow
). What I mean by RAG 2.0 is that, in the not-so-distant past, the prevailing wisdom was that to increase the accuracy and usefulness of AI generally, RAG was required, wherein specific content was embedded and made available to be integrated into the normal AI workflow.
That didn‚Äôt last long, did it?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The race for the ‚Äúperfect AI sidekick.‚Äù
It helps to think about where we have come from so we can better understand why ‚ÄúAgentic AI‚Äù is even a thing. When ChatGPT first took the world by storm, quickly followed by a slew of competitors, the race was on to build the
perfect model that could do and answer
everything
.
A single model that could do whatever we wanted at a moment‚Äôs notice. As the adoption of AI began to infiltrate every single business and job, two things became apparent:
where AI was lacking.
Being good at everything means you‚Äôre not great at very specific things.

- Being good at writing a book means you might not be the best at making art for the cover.
- Complex human-centric decision-making processes use lots of relevant context.

Go look up this information, pull the data, compile it, make a decision, etc.
In essence, Agentic AI is tech‚Äôs way of making AI perform better,
based on the currently existing architecture.
Instead of having a single LLM attempt to solve a specific problem, perhaps we can insert ‚Äúagents‚Äù (other AI bots) into the workflow,
whose sole task is a specific one.
This is all fed back into a single context to achieve the best possible result, incorporating all relevant information and data.
It probably functions more like how we humans solve problems, gathering a lot of specific data based on our experience, and piling it all together to make a decision.
So what does that mean in real life?
In real life, this Agentic AI approach is built to be sort of ‚Äújob‚Äù or ‚Äúproblem space‚Äù specific. It‚Äôs really about solving complex problems in a real-life way.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
An agent might ‚Ä¶
pull data from a database
go read a website
look through a company knowledge base
peak inside a vector database
some other specific task
etc, etc.
All the results can be pulled together and run through a controller LLM/AI that can now give a more accurate ‚Äúresult‚Äù based on the work of the other agents.
Share
An Agentic AI workflow is just another Data Pipeline.
I do hate the burst the bubble of the endless stream of SaaS vendors who are running down the long hallways of LinkedIn trying to sell you the next magic AI assistant built using ‚ÄúAgentic AI.‚Äù
What they are trying to say is that some under-appreciated engineer spent a weekend ‚Äústitching together‚Äù a few different ‚Äúfunctions‚Äù and wrapped it all inside a single AI ‚Äúcall.‚Äù (
which I will do for you soon ‚Ä¶ on a weekend
)
If you‚Äôve been struggling to follow me so far, thinking of an
Agent
as a ‚Äú
method
‚Äù or ‚Äú
function
‚Äù is fine with me. Of course the implementation details will vary widely, depending on the agent itself, but in the end ‚Ä¶
input ¬ª output ¬ª pass it along.
input ¬ª output ¬ª pass it along.
input ¬ª output ¬ª pass it along.

>>> put it all together
Just some question(s) being feed into the LLM, that question or ‚Äútask‚Äù being broken up into parts, being feed to agents, who in turn do a thing and return a thing ‚Ä¶ than their fearless AI context controller will munge it all together and spit it back out.
It won‚Äôt be long before there are DAGs for building Agent workflows if I had to guess.
Because we can, and to prove to you that Agentic AI is not magic, but just the hard Engineering work of defining specific Agents to do a thing, and tying it all together, let‚Äôs just create our own Agentic AI workflow.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Building an Agentic AI system.
Ok, so we are going to use
LangGraph
+
LangChain
and OpenAI to build a multi-agent system workflow, just to prove out how these concepts we‚Äôve talked about, work in the real world, and there is no dark arts involved.
All this code is available on GitHub.
So we (me) will be building a self‚Äëcontained demonstration of a
supervisor‚Äërouted multi‚Äëagent workflow
built with
LangGraph
+
LangChain
. Dang, that sounds fancy doesn‚Äôt it?
Maybe Meta should offer me a few million dollars to do this.
This GitHub project freely available to you
spins up (
via Docker Compose
):
Postgres
(seeded sample
transactions
table)
App container
running a LangGraph graph with three worker agents:
SQL_AGENT
‚Äì queries Postgres through an LLM‚Äëgenerated SQL tool
WEB_AGENT
‚Äì performs DuckDuckGo search + optional page fetch
SYNTHESIZER
‚Äì composes final answer
A
Supervisor (controller)
node that decides which agent acts next and when to finish.
The example objective (
modifiable
) asks for
structured spending insights + a current external headline
, forcing cross‚Äëagent collaboration.
Here is a visual of how this multi-agent (Agentic AI) workflow looks conceptually.
+------------------+
          |    SUPERVISOR    |
          | (decide next)    |
          +---------+--------+
                    | (route: SQL_AGENT / WEB_AGENT / SYNTHESIZER / FINISH)
      +-------------+--------------+
      |                            |
+-----v------+              +------v------+             +--------------+
|  SQL_AGENT |              |  WEB_AGENT  |             | SYNTHESIZER  |
|  (DB SQL)  |              | (search+fx) |             | (final blend) |
+-----+------+              +------+------+
      |                            |
      +-------------+--------------+
                    |
               (back to)
               SUPERVISOR  --> FINISH
This makes sense right? Add your own context here that works for you. We basically want a few different agents to do specific tasks, and then bring it all together at the end so we can ‚Äú
get the best result possible.
‚Äù Maybe you business context would be something different, use your imagination.
The directory of the repo looks as follows.
I just want to point out there is a lot of ‚Äú
boiler plate
‚Äù and ‚Äú
routing
‚Äù code.
It‚Äôs about weaving the spaghetti dinner together more than some new novel AI thingy.
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Environment & model settings
‚îÇ   ‚îú‚îÄ‚îÄ tools.py           # SQL & web tools
‚îÇ   ‚îú‚îÄ‚îÄ agents.py          # Worker agent constructors & prompts
‚îÇ   ‚îú‚îÄ‚îÄ memory.py          # Lightweight conversation state
‚îÇ   ‚îú‚îÄ‚îÄ graph.py           # LangGraph build (state + nodes + edges)
‚îÇ   ‚îî‚îÄ‚îÄ run.py             # Entry point (sets objct & streams graph)
‚îî‚îÄ‚îÄ postgres/
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îî‚îÄ‚îÄ init.sql           # Schema + seed data
For the concepts we are talking about today,
multi-agents
, let‚Äôs just focus on the agents themselves, what they are doing and how they work together.
I suggest you
go browse the code
, but here is a an overview.
Graph-Based Workflow (
LangGraph
)
The system uses
LangGraph
to create a directed graph where:
- Nodes: Represent agents or decision points

- Edges: Define the flow between agents based on conditions

- State Management: Maintains conversation history and context across agent interactions

- Recursion Control: Prevents infinite loops with configurable recursion limits
Tool Integration
Each
agent
has access to specialized
tools
:
- SQL Tool: Connects to PostgreSQL database for transaction analysis

- Web Search Tool: Searches for current market trends and financial news

- Fetch Tool: Retrieves detailed content from web pages
How It Works
Supervisor Entry
‚Äì Graph entry point is
supervisor
, which inspects conversation state + objective.
Decision
‚Äì Supervisor LLM outputs either an agent name or a termination marker (interpreted as FINISH).
Worker Execution
‚Äì Selected worker runs via its
AgentExecutor
with only its allowed tools.
Conversation Update
‚Äì Worker output appended to shared state.
Loop
‚Äì Control returns to supervisor until it selects
SYNTHESIZER
and then finishes, or decides it already has enough context.
The
SYNTHESIZER
agent integrates prior SQL + web outputs into the final narrative.
Let‚Äôs just give a quick overview of what is inside each of the Python files that make up this Agentic AI workflow.
agents.py
graph.py
tools.py
memory.py
They all work together to make up the entire system.
1. Agent System
Supervisor Agent: Uses a structured prompt to decide which worker agent should handle tasks

Worker Agents: Each has a specific role, instructions, and access to relevant tools

Prompt Engineering: Carefully crafted prompts ensure agents use their tools effectively
2. Graph Workflow
Node Creation: Dynamically creates nodes for each worker agent

Conditional Edges: Routes between agents based on supervisor decisions

State Management: Maintains conversation context and user objectives
3. Tool System
SQL Database Tool: Executes SQL queries against PostgreSQL

Web Search Tool: Performs web searches for current information

Content Fetch Tool: Retrieves detailed content from URLs
4. Memory Management
ConversationState: Tracks all agent interactions and outputs

Message History: Maintains chronological conversation flow

Context Preservation: Ensures agents have access to previous insights
As you can see, it‚Äôs mostly about setting up a workflow and ‚Äúpipeline‚Äù so our Agentic AI system can call the agents correctly and pull all the data together.
Honestly, most of this stuff is boilerplate and would stay the same throughout any AI Agent workflow we made, it would be more about swapping out different agents that we might want for other things, like looking inside a vector database, going through our Google docs, hitting some API, whatever.
So enough with that stuff, does it work?
git clone git@github.com:danielbeach/agenticAIagent.git
cd agenticAIagent
export OPENAI_API_KEY=sk-...  # your key
docker compose up --build
You can see the question I hardcoded into this example.
I will save you the pain of giving you the entire output, I suggest you clone the repo, export our
OPENAI_API_KEY
and run
docker compose up
to see what happens yourself. Here is a little bit of what is printed to STDOUT.
Basically ‚Ä¶ it works. It might seem overwhelming, but not really, once you step through the code, it‚Äôs not that many lines. We are build a few agents, chain them together in a graph, etc, etc.
The sky is the limit at this point as you can see. You could, for whatever your use-case is, build specific agents to do other things, look over StackOverflow, read DuckDB docs, go read some database, etc, etc.
The obvious thing here is that that you can make an ‚ÄúAgent‚Äù out of whatever your little heart desires. This sort of thing really can make AI better at solving specific tasks.
Just a whole lotta for-loops and coordination between tasks.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Closing thoughts.
I hope I was able to demystify ‚ÄúAgentic AI,‚Äù stripping away most of the hype to show it‚Äôs just a little engineering:
orchestrating multiple narrow, tool‚Äëusing AI
agents
instead of leaning on one monolithic LLM.
Rather than magic, it‚Äôs a familiar data / ETL pipeline pattern: break a high‚Äëlevel goal into sub‚Äëtasks, route work to specialized components, gather outputs, synthesize a final answer.
W
e used LangGraph + LangChain and built
a
Supervisor
node that iteratively decides which worker to invoke next (SQL agent for Postgres queries, Web agent for search + fetch, and a Synthesizer to compose the final response).
Each agent has tightly scoped tools and prompts; shared state (memory) preserves intermediate results; the graph enforces control flow and prevents loops. Most of the code is reusable boilerplate‚Äîswap in other agents (vector DB lookup, internal docs crawler, API caller) as needed.
‚ÄúAgentic AI‚Äù is just structured decomposition + routing + tool execution + aggregation. Treat agents like functions:
input ‚Üí output ‚Üí pass along
, repeated until the supervisor judges the objective satisfied.
In the end, Agentic AI is an extensible pattern for scaling specificity and reliability‚Äîessentially a programmable workflow wrapped around LLM calls‚Äîpromising better, more context‚Äërich results through modular specialization.
Key Takeaways
Not magic: it‚Äôs modular orchestration of specialized LLM-powered tools.
Supervisor pattern routes tasks; workers stay narrowly focused.
LangGraph models the workflow as nodes (agents) + conditional edges.
Memory/state glue the steps; prompts + tool limits enforce role discipline.
Easy extensibility: add/replace agents for new data sources or functions.
Think ‚Äúdata pipeline / DAG‚Äù applied to AI reasoning and action execution.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
168243548.data-team-strategy-and-collaboration,Data Team Strategy & Collaboration,spicy eh??!!,2025-07-27 13:34:01.001-05,"I‚Äôm not sure a person like myself (
who daydreams about living in a mountain cabin with no one around for miles
) is the right one to give you any sort of lesson about Data Team Strategy and Collaboration, but you know, I do what I have to.
How do you stop playing data whack‚Äëa‚Äëmole and start operating like a data team that gets !@$$#@ done?
TL;DR
:
Great data teams run on a clear mission, a shared roadmap, crisp interfaces, and lightweight rituals that make collaboration boringly predictable.
Surprise, surprise, it takes more than good Software Engineers to deliver value while keeping a happy team.
I have personally worked with data for longer than I would like to admit, from classic, slacks-wearing corporate data and software teams to ping pong- and beer-drinking startups, and a few in between.
These experiences have taught me a few things about humans, companies, and how to best solve problems and accomplish big tasks while dragging people along kicking and screaming.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Why ‚ÄúStrategy‚Äù Matters (and why ‚Äúcollaboration‚Äù isn‚Äôt just meetings)
Most data teams are stuck reactively shipping dashboards, patching pipelines, and answering urgent Slack messages (‚Äú
Can you pull this real quick?
‚Äù). Strategy is how you say
no
with confidence, align around the
few things that matter
, and create
reusable systems
instead of bespoke one-offs. Collaboration is the way you keep that strategy alive day-to-day‚Äîwho talks to whom, when, and with what artifacts.
Without a strategy, collaboration becomes chaos. Without collaboration, strategy becomes shelfware.
Classic Data Team Struggles
Data teams are notorious for not delivering anything on time ‚Ä¶ correctly.
It‚Äôs always fires to put out, explaining to do, and never enough time to actually deliver business value via some pie-in-the-sky project on time and on budget.
So, even though we all want to bury our collective heads in the sand and just keep doing what we always do, if you decide to do that, your world will eventually crumble around you.
I think, instead of getting bogged down in the mire of ‚Ä¶

- ‚ÄúWell, the Product team doesn‚Äôt give us clear direction or priorities.‚Äù

- ‚ÄúOur budget doesn‚Äôt allow us to buy X, Y, Z, which would solve our problems.‚Äù

- ""We ever get the time to finish the projects we start working on.""

- ""My staff isn't technical enough to solve these problems.""
We can simply lump all these and the myriad other problems into the category of poor excuses and problems that simply need to be overcome.
Most data teams and many leaders fall into the trap of just focusing on technical tools and outcomes, as if migrating to Databricks will make everyone‚Äôs life so easy that you will sit around campfires singing songs and holding hands.
Yeah ‚Ä¶ not.
The Four Pillars of an Effective Data Team Strategy
Here is an outline that I would give to someone who wants to lead a Data Team to success. Most people and teams simply don‚Äôt want to invest the time up front to set everyone up for success. It‚Äôs easy to just do what you‚Äôve always done. Change is hard.
Mission & Outcomes 
    ‚Äì Why do you exist, and how do you know you‚Äôre winning?

Operating Model 
    ‚Äì How do requests flow? Who owns what? What gets prioritized?

Technical Architecture & Standards 
    ‚Äì What tools/tech are ‚Äúblessed‚Äù? How do you keep the ecosystem healthy?

Rituals & Communication 
    ‚Äì How do you stay aligned without drowning in meetings?
Let‚Äôs unpack each.
1. Mission & Outcomes: Your North Star(s)
Write a one-sentence mission that answers:
‚ÄúWe deliver ____ to ____ so they can ____.‚Äù
Example:
‚ÄúWe deliver trustworthy, self-serve data models and decision-ready insights to product, marketing, and finance teams so they can ship features faster, optimize acquisition spend, and forecast revenue accurately.‚Äù
Then, tie that to measurable outcomes:
% of key metrics defined in the catalog and covered by data tests
Dashboard adoption / weekly active readers
Lead time from request to delivery (median/90th percentile)
Data pipeline incident rate and MTTR
What I‚Äôm trying to tell you is that if you spend 1 day out of 365, planning at a high level what you want to accomplish ‚Ä¶ you will be ahead of 99% of other people.
The other key is to ensure that you actually come up with some measurable targets that you can hit.
We will improve our MLOps is ‚Üê
NOT GOOD
We will use MLflow to do batch prediction in our pipeline ‚Üê
BETTER
It‚Äôs the age-old adage:
if you don‚Äôt aim for something, you will hit it every time. Not rocket science, my friend, make a plan with measurable targets, and go for broke.
This will also have the effect of improving your team dynamics. When everything is clear to everyone, there are no surprises, people are not wandering around aimlessly, and accountability is present.
This WILL improve a team‚Äôs culture and ability to deliver.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
2. Operating Model: From Intake to Impact
A solid operating model turns random requests into a steady flow.
Key components:
Request Intake & Triage
: A form or JIRA template that captures ‚Äúwho, what, why, urgency.‚Äù (If you‚Äôre allergic to forms, at least a Slack workflow.)
Prioritization Framework
: Use impact vs. effort, tie to OKRs, or adopt a DACI/RICE style scoring. Make it transparent‚Äîpublish your weekly prioritization list.
Ownership & Interfaces
: Define product owners for domain models, platform engineers for infra, analysts for BI. Use
RACI
or
DRIs
for every recurring process.
Delivery Process
: Lightweight PRDs or decision logs for bigger projects, code review standards, staging/prod environments, and deployment checklists.
Feedback Loop
: Post-delivery surveys (‚ÄúDid this solve your problem?‚Äù) and dashboards on your own performance.
Sample Intake Questions (Keep it short!)
What decision will this data enable?
What‚Äôs the deadline (real one, not ‚ÄúASAP‚Äù)?
Who will use it and how often?
What existing data/products have you tried?
What does ‚Äúgood enough‚Äù look like?
Okay, to bring it back down to earth again, what I‚Äôm talking about is setting up a system where we have a template to follow and get things done.
If you let everyone kind of free-wheel, that sort of works, but it will create friction within and outside the team. You simply have to have SOME sort of process that you can follow when working on projects and assigning tasks.
This will increase the chance of success and finishing things on time, correctly, rather than just throwing things into the wind and waiting to see where they land.
It‚Äôs ok to allow a little leeway to people and teams that have earned it, but keeping people on the straight and narrow path is a good thing.
3. Technical Architecture & Standards
Strategy dies when every tool is fair game and every engineer builds their own naming convention or way of solving problems. Curate, don‚Äôt collect. We simply must establish some sort of standard(s) or bar of expectations about how data teams will operate.
What does a good PR look like?
How do we plan projects?
What kind of tooling is acceptable?
How do we communicate changes?
Decide & document:
Storage & Modeling
: Lakehouse? Warehouse? Both? What formats (Delta/Iceberg/Parquet)? How do you do versioning, schemas, and governance?
Transformation Layer
: dbt, Spark, Fivetran + notebooks‚Äîwho owns what and where logic lives.
Testing & Quality
: Data validation (Great Expectations, dbt tests), SLAs for freshness, and incident pages.
Observability & Lineage
: Choose a lineage tool (OpenLineage, Unity Catalog) and standardize usage.
Serving & BI
: Standard dashboards in, say, Looker/Mode/Hex; semantic layers for consistent metrics.
ML/Advanced Analytics
: Feature stores, model registries, and clear handoff points between data & ML teams.
I‚Äôm sure it will be different for every team, but having some clear guidelines will make things more cohesive, and problems will be solved in a predictable manner. This sort of thing will make people happy in the long run.
The ‚ÄúBlessed Stack‚Äù Document
Create a single wiki page (or Notion doc) that answers:
‚ÄúIf I build a new pipeline, what should I use and how?‚Äù
Include code snippets, linting rules, and examples of ‚Äúgood‚Äù repo structure.
Nothing will bring chaos and hard feelings into a team quicker than simply saying, ‚Äú
You do you, just pick something and get it done.
‚Äù Yeah, we want to give Engineers the ability and freedom to choose the correct way to solve problems, how they see fit.
However, if you are a Databricks team and someone decides they need to use Redshift for something, you're going to have problems.
Simple tech stacks always deliver better than a hodge-podge of soup like your Grandma makes.
4. Rituals & Communication: Lightweight, Predictable, Useful
You can‚Äôt collaborate effectively without a steady cadence of both asynchronous and synchronous touchpoints. Everyone might dread the daily stand-ups; that‚Äôs fine, but living in a tower where communication is lacking will lead to failed projects and unmet expectations.
Writers of code hate talking to people (
and people in general
), so you might have to force this one a little bit, but it will be worth it in the long run.
You would be surprised how many problems you can avoid and solve, simply with a 5-minute conversation with the ‚Äúright people in the room.‚Äù
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Suggested Rituals:
Weekly Planning / Standup
(30 min): Review request queue, assign DRIs, call out blockers.
Monthly Stakeholder Sync
(45 min): Show what shipped, remind everyone of strategy/OKRs, gather feedback.
Metric Council
(Monthly): Define or resolve disputes over business metrics.
Incident Postmortems
: Short, blameless write-ups for any data outage > X hours or > Y users impact.
Office Hours
: Open Zoom/Slack sessions for ad-hoc help.
Collaboration Patterns That Actually Work
That‚Äôs one of those dirty words to almost anyone who works in software. The dreaded collaboration talk. The thing is ‚Ä¶ if you don‚Äôt talk, you will end up with unmet expectations and missed projects and deadlines.
Most senior engineers recognize that many times the technical aspect(s) of any big project are not really what end up making things difficult or causing projects to fail.
It‚Äôs the inability of people to communicate well and regularly with each other to understand what the goals, expectations, and outcomes are.
Domain Ownership + Central Platform
: Let squads own ‚Äútheir‚Äù data models and KPIs, while a platform team owns ingestion, governance, and tooling.
Product Thinking for Data
: Think in terms of users, problems, and outcomes‚Äînot just tables and jobs.
Documentation-as-You-Go
: Write docs in PRs, keep README.md files in each repo, and require links to docs for every new model.
Decision Logs Over Slack Scrollbacks
: Use ADRs or lightweight docs to record ‚Äúwhy‚Äù behind changes. Link them in PRs.
Async First, Meetings Second
: Default to written updates, and reserve meetings for alignment or decisions.
Small, Cross-Functional Swarms
: Spin up temporary squads for high-impact projects with a clear exit criterion.
Anti-Patterns to Kill Quickly
Ticket Chaos
: Data team as the ‚Äúyes to everything‚Äù service desk.
Unowned Dashboards
: 200 dashboards, 0 owners, and a weekly ‚Äúwhich metric is right?‚Äù fire drill.
Tool Sprawl
: Every analysis lives in a different tool with no lineage or version control.
Shadow ETL
: Sales ops quietly building mission-critical pipelines in Excel macros and no one knows‚Ä¶ until it breaks.
The Hero Engineer
: One person holds all the tribal knowledge; burnout is inevitable.
Templates You Can Steal
1. Project One-Pager (for anything > 1 week of work)
# Project Name
**Problem:** What decision/problem are we solving?
**Users/Stakeholders:** Who cares and why?
**Scope:** What‚Äôs in / out?
**Success Criteria:** What does ‚Äúdone‚Äù look like, in measurable terms?
**Risks & Assumptions:** What could derail this, what are we assuming?
**Timeline & Milestones:** Key dates.
**Owner / DRI:** Person accountable.
**Links:** GitHub repo, dashboard Figma, ADRs, etc.
2. Architecture Decision Record (ADR)
# ADR-006: Adopt Iceberg for Analytic Tables
**Context:** We need table format standardization for time-travel & schema evolution.
**Decision:** Use Apache Iceberg for all new analytical tables; migrate top 20 legacy tables in Q4.
**Consequences:** +Better schema evolution, -Migration cost, need to train team.
**Status:** Accepted (2025-07-24)
3. Data Incident Postmortem Lite
**Incident:** Stale revenue metrics in dashboard
**Start/End:** 2025-06-03 10:15 ‚Üí 14:30 CT
**Impact:** 34 users saw wrong data; finance halted forecast update
**Root Cause:** Failed upstream load job; missing alert on DAG failure
**Fix:** Added circuit breaker + Slack alert
**Prevention:** Add dbt test for freshness; weekly DAG audit
Collaboration Checklist
Mission statement written and reviewed with stakeholders
Quarterly OKRs set (and not just about ‚Äúdeliver X dashboards‚Äù)
Single intake path defined (form or Slack workflow)
Prioritization criteria published (impact/effort, RICE, etc.)
Owners assigned for top 20 data sets and dashboards
‚ÄúBlessed stack‚Äù doc exists and is kept current
Data testing/observability standards in place
Monthly stakeholder sync on calendar
ADR template in repo and being used
Incident postmortems logged and searchable
If you checked fewer than 6 boxes, you have a strategy/collaboration gap (or three). Pick one gap, fix it this quarter, and build momentum.
Bringing It All Together: A Simple Flow
Intake & Scope
‚Äì A stakeholder submits a request via form (auto-Ticket created).
Triage & Prioritize
‚Äì Data PM/Lead scores it vs. OKRs, publishes weekly plan.
Design & Decide
‚Äì Mini-PRD and ADR written as needed.
Build & Review
‚Äì Code, tests, deployment; docs updated in PR.
Deliver & Educate
‚Äì Walkthrough video, usage notes, data catalog entry.
Measure & Iterate
‚Äì Track adoption & performance, collect feedback, improve.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Final Thought: Make Strategy Visible
The worst place for your strategy is in a PDF no one reads. Put your mission, OKRs, project list, and key docs in a single, easy-to-find home: a Notion page, README in your data repo, or an internal ‚ÄúData Portal‚Äù landing page. Add a link to it in every dashboard footer. Make it
impossible
to say, ‚ÄúI didn‚Äôt know where to look.‚Äù
TL;DR:
Stop reacting to every ping. Define a mission, standardize how work flows in, pick a blessed stack, and adopt simple rituals so collaboration becomes predictable instead of chaotic.
Most data teams operate in a state of constant firefighting‚Äîshipping rushed dashboards, fixing broken pipelines, and fielding endless ‚Äúquick asks.‚Äù
The cure is strategy plus disciplined collaboration. Strategy clarifies why the team exists and what winning looks like (
mission + measurable outcomes
).
Collaboration operationalizes this strategy through an operating model (intake, prioritization, ownership, delivery, and feedback), a curated technical stack with documented standards, and lightweight yet consistent rituals (weekly planning, monthly stakeholder syncs, metric councils, post-mortems, and office hours).
Turn your data team from a reactive help desk into a product team with a mission, a roadmap, a blessed stack, and boringly reliable rituals."
168012911.data-engineering-central-podcast-410,Data Engineering Central Podcast - Episode 8,"Iceberg, Databricks, DuckLake, and more.",2025-07-10 16:22:41.573-05,"Hello! A new episode of the Data Engineering Central Podcast is dropping today, we will be covering a few hot topics!
Apache Iceberg Catalogs
new Boring Catalog
new full Iceberg support from Databricks/Unity Catalog
Databricks SQL Scripting
DuckDB coming to a Lake House near you
Lakebase from Databricks
Going to be a great show, come along for the ride!
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
167912000.managing-ml-model-lifecycles-for,Managing ML Model Lifecycles for Dummies,MLOps with Unity Catalog,2025-07-14 07:14:30.133-05,"With the rise of AI and the word ‚Äú
model
‚Äù becoming almost mainstream, I figured it might be helpful for me to do a mini-series on some basis around MLOps (
Machine Learning Operations
).
Probably one of the biggest misconceptions I see when it comes to Machine Learning in general (
as someone who has been working with it in production for over a decade
) is that people think it‚Äôs all smoke, mirrors, and black magic.
The truth is very different.
Sure, I‚Äôve seen plenty of Data Scientists and Machine Learning Engineers doing their black magic with model configs while pontificating on p-values. But that is the smallest portion, and arguably the easiest part of putting Machine Learning models into production.
The hard part is MLOps.
Model Lifecycle with Unity Catalog
Today, I want to focus on what we will refer to as ‚Äú
model objects.‚Äù F
or those new to Machine Learning,
this is simply the file object that is the ML model itself
, used by some ML pipeline. This could take the form of a
Spark batch or streaming pipeline
model served via REST endpoint etc.
When a Data Scientist or ML Engineer develops (trains) a new model, the output is a physical object file. This object is what is used to ‚Äúpredict‚Äù upon. The details may vary depending on what ‚Äúframework‚Äù was used to create the model ‚Ä¶ Spark, XgBoost, etc.
It doesn‚Äôt matter how that model is being served for our discussion, but we want to think about how we manage that model as new versions are released, etc, and more models are added to the ‚Äúmix.‚Äù
It might seem trivial on the surface, so insignificant that you don‚Äôt really ‚Äúthink about‚Äù it when developing some new ML Models, you must think in your head, ‚Äú
I will store them in S3.
‚Äù Nothing could be more detrimental to your long-term MLOps than this sort of mindset.
What a good Data or Software Engineer will think to themselves is ‚Ä¶ ‚Äú
I need git for ML models.
‚Äù This would be a good thought. Let‚Äôs take a look at one of the easiest-to-use ML model lifecycle tools available today,
Unity Catalog
.
Unity Catalog is best known through Databricks, where it is an integral part of the Data and AI platform; however, it is also available as an open-source tool.
We will use Databricks Unity Catalog in the following discussions.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Unity Catalog/MLflow & ML Model Lifecycle ‚Äî Key Takeaways
Unity Catalog (UC) brings the same centralized governance you already use for tables and files to
every stage of an ML model‚Äôs life
from registration to retirement
by combining the MLflow Model Registry with UC‚Äôs fine-grained security, lineage, and cross-workspace visibility.
However, instead of regurgitating the marketing drivel often spewed forth by the powers that be, I want to take a more practical approach to help the average user understand why using something like Unity Catalog/MLflow to manage ML model lifecycles can be a game-changer.
Sometimes, the benefits of using tools like Unity Catalog and MLflow are so obvious and straightforward that they are often overlooked.
Let‚Äôs give a real-life example.
Model tracking and versioning
One of the most annoying and surprising situations you will encounter within organizations that employ some level of Machine Learning, often without much rigor, is the disorganization and confusion surrounding the storage and management of
model objects.
This might not seem like a big deal on the surface, but this is the 101-level baseline of managing MLOps.
If you have one Data Scientist and one Data Engineer managing one or two models, the entire ‚Äú
store models in S3
‚Äù approach can work,
but it quickly falls apart
.
This poses a serious risk and has downsides, especially for teams with multiple models and people working on them.
People can write, move, forget, and delete any model or version of a model.
discovery of models
lack of control
who can access, delete, update models, etc.
lack of metadata tied to the model
tracking multiple models
tracking multiple model versions
Sure, there are more ‚Äú
complicated
‚Äù reasons to use Unity Catalog and MLflow for other MLOps problems related to models. Still, honestly, if you don‚Äôt have this baseline, it‚Äôs pointless to move to different things.
I‚Äôve seen it too many times.
People lose track of models
People forget what model and what version were stored where
Someone overwrites a model
Someone deletes a model version by accident
People write models to different locations and in different ways
Unclear what models and versions are development vs production.
It seems trivial, but it isn‚Äôt; the more people you add to the mix, the bigger the mess becomes. It spirals out of control.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Yet, a more excellent way.
It can be challenging not to get drawn down rabbit holes into Alice‚Äôs Wonderland when starting to use Unity Catalog and MFflow to manage ML models. These tools are large and complicated, but I‚Äôll give you the 10,000-foot view to give you an idea.
Here are a few key concepts when working with Databricks‚Äô UC and MLflow.
Using MLFlow you can Log model train/test runs.
This concept is called an
Experiment
. Consider this: when developing models, you have various essential components that should be logged and associated with a specific model/code.
input/output
configs
code
plots
datasets
blah
Of course, it is very easy to ‚Äúregister‚Äù models to the UC in Databricks if they have been developed inside an Experiment and ‚Äúlogged‚Äù.
If you have already pre-existing ML models stored somewhere, cloud storage, etc, it‚Äôs easy enough to register those models to Unity Catalog.
This can all be done via the UI if one would rather attempt such things that way.
Either way, in the end you will get all your models registered in a single spot, searchable, discoverable, nary to be deleted by a wayward Data Analyst piddling around on s3.
And the wonderfulness doesn‚Äôt stop there. We can explore each model, looking at the versions, seeing things like the list of input features, the outputs from the model, etc.
This might seem kinda obvious to those who haven‚Äôt worked much around ML, but to those who have, like me, for over a decade, you would be surprised how novel this sort of fine-grained control and tracking of ML models is.
You have to remember ‚Ä¶
MLOps has been a small subset of Data Engineering for a long time.
meaning the tooling options have always lagged way behind
Many Data Science teams developing models DO NOT come from a strong Software Engineering background that lauds best practices.
Many times is difficult to get people do the ‚Äúbasics‚Äù before trying to do the ‚Äúcomplicated.‚Äù
Again, I‚Äôm not trying to teach you HOW to use MLflow and Unity Catalog here, just trying to explain the very basics of the WHY you would want to do such a thing, and WHY you should care at all.
With the rise of AI and the increasing proliferation of Machine Learning in general, soon, it‚Äôs going to be simply expected that a good Data Engineer understands the basics of MLOps, especially related to the lifecycle of model objects.
The truth is,
it doesn‚Äôt matter much what tool you choose to manage your ML models
; the point is that you should understand the pitfalls of NOT managing them well and the long-term benefits of having a clearly defined catalog to manage the chaos.
If you let people dump their models, code, and outputs wherever it strikes their fancy, God help you in the future. Your doom will come.
Just like the code you write needs GIT, the models on any platform need a place to live.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
167816800.sqlmesh-for-dummies,SQLMesh for dummies.,Too afraid to ask?,2025-08-17 20:18:02.041-05,"‚ÄúSQLMesh is a next-generation data transformation framework designed to ship data quickly, efficiently, and without error. Data teams can efficiently run and deploy data transformations written in SQL or Python with visibility and control at any size.
It is more than just a
dbt alternative
.‚Äù -
SQLMesh
I ain‚Äôt going to lie to you. When I first decided that I should probably get around to checking out SQLMesh, if for no other reason than keeping up with the Joneses, when I read ‚ÄúIt is more than a dbt alternative,‚Äù I laughed out loud.
To be honest, something like this tells me all that I need to know. Most people are probably using SQLMesh as a
dbt
alternative. Especially with the
dbt-core
vs
dbtLabs
huffaluff I‚Äôve heard about.
Apparently
r/dataengineering
thinks dbt-core was murdered. Strong words.
Don‚Äôt you just love a good fight?
Anywho, this just happened to be about the same time. Dbt users get angry, SQLMesh rides forth in all its shining glory to save the day. Good times.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Yeah, yeah, so what is SQLMesh?
Ok, so let‚Äôs start with SQLMesh concepts. I‚Äôm not anticipating anything too crazy here. So we sorta mentioned this, but here is the academic description of what SQLMesh actually is.
‚Äú
A Python-based, DevOps-style data transformation framework
for building scalable, reliable pipelines using SQL or Python.‚Äù
Ok, so we can just go ahead and say out loud what everyone is thinking.  Just another data transformation (ETL) framework?
Yes and no.
In one sense, every new tool we encounter today, including SQLMesh, is vying for a spot as ‚ÄúTHEE data tool used to transform data.‚Äù At the end of the day, that is pretty much all we do, day in and out, ad infinitum.
Pull data here. Move it there. Transform it. Same old story.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
SQLMesh on Databricks. Learning the obvious way.
What I usually do when trying to pick up the concepts of a new tool is to write some code. We could read docs all day, but that only gets you so far.
People can say whatever they want, but the proof is in the pudding.
The following commands with uv will make a new project, add sqlmesh, and init sqlmesh.
Once you run the sqlmesh init command, you get a walk-through on the CLI.
(
Note the option to migrate/use a dbt project
)
We select numro uno, and then we have an option to choose our engine, in this case Databricks. Once this is done, we can open up the folder in our favorite Editor. This is what we get.
Looks at the directory structure tells us a lot about SQLMesh, mostly that it‚Äôs a new version of dbt ‚Ä¶ :)
So, I‚Äôm kinda surprised SQLMesh didn‚Äôt ask me for Databricks connection information when I selected that option. But the reason becomes clear later. They don‚Äôt even throw a bone to a new guy. Buggers.
When I cracked open the config.yaml (
a pure guess on my part
), I got a link for Databricks + SQL Mesh connection options.
It appears we have two options ‚Ä¶
SQL only connection
SQL + Dataframe connection (Databricks Connect) ((which we must install ourselves))
I love me some Dataframes, but for ease of use, lest just go with SQL only option because I know most of you are Milk Toast Data Engineers.
uv add ""sqlmesh[databricks]""
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Connecting SQLMesh to Databricks Compute
So probably just like any other ‚Äúthingy‚Äù you want to connect to Databricks, you and I will need ‚Ä¶
PAT token
Compute URI from SQL Warehouse, All Purpose Compute, etc?
I will use the Start SQL Warehouse in my personal Databricks account, it‚Äôs easy enough to locate the connection information.
Once we have this info, back over to the config.yaml to add this information back in, along with our PAT (token).
Writing our first SQLMesh script.
So, now that we have the connection setup and ready to go, it‚Äôs time to just do some simple stuff and see how the concepts work out in real life.
We are going to use the
classic Divvy Bike trip dataset
, some of which are sitting in my s3 bucket.
Since this dataset has info about daily individual rides, I figured we could write an easy first ‚Äúmodel.‚Äù
models/divvy_rides_daily.sql
Above you can see the added file in the `models` folder, along with the code. Nothing exciting there.
What next?
Well, it‚Äôs good to get an error the first time, keep a vibe coder humble.
Let‚Äôs change to
INCREMENTAL_BY_TIME_RANGE
Try again.
Ok then, let‚Äôs sway out the model name to
sqlmesh_example.divvy_rides_daily
Try again.
Dang, I haven‚Äôt had this many errors since list time I compiled a Rust project.
This error seems strange to me, I know that CSV is correct and in that location, so I wonder if this is some sort of other error that is not being handled or understand well by SQLMesh, so it‚Äôs surfacing some nonsense.
I made sure to export AWS creds
I made sure Databricks can read that file via Notebook.
I noticed one thing when testing this ‚Ä¶
I should have been using
read_files()
and not
read_csv_auto()
This did not fix the same error, while looking at the file in Databricks, I did noticed I had the ride_start column with the wrong name, it‚Äôs actually started_at.
Part of me feels like SQLMesh isn‚Äôt even connecting to Databricks, but what do I know. Maybe I should just run
sqlmesh run
and see what happens.
There we go, an error that I can poke at.
Environment not found, I can do something with that. Maybe something wrong with my config YAML.
It seems like either the docs I read led me astray or I didn‚Äôt pay attention. Apparently I missed the concept of environments and didn‚Äôt set anything up.
Let‚Äôs try to fix this. Seems like there is nothing wrong with my ENVs and I just need to learn how to write a proper SQLMesh model.
Now, running sqlmesh plan gives something I can work with!
Let‚Äôs try running it now.
sqlmesh run
Well apparently that worked because now I have a bunch of junk in my Databricks Catalog related to this that I‚Äôm going to have to clean up later.
:(
I think I‚Äôve played around enough to get an idea.
Well, not that I‚Äôve cut my teeth on SQLMesh a little, and a very little at that, I‚Äôm patently unqualified to give me thoughts but I will do that anyway.
Also, I‚Äôve poked enough at DBT in the past to be able to think about that, as I write this, and so have at least a beginners perspective on SQLMesh in that frame.
I can understand the gap that SQLMesh is trying to fill.
The SQL ‚Äúmodel‚Äù SQLMesh isn‚Äôt in itself ground breaking.
SQLMesh seems to excel where dbt doesn‚Äôt.
configs
DevOps
Dev/Prod CI/CD etc.
Having models focus on freshness and intervals
I could be wrong here, but this my take.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
üîπ Key Advantages SQLMesh Brings
Environments & Plans
dbt:
uses schema naming conventions for ‚Äúdev/prod.‚Äù Switching contexts can be clunky.
SQLMesh:
explicit environments (
dev
,
prod
, feature branches). You can plan in dev, validate, then promote to prod ‚Äî no schema hacks.
Incremental by Time Range
dbt:
incremental
models, but you write your own
is_incremental()
filters; more ad-hoc.
SQLMesh:
native
INCREMENTAL_BY_TIME_RANGE
with enforced
@start_date/@end_date
macros, so scheduling, backfills, and re-runs are consistent.
Automatic Diffing
dbt:
you ‚Äúrun‚Äù and hope your SQL compiles, then inspect.
SQLMesh:
sqlmesh plan
diffs your project vs. deployed state and shows model-by-model changes before you apply.
Static Analysis & Linting
SQLMesh does column-level resolution before executing.
dbt only resolves at runtime ‚Äî errors show up when queries run in the warehouse.
Built-in Scheduling
dbt core:
no scheduler; you rely on dbt Cloud or an orchestrator (Airflow, Dagster).
SQLMesh:
models declare
cron
and batch size, and
sqlmesh run
respects those, so scheduling is intrinsic.
Execution Engines
dbt:
tied to warehouse SQL dialects.
SQLMesh:
runs in multiple engines (DuckDB, Spark, Databricks, BigQuery, etc.), with a more portable intermediate representation.
üîπ Where dbt Still Leads
Ecosystem & Community
: dbt has a massive ecosystem (packages, dbt Cloud, integrations).
Adoption
: many analytics orgs have standardized around dbt.
Simplicity for Analysts
: dbt is more approachable if you‚Äôre just doing SELECTs and don‚Äôt need complex scheduling/state semantics.
üèÜ TL;DR
Use dbt
if your team is mostly analytics engineers, you want a big community, and you‚Äôre comfortable plugging into an external orchestrator.
Use SQLMesh
if you want:
First-class dev/prod environments without schema hacks
Reproducible plans/diffs before deployment
Stronger guarantees around time-based incremental models
Built-in scheduling and metadata/state management
Think of
SQLMesh
as
‚Äúdbt + versioned environments + built-in scheduling + static analysis‚Äù
, while
dbt
is
‚ÄúSQL modeling + big ecosystem + simplicity.‚Äù"
166941914.apache-iceberg-on-databricks,Apache Iceberg on Databricks,even if it is a bad idea,2025-07-01 20:12:33.477-05,"At this point, I‚Äôm not sure if the news
that Databricks has full support for Apache Iceberg
is just a token, an offering to appease the rambunctious rabble and data gods, a way to quiet the complainers and keep lightning from falling from the sky.
Not sure. Maybe all of the above.
On the other hand, it can be seen as a
testament to Apache Iceberg's enduring strength
, as well as its widespread acceptance in the Lake House ecosystem as a tier-1 option.
However, I find it inferior to Delta Lake, primarily because it lacks full support in the broader Data Engineering tooling landscape, such as DuckDB.
It is what it is.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
What, you think it‚Äôs funny a newsletter sponsored by Delta Lake is writing about Apache Iceberg???!!! That should tell you something about the shifting landscape of the Lake House.
Today, we shall be the seekers of truth; we shall plumb the depths of Apache Iceberg on Databricks to see if the truth will set us free. Can you read and write Iceberg data with third-party tooling, both inside and outside the Databricks compute platform?
In theory, this should be possible thanks to Unity Catalog and the new Iceberg REST API.
‚ÄúFirst, you can now read and write Managed Iceberg tables using Databricks or external Iceberg engines via Unity Catalog‚Äôs Iceberg REST Catalog API.‚Äù -
announcement
I‚Äôm not sure if this will slowly melt the ice-cold (
pun intended
) hearts of all those angry Apache Iceberg trolls who‚Äôve been ticked at Databricks for not supporting Iceberg more.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Setting the Stage.
Lucky for us, those nerds over at Databricks never fail to serve up a
pile-o-docs
to go along with every new thingy they release, full support for Apache Iceberg being no different.
A few notes that I don‚Äôt care about, but that I feel obligated to list.
Apache Iceberg in Databricks support can be done via two routes.
Unity Catalog managed Iceberg
‚Äú
Foreign Catalog
‚Äù table.
You can create Iceberg tables via normal SparkSQL with DBR runtime, or with an external engine.
This sort of external engine + Databricks Iceberg access requires you to
throw salt over your left shoulder, speak the secret incantation in Latin, and meet a wizard under a tree at midnight.
Enable
External data access
for your metastore
Grant the principal configuring the integration the
EXTERNAL USE SCHEMA
privilege on the schema
Authenticate using a Databricks personal access token
Aka ‚Ä¶ they want you to get tired of such things and use Delta Lake ‚Ä¶ (
not going to lie, that‚Äôs what I think you should do
).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Trying it out with code ‚Ä¶ and Polars + PyIceberg.
To make sure we can do what Databricks says we can, let‚Äôs use Polars + PyIceberg (
because Iceberg sucks
) and on our local machine, seeing if we can reach out and ‚Ä¶
create a Unity Catalog Iceberg table
write data to that table
read data back from table
Again ‚Ä¶ this will be done with compute and tooling
OUTSIDE
of the Databricks environment. I mean this is the whole point behind that great announcement after all.
Let‚Äôs start by using the
Divvy Bike Trips open source dataset
.
Should be simple enough to turn this data into a Databricks Iceberg table with PyIceberg and Polars. If you run something like the following in a Python:latest Docker image ‚Ä¶ this will get you up and running.
docker run -it python:latest /bin/bash
>> apt-get update
>> apt-get install vim wget
>> wget https://divvy-tripdata.s3.amazonaws.com/202505-divvy-tripdata.zip
>> unzip 202505-divvy-tripdata.zip
>> vim test.py # copy code in here
>> pip3 install polars pyiceberg pyarrow
>> python3 test.py
And the code ‚Ä¶
Notice how straight forward this is, all being done in a Docker container on my machine, reaching out to my Databricks Unity Catalog setup ‚Ä¶
Well, this is interesting. Every time I run the code I get some conflicting errors from pyiceberg/Databricks.
I‚Äôm getting both
NotAuthorized
, but all
TableAlreadyExistsError
. The
TableAlreadyExistsError
must not be true because I tried random and different table names and the same error came through. Must be something with permissions.
I did create a brand new PAT (token) just for this, so that‚Äôs not it.
External data access is turned on ‚Ä¶ see below
I mean I guess I can grant myself, the Admin, more permissions explicitly?
GRANT EXTERNAL USE SCHEMA ON SCHEMA confessions.default TO `dancrystalbeach@gmail.com`
Well, that did the trick. Maybe I should just follow my own instructions in the first place.
Look-ye-thar.
A nice Iceberg table appears in my Databricks Unity Catalog ‚Ä¶ and I wouldn‚Äôt even know it was an Iceberg table.
Of course, we can interact with this Apache Iceberg table in a Notebook with Databricks SQL like we would with any other Delta Table or whatever.
What about reading this data from our local machine with Polars?
and the results ‚Ä¶
There you have it, Databricks didn‚Äôt lie to use. All hail the new
Thanos
of Catalogs. Unity Catalog wins all.
To be honest, you can setup a Databricks account and run it for basically no cost (I do), and now you have an catalog you can use to manage all our Apache Iceberg tables???
No setting up a third party catalog and managing it, you get all the benefits of Databricks ‚Ä¶ AND an easy to use and bullet proof Apache Iceberg Catalog (Unity), that works with Delta Lake too??? The future is here my friend.
Why the crap would you use anything else is beyond me.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
166897198.polars-in-production-on-aws-lambda,Polars in Production on AWS Lambda,ramblings of an old Data Engineer,2025-06-30 07:30:50.933-05,"The mental whiplash I experience inside my head when writing various articles to keep you fickle folk entertained sometimes gives me a headache. For my sanity, I prefer to switch between simply pontificating on writing some random bit of code, and rather heady topics like how not to suck at writing code.
Today, we shall walk down the well-worn path of a random piece of code. In this case,
running Polars in production on AWS Lambda
.
Why, you might ask? Because most of the ‚Äúcreators‚Äù and ‚Äúinfluencers‚Äù that I‚Äôve run into over the years might start out technical, but then they end up wandering down the road of theory ‚Ä¶
never to be seen again
.
In my opinion, the content then becomes bland, boring, and disconnected from reality, like the reality of running actual Polars code, in actual production, on an actual AWS Lambda.
We will leave the hello-world crap for the birds.
Let‚Äôs talk about real stuff.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Polars in production.
I've been running a sprinkling of Polars in production for a year or two now, with varied success. The truth is, when tools are new and fresh off the Frankenstein block, there are a lot of rough edges that the mistakes of a thousand users haven‚Äôt worn off.
Such was Polars when I found that spunky upstart before it was ‚Äúcool‚Äù to write about using Polars on your laptop.
I mean, even those lowlifes on
r/dataengineering
are still wondering if anyone is
actually
using Polars in production.
Most of those hobbits are probably still using Pandas like a bunch of ancient sissies stuck worshipping the old gods. Not sure what they are waiting for or what they see in the old ways. Probably just lazy.
Anywho.
Polars.
Strange problems.
Recently, I had to perform the simplest of tasks, although it was strange, I must admit. We were receiving CSV files that contained rows with specific columns that were empty because the following values matched the first one, until a different value finally appeared.
Let me show you.
+-----------+-----------+---------+
| City      | Event      | Attends |
+-----------+-----------+---------+
| Chicago   | Marathon   | 5000    |
|           | Parade     | 8000    |
|           | Concert    | 12000   |
| Austin    | Festival   | 10000   |
|           | Rodeo      | 7000    |
+-----------+-----------+---------+
‚Ä¶ and what it needs to look like is this ‚Ä¶
+-----------+-----------+---------+
| City      | Event      | Attends |
+-----------+-----------+---------+
| Chicago   | Marathon   | 5000    |
| Chicago   | Parade     | 8000    |
| Chicago   | Concert    | 12000   |
| Austin    | Festival   | 10000   |
| Austin    | Rodeo      | 7000    |
+-----------+-----------+---------+
Except this with multiple columns, very strange indeed. You can think about this problem as ‚Äúforward-fill.‚Äù We need to forward fill any NULL values until the following valid input for each column.
I decided to solve this problem using Polars and an AWS Lambda, with an S3 trigger to process incoming CSV files using Polars. I‚Äôm sharing this seemingly strange and boring problem so I can talk about
WHY
I did what I did, and what I found when using Polars.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The Code.
This code is simple, but that‚Äôs kind of the point. Part of the beauty of solving a problem in a
business context
is doing it reasonably, without spending inordinate amounts of time doing so‚Äîall the while, keeping the entire process easy and simple, including the architecture.
It‚Äôs easy to get carried away with fancy things when thinking about a problem like forward fill.
I mean we could have used PySpark to solve this problem. But why use Spark to solve a problem with singular, smallish CSV files coming in?
Either way, using a fast Lambda with Polars seemed like the right approach.
The first thing I found when deploying a Polars AWS Lambda was that you immediately got an error about the code puking.
called `Result::unwrap()` on an `Err` value: Custom { kind: Other, error: ""error initializing temporary directory: could not load $USER or $HOME environment variables consider explicitly setting POLARS_TEMP_DIR"" }

called `Result::unwrap()` on an `Err` value: Custom { kind: Other, error: ""error initializing temporary directory: could not load $USER or $HOME environment variables consider explicitly setting POLARS_TEMP_DIR"" }
Should we blame AWS or Polars? Or both??
I blame Polars.
Clearly, the majority of Polars users are working on their laptops and rarely encounter errors like this (when deploying it in
production
).
Or none of them are using the Lazy features maybe.
A simple line of code fixed this issue, aligning Polars to find a tmp directory that jives with an AWS Lambda.
os.environ[""POLARS_TEMP_DIR""] = '/tmp'
Like I said, always rough and strange edges on things that are newish, and yes, Polars is newish in Production environments for
most
of the world.
I also found this logic a little awkward to write, maybe it‚Äôs fitted more to SQL?
The whole
pl.col(‚Äúsomething‚Äù).str.strip_chars()
is annoying as crud. Again, its newness is showing its true colors.
When something is used frequently and becomes annoying, you end up with features like PySpark and
trim()
.
Another super annoying part of Polars is the whole infer schema thing. When working with large CSV files, Polars tries to infer a schema and 80% will actually get it wrong.
It will assume an INT in the case of a STRING, and promptly blow up.
df = pl.scan_csv(f""s3://{bucket}/{key}"", has_header=True, infer_schema=False)
Typically, Polars will default to infer the schema and guess what
infer_schema_length
is set to? 100. Blah. But who wants to sniff 1000 rows for all the columns just for the data types?
Not me Sunny-Jim.
Set
infer_schema=False and go on your merry way. It‚Äôs a flipping CSV ‚Ä¶ everything is a STRING you ninny.
But, lest you think I‚Äôm being hard on Polars, I am not, it‚Äôs new, I get it, and the variety of problems this tool can solve is incredible. It‚Äôs fast, reliable, and can process mass amounts of data.
For example, look at all the options they give you when scaning a simple CSV file.
I mean, I‚Äôve never seen anything like it ‚Ä¶ and that‚Äôs just a CSV file. Now there is a tool made for the masses.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I‚Äôm quite sure one could, with little effort, build an entire Data Stack with simply AWS Lambda and ‚Äúlazy‚Äù (streaming) Polars. It would be ‚Ä¶
cost-effective
easy and simple
powerful and fast
There is a lesson in there somewhere for us all."
166342788.data-cleansing-for-dummies,Data Cleansing for Dummies,"yeah, that's what I said.",2025-07-07 10:52:51.484-05,"It‚Äôs been a while since I did any ‚Äú
back to the basics
‚Äù posts. I imagine all you lovely readers sitting there with your giant brains picking apart everything I say. Who‚Äôs got any time for the simple stuff??
Recently, someone requested I talk(write) about Data Cleaning at a high level, for Data Engineers, Analysts, Scientists, and the like. It seems like a simple topic,
but it‚Äôs one that's ignored chiefly on Data Teams.
Do you mean to tell me that, living in the Age of AI, SaaS products have reached the zenith of technological advancement, and ‚ÄúData Quality‚Äù tooling is at your fingertips, requiring zero work or setup ‚Ä¶ and we still have data quality problems?
Of course. Surprised?
I hope not.
This week‚Äôs newsletter is sponsored by
networthtracker.io
Interested in FIRE? Want to kick your boss to the curb?
networthtracker.io
is a free website where you can simply and easily play with your financial numbers to divine your net worth and where it‚Äôs headed.
Back to our regularly scheduled programming.
Data Cleansing - starting from scratch.
Okay, so data quality, data cleaning, and data governance are all the same.  Since before time began, it was decreed that all data should henceforth be dirty, confusing, and full of surprises.
Having done this ‚Äúdata thing‚Äù for over two decades, with many promises and grand statements from a plethora of talking heads and vendors, we find ourselves in the same place. Data is often considered dirty, and data quality is frequently overlooked and ignored as useless by the majority of Data Teams.
Sure, they might give it some lip service, but lip service is all it is.
I‚Äôm going to propose that there are two levels of Data Cleansing (call it Data Quality if you want).
Low Level (DDL, constraints, etc.)
High Level (SaaS tooling, alerts, etc.)
Most people mistakenly assume that to start with data cleansing, one must find the latest DQ SaaS or open-source tool and get to work. This is rarely the case;
it is detrimental, ignores the basics, and leads to lackluster results
.
‚Äú
All good data cleansing and data quality work starts with a solid foundation built upon data types and constraints.
‚Äù - me
So, where should you start?
Foundations of Data Cleansing and Quality
When starting with a new data set, there is likely a plan to ingest that data into a process that will assume or look for certain things within that data.
You must start with the basics.
review columns to settle on Data Types
understand NULLable columns vs not
understand column data (constraints)
Let‚Äôs use the
example of Divvy Bike Trip data
, and walk through that. If you think this sort of idea of DDL, data types, and constraints is getting ahead of ourselves,
but you can‚Äôt clean data you don't understand
.
Spending time to understand data upfront will make data cleansing make more sense and go smoothly on the backend.
It‚Äôs incredible to me that about 1% of data workers when dealing with a new dataset, or working on ‚Äú
data cleansing
‚Äù ‚Ä¶ will NOT take the following steps.
decide on DDL/Data Types
understand where constraints apply
understand where NULLs apply
Sounds too easy? Yeah, well, how are you supposed to ‚Äúclean‚Äù datasets that you don‚Äôt understand
WHAT
the data is
SUPPOSED
to look like??
In our example case, we could say ‚Ä¶
This is not rocket science; it's simply understanding the data before us and what we can infer from that data. To avoid having to do this work manually, consider using a tool like DuckDB for your data exploration.
I mean, you are smart folk, you get the picture.
So what's next?
You tell me. Of course, once you have done this sort of data exploration and understand what you are dealing with, the data cleaning and quality part becomes much more apparent, and the puzzle pieces are more likely to fall into place.
In the Lake House world we live in,
using tools like Delta Lake
or Iceberg, this sort of data cleaning via constraints and checks has tier 1 support.
The sky is the limit these days; there are plenty of open-source tools that can make data cleaning a fairly easy and fun process. DuckDB, Polars, etc, these tools make munging data straightforward.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What are other low-level data cleaning steps?
What else can we do besides data types, constraints, etc to wrestle the data into submission?
apply obvious and descriptive column headers
DESCRIPTION ‚Üí product_description
CUR ‚Üí currency
add descriptions and comments to code and data.
work with Product (or whoever) to understand data more deeply.
Of course, with these basics we are just scratching the surface of data cleaning and quality, the problem is that everyone wants to skip these steps and jump straight to the ‚Äúfun‚Äù and ‚Äúfancy.‚Äù
Fun and Fancy won‚Äôt work well if you feed it slop.
Of course, there are always the plethora of data cleaning and quality tools available, some of which I‚Äôve talked about.
More fancier stuff.
Once you have mastered the basics and are ready to go on to glory, the data world is your oyster, and there is no lack of tools like Soda, Great Expectations, DQX, and the like to spend the rest of your life playing with.
Eventually, you would want ‚Ä¶
alerting
monitoring
observability
auto-healing
quarantine
etc
.
But, to be honest, most people haven‚Äôt done the simple stuff we talked about, and you can buy and use whatever fancy tool you want, and your life will be stuck if you don‚Äôt use data types, constraints, and checks to control your data cleaning and quality.
Books could, and have, been written on each of those bullet points above. I suggest a more methodical and nuanced approach that begins with tight schemas and proceeds slowly from there.
Best of luck, you bunch of dirty data hobbits.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
166237954.rust-python-perl-cobol-php-oh-my,"Rust, Python, Perl, COBOL, PHP ... oh my.",a tale of some languages (Anonymous Rust Dev),2025-06-23 07:17:26.951-05,"We all love our little language wars:
the vitriol, the expletives, the dank memes that accompany them.
You may recall, as your friendly neighborhood
Anonymous Rust Dev
, that I've been magnanimous and kind to the Python language. This is because I'm above it all - I can find the beauty in any language if I look hard enough, and I'm willing to give credit where it's due.
But even I have my breaking point, which I'd like to explore today. How clunky does a language need to be to make itself my foe?
The baselines
Well, I can't sit here and talk up a language without the receipts. So I'll set some ground rules that I'll use as the basis for my ""highly objective"" analysis.
It's okay to use third-party libraries, but more points go to a well-organized dependency-management ecosystem, and deducted for having to scrape snippets out of some decades-long-lost comment buried in an
experts-exchange.com
thread.
Code (including dependencies) should be readable and lend itself well to understanding the problem it's solving.
Comments are an insufficient crutch to carry a language's expressivity; the code itself needs to be self-explanatory to such a degree that a C-suite executive could look at it and understand at a high level what's going on (assuming good variable and function naming conventions are followed).
While I don't kowtow to language popularity (else, I'd probably be a Java dev), points will be deducted if a language is obscure and nobody is left who knows how to maintain our code in the future.
Whatever is chosen should be backed by a standard, and we won't favor variants.
Meaning, if we chose something like SQL, we'd base our results off of the standard and not some flavor like MySQL.
The difficulty of creating a new project and building it is assessed. One-liners are favored, while tedious hand-edited build scripts or arcane commands are penalized.
No points removed for reaching into the third-party ecosystem to find a useful build tool that manages this pain (e.g. using
yarn
instead of
npm
).
Point 3 automatically rules out some obvious choices, such as
Brainfsck
or
ArnoldC
. Also, despite what I say in #4, I don't think SQL qualifies for this discussion, as it's not technically a ""programming"" language (probably
no love lost here
, anyway).
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Getting Started
Regarding the exercise in question, we aim to transform a simple CSV file into a semi-structured JSON file. Our data source will be located in
/tmp/test.csv
and have the following raw contents:
last_name,first_name,age
Jones,Steve,3
Smith,Nick,50
Yeah, it's hardly the fanciest thing out there, but we're not looking to make this a complicated test.
Output will be saved to
/tmp/test.json
. I don't particularly care if it's formatted for readability or compactness, though in the real world, you'd want a library capable of managing such details.
First contender: Rust
Our journey here starts in the CLI:
$ cargo new csvtojson_rs && cd csvtojson_rs
$ cargo add serde serde_json csv --features serde/derive
My
src/main.rs
file:
use serde::{Deserialize, Serialize};

const PATH_TO_CSV_FILE: &str = ""/tmp/test.csv"";
const PATH_TO_JSON_FILE: &str = ""/tmp/test.json"";

#[derive(Debug, Deserialize, Serialize)]
struct Person {
    pub last_name: String,
    pub first_name: String,
    pub age: i32,
}

fn main() {
    let reader = csv::Reader::from_path(PATH_TO_CSV_FILE).expect(""should open test csv file"");
    let mut output = vec![];

    for row in reader.into_deserialize() {
        let person: Person = row.expect(""should be valid person"");

        output.push(person);
    }

    std::fs::write(
        PATH_TO_JSON_FILE,
        serde_json::to_string(&output).expect(""should serialize to JSON""),
    ).expect(""should write file to disk"");
}
In this example, I did the work of defining the schema via the
Person
struct, and handled the possible errors using the less-than-graceful approach of
.expect()
. Whatever, this is a get-it-out-the-door effort.
$ cargo run
In this Rust-based example, I leaned very heavily into third-party crates for serialization and deserialization for the different formats. In particular,
serde
carries us, handling both the CSV and JSON side of the equation under the hood.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Second contender: Python
Again, from the CLI side:
$ uv init csvtojson_py && cd csvtojson_py
...and our resulting
hello.py
file:
import csv
import json

PATH_TO_CSV_FILE = ""/tmp/test.csv""
PATH_TO_JSON_FILE = ""/tmp/test.json""

def main():
    recs = []
    with open(PATH_TO_CSV_FILE) as csv_file:
        csv_reader = csv.DictReader(csv_file)
        for row in csv_reader:
            recs.append(row)

    with open(PATH_TO_JSON_FILE, 'w') as json_file:
        json_file.write(json.dumps(recs))

if __name__ == ""__main__"":
    main()
Execution is simple, too:
$ uv run hello.py
You'll note that, unlike Rust, the JSON and CSV libraries are components of the Python Standard Library. This makes sense from both Rust and Python's perspective - in the case of Python, they're both so ubiquitous that making them a well-supported function of the language runtime costs little and offers much.
In Rust's case, owing to its use as a systems language, many of its applications are built without the benefit of the standard libraries (""no-std""), which is frequently seen in embedded applications; in that case, serde (and its derivatives like the CSV and JSON libraries) are decoupled from the standard library and can be used in no-std environments.
A blast from the past - Perl
Okay, I know people still use perl for some reason, which thankfully also has its own third-party ecosystem (CPAN) to get around having to regex our way through a delimited text document.
If you're familiar with CSV, you know that there are many variants and idiosyncrasies (optional text wrapping, multiline values, escape characters) to complicate things, and having to write a regex to handle this (which I believe is the perl way to do it) would not be pleasant.
In case you're curious about what I feared having to write:
this SO answer
gives a good representative example.
I've never used CPAN before, so had to hit up the
installation
instructions to do this. At least here in Kubuntu, I have no problems just firing up
cpan
or
perl
at the CLI, but chose to go through their
cpanm
installation process, which forced me into a sudo prompt:
$ cpan   # Ran this guy first just to force me through its onboarding prompts; note the profile script recommendations
         # it gives at the end of the process.
$ sudo cpan App::cpanminus
With that done, let's try creating a new project:
$ mkdir csvtojson_pl && cd csvtojson_pl
$ sudo cpan Text::CSV   # Got yelled at when I tried it without sudo; very frustrating.
To bootstrap this process, since again I'm new at this, I wanted to first know whether I could read my file, and what I was looking at data-wise:
use Data::Dumper;
use Text::CSV qw(csv);

my $doc = csv(in => ""/tmp/test.csv"", headers => ""auto"");

print Dumper($doc);
Running my script gives me:
$ perl script.pl
# $VAR1 = [
#           {
#             'age' => '3',
#             'last_name' => 'Jones',
#             'first_name' => 'Steve'
#           },
#           {
#             'age' => '50',
#             'last_name' => 'Smith',
#             'first_name' => 'Nick'
#           }
#         ];
Okay, this is what I like to see. Now to figure out the JSON part.
First, I went to install the JSON library, but CPAN told me it was already installed?
$ sudo cpan JSON
Looking at the
docs
, it seems straightforward to use, so I'll just give it the old community college try:
use Text::CSV qw(csv);
use JSON;

my $csv_path = ""/tmp/test.csv"";
my $json_path = ""/tmp/test.json"";

my $in_doc = csv(in => $csv_path, headers => ""auto"");
my $out_doc = encode_json $in_doc;

open(JSON_OUT, "">"", $json_path) or die $!;
print JSON_OUT $out_doc;
close(JSON_OUT);
Working with the file API kind of reminds me of VB6. Otherwise, this was surprisingly less ugly and painful than I originally thought it would be.
Telling my code
do something... or die
does feel a bit fatalistic, and I never got to
chomp
on anything, but CPAN smoothed over a lot of pain points. While I'd probably never choose to author a new perl script in the future, I can at least rest assured that if the occasion does arise it's not as gnarly as I feared.
Actually, the bad rep that perl has with me seems undeserved, but
the internet
acknowledges some good reasons for its general reputation.
Also worth mentioning: perl (v5) is almost certainly available out-of-the-box on *nix systems, so if you (like me) hate writing bash scripts, this might be a good alternative.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
COBOL
I'm doing this one on a dare. My compiler of choice is
OpenCOBOL
GnuCOBOL
. I wrote some of this stuff back in my college days, and I don't remember it being all that bad.
Just to be clear, I think greenfield development in COBOL would be stupid. However, just as C++'s death in the face of Rust is very premature, so also is COBOL's demise, given just how much running (and working, I might add) code is out there in the wild today. You don't fix what ain't broke, and that means this language still might be worth picking up even in 2025.
Looking at GnuCOBOL's
docs
, we learn how to install it. Well, I went ahead and already tried it with
apt install gnucobol
and was met with success, but if you want to try building from source then this is a good starting point. I also suspect you'll find a docker container if you don't want to install this stuff on your computer.
Thinking differently about files
Okay, so disclaimer: my trivial test CSV file turns out to be a happy accident. COBOL has a function called
UNSTRING
that does the heavy lifting for us, but would have struggled with some of those earlier-mentioned idiosyncrasies I brought up in the perl section. For our purposes today, I think we'll have no trouble, but I suspect that arcane file formats might not be COBOL's greatest strength.
Which takes us to JSON, which is surprisingly supported in COBOL via commands like
JSON GENERATE
or
JSON PARSE
. I guess I shouldn't be terribly surprised; as stated already, people still write COBOL, and the language has evolved to help modern developers keep writing code for the current age.
Well, now that the feasibility study is complete, let's give it a go.
$ mkdir csvtojson_cob && cd csvtojson_cob
...and our
program.cob
file:
IDENTIFICATION DIVISION.
       PROGRAM-ID. CsvToJson.
       AUTHOR. AnonymousRustDev.
       
       ENVIRONMENT DIVISION.
       INPUT-OUTPUT SECTION.
       FILE-CONTROL.
           SELECT CSVIN ASSIGN TO ""/tmp/test.csv""
                  ORGANIZATION IS LINE SEQUENTIAL.
           SELECT JSONOUT ASSIGN TO ""/tmp/test.json"".
       
       DATA DIVISION.
       FILE SECTION.
       FD CSVIN
           BLOCK CONTAINS 0 RECORDS
           DATA RECORD IS TEXT-LINE.
       01 TEXT-LINE PIC X(1000).
       FD JSONOUT
           BLOCK CONTAINS 0 RECORDS
           DATA RECORD IS JSON-TEXT.
       01 JSON-TEXT PIC X(1000).

       WORKING-STORAGE SECTION.
       01 PERSON-RECORD.
           02 last_name PIC X(20).
           02 first_name PIC X(20).
           02 age PIC 9(2).
       01 JSON-CURRENT PIC X(255) VALUE SPACES.
       01 COUNTER PIC 9(1).
       
       PROCEDURE DIVISION.
       Begin.
           OPEN INPUT CSVIN.
           OPEN OUTPUT JSONOUT.

           STRING '[' INTO JSON-TEXT
           WRITE JSON-TEXT.

      *    Skip the first (header) row:
           READ CSVIN
           END-READ

           PERFORM UNTIL EXIT
                READ CSVIN INTO TEXT-LINE
                    AT END EXIT PERFORM
                END-READ

                UNSTRING TEXT-LINE DELIMITED BY ','
                    INTO last_name, first_name, age
                END-UNSTRING

                JSON GENERATE JSON-CURRENT FROM PERSON-RECORD
      *             Without the omit, the entity type name is the parent key
                    NAME OF PERSON-RECORD IS OMITTED
                    ON EXCEPTION
                        DISPLAY 'No can do.'
                    NOT ON EXCEPTION
                        DISPLAY 'generated'
                END-JSON

                IF COUNTER > 0 THEN
                    STRING ',' DELIMITED BY SIZE,
                        function trim(JSON-CURRENT) DELIMITED BY SIZE 
                        INTO JSON-TEXT
                    END-STRING
                ELSE
                    STRING function trim(JSON-CURRENT) DELIMITED BY SIZE 
                        INTO JSON-TEXT
                    END-STRING
                END-IF
                WRITE JSON-TEXT

                ADD 1 TO COUNTER
           END-PERFORM

           INITIALIZE JSON-TEXT
           STRING ']' INTO JSON-TEXT
           WRITE JSON-TEXT.

           CLOSE CSVIN.
           CLOSE JSONOUT.
...I hate it.
That was the worst programming experience I think I've had in years. I couldn't figure out how to create a variable-length array, so ended up having to mash JSON records together by hand.
And finding a version of GnuCOBOL that actually supports the JSON GENERATE command correctly took effort; much time was lost just troubleshooting why my strings failed to actually exist until I stumbled upon a
v4.0 Docker container
.
Try to google for anything COBOL-related, and IBM hits are at the top of the list, except everything they give you to work with is a
cryptic BNF diagram
with little by way of example usage (yes, I'm that stupid, I need examples to figure stuff out). Like, I grew up in the early 90s era with a DOS 4.0 manual explaining how to use the
debug
command to hand-roll assembly, and that was a cakewalk by comparison.
Somebody please brain me with a rusty spork if I ever suggest writing COBOL again. And I'm sure some avid COBOL dev looks at what I wrote, shakes their head in disdain, and my feelings are only even more validated in the process; no part of this process was intuitive or easy, and my code is obviously ham-fisted.
PHP
I needed a palate cleanser after that last one, so I reached for a breath of fresh air: PHP, the beloved and ubiquitous language everybody reaches for when... uh, Wordpress, I suppose.
First, if you're on Linux or Mac, chances are you either have PHP or can get it very quickly through a package manager. Windows has installers too, so low-hanging fruit.
<?php
$csv_in = trim(file_get_contents('/tmp/test.csv'));

// So it goes without saying that there are libraries or other techniques for this,
// but I'mma hand-roll this beast:
$lines = explode(""\n"", $csv_in);
$first_line = array_shift($lines);
$object_keys = explode(',', $first_line);

$records = [];

foreach($lines as $current_line) {
    $as_split = explode(',', $current_line);
    $records[] = array_combine($object_keys, $as_split);
}

file_put_contents('/tmp/test.json', json_encode($records));
For those already in the know, it must be painful seeing me not reach for Composer. There is a perfectly useful
CSV library
that I've had great success with, and others may already be aware that some of the
explode
stuff would go away if I used the shorthand helper
str_getcsv
. Nonetheless, it only gets simpler than what I did above (at least for our trivial CSV file).
Weighing in our contestants
Now that you've seen the code and the struggles involved, here's how I'd rate things in terms of my earlier bullet points:
Third-party ecosystem
For this one, the only one that doesn't get any points is COBOL - not because no ecosystem exists, but I've spent far too much time trying to get any meaningful information out of that community. For everybody else, full points awarded for having a rich and diverse ecosystem from which to pull any needed already-solved solutions.
Code as prose
It's hard to be fair here - it's such a subjective thing, after all.
First, I wanted to award full points to COBOL, as this was its original claim to fame after all. But, looking at what I wrote earlier, I don't think this is necessarily true - there might be a lot of English in there, but it isn't exactly readable outside of the
PROCEDURE DIVISION
portion.
The
FD
declarations feel like they're trying too hard to be human-readable, but contrast with the
PIC
declarations (I think it means ""picture""? Like my data is some kind of drawing?).
Meanwhile, I came into this thinking perl would be cryptic and arcane, but it ended up feeling a lot more like PHP or Python in my own opinion.
And, let's not lie to ourselves, the Rust language has a reputation for being complex. My example doesn't really demonstrate just how quickly the code can get gnarly to a novice reader, but if you're just encountering it for the first time today in this article, I'd assume you'd rate it lower than the ""P-languages"" (perl, Python, PHP). In the interest of fairness, I'll make that judgment call for you here and now - the ""P-languages"" win, followed by Rust, and finally COBOL.
Obscurity
COBOL and perl are the obvious examples here of ""yesterday's news"" in terms of languages, with the former being a glaring example of something that people want to distance themselves from. Yes, people still learn COBOL in this day and age, but it's a rarity and not representative.
Likewise, despite its availability, perl has a lot going against it. Aside from a few old GNU types, there probably aren't a lot of advocates for the language floating around, and my casual searching for info while composing this article tells me that even seasoned perl devs find it to be flawed.
Many probably wish PHP would go the same route, but it unfortunately continues to improve over time and is actually a halfway decent language. I'd actually put it a bit higher than Python, particularly where OOP and types are concerned, as in my own experience the Python type hints are more of a gentle suggestion than something that's actually enforced.
Rust is soon to have its day, evidenced in incremental grabs of market share year-over-year. It's not yet found a mainstream place - a lot of developers wish there were more Rust jobs than there actually are, and businesses are still hesitant to adopt it for assorted reasons (e.g. the relatively smaller talent pool than a ""boring"" language like Java).
And then there's Python, which is easily the most popular of the pack. I won't even spend any time discussing it, it's pretty self-evident.
Standards
GnuCOBOL feels like it's trying to be the MySQL of the COBOL community. I assume that all the enterprise-level COBOL devs are running IBM or some other mainframe flavor, and my impression is that GnuCOBOL is the spunky new kid that's racing to catch up to the old guard. The trouble I had with
JSON GENERATE
really drives this home - while I did eventually succeed with that v4.0, the one that came out of my Ubuntu repository left me with
this mess
.
Launching a new project
Thanks to
uv
, I won't take any points away from Python; normally, my experience with
venv
and
pip
leave much to be desired, but today's exercise shows how easy it can be.
Had I bothered with
composer
, the PHP example would have been fairly straightforward.
Even with perl, the CPAN repository is perhaps the grandpappy of library management as we know it today.
Rust's
cargo
is perhaps my own favorite, on par with
uv
with regards to its ease-of-use.
That leaves COBOL. I'm really not sure how to categorize this; all I did was create a .cob file and call it a day, but I have no idea how this scales when you start adding dependencies. I hope to never find out.
Is there a takeaway here?
For me, this was a chance to play with some stuff I normally don't get to touch. I'm not about to abandon my current ways of doing things after this exercise, but hopefully for those on the fence you can see that it's not really all that scary out there in terms of other languages.
Granted, I didn't touch some of the weirder stuff, and probably skipped over your favorite language in the process (any Ada fans out there? Fortran? Lua?). Feel free to write your own article espousing how I glossed over the greatest language known to man, and I will gladly read it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
165909036.golang-with-duckdb-and-more,"Golang with DuckDB, and more.",absent from Data Engineering,2025-06-17 17:59:55.55-05,"Recently, I was reading through some documentation on DuckDB and came across information about Golang. I‚Äôve never gotten bit by the Golang bug, as clearly seen when looking at my
109 public GitHub repo, only 5 of those little blighters are written in Golang
.
That, and it was 2022, the last time some Golang slipped through my fingers.
Why is that?
I‚Äôve found more use with Rust than Golang,
but I‚Äôm no Golang hater
; I can see the beauty in its simplicity and speed. Sometimes, it is quite confusing as to WHY it never caught on in Data Engineering or data context.
I mean, I did write an article about why Golang lost to Rust in the battle for Data Engineering supremacy. One would like to think it boils down to something as simple as Garbage Collection, but that can‚Äôt be true in a data world awash with Python.
(code on GitHub)
Clearly, the reasons must be more complicated and deeper as to why Golang couldn‚Äôt get its claws into the data space.
Even the hungry rabble on Reddit can‚Äôt deny that
Golang simply isn‚Äôt suitable for most Data Engineering work.
In theory, you CAN do anything you like, but there is a difference between being able to do something vs
should you do something.
To me, it‚Äôs clear why Golang hasn‚Äôt caught on in data, and the above Reddit comment gives you a glaring clue as to why.
Integrations
and
interoperability
.
Data is a land built upon the bones of a thousand tools; SDKs and integrations are at the core of a data engineering toolset.
The simple truth is that for a long time, Golang simply lacked first-class support for the myriad of tooling, SDK, and APIs that make up most of what Data Engineer is ‚Ä¶ namely, using a plethora of tools to manipulate ‚Äústuff.‚Äù
Sure, that may have changed now, but it‚Äôs too late; the fact that Polars, DataFusion, etc., etc. got to the SQL and Dataframe-centric world of Data Engineering first sealed the fate of Golang in data. Sad but true.
Back in 2022,
I did test the ONLY option at the time for Dataframes in Golang
‚Ä¶ and it wasn‚Äôt pretty. Slow, clunky, and just not fun. No one in their right mind would put themselves through that pain when much better options exist.
Anywho, time grows short, and I wax poetic; today, I thought I would like to revisit Golang in regards to some of the more popular tools today, and see what gives after a few years have passed.
DuckDB. Delta Lake. Dataframes. ‚Üê Golang.
Has anything changed? Probably, let‚Äôs find out.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
DuckDB with Golang.
I‚Äôm honestly just curious, more than anything, what it looks like to write some Golang these days and attempt to use a newish package like DuckDB.
Is the code clean and nice? Is it a pain? What about versions and working stuff together?
I don‚Äôt know; it doesn‚Äôt seem special by any means; one could argue that boring is good.
Using Golang doesn‚Äôt seem much different than using Golang in Python or whatever else, that probably says more about DuckDB than Golang.
I want to use something new like DuckLake with Golang, but the default Golang-DuckDB package doesn‚Äôt come with the latest DuckDB version that would ALLOW you to use DuckLake + Golang. I tried. Doesn‚Äôt work.
I fought it for a little bit but gave up.
Indeed, this is one of the main reasons Golang is not widely adopted in Data Engineering. When the community is smaller (e.g., Golang) than Rust, Python, Scala, Java, etc.,
in terms of data-centric tooling, it will fall behind, and it‚Äôs a self-fulfilling prophecy.
BTW, below is my Docker container if you‚Äôre interested in doing the same thing.
I guess that‚Äôs sorta the point with Golang and tools like DuckDB and DuckLake. DuckDB is relatively new, DuckLake even newer, and Go is somewhat obscure in the context of data engineering and tooling. Hence, I encountered strange errors related to the official DuckDB Golang package.
(Code on GitHub)
One could say I‚Äôm not trying very hard, and that is true, but at this point in Data Engineering, I don‚Äôt need to try hard. Every other known tool and platform in Data Engineering runs DuckDB like a champ without.
Share
Golang with Dataframes.
After my weak attempt with Golang and DuckDB, I will pretend like that never happened. I was curious after a few years, if Golang had finally caught up in what I would call the ‚ÄúDataframe‚Äù centric space.
This is important.
To be taken seriously in the Data world, a language would have to have at least a few easy-to-use Dataframe tools. Why? Because the data world revolves around tabular data, Dataframes or SQL is the preferred abstraction.
gota
is at the top of the list on Google. Simply looking at the examples makes my toes curl. There isn‚Äôt a soul on God‚Äôs green earth who would raise their hand and say they want to filter a Dataframe like this.
I mean,
even writing Rust with Polars to do Dataframe stuff
isn‚Äôt that ugly.
Heck, if you google ‚Äúgolang with sql‚Äù you get a whole lotta nothing. And that, my friend, is where Golang + data goes to die a slow death, or fast one.
Listen Up.
Hey, man, I‚Äôve got nothing against Golang. I‚Äôve written it a few times; it seems nice and has a low learning curve compared to Rust. So, why did I choose to learn Rust instead of Golang?
Community. Tooling. Wide support. Integrations.
Just saying.
I know Golang has its use cases, has been used to build awesome things, and probably will be used more in the future. But that future isn‚Äôt in Data Engineering I don‚Äôt think.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
165725465.lakebase-from-databricks,Lakebase from Databricks.,OLTP meets OLAP,2025-06-12 17:49:19.538-05,"Well, the future is here; it‚Äôs swallowed and chewed us up, spit our poor bones out into a steaming pile of
bits
and
bytes
(probably in Rust). I‚Äôm sure that every author of code, at some point in their troubled existence, has turned their eyes to the sky, wondering when all the new fancy things will stop raining down upon our heads.
When you‚Äôve finally learned to master one new tool or concept, another one sneaks up behind you and gives you the old whack-a-do.
When I thought the Lake House was the latest and greatest concept and technology(s) that wouldn‚Äôt be outdone anytime soon, we have yet another new arrival on the deck vying for our mental capacity.
Drumroll ‚Ä¶ enter ‚Ä¶ the
LakeBase
(
a Postgres OLTP engine
).
Leave it to Databricks to throw a wrench in the smoothly running cogs of data teams around the world, and that they did at the
2025 Data and AI Summit
.
I can almost hear the clickety-clack of the callused fingers of a thousand starry-eyed CTOs and middle managers whose bulging heads are about to explode at the thought of
adopting a LakeBase
and
deleting a few decades‚Äô worth of technical detritus
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
LakeBase ‚Ä¶ the background a decade in the making.
Ok, I‚Äôm going to do my best to cut through the marketing hype and buzz of excitement rattling around the inter-webs, gather the musing of the masses from Linkedin, read the docs,
and serve you up a steaming pile of truth
‚Ä¶
because ‚Ä¶ you know ‚Ä¶ the truth will set you free.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Ok, so what is
Lakebase
‚Ä¶ and why do we have people angry about it and others calling it manna from heaven??
‚ÄúDatabricks Lakebase is a
fully-managed PostgreSQL OLTP engine
that lives inside the Databricks Data Intelligence Platform. You provision it as a
database instance
(a new compute type) and get Postgres semantics‚Äîrow-level transactions, indexes, JDBC/psql access‚Äîwhile the storage and scaling are handled for you by Databricks.‚Äù -
docs
Key capabilities
Postgres‚Äìcompatible
: standard drivers, psql, extensions roadmap.
Managed change-data-capture
into Delta Lake so OLTP data stays in sync with BI models.
Unified governance
via Unity Catalog roles & privileges.
Lakehouse hooks
: can feed Feature Engineering & Serving, SQL Warehouses, Databricks Apps, and RAG pipelines out of the same rows.
docs.databricks.com
Elastic scale
: separate storage and compute lets you grow read/write throughput without dumping and re-importing data.
Ok, so now we know what Lakebase is ‚Ä¶ managed, scalable ‚Ä¶ Postgres inside the Databricks environment.
Clam down, I can see the wheels turning in your head.
Is this a huge deal? Yes and no. It depends on who‚Äôs asking and what your use case is, like anything else.
To gain context for why Lakebase is a concern that has everyone freaking out, we'll do a brief history lesson. Heck, it‚Äôs not that much of a history lesson, as much as a ‚Äút
hing is what happens today.
‚Äù
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
OLAP vs OLTP - always at odds.
The story begins with the advent of RDBMS databases, along with the classic Data Warehouse, the roots of our current Lake House live inside concepts like Kimball‚Äôs
Data Warehouse Toolkit
.
Cira 2007 the world was going running full bore on SQL Server, Oracle, Postgres, and MySQL. OLTP databases were where all transactional processing took place, and we used Data Modeling to shift into an OLAP mindset that was suited to serve up analytics and large-scale analysis.
Only a select few managed to run Hive, Pig, and other HDFS systems, which were more problems than they were solutions.
But it simply wasn‚Äôt enough, from either to storage or compute viewpoint.
Enter the Data Lake, where giant dumps of Parquet, Avro, CSV, and JSON files sat in S3, with bespoke Spark, BigQuery, and Athena ground their teeth on TB‚Äôs of messy data.
The
Lake House
saved us from that hell.
What has never really changed over the years?
The line between OLAP and OLTP has always been clear
, regardless of what new sauce some vendor added to their project.
This sort of stark contrast and separation in the architecture and environment between, say ‚Ä¶ an
RDS Postgres instance
‚Ä¶ or
Aurora cluster
for that matter ‚Ä¶ and the Lake House platform ‚Ä¶ has been obvious and painful.
We always kept OLTP low latency workloads inside SQL Server, Postgres, etc. They were, and are, the beating heart of many applications and companies.
Lakebase is attempting to narrow that gap.
What OLTP? Want OLAP? They want to give you both, on the same platform ‚Ä¶ aka in an architectural drawing these boxes set right next to each other so that all sorts of technical barriers are broken down ‚Ä¶ and innovation increases.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
While the acolytes fight, we ponder.
This my fickle friend, is why people are excited about Lakebase. Yes, managed Postgres instances, and clusters have been available for some time.
But honestly, that was never the problem in the first place.
At least that is the idea, and idea is probably is at this point in the marketing cycle.
But, at this point I‚Äôm assuming you can imagine
WHY
the excitement is building
inside the Databricks community
. Keyword ‚Ä¶ Databricks community.
If you are a company heavily invested in Databricks, eating up the AI excitement like popcorn, and you also have a few Postgres RDS instances running inside AWS to support your web app ‚Ä¶ of course you are going to sit up and take notice.
The simplicity of running both your OLTP and OLAP workloads inside a single platform, all with Unity Catalog, the data from each box can almost kiss each other.
It‚Äôs the ultimate complexity reduction.
The real question to be answered yet is ‚Ä¶ can it actually perform and deliver in real life? I dont know.
Share
Technical Overview.
I don‚Äôt intend this to be a technical introduction or how-to-guide for Lakebase, you can be sure that is coming soon enough from yours truly, but we can glean over the docs to get some initial insight.
It‚Äôs clear that Databricks, as per normal, has integrated this well into their Platform. Right now a ‚ÄúDatabricks instance‚Äù is just a new type of compute you can select.
They make it easy to spend money don‚Äôt they?
Heck, if you want read only nodes and HA setup, they got that too!
The fine print tells us we get a 2TB max size for an instance ‚Ä¶ which isn‚Äôt as big as you think it is, a lot of folk would go over this max. Also, the 1000 concurrent connections will turn some people off, but not the average user.
What is very attractive to many folks is the advent of
Synced Tables
, the ability for a UC Table to magically be available in your Lakebase Postgres table. Very nice. (
simplifies a lot of exiting workflows)
Heck, you know all those SQL junkies will pee their pants when they find out you can use a flipping Notebook to query that Lakebase OLTP database.
Imagine the havoc they will cause. They always find a way.
I mean look at this Python example Databricks gives.
Amazing days we live in.
I for one am looking foward to using Lakebase in the future. Anyone on the Databricks platform who also uses an RDS instance, for example, would be crazy not too.
You know the part you aren‚Äôt going to like?
The price
. Ouch. Ain‚Äôt no free lunch around these here parts.
But, before all you hobbits go screaming off into the sunset complaining about the price ‚Ä¶ have a little nuance will you?
Yeah, the cloud and any SaaS is expensive.
Guess what? A lot of CTO‚Äôs and Principal Engineers understand that complexity comes at an extremely high cost.
A person must look at the reduction in code, developer hours, breakages, maintenance, and the roadblocks that data silos cause. Can you put a price tag on innovation? What do you GAIN from the expensive toolset that allows you to innovate and do business better than others?
That‚Äôs the question that everyone will have to ask themselves.
Is Lakebase worth it?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
165210253.nushell-in-a-nutshell,Nushell in a Nutshell,guest post by Anonymous Rust Dev,2025-06-09 07:05:24.809-05,"It's the Anonymous Rust Dev back unbidden, trying to pry the Python from your cold... errr, I mean, sell you on new things.
You know the drill... A file comes across your desk, and you want to know what's in it. Perhaps you're one of those individuals who receives a 4 GB text document and attempts to open it with Excel or Notepad.
You may have a bash command that you reach for to grok its contents, or a fancy GUI application, or perhaps with all those fancy Python skills, you code up a throwaway application to scry its contents. And you've even reached for something like
xonsh
, allowing you to ply your Python skills directly within the shell.
I submit, however, that there may be
a better way
. In today's episode of ""Not Another Rust-based Tool,"" I present
Nushell
.
Obviously, one person's ""
better
"" is another person's no-thank-you.
Hopefully, I can win you over to my side with some persuasive arguing
...
New shell?
Okay, it's time for a confession. When presented with the above problem, in the past, I would have reached for some unconventional tools. For instance, the
NodeJS repl
is a useful tool for looking around in JSON files.
Firefox does a bang-up job of graphically representing my XML documents. And let's be clear: this isn't a new problem - several years' worth of developers beating their heads against files have given us tools like head or
yq
that can be strung together in the command line.
Perhaps my favorite (and most controversial) pick, though, has long been
Microsoft PowerShell
. Yeah, you heard me right, I'm one of those M$ fanbois - or, rather, used to be.
Because at the end of the day, I am more interested in working with data structures than arcane text parsing utilities (
looking at you,
awk
and
xargs
). My favorite part of the Perl language is where you tell the commands to
die
- I love me some regex, but no way am I using that language to wrangle with a text document.
So, why PowerShell? The answer is simple: everything is backed by a data structure, even if it's something simple like
System.Object
. Once you are in the rich type system of .NET, you also get the opportunity to use its vast system libraries to work with objects.
Below is an example of how I might scrape a folder's worth of XML contents for a particular node, grab the first 10 hits, and tabulate the results:
Maybe then I want to know, after getting a peek, what the volume is - I can build on those results with a fancier piped command:
Okay, so that gets clunky fast, and admittedly, it took a bit of effort to write. I've lived with this pain for some time, so when I started hearing whispers of Yet Another CLI that does the same kind of work, I was already feeling a bit jaded.
Also, this happened at the same time as I was trying to learn the Fish shell (
also Rust-based; can't let an opportunity slip to plug it
). Well, whatever, I'll at least see what the fuss is about.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
So,
Nushell
... The landing page has a few quick illustrations and some installation commands for some hand-picked package managers. What immediately becomes apparent, though (
partly because they literally say it in the heading
) is that the data is structured.
In one example, they pipe the results of an HTTP GET into the
get
command, which only works because it implicitly parses the document (JSON-based):
Or, maybe I don't want all that and just want the license name:
Above, in a single terse and easy-to-read statement, I've fetched a JSON document, parsed it, and run a nested property selector. Of course, you could have accomplished the same with other tools, e.g.:
Thus far, it has not been a game changer, but I have to admit that the
Nushell
version looks nicer. We're going to need to look harder to find a justification to stay on this course, I think.
Crash course
A disclaimer - I'm new to this, so I need to review the documentation as I proceed. The
Nushell Book
will present more information than I plan to give here, and I'd recommend to the curious that they check it out for themselves.
That said, I can hand-pick some of its elements to pave the way.
Some random examples
First, as a shell, it's designed to be usable by anyone who's used the likes of bash or zsh. Many (not all) of the same commands (
ls
,
cd
, etc.
) have Nushell-specific implementations, and it will fall through to system commands (
for instance,
cat
) when it doesn't itself have a built-in alternative.
Unless you're doing some kind of fancy bash semantics, chances are you could switch shells without having to learn anything new, and get all the fancy UI benefits from how Nushell formats results. For illustration, consider a directory listing in bash vs. nu:
vs
You'll instantly notice the
#
column - tabulated results come back with row index, making it easier to work with a single result. You might also notice less information coming from Nushell, which by default holds a bit back -- we can force the issue with an argument:
The first apparent difference in this output from bash is just how
readable
it is - sizes and times have type-aware context, the table columns are more clearly delineated, and if you're foolish enough to run the same command on a narrow screen:
...it collapses remaining columns, keeping things presentable. So, what if I want to know more about that #0 (access_log) entry? I can pipe that index into the
get
command we saw earlier to learn more:
Nice. What if I wanted to know more about that
modified
property?
The getter correctly navigates object hierarchy in the same way
jq
or
yq
would. Additionally, now that we have more screen real estate, it gives us the backing time that backs that ""hour ago"" language. Under the hood, that's because we are dealing with the
Datetime
data type.
Likewise, with the file size:
Maybe you don't want it to automatically choose the units? You can use some math to coerce the units:
Oh yeah, we are indeed doing math on the command-line prompt:
Here, you can see me naively trying something and failing. These are its Rust roots in action - informative error messages will guide you to the correct answer, as happened here.
I attempted to use the wrong math operator, which the authors correctly thought I might try and guided me to the appropriate selection.
Now, what happens when we use a command that isn't directly offered by Nushell? I happen to know of one such example:
Lame. It just piped out to the boring text-based
df
shell command. But, Nushell has our back in situations like this:
Don't get too excited just yet - the fact that those numbers are left-aligned (
including the Use%
) tells me that this was parsed as text. However, once again, we have a Nushell way to work with this data:
Tables
Tabular data is a
first-class citizen
in Nushell. Our
ls
and
df | detect columns
illustrations above come back in this format, and give us enormous opportunity to work with data in the command line.
What if we didn't want to know the Used or Use% figures in the first place?
Perhaps it would be easier just to have instead selected the columns we wanted:
Filtering is intuitive, as long as you're aware of the available
operators
(also see
this
if coming from another language):
And, we can sort this stuff as well (but remember the fact that this data is currently text-based):
If you've followed me this far, you may be wondering about some of the practical implications of this tabular data. Out of the box, there is built-in support for various data types (see
Opening files
for the current list).
It's a small but useful list at the moment includes popular formats like CSV/TSV, JSON, yaml, and even Excel documents. Perhaps my own favorite would that it also reads from
SQLite
, complete with the ability to specify the data source via query - alternatively, if you just open the database, you end up with a hierarchical object where the tables are represented as key-value pairs, where the key is the table name and the value the table's contents.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Reading data
Alluded to above is the fact that we can import data from sources. Early on we saw the
http get
command, just now the
open
command, and somewhere along the way we even composed a table de novo. Maybe the format you're looking for wasn't in the supported list I linked to, or you want a better way to get data.
Well, as I said, this is a Rust-based project, so of course they had to go and make a way for you to
use Polars
. It requires an additional plugin to make it work, which the linked page gives more help on. I'm sure the creative among you can figure out how to access even more file formats from this point, including Parquet.
In a pinch, you can also just pipe your content in:
Finally, if you have the Rust chops and think something important is missing from those formats, you could always write your own plugin.
Lists, and records
Not everything's a table. I spent the most time on tables because I expect that's where my audience is at, but we also need to consider other types.
Lists are exactly as you'd think - heterogeneous collections of values.
A simple transformation:
...and going a step further, creating an array of ""records"", leads to an accidental Table:
In that last example, we effectively ended up with a value akin to:
A record is defined with a
syntax largely similar to JSON
. Getting around in these should hopefully be intuitive, but the
docs
may nevertheless offer some insights.
The big thing that I think will trip people up is the array accessor - don't use
$var[3]
to access the
3
index, but instead use the dot operator
$var.3
(or the earlier mentioned
get
command,
$var | get 3
). Instead, think of arrays/tables as objects whose keys are the positional indexes.
Revisiting the GitHub repo illustration (
http get https://api.github.com/repos/nushell/nushell
), we'll find a JSON document similar to the following (redacted heavily for brevity):
Treating this stripped-down document as an object, we'd end up with:
From here, we could access individual properties:
That JSON-like syntax being used is called
NUON
, which is a bit looser in some regards (e.g. automatic string inference allowing for optional quotes, comments are supported).
Saving (and converting) data
I didn't find much help from The Book, but I did take a stab at guessing what the command might be called. As with
open
, there's a corresponding
save
:
Reading between the lines, it seems to be file-type aware. Building off our last CSV example, let's see if we can cast our table to JSON:
Be aware of the
Formats
list - in particular, the
to <xxx>
commands, as they will indicate the valid output formats you can use to save files. Really, relying on the
save
command to cast your data is shorthand for
$data | to <fmt> | save --raw <dest>
- just in case you need to take ownership of the format.
Scripting
It's a shell, so of course it comes with its own scripting layer. It takes cues from its daddy Rust, in case you're wondering what's up with that goofy
closure
syntax we saw in our column casting illustrations earlier.
Variables
The simplest (and perhaps most useful to you) thing you might need to know right now is how to do assignments:
All those times I went through that
df | detect columns
business, I could have just instead done something like:
But, keep in mind Nushell's Rust roots. By default, variables are immutable:
Flow control
All the standard stuff (
if
/
else
,
for
,
while
) can be expected to do the usual stuff. As with Rust, parentheses are optional around the block conditions.
Unlike Rust, though, most of those commands are statements, not expressions (meaning, no return values). The exception here is the
match
expression, which occupies the same role as
switch
would in other languages:
Of course, you're not required to have
match
return anything:
Functions
Not everything needs to be a closure. In fact, they're hard on the eyes, and there's a better way (if you don't need the closure scope, that is).
And, with strong types:
Whether you care about types or not is your business, but if you accept loose values you should be prepared to deal with the ramifications (e.g. casting in your functions).
I'd recommend getting familiar with The Book's section on
Scripts
if this seems like something you'd want to pursue.
Takeaways
Changing shells can be an ordeal, particularly if you don't know the advantages of doing so. Hopefully, you've seen how easily you can explore your data without reaching for a plethora of other tools, as well as the quality-of-life improvements it offers over boring old Bash.
Its native support for structured data and assorted data types, coupled with an expressive programming syntax, should hopefully sway you to consider this in lieu of whatever boring shell your computer currently has going on.
This article only scrapes the surface, and there are some awesome features (e.g.
parallelism
) I didn't even get to touch here. Also, while I chose to focus on the ETL side of things, this is also a fully functional CLI shell intended to do almost everything that your current shell of choice does with minimal pain and lots of eye candy.
The
Coming to Nu
section should hopefully make a transition to Nushell a straightforward and easy process if you're willing to give it a try.
Also, an important aside: nobody seems to be talking about how ""Nushell"" sounds like ""Nutshell"", which when translated to German is ""Nussschale"" - which also sounds suspiciously like Nushell to me. I feel like there's a missed opportunity there somewhere.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
165014999.duckdb-enters-the-lake-house-race,DuckDB enters the Lake House race.,my two cents on DuckLake,2025-06-05 13:57:34.583-05,"I‚Äôve been muddling about this one for a few days now, and I'm not sure what to think. One minute, I think it‚Äôs a great idea; the next minute, I turn up my nose to what seems to be another pointless addition to the morass of the Lake House world we live in.
One can only assume, at this point, that the truth lies somewhere in between.
MotherDuck
, DuckDB ‚Ä¶ whatever, same thing,
has doubled down and submitted yet another Lake House format
to be added to the already crowded field.
Thanks to
Astronomer
for sponsoring this newsletter! I‚Äôve worked with Apache Airflow for years, and
Astronomer‚Äôs Astro platform
is hands down the best way to run production-grade Airflow. Their commitment to simplifying data orchestration makes content like this possible.
Sign up for a free trial of Astro below.
The only thing I can do at this point is to introduce you to this new Lake House tool and provide some realistic commentary on this latest development.
I will give it to you like your grandmother‚Äôs comments at Christmas, raw and unadulterated.
As I lie here on my back porch watching the rain pitter-patter on the windows, the thought of dealing with and writing about yet another Lake House format has me dreary and about as excited as the wet and soggy maple tree staring back at me.
But, for you, my dear reader, I will do my best.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
How did we get here, and what is DuckLake?
The simple fact is that we live in a Lake House world now, and there is no going back. It‚Äôs here to stay, at least for the next decade or two.
Like it or not, your CTO has followed the lemmings down the long and winding Lake House + AI road, and there is no turning back. This has all happened fairly quickly and relatively recently, causing a new tech data gold rush of sorts.
I‚Äôm sure some bumbling acolytes will disagree with me, but as of today, you have three serious options when it comes to choosing a
Lake House Storage format
.
‚ÄúIn the context of data platforms, a
Lakehouse
(or
data lakehouse
) is a modern data architecture that combines elements of
data lakes
and
data warehouses
to support both
structured
and
unstructured
data, enabling
analytics
,
machine learning
, and
BI
on a single platform.‚Äù - ChatGPT
When you read a simple summary of what a Lake House is, it makes sense why EVERYONE would want to have their dirty little fingers in that pie.
You as a vendor, want to be involved in the compute and
STORAGE
of the Lake House architectures as they are being built.
It‚Äôs not rocket science, folks. The Lake House is where everyone is heading; there is still a lot of migrations happening and back-and-forths.
The question that DuckLake brings to the forefront is ‚Ä¶
Is the race to be part of the major Lake House formats finished, or is there room for more movement?
So,
DuckLake
from DuckDB,
can it be a serious contender and addition to the already crowded Lake House storage formats?
This is the question everyone is asking. How much is hype, how much is reality?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Diving into DuckLake from DuckDB
Ok, before we dive into
DuckLake
itself, I still have a few comments that I think will add some color to this tool, why it is here, and what will happen, or not happen with it.
I‚Äôm just going to list them for brevity.
the ‚ÄúCatalog‚Äù has become the achilles heel of many Lake House implementations (
especially Iceberg
).
DuckDB doesn‚Äôt play nice with Apache Iceberg (no write support)
Delta Lake is the clear implementation winner right now
(support across broad spectrum of tools for read/write)
AWS + GCP + Cloudflare + Everyone is adding major support for Iceberg.
but it‚Äôs very fragmented
they are semi-open-source solutions
My guess is that DuckDB looked at the Lake House format landscape and saw the obvious fragmentation, frustrations with Iceberg catalogs, and air juggling between formats.
I mean, take for example the number of different articles I have written on Lake House storage formats and catalogs. It shows you the current chaos.
What?! An Iceberg Catalog that works?
Apache Polaris (Iceberg Catalog) ... with Daft
Cloudflare R2 + Apache Iceberg + R2 Data Catalog + Daft
Delta Lake vs Iceberg. UniForm and Unity Catalog.
Polars and DuckDB release Unity Catalog (Delta Lake) integrations. Who lied? Who didn't?
SaaS Vendor Lock In
Amazon (S3) Tables
Lord have mercy. Apache XTable.
Delta Lake vs Apache Iceberg. The Lake House Squabble.
Dang, we keep getting side tracked, we will get there eventually.
Oh yeah, DuckLake.
So what
IS
DuckLake?
‚Äú
DuckLake is an open Lakehouse format that is built on SQL and Parquet. DuckLake stores metadata in a
catalog database
, and stores data in Parquet files.
‚Äù -
DuckDB
So it really is just another Lake House format. How is like most other formats? Well, it‚Äôs based on parquet files stored wherever you like.
The big differentiator?
The catalog ‚Ä¶ aka metadata data can be stored in a SQL Database. Why is this a thing? Because everyone ‚Ä¶ like everyone ‚Ä¶ probably runs a Postgres database somewhere.
DuckLake‚Äôs ability to use a SQL database as a catalog makes it approachable and easy to use.
Unlike Iceberg and Delta Lake, you don‚Äôt need to get an EC2 instance up and running, install and manage your own Catalog service/API.
This my friends, is what it comes down too. DuckLake is just another Lake House format with parquet storage on the backend.
Nothing new there.
It‚Äôs the catalog and metadata management that DuckDB recognized has been a huge frustration for Lake House users.
‚ÄúThe basic design of DuckLake is to
move all metadata structures into a SQL database
, both for catalog and table data.‚Äù
Databases are the bread and butter of DuckDB are they not? I mean their tagline in their own website they call themselves a ‚Äú
database system
.‚Äù
Wait ‚Ä¶ there‚Äôs more.
So, is this truly a incredible and ground breaking new way to approach the Lake House and catalog woes from DuckDB? Everyone makes it sound like that, but it isn‚Äôt totally true.
Apache Iceberg appears to long have had support for SQL Catalog
backend (
in Postgres etc
).
Even the
pyiceberg
website lists Postgres support for a SQL Catalog backend.
So why isn‚Äôt everyone doing that?
This is a good question, why is everyone tripping overthemselves about DuckLake and a SQL based catalog when Apache Iceberg already has that option?
The answer is easy and obvious, not ever problem is a Engineering problem, some problems are human.
The answer is because Apache Iceberg has been beaten down and abused by various SaaS vendors for their own use, it‚Äôs always lagged behind development wise, from formats like Delta Lake, it‚Äôs had a
fragmented and divided community
.
DuckDB has rightly recognized those problems and decided to offer an alternative solution that WILL BE much easier to use and more developer friendly.
Simple as that.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Downsides to DuckLake?
It‚Äôs a little early to poke the stick into DuckLake and pontifcate about all it‚Äôs shortcomings ‚Ä¶ here is a bullet list of some my off the cuff reactions when they first released it.
By naming it ‚Äú
DuckLake
‚Äù they cut their potential users in half.
You think people using Databricks and Snowflake will seriously use something called ‚ÄúDuckLake‚Äù in conjunction with non DuckDB compute?
unlikely.
If DuckDB treats and develops DuckLake without the broader community in mind, it will never move past DuckDB users.
(
for example there is no Apache Iceberg DuckDB write support today
) ‚Ä¶ (
now we know why
)
Don‚Äôt get me wrong, I think DuckLake is genius idea from DuckDB and is the
well deserved stick in the eye
of Apache Iceberg for sucking so much when it comes to catalogs.
Should we play with the code?
Not sure if we will do or see anything amazing here,
and all this code is available on GitHub in a repo
for you to play with, but here goes nothing.
Let‚Äôs try to use the setup with DuckDB + DuckLake + Postgres, using Docker and see what happens.
Now for a `docker-compose` setup that will use Postgres as the backend for our DuckLake.
Easy enough eh ‚Ä¶ now we can write some DuckDB and Python code to install and load DuckLake as well as create a table, and then query the BACKEND of the tables created in Postgres by DuckLake to see what is happening.
A lot of code, but simple enough. Honestly, using DuckLake is as easy as these few lines right here.
In typical DuckDB fashion, it could not be simpler.
Work‚Äôs like a charm, i
f you clone my GitHub repo
and run `
docker-compose up ‚Äîbuild
` you will get the following print outs, abbreviated here.
Not going to lie, pretty slick and easy to use, a perfect tool for those going all in on the DuckDB bandwagon. They did a better job than Apache Iceberg, like it or not.
If you want people to adopt your Lake House solutions they have to be ‚Ä¶
approachable
easy to use
extensible
DuckLake checks all those boxes.
They even went out of their way to produce a nice website and documentation.
I don‚Äôt know ‚Ä¶ is DuckLake earth shattering? Once you get past all the hype, which can be hard,
no, not really, but yes a little.
In classic DuckDB fashion (taking SQL and making it easy to use and powerful), clearly they have done the same in thing in Lake House format world.
They took a concept that already was here, a Lake House, including a SQL Catalog, and just did it better,
with a cleaner and more approachable implementation.
In the end, that is what wins the hearts and minds of Data Engineering users at large. Give us something that works, is powerful, and is clean and simple, and we will use it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
164251337.apache-iceberg-rant,Apache Iceberg Rant.,we have been saved from catalog hell,2025-05-26 07:44:21.412-05,"Hello, my fair-weathered friends and readers! I am gone on vacation this week with my family, probably at this moment lying in the sand on a beach (
Lord willing the creek don‚Äôt rise
),
not thinking of you all.
Anywho, be that as it may, I didn‚Äôt want you to miss my pretty face, so here is a video of me ranting about Apache Iceberg, something I‚Äôve had a lot of practice doing and enjoy quite thoroughly.
For all you free-loaders out there, you can get 20% off to celebrate Memorial Day.
https://dataengineeringcentral.substack.com/Merica"
164094171.databricks-sql-scripting,Databricks SQL Scripting,SQL Server infiltrates the Lake House,2025-06-02 07:03:20.701-05,"I would be lying if I said a little bit of me didn‚Äôt die when I first read this, that is,
SQL Scripting
released by our friends at Databricks.
Don‚Äôt get me wrong, I hold no deep-seated grudge towards Databricks for doing the obvious.
You must give the masses that which they desire lest they turn and tear you.
We Data Folk are simply gluttons for punishment, our own worst enemies, yet we get the job done most of the time, eh? Why did I die inside just a little bit?
Because I‚Äôm from the old days, like the old, old days when SQL Servers roamed the earth, spewing TSQL and Stored Procedures across the land with SSIS riding on its‚Äô back.
Some of you will know the dark, evil days of which I speak; others of you were born in light and simply are unaware of the ages past in which Business Intelligence Engineers and Data Developers fought hand-to-hand combat with SSMS, deadlocks and other such
terrible beasts
.
Can you tell I might have a little PTSD?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What exactly is SQL Scripting in a Databricks/Spark context.
Ok, so enough with the old man stories, I can‚Äôt help it, what exactly is SQL Scripting in this context, and who cares?
‚ÄúYou can employ powerful procedural logic using SQL/PSM standard-based scripting syntax. Any SQL script consists of and starts with a
compound statement
block (
BEGIN ... END
). A compound statement starts with a section to declare local variables, user-defined conditions, and condition handlers, which are used to catch exceptions.‚Äù
-
Databricks
This abomination ‚Ä¶ ehhh ‚Ä¶ I mean super helpful procedural SQL can be encapsulated as ‚Ä¶ you guess it,
a stored procedure
. It‚Äôs funny how history always repeats itself.
‚ÄúUsing the
CREATE PROCEDURE
statement you can persist a SQL Script in Unity Catalog. You can then
GRANT
the access to the procedure to other principals. Those principals can then use the
CALL
statement to invoke the procedure.‚Äù
-
SQL Stored Procedure
Let me drop a simple example here so you can see both SQL Scripting and
SQL Procedure
in code so it hammers the point home of what we are dealing with.
Both these examples are from the
official Databricks docs
; they aren‚Äôt rocket science and mostly just show you what is possible.
source Databricks docs
I mean, as you can see from these examples, you can pass in whatever params you like and can return whatever ‚Ä¶ and the sky is the limit to what goes between the
BEGIN
and
END
statements.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
So what‚Äôs the problem grumpy pants?
This is sort of a loaded question, at least for someone from my generation anyways. To be honest, we have lived through this sort of architecture that drove all sorts of pipelines for decades ‚Ä¶
there was a reason it was abandoned.
There is a thin-red line between HOW and WHERE we should solve certain business problems.
Let me put it like this ‚Ä¶ we have two general options as programmers to solve problems that present themselves. Each has it place, and they both can work in unison, walk hand and hand, wherein the best results lie. If you start to force square pegs in round holds, things will go bad ‚Ä¶ eventually.
Your two options are ‚Ä¶
SQL
SQL is best for working on tabular (sets) of data, crunching uniform type transformations across wide swathes of data. It just works well for this sort of thing.
Classic programming language (Python, Java, Rust, Golang, etc)
Programming languages work well on minute and specific procedural tasks and data manipulations.
Methinks this is where the stored procedure went wrong in classic SQL Server warehouse environments. It was something that was inherently used to be incongruent to SQL. Not all the time, just 50% or more of the time.
Also, stored procedures suffered from another MAJOR implementation flaw.
Invisibility
and
Obscurity
.
The question is,
can Databrick‚Äôs SQL Scripting avoid these mostly human implementation problems?
I suppose one could argue this is no inherent reason that simply encapsulating code inside a thing is bad, it‚Äôs HOW it‚Äôs done that creates the problem.
What used to happen back in the day is that ‚Ä¶
some long and complex SSIS package would run
various tasks would run
a series of stored procedures with TOO much logic would run
the stored procedure itself if mostly invisisble
someone has to go find it
debug it
etc
And that‚Äôs if everything worked perfectly. What you think stored procedure hell is something from the past?
Guess again.
Stored procedure hell is alive and well.
If this sort of ‚ÄúSQL scripting‚Äù is so destructive, than why is it still going on, and why is Databricks bring such features to their users?
Because it‚Äôs humans that make bad decisions ‚Ä¶ not code.
Let‚Äôs go back to the definition of what SQL Scripting is ‚Ä¶
‚ÄúCreates a procedure in Unity Catalog that takes or modifies arguments, executes a set of SQL statements, and optionally returns a result set.‚Äù -
Databricks docs
I mean it sounds like a powerful abstraction ‚Ä¶ because it is.
Powerful things can be used for both good and bad purposes
. It‚Äôs up to the Engineers to make correct decisions about HOW and WHEN to use a stored procedure.
It‚Äôs probably true that SQL Scripting and stored procedures in general suffer from the same fate and pitfalls they have for last few decades.
If you cram too much logic in one place, it sucks.
If you hide and obfuscate data pipelines and code, it sucks.
There is no free lunch, the fancier you decide to be the more you will pay the price later on down the road. This feature will suffer from the same problems your SQL right now suffers from ‚Ä¶ except that SQL Scripting will exacerbate the problem.
keep all logic clean and simple
put everything in source control
make sure all step(s) of a pipeline are observable and discoverable.
What is my opinion of Databricks SQL Scripting and the introduction of stored procedures into the Databricks landscape? I have none.
I‚Äôm sure there are wonderful use cases. There are also a lot of trigger happy engineers who write bad code, and they will indeed, as in other circumstances, abuse this feature."
163944714.what-an-iceberg-catalog-that-works,What?! An Iceberg Catalog that works?,who would have thought ...,2025-05-20 09:37:52.534-05,"I will be the first to admit, in an unapologetic way, that working with Apache Iceberg is far from a pleasant experience once you move past the ‚Äú
playing around on my laptop
‚Äù stage. The tight, inflexible relationship between Iceberg and a catalog, combined with the lack of easy-to-use catalogs, created a pile of junk.
The only beam of light in the dark tunnel of despair that is Apache Iceberg in production has been Cloudflare‚Äôs wonderful but completely ignored R2 Data Catalog ‚Ä¶ that is, until now.
May the data gods bless
Julien Hurault
and the new
boring
Apache Iceberg catalog he has made, apparently something the behemoths of Iceberg were unable to do ‚Ä¶ like AWS, etc.
Enter the
boring-catalog
.
‚ÄúA lightweight, file-based Iceberg catalog implementation using a single JSON file (e.g., on S3, local disk, or any fsspec-compatible storage).‚Äù
-
boring GitHub
I‚Äôm sure some of you unbelievers and whiners who‚Äôve never actually tried Apache Iceberg in a cloud-like production environment will say in a very simpering voice ‚Ä¶ ‚Äú
But we‚Äôve had SQLite catalogs for Iceberg forever ‚Ä¶ it has a REST catalog blah blah blah.
‚Äù
Clearly, if that‚Äôs what you think, you can‚Äôt be argued with because you‚Äôve never actually written any code to use Apache Iceberg (totally open-source) in a cloud-like environment.
Apache Iceberg‚Äôs catalogs are in desperate need of ‚Ä¶
Easy-to-use implementations
Implementations that don‚Äôt need Google-scale and a server to run
Work in the cloud
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Essentially, what is needed as a first step is a viable catalog that works in the Cloud without a major setup routine, which can support the 80% use case for small to medium-sized data teams.
*Most* people aren‚Äôt crunching Petabyte data with thousands of workers ‚Ä¶ they don‚Äôt need a high-performance Apache Iceberg catalog on a set of servers ready to handle anything.
They need a simple, easy-to-use catalog that just works.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Kicking the tires on
boring-catalog
.
I hope that this will be straightforward, a very
boring
(pun intended) code and example walk-through of using boring-catalog in a Cloud environment to read AND write to Apache Iceberg using a variety of tools.
If it doesn‚Äôt work as intended, we can still be pleased that people are acknowledging the fact that Iceberg has some significant catalog hurdles to overcome if it hopes to reach Delta Lake-level parity and usage.
Here are some notes from GitHub about boring-catalog that might help frame what‚Äôs about to happen.
Boring Catalog stores all Iceberg catalog state in a single JSON file:

- Namespaces and tables are tracked in this file

- S3 conditional writes prevent concurrent modifications when storing   catalog on S3

- The .ice/index file in your project directory stores the configuration for your catalog, including:
    - catalog_uri: the path to your catalog JSON file
    - catalog_name: the logical name of your catalog
    - properties: additional properties (e.g., warehouse location)
Let‚Äôs get everything set up with `
uv
`, and then we can go off to the races.
uv init boringiceberg
uv add pyiceberg duckdb polars boringcatalog
Good first note and problem, I need a newer Python version, at least 3.10, to use boring-catalog. Easy enough to fix that problem.
uv init boringiceberg --python 3.11
uv add pyiceberg duckdb polars boringcatalog
Next, much like any other Iceberg catalog, we need to set up a warehouse, in our case, we want it located in s3 somewhere.
(boringiceberg) danielbeach@Daniels-MacBook-Pro boringiceberg % ice init -p warehouse=s3://confessions-of-a-data-guy/bingbongdingdong
Error initializing catalog: Failed to initialize catalog at s3://confessions-of-a-data-guy/bingbongdingdong/catalog/catalog_boring.json: When getting information for key 'bingbongdingdong/catalog/catalog_boring.json' in bucket 'confessions-of-a-data-guy': AWS Error ACCESS_DENIED during HeadObject operation: No response body.
Aborted!
I figured this might happen; the docs don‚Äôt mention credentials, and I have multiple AWS profiles stored in my .aws configs. Running the help command doesn‚Äôt indicate anything for me to point to a specific .aws profile.
(boringiceberg) danielbeach@Daniels-MacBook-Pro boringiceberg % ice --help
Usage: ice [OPTIONS] COMMAND [ARGS]...

  Boring Catalog CLI tool.

  Run 'ice COMMAND --help' for more information on a command.

Options:
  --version  Show version and exit
  --help     Show this message and exit.

Commands:
  catalog           Print the current catalog.json as JSON.
  commit            Commit a new snapshot to a table from a Parquet file.
  create-namespace  Create a new namespace in the catalog.
  duck              Open DuckDB CLI with catalog configuration.
  init              Initialize a new Boring Catalog.
  list-namespaces   List all namespaces or child namespaces of PARENT.
  list-tables       List all tables in the specified NAMESPACE, or all...
  log               Print all snapshot entries for a table or all tables...
Let me export my AWS creds into the environment and see if that does the trick.
Perfection.
So, as long as that is working, we should be able to write some Python now and see what happens.
This code doesn‚Äôt work unfortunately ‚Ä¶
Traceback (most recent call last):
  File ""/Users/danielbeach/code/boringiceberg/.venv/lib/python3.11/site-packages/pyiceberg/catalog/__init__.py"", line 922, in _resolve_table_location
    return self._get_default_warehouse_location(database_name, table_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/danielbeach/code/boringiceberg/.venv/lib/python3.11/site-packages/pyiceberg/catalog/__init__.py"", line 935, in _get_default_warehouse_location
    raise ValueError(""No default path is set, please specify a location when creating a table"")
ValueError: No default path is set, please specify a location when creating a table
Even though the table location is an optional None in the source code, which makes sense, we are passing in the full namespace and table name to the catalog, it should be smart enough to create the location itself without me telling it where to go.
Either way, I can do its job for it and tell it where to store the table. That seemed to do the trick; updating the following line with a specific table location worked.
catalog.create_table(identifier=(""pickles"", ""trip_data""), schema=data.schema, location='s3://confessions-of-a-data-guy/bingbongdingdong/')
Ok, so now we have created an Iceberg catalog and warehouse and created a table inside it. Let‚Äôs see if we can write some trip data to it.
(
You will note I used the
Divvy Bike trip dataset
, reading it via Polars and converting to Arrow and using that for the schema of the Iceberg table
).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Now, we can see if the real test works. Can we use something like Polars to write directly to this Apache Iceberg table using our
boring-catalog
.
All we are going to do, is in the above code use ‚Ä¶
df.write_iceberg(table)
.
So easy. It‚Äôs almost like we are using Delta Lake.
Here is the code to read the data back to prove it worked.
warnings.warn(
pyarrow.Table
ride_id: large_string
rideable_type: large_string
started_at: large_string
ended_at: large_string
start_station_name: large_string
start_station_id: large_string
end_station_name: large_string
end_station_id: large_string
start_lat: double
start_lng: double
end_lat: double
end_lng: double
member_casual: large_string
----
ride_id: [[""AF3863596DF9D94B"",""8B38081EBE918800"",""1C7F1DE826BBBC8D"",""CAD23D69A79A6C3B"",""BE241E601482E0AB"",...,""3A350F6EF7EBBE32"",""D1D3DE0B041A0619"",""E23151BA351A9F0C"",""D932E1160214FD7F"",""A49BBBC483A8C824""],[""7F89DAF53151591F"",""F868AC218A8DCDD3"",""32C1295AEFCCD873"",""106A7E3CB0DCFC59"",""83FF6EE3387486EA"",...,""42A3DC25B6F1A61C"",""EB4A20D2CF7B843C"",""2AD698618BF66003"",""5B1C269FB6352DFA"",""1C7B8011918D458C""],[""62359148F3BD42E9"",""82417490E9339CE8"",""D8B6BD81A778C228"",""3B5187D1B7DCEFFD"",""9ADDBDD39ECAD340"",...,""00EE7C31C568974F"",""B6DBC7FD6DD0DEF3"",""B099E6E3285BE09B"",""76BD307B9C12E25F"",""420F78CF3AD3E7FF""]]
rideable_type: [[""classic_bike"",""electric_bike"",""electric_bike"",""classic_bike"",""electric_bike"",...,""electric_bike"",""electric_bike"",""electric_bike"",""electric_bike"",""classic_bike""],[""electric_bike"",""classic_bike"",""classic_bike"",""electric_bike"",""electric_bike"",...,""electric_bike"",""classic_bike"",""classic_bike"",""electric_bike"",""electric_bike""],[""classic_bike"",""classic_bike"",""classic_bike"",""classic_bike"",""electric_bike"",...,""electric_bike"",""electric_bike"",""electric_bike"",""electric_bike"",""electric_bike""]]
started_at: [[""2025-04-27 14:29:34.619"",""2025-04-23 17:48:51.863"",""2025-04-05 17:55:30.845"",""2025-04-03 08:22:04.493"",""2025-04-15 06:09:55.293"",
Dang, what a nice tool.
That‚Äôs the real deal right there. Production ready tomorrow.
Share
What the crap?
Now riddle me this you hobbit, why is it that it took a lone Data Engineer wandering this broken world, this far into Apache Iceberg being around, to write a simple to use cloud-compliant Catalog that actually works???
Let‚Äôs be honest, we have all sort of massive companies like AWS (with S3 Tables) and others trying to take advantage of the Iceberg hype. We have a myriad of utterly complex Icberg catalog options that you can install and setup that require magic and servers.
You know what we didn‚Äôt have until now?
I realistic and reliable simple cloud enabled Iceberg Catalog that you could use in an TRULY open-source way.
It‚Äôs incredible
yet predictable
that the entire Apache Iceberg universe of talking heads and companies are EACH taking advantage of Iceberg users without actually putting DEVELOPERS FIRST and providing a real, simple, and workable catalog option into the open-source space.
The Iceberg community at large needs take a note from the page of Databricks and Delta Lake, like it or not. They have put open-source usability and developers first, and it shows in their product.
I highly suggest you send a thank-you note to
Julien Hurault
and follow him on Linkedin and check out
his Substack
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
163580493.how-do-eat-a-data-platform,How do eat a Data Platform?,... one PR at a time.,2025-05-19 07:16:14.496-05,"I get this sort of question ‚Ä¶ well ‚Ä¶ half question, half asking for advice in various forms on a reasonably consistent basis. It usually takes the form of ‚Ä¶
‚ÄúI have this idea, this feature, this thing I want to do or I was looking at. I identified three problems: X, Y, and Z. We need to update A, B, and C thingies, at which point our Data Platform will reach nirvana, and we will all be sent to Heaven.
It can come in various and manifold forms, but usually in the form of one of the following two.
Adding or adopting this new feature/tech requires the change of X, Y, and Z.
I found this problem while looking at another problem; I need to fix A, B, and C.
It‚Äôs all or nothing for this; we go all the way or nothing. We simply HAVE to do this NOW.
The truth is, at most companies, this is the number one reason that ‚Äú
things don‚Äôt get done
,‚Äù or ‚Äú
things never improve,‚Äù or perhaps it‚Äôs that projects always languish and never finish, or, even worse
, they always disappoint in the end.
It‚Äôs easy to point fingers and blame when it comes to Software and Data projects, especially when the two cross paths, but often,
the answer to the problem(s) is more straightforward than one might suppose.
It‚Äôs the classic problem of ‚Ä¶
Too much going on at once, no focus.
Everything must be done now.
Inability to take incremental steps.
Choosing new and shiny over old and consistent.
Can‚Äôt see the big picture.
Impact vs amount of work required.
We could say it in a thousand ways, could we not? It all comes back to the same old problems ‚Ä¶ Software engineers prioritizing their interests and desires over the overall good of the Data Platform and the company at large.
Look, I get it.
If you spend too much time watching and reading about what others are doing, such as on Instagram for software engineers, you may feel like you are falling behind and that you have to implement X new tool or feature.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Do you doubt that the Software world as we know it is filled with doom and gloom? Failed project after failed project. I quickly asked ChatGPT to summarize recent studies on the topic,
which paints a gloomier picture than one would imagine.
I mean, how is this possible? We‚Äôve been writing software for 50+ years!! AI has been around for a few years now; it can write perfect code for us; what gives???!!!
It is not different for us working in a ‚Äúdata context‚Äù. We suffer from the same problems.
THEE main problem.
It‚Äôs me and you ‚Ä¶ that‚Äôs what the problem is. It‚Äôs a human problem. Not sofware.
TDD doesn‚Äôt fix it.
Clean Code doesn‚Äôt fix it.
The Pragmatic approach doesn‚Äôt fix it.
Rust doesn‚Äôt fix it.
< insert popular thing > doesn‚Äôt fix it.
It‚Äôs funny. You will have seasoned Software Engineers look you in the eye and tell you if you adopted the TDD approach and bowed before the testing god, all will be well.
Perhaps it‚Äôs functional programming; maybe that‚Äôs the missing key if we just adopted it. The list will continue indefinitely. Endless books, blogs, classes, and talking heads will move from one topic to the next until the world melts in a fervent heat.
It‚Äôs a human problem.
Solve the real problems.
Do you want to build reliable and scalable software systems that don‚Äôt become part of the undeniable statistics of failure?
Then, you must embrace your humanity.
Be honest about how you are tempted to operate, recognize these things in others, and work against the bad habits, and don‚Äôt eat the forbidden apple of new and exciting technology that simply must be eaten right now, or all is lost.
The solutions are simple, and we listed some of their opposites when we first started. Here is a list of actionable steps anyone can take tomorrow to calm the madness and deliver high quality software projects on-time, on budget, that are a success.
Develop a Maniacal Focus
Do not be distracted by every new toy, tool, or feature. New things, carefully considered, can be good but swallowed without a second thought? You‚Äôre number one downfall.
Don‚Äôt chase every squirrel that crosses your path.
Be Calm and Methodical
refuse to be pushed or rushed into decisions.
give everything a day or week to percolate.
spend the time you need to understand a problem and solutions.
Take Small and Incremental steps towards big goals.
including in your PRs
Seek Simplicity
learn to abhor the complicated love the simple.
question every assumption at all times.
Align yourself with Business Impact.
every engineer wants to do the new cool thing or fix the apparent problem.
Impact over desire every time.
I‚Äôm tempted to keep rambling, but the problem is I won‚Äôt know when to stop. I think by this point,
you will start to see the problem(s) and understand the very HUMAN solutions.
Your goal should be to inject your humanity and common sense into every software project that you are a part of, from the big overarching goals to the mundane everyday task of ‚ÄúI found this bug,
and I want to fix it.
‚Äù
It won‚Äôt be easy for you or others. Humans will be human. They will want to work on the new tool. They will want to fix X, Y, Z and refactor A, B, C to be better while they are at it.
Best of luck!"
163175767.postgres-to-delta-lake-and-back-again,Postgres to Delta Lake ... and back again.,back and forth,2025-05-12 07:17:33.193-05,"Strangely enough, or maybe not so strange, when you find yourself working in a Lake House environment, it‚Äôs not uncommon to find the odd Postgres database hanging around the edges. You know, like the third wheel at the party in the corner looking awkward.
I think it‚Äôs one of those things; the whole RDMBS ‚Üê ‚Üí Lake House is something everyone does, and no one talks about for some reason. Too dull, too many ways to do it? Who knows.
On a semi-regular basis I find myself doing one of two things.
pushing data from Delta Lake into Postgres
pull data from Postgres into Delta Lake
I mean, it‚Äôs the classic Data Engineering problem, right? Running old and new technology beside each other makes
them play nice together!
Very often, our data systems as a whole are made up of various pieces and parts, and the Lake House + RDMBS is one that often crosses paths.
So today, we will examine this problem and try to solve it using several different technologies. We will s
ee which one we like best.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The classic Spark + Postgres.
Surprisingly enough, I see a lot of people reach for that old trusty friend of us all,
psycopg2
. Pre-Lake-House world when Data Engineering started to become a thing and the Data Warehouse roamed the land, psycopg2 was the tool of choice.
A lot of folk still us it, stuck in the old way of their forefathers.
When it comes to PySpark, the current king of the Data Engineering hill, provides something nice called the
.jbdc() writer on a DataFrame
.
I mean this beautiful snippet of code you see following allows the simple muggle to take a DataFrame and push it straight into a Postgres table, existing or not, append or truncate etc.
This has been my go-to for a long time, it‚Äôs just so simple and easy to use. I don‚Äôt care about the speed or efficiency that much (
it is annoyingly slow
), just let the method rip and it will get the job done.
It‚Äôs hard to argue with simple code, you can keep your fancy stuff.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Other options?
Curious about other options, I would hope there are plenty considering the lineup of new tooling we have to work with these days, DuckDB, Polars, Daft and the like.
Since we have already talked about
DuckDB + Delta Lake
, it shouldn‚Äôt be that hard to extend it to Postgres eh?
Can DuckDB be used?
What‚Äôs even more impressive for all you Postgres lovers is that DuckDB supports that wonderful COPY command, back and forth between DuckDB and Postgres.
Kinda hard to argue with that one. Little bit more code using DuckDB, but it is an option.
Polars with Postgres and Delta Lake.
I mean I expect a lot out of Polars simply because it is Rust based and also made to act a lot like good ol‚Äô PySpark, so we should have a simple and smooth option.
Of course Polars did not disappoint.
The DataFrame write_database() option
pretty much mirrors Spark in it‚Äôs simplicity and implementation.
Also, since Polars has such a seamless integration into Delta Lake, this one is a kind of a no brainer as well.
The old fallback.
Of course you can fall back onto that old hunka-junka psycopg2 with plain old Python. One could read Delta Lake with any package, deltalake, polars, duckdb, daft, spark, etc ‚Ä¶ and simply collect the records as a Arrow table or Python list.
From there it isn‚Äôt that hard to write a performant INSERT statement if you know what to do and what to reach for.
This is probably much faster than many of the alternatives."
162695553.ai-is-never-going-to-take-your-job,AI is NEVER going to take your job.,AI ain't going to get me.,2025-05-08 15:58:06.562-05,"I‚Äôm going to give you the spoiler up front.
AI isn‚Äôt taking your job anytime soon
. If you think otherwise, you‚Äôve been watching too much YouTube and reading the wrong stuff on LinkedIn. Heck, maybe you simply
HAVEN‚ÄôT
been using AI enough on more than junior-level problems.
AI is good at writing boring, easy code.
AI is terrible at new or cutting-edge things.
AI itself requires massive amounts of Data Engineering.
AI lets you focus on other things (
arguably the more important things
)
The truth is I wish AI were better, those talking heads keep saying how every benchmark gets better and better, but the reality on the ground is much different.
Can AI do a lot of non-value-added things in a programming context?
Of course, it can, just like it can do a lot of mundane tasks in many areas.
AI, make me a marketing plan for X, Y, Z.
AI, write me an email or context that says this or that.
AI, write me this Python script.
The internet, particularly GitHub, is full of horrible ideas and code. AI has learned this along with everything else, being not the wiser.
Yes, AI (
Cursor, CoPilot, ChatGPT, whatever
) can write code like there is no tomorrow.
It‚Äôs not good code.
It can‚Äôt solve complicated problems that spill across many different areas. It isn‚Äôt very good at remotely new topics (
like the last few years
).
Yes, AI can write you Postgres queries, SQL queries, and a Spark data pipelines.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I did a little test.
I was recently working on a project that is all very obvious, that is, how poor AI is at non-trivial tasks. I will summarize it as much as possible.
Using Spark code to combine various datasets
CSV, TXT, Delta Lake, etc
Get pgvector setup on PostgreSQL
Get code to combine various datasets and create embeddings
Store the embeddings into the pgvector table
Query data back out in RAG application
Write a RAG application using recent tooling
Basically, AI was slightly worse than a junior developer at doing ALL these things. Just to be nice, I spent about a day on it, tweaking prompts and giving it links to blogs where pieces of the solution were in play.
It was just too much.
Sure, if I had asked AI how to make me a doubly linked list in Python, it would have spit it out with no problem.
Real-life stuff?
Not going to happen in a million years.
AI totally sucked at the following things.
understanding how to use newish tools like Polars, DuckDB, Daft for processing data.
understanding new storage tools like Delta Lake and how to interact with them.
how to use a newish technology like pgvector
probably a combo of too new and too obtuse
How to write performant Python that could interact with Postgres in a way that ‚Ä¶
didn‚Äôt OOM
would finish in this century
Understanding the recent developments in building RAGs
things like langchain
interacting with embedding and chat endpoints
And the list goes on.
AI will trick you.
The funny thing is AI will trick itself and you into thinking it actually knows what it‚Äôs doing. It will be confidently wrong and close at the same time.
Prompt with some errors?
‚Äú
Sure, let me fix that for you, I know what‚Äôs wrong.
‚Äù
I‚Äôm confident that tweaking prompts and inputing errors for a week would have not gotten you to the end, and the result would have been a blob of code you could never fix or understand.
Because you didn‚Äôt write it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
162626492.dataframe-sql-column-manipulation,DataFrame / SQL Column Manipulation.,"Spark, Polars, etc.",2025-05-06 14:26:02.828-05,"Sometimes, we should talk about the boring, even though we don‚Äôt want to. It‚Äôs part curiosity, part ritual to see if anything has changed much. We all get stuck in our old ways; learning new tricks is hard.
Part of this was spurred by some recent chatter online about PySpark/Spark and how people
SHOULD
be adding new columns, per all the internet savants.
Honestly, I‚Äôm not sure if I care that much anymore. At a certain point,
I outgrew the need to optimize every single piece of code
‚Ä¶ unless that code was causing problems and
NEEDS
to be optimized.
When you are young and full of spit and vinegar, as you well should be, you want to take on the coding world. Every single piece of code is agonized over, optimized, and squeezed for every drop of bits and bytes until it bleeds.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
It‚Äôs about priority, working on what the business needs, providing value, and not doing whatever whim strikes you on that day because you had too much coffee.
Did you like my rant?
Sorry, not sorry.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Ok. Columns.
I mean, if you grew up listening to
Limp Bizkit
like me, then there is a good chance you are old, wizened, and raw dogging Spark pipelines using
.withColumn()
, like tomorrow is your last day on earth.
This is the most classic use of adding columns. It is popular because it allows you to easily add complex business logic without a lot of fluff. Of course, this is what all the smart engineers will tell you is worthy of getting your knuckles cracked by your grandma.
After getting a good wrap on the hands, you quickly move on to using
.withColumns()
‚Ä¶
Even though it looks kinda messy, we do what we have to. Then, if you are feeling spicy and just want to mess with someone ‚Ä¶ you whip out a little
.selectExpr()
‚Ä¶
I mean, aesthetically, this is probably the most pleasant option for people, although it is the least used because it looks more like SQL!
Every once in a great while, you might see something strange.
Let‚Äôs be honest; most hobbits do spark.sql(‚Äú‚Äú) and are never seen again.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Polars and columns.
The new kid on the block,
Polars, simply follows the Spark patterns
that came before them. The
.withColumns()
combined with Expressions.
I guess that most people probably don‚Äôt like this style; it seems that people avoid expressions. People probably like the approach better.
You can do the classic select + expression in Polars as well.
Even Polars as SQL at this point.
I think what it comes down to is if you are a DataFrame or SQL person. What does most of the codebase consist of?
If we ask ChatGPT to summarize it all for us, it pretty much tells us the same thing we‚Äôve looked at, and knowing it‚Äôs training data (
everyone‚Äôs GitHub repos
), you can get a good idea of what is popular.
Sure, the talking heads want us to always use .withColumns etc, but honestly, who cares? If you‚Äôre literally adding 200 columns to a dataset, then you‚Äôve got other problems besides how you decide to add them.
I like the readability option when it comes to choosing. The
selectEXPR
of some sort is my default; it‚Äôs a good mix between the verbose DataFrame option and the classic SQL-readable code."
162365203.ray-on-databricks-distributed-python,Ray on Databricks. Distributed Python.,Machine Learning and AI Mastery,2025-05-01 10:39:03.792-05,"I haven‚Äôt used
Ray
much in my life, just a few times. Recently, when working on some LLM stuffy-stuff, I managed to find myself setting up a
Ray cluster
on Databricks for some distributed ML/AI work.
Lest you think I am some AI savant, tis not true, I still hold to the old axiom that 90% of all Machine Learning, including that fancy LLM stuff, is mostly the same ole‚Äô Data Engineering.
So, what I want to do today is nothing fancy: introduce you to
what
Ray is,
why
, and
where
you would use it. Then, I will show you some code examples of how I used Ray on Databricks to fine-tune an LLM model ‚Ä¶ to help drive home the concepts of what Ray provides.
This is what ChatGPT thinks about Ray and its relation to data and ML.
I agree with this assessment. What Ray seems to solve, in my experience ‚Ä¶
making regular Python distributed on a cluster
emphasis on Machine Learning/AI
I want to walk through a code example at a very high level and just give you some high-level thoughts. I‚Äôm not trying to teach you HOW to run Ray on Databricks, but maybe spark (pun intended) some interest on your end and give you some basic knowledge.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
High-Level LLM Fine-Tuning Example - Ray on Databricks
The first thing to note is that Databricks has good introductory notes on how to run Ray on Databricks;
read them
.
Note ‚Ä¶ since Ray is ML-specific, you will want two things ‚Ä¶
use a GPU Databricks cluster
use DBR version ML 15.0 or >
With Databricks Runtime ML 15.0 onwards, Ray is preinstalled on Databricks clusters.
Also, with
most
machine learning and AI use cases, you will be using
WAY
too many Python packages, and you will have no choice. This will require some trial and error to get all the packages installed and working with the correct versions, etc.
This is very common for
ALL
ML workloads and nothing new. In my case, I was fine-tuning a BERT model. This is an example of the imports I had to use.
If you are working on a similar project, plan time into the project to get a Cluster running without errors. Next, if using an ML GPU cluster, you will most likely have to match your Ray cluster configs to your cluster size, etc.
(number of works, GPUs, CPUs available, etc.)
Next, if we have a working Cluster and Ray is ‚Äúworking‚Äù without errors, we have to set up the ‚Äúflow‚Äù of the ML Pipeline that is specific to Ray and run the code in a distributed manner.
The high-level steps will be ‚Ä¶
Set up your dataset and model.
Define your single-worker training function. (
the part that needs to be distributed
)
I am glossing over many details, but it is that simple with Ray; the complication comes from what ML library you are using and WHAT you are trying to accomplish.
Below is an example of Ray training a BERT model.
It‚Äôs obvious what this code is doing, isn‚Äôt it?  Ray provides a Torch Trainer out of the box that we can use with BERT. But the magic really happens in the
train_func
.
This is how Ray will distribute the work.
I know this looks like a lot of code, but it‚Äôs not, it‚Äôs just standard ML stuff ‚Ä¶ wrapped up inside a function that Ray can throw onto a Worker.
So, if you have worked on ML workloads, this won‚Äôt be an unfamiliar code to you. Training most models ... including LLMs is much like classic ML.
pick your tooling of choice, torch, etc.
massage your data to prep it for your model
tokenize your data (LLM specific)
write the training code to suit your need
It‚Äôs really about taking <
whatever Python code
> and wrapping it in a function to be distributed. This can be seen in the below Ray example.
We have a Python thing ‚Ä¶ and want to speed it up by distributing it on a cluster.
Why use Databricks?
Well, that‚Äôs obvious. I can speak from experience; creating your cluster from scratch and installing things for ANY tool isn't trivial.
Databricks ML runtimes (15.0+) come with Ray ready, and the Cluster is ready for use. This slices about 20%+ of work time off the project from the start. You can focus on getting your code running, not configuring clusters.
Also, Ray is helpful because it can distribute specific use cases that don‚Äôt work well with Spark, like LLM fine-tuning, etc.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
162012220.becoming-a-better-data-engineer,"Becoming a ""better"" Data Engineer",being more competitive ,2025-04-28 07:22:15.474-05,"I get this question in one form or another, as regularly as my mother calling me on Mondays to check-in. It usually is formed something like ‚Ä¶
How can I be better at my job?
How can I be more competitive (in this job market)?
How can I get a better job?
How can I move to a Senior+ level role?
How can I grow?
How can a person beat all the other people just like you running around out there? Maybe you‚Äôre looking for a new job. Perhaps you‚Äôre falling behind at work. You‚Äôre getting old and feeling left behind.
I get it.
I‚Äôm old.
Every day that goes by, I get another grey hair, and there are six new Engineers who are ‚Äúsmarter‚Äù than me. So how am I still hanging on by my fingernails when crossing the Big 40 mark? Good question.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
I have no idea if this will be helpful, but I'll be sure to give you a 10-second overview of my background.
self-taught (no CS degree etc.)
worked at mostly startups (no FAANG or MAANG or whatever)
started working when SQL Server was your Data Warehouse
started as a Data Analyst
That proves anything is possible for anyone with a bit of elbow grease.
Also, I wanted to list many articles I‚Äôve written on this topic generally.
Supercharge your Data Engineering Career.
Transitioning to Senior Engineer
When a good engineer goes bad.
Data Engineer vs Senior Data Engineer
What makes ""smart"" engineers so stupid.
My Journey as a Data Engineer - Visually
Maniacal Focus
New Year's Resolutions for Data Engineers.
What is a ‚ÄúGood‚Äù Data or Software Engineer?
What I‚Äôve Learned After A Decade Of Data Engineering
The Best Piece of Software Engineering Advice
The Difficulties of Senior Engineer ‚Ä¶. are not Engineering
4 Ways To Setup Your Data Engineering Game.
Before I offer any advice, we shall do the most classic thing and send all the above articles to ChatGPT and ask our new overload AI to summarize some of this career content for us.
AI - TL;DR
Across all twelve essays, the storyline is consistent:
Exceptional data engineers differentiate themselves less by exotic tech and more by decisive action, relentless learning, disciplined focus, and human skills that scale their impact through others.
Nail those, and titles, pay bumps, and leadership roles follow almost automatically.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Mindset Shift - code to code+
I've given some thought to how to summarize my advice to someone looking to grow to the next level,
whether it's a new job, their first job, or simply a promotion.
It isn't easy; everyone and every situation are different.
From my point of view there is no shortage of talking heads on every social platform under the sun talking about career advancement and how to nail your next interview.
I‚Äôm so much interested in that topic, as much as how to build a long-term sustainable and well-rounded career.
All I can do is share what‚Äôs worked for me. I‚Äôm generally old school, so a lot of my career advice would be simply to
‚Äúwork hard.‚Äù
In our fast-paced digital culture of ease and comfort, most people don‚Äôt work hard; if you decide to work hard at whatever you‚Äôre doing ‚Ä¶ you will automatically be ‚Äúbetter‚Äù than most people.
Mostly, what it comes down to if you want to build a long-term sustainable career in Software and differentiate yourself from AI and the thousands of other (often better) programmers ‚Ä¶
is to stop worshipping at the feet of the Code God.
Yes, you should be an above-average programmer, but that has lost its luster in the face of AI that is cranking out code faster than you ever will.
But you have to move beyond the code.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
8 Universal Habits leading to the better you.
Decide, then own the outcome.
Clear, data-backed calls beat committee paralysis.
Read, learn, repeat.
Staff-level books, specs, and docs compound faster than tools.
Move before you turn into a fossil.
New roles or responsibilities every ~2 years keep skills fresh.
Test & document your data pipelines.
They‚Äôre the forcing functions of quality thinking.
Practice deliberate kindness.
Empathy turns raw IQ into team-scale impact.
Broadcast what you know.
Blogs, brown-bags, and code reviews turn you into a multiplier.
Guard your calendar.
‚ÄúManiacal focus‚Äù on the highest-leverage task beats 10 busy days.
Stay humble.
The moment you
must
be the genius, you‚Äôve stopped learning.
Yes you need to take the time to become a technical savvy Engineer, but this probably includes reading more documentation, POC‚Äôing new tools and technology, and working on high level architecture and design.
Your second focus
should be all the NON-technical things
we just got done talking about. Yeah, there is a lot of them, but also, the climb to the top isn‚Äôt easy either.
‚ÄúBe good at software‚Äîbe
better
at everything around it.‚Äù
‚ÄúFocus is saying
no
90 % of the time.‚Äù
‚ÄúYou don‚Äôt need to be the smartest person in the room; you need to make the room smarter.‚Äù
Can you shortcut this path? Not really. But you can start today at whatever job or position you are in. Typically, extra effort will be noticed eventually, if not, move on, don‚Äôt waste your time.
Work hard at your craft, love what you do, don‚Äôt get comfortable, always be pushing forward to learn more both technically, and non-technically.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
161746114.cloudflare-r2-apache-iceberg-r2-data,Cloudflare R2 + Apache Iceberg + R2 Data Catalog + Daft,the S3 Table killer?,2025-04-20 20:07:59.292-05,"Sometimes, we do things without thinking because that is what our forefathers have taught us; we walk the well-trod path before us without question ‚Äî lemmings doing lemming things.
S3 from AWS is one of those things. We use it a thousand times a day; it is a critical path and component of our Data Platform and Pipelines.
If it disappeared, we would come to a collective grinding halt.
It costs a lot of money too.
I‚Äôve been meaning to check out
R2 from Cloudflare
to see if it is what they say ‚Ä¶ a s3 killer. Then they did something that made me sit up in bed and scream.
A managed R2 Data Catalog
for Apache Iceberg that is built right into the bucket.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
R2 storage and buckets from Cloudflare.
Let's start from the beginning, see what this storage option from Cloudflare is, is like and how it compares to AWS s3; then we shall explore the new R2 Data Catalog for building Apache Iceberg tables in the cloud.
‚ÄúCloudflare R2 is
a global object storage service offered by Cloudflare that allows users to store and access unstructured data without incurring egress fees
. It is designed to be S3-compatible, making it easy to migrate existing applications or build new ones that utilize object storage.‚Äù
- AI
One of the most attractive parts of Cloudflare‚Äôs R2 storage is that they are realists.
They made it S3 API compatible
,
which is simply a must
. This makes the migration of code and data, as well as the onboarding of new Engineers, much simpler and straightforward.
I hate to touch on pricing because, most of the time, you need a degree from
Gandalf‚Äôs School of Wizardry
to understand any cloud tool's minute and hidden costs. Being that is what it is, let‚Äôs take them at their word and compare them to s3.
R2 charges based on the total volume of data stored, along with two classes of operations on that data:
Class A operations
which are more expensive and tend to mutate state.
Class B operations
which tend to read existing state.
For the Infrequent Access storage class,
data retrieval
fees apply. There are no charges for egress bandwidth for any storage class.
As compared to AWS s3.
I will let you smart people figure out the rest yourselves. I set up a personal account with Cloudflare; they give you some free storage, etc.
Next, let‚Äôs just set up a new R2 bucket and see if we can get the R2 Data Catalog going.
Wait, I‚Äôm getting ahead of myself
. We‚Äôve been talking about R2 for storage, but we haven‚Äôt talked much about the
Apache Iceberg
and managed
Data Catalog that Cloudflare is now providing.
‚Äú
R2 Data Catalog
in open beta, a managed Apache Iceberg catalog built directly into your Cloudflare R2 bucket ‚Ä¶
R2 Data Catalog exposes a standard Iceberg REST catalog interface, so you can connect the engines you already use, like
PyIceberg
,
Snowflake
, and
Spark
. And, as always with R2, there are no egress fees, meaning that no matter which cloud or region your data is consumed from, you won‚Äôt have to worry about growing data transfer costs.‚Äù
This is exciting indeed. Yet another SaaS and Cloud behemoth is entering the Lake House race, getting involved in the ongoing Storage Format War, and making Big Data at scale as simple as a click of a button.
But this is nothing new to you, my fickle reader.
Everyone under the sun has been trying to make it as frictionless as possible to build and manage Lake House tables in the cloud.
Cloudflare‚Äôs entry into the fray is good for competition and will push others to up their game.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Now that we have the boring part out of the way, let‚Äôs get to actually creating an Apache Iceberg table in R2, using the provided Data Catalog.
Should be fun!!!
Create R2 Bucket and Apache Iceberg table with R2 Data Catalog.
So, if you are new to Cloudflare,
they have a tool called Wrangler
that you can use on the CLI to run many commands, think of it like the aws cli many of you use (requires NPM to install Wangler).
>> brew install node
>> npm create cloudflare

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üëã Welcome to create-cloudflare v2.44.1!
üß° Let's get started.
üìä Cloudflare collects telemetry about your usage of Create-Cloudflare.

Learn more at: https://github.com/cloudflare/workers-sdk/blob/main/packages/create-cloudflare/telemetry.md
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

‚ï≠ Create an application with Cloudflare Step 1 of 3
‚îÇ
‚ï∞ In which directory do you want to create your application? also used as application name
... blah, blah, blah
Now, we already have a R2 bucket ‚Ä¶
What we need to Enable R2 Data Catalog on your bucket.
>> npx wrangler r2 bucket catalog enable testing-iceberg-r2
Successfully logged in.
‚ú® Successfully enabled data catalog on bucket 'testing-iceberg-r2'.

Catalog URI: 'https://catalog.cloudflarestorage.com/3ee64e77beb1e2c68a3ae7c1cd4d232e/testing-iceberg-r2'
Warehouse: '3ee64e77beb1e2c68a3ae7c1cd4d232e_testing-iceberg-r2'

Use this Catalog URI with Iceberg-compatible query engines (Spark, PyIceberg etc.) to query data as tables.
Note: You will need a Cloudflare API token with 'R2 Data Catalog' permission to authenticate your client with this catalog.
For more details, refer to: https://developers.cloudflare.com/r2/api/s3/tokens/
Well, that was way too easy wouldn‚Äôt you say? Nothing works the first time without problems, what a breath of fresh air.
We can see from the notes above that we will need an API token to assist with authentication.
I went ahead and used the
docs to create tokens
, one nice thing is that they give you s3 compatible tokens ‚Ä¶ aka if you are using or migrating s3 code.
Share
Creating Apache Iceberg table on Cloudflare‚Äôs R2.
Next, let‚Äôs get to actually creating an Iceberg table on R2, and to prove out that it works well, let‚Äôs use my Databricks account and
Daft to do this
.
Let‚Äôs be real, if this is going to be used in production, we have to be able to do it like this ‚Ä¶ not on someones laptop. It‚Äôs the Cloud or bust.
We will use
BackBlaze hard drive dataset
as our base data to create an R2 Apache Iceberg table with the R2 Data Catalog.
That‚Äôs the data. This is how we can create our Iceberg table on R2. You can get the R2 Details needed from the Cloudflare Dashboard.
I to comment on this ‚Ä¶ how easy is this?? That is the beauty of working with Iceberg on Cloudflare‚Äôs R2 ‚Ä¶ it‚Äôs idiot proof, even I can do it.
Unless you‚Äôve spent time like me piddling around with Iceberg using LakeKeeper, Polaris, etc etc, it‚Äôs hard to appreciate the utter simplicity with which R2‚Äôs Iceberg Catalog makes working with Lake House tables ‚Ä¶ all stored, served, managed via Cloudflare.
Next, let‚Äôs see if we can read this empty R2 Iceberg table with Daft on a Databricks Cluster.
Look at that! Just as simple and easy as could be.
Well, can we go a step farther and write a bunch of data into our R2 Iceberg table with Daft?
Smooth as butter my friend, that‚Äôs what I like to see (
Almost as smooth as Delta Lake
). I have to say Cloudflare‚Äôs R2 and their top-notch Apache Iceberg setup is the cream of the crop.
Pair it with something like Daft and you have production-ready Lake House just begging to be beat with queries.
I mean I wish I had more complicated code and fancy backflips to make me look smart and wise. Too bad Cloudflare did such a good job, made it too easy.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
161263643.lakekeeper-iceberg-rest-catalog,LakeKeeper: Iceberg REST Catalog,... another catalog for the mix,2025-12-08 06:55:15.004-06,"In a continuation of a topic I‚Äôve written about off and on over the last few months (
Apache Iceberg
and
Catalog options
), I‚Äôm taking another stab at finding ‚Äúgood‚Äù and ‚Äúapproachable‚Äù Catalog options that support Apache Iceberg.
By ‚Äú
good
‚Äù and ‚Äú
approachable
,‚Äù I mean something that can be easily installed and run in a semi-production-ready state by an A
verage Engineer
in a reasonable amount of time with a small amount of troubleshooting.
The truth is I am a glass-half-empty kind of guy regarding most technology. Nothing angers me more when a talking head declares the next most remarkable thing is released and
shows how well it worked on their laptop.
I prefer to try tools out in an environment
where the hidden rocks pop out of the water and sink your happy boat.
Amazingly, all a person is required to do is use a Linux server and some AWS credentials to do just that.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Lakekeeper - an overview.
Today, we will review and test one of the new Iceberg Catalogs on the block,
Lakekeeper
. It‚Äôs an Apache-licensed (
makes me feel fuzzy inside
) REST catalog that can help when running Iceberg Lake Houses in the cloud.
It‚Äôs based on
iceberg-rust
, which should perk up the ears of all you bandwagon hopping Data Engineers.
I suppose there isn‚Äôt much to say before simply diving into the deep end, but here is the general plan.
Install
Lakekeeper
on a Linux server in the Cloud
Get it running and access the UI
Add AWS creds to
Lakekeeper
so our Iceberg tables are stored in S3
Connect to
Lakekeeper
with some Python, etc., and read/write some tables.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Getting the Lakekeeper ball rolling.
Okay, let‚Äôs start by getting a nice
Linode
Linux server up and running, which we can use to install Lakekeeper and related tools.
What do you think? Two CPUs and four GB of RAM should be enough to run a REST catalog. The first thing we will do is SSH into our new machine and
set up UFW
to block all incoming traffic except from my local machine;
we have to keep the hobbits out of that pantry, so to speak.
Ok, let‚Äôs see if we can get Lakekeeper installed on our little server kingdom, check out what docs we can find, and see what our options are.
One of the nicest things you will find about
Lakekeeper is the good documentation.
Never take that for granted.
Options for installing Lakekeeper?
Docker-compose (
you will have to enable the network to be available outside Docker
)
Kubernetes
Binary
We are going to try the docker-compose way, it seems the easiest place to start and the least amount of effort. Just need to install Docker and docker-compose on the server.
Well, that was a mouthful.
First problem right off the bat.
When in doubt, restart.
Worked that time ‚Ä¶ shrug.
Now, I forgot to check the Docker Compose.YAML file to ensure it was exposing and binding the correct ports so the Lakekeeper UI would be available.
Let‚Äôs just try to hit the UI from my local machine and see what‚Äôs up. Look-yee-thar.
Let‚Äôs clickety-click Add Warehouse.
We can now add our new pickles library with our S3 Credentials.
Not bad, fill out our bucket, region, etc.
Sooo ‚Ä¶ what say you, shall we give ‚Äòer the old college try? Basically, we can use some Python and try to hit http://172.233.221.242:8181/catalog/ and create an Iceberg table backed by our S3 Warehouse.
Let‚Äôs create a small test CSV, load it with Daft, and see if we can accomplish something with it.
Something like this should do the trick, I think.
Okay, so that code ran without throwing an error; let‚Äôs do the real test and see if we can write data into that table and then read it back from the catalog.
Our first error occurs when we write data in the table. This makes me wonder if the table was created in the first place. We should check the Lakekeeper UI.
When I check the Lakekeeper UI, there are no tables listed, but I realize now that I need to create a namespace. I‚Äôm curious why the first command didn‚Äôt blow up.
Let‚Äôs create a default namespace and try again.
Again, I run this same code above without error, but no tables appear in the UI. It‚Äôs clearly not even getting far enough to create the table based on the dataframe.
It‚Äôs hard to troubleshoot when you can‚Äôt even get an error. Maybe we can check the Docker logs on the server.
Better yet, let's kill the current Docker Compose setup and try it again from scratch. When in doubt, kill everything and try again. I restarted everything and reset the AWS creds, warehouse, etc.
Well, this makes sense; when I get back to the server, I can see it's dead. I retried a few times, but the `trio` ‚Ä¶ whatever they are using that for keeps dying.
I tried docker-compose up and down a few more times, and finally, randomly, the processes didn‚Äôt exit and seemed to be running again.
Honestly, at this point, I wasn‚Äôt sure if the problem was with Lakekeeper or possibly Daft/pyiceberg. So I figured I would switch to PyArrow and run the code below.
That was the key; it worked fine when combined with PyArrow and PyIceberg. Our table appeared in the UI.
Let‚Äôs try writing data to it and reading it back.
No such luck. I'm complaining about signatures. The error appeared to be coming from the s3fs package.
I know the creds are correct on the catalog. Why no table love?
Strangely enough, if I print the pyiceberg table from the catalog, it appears to work that far. Just has problems when we try to actually write data into it (
backed by s3
).
I updated the AWS creds via the Lakekeeper UI just to be safe.
Well, that‚Äôs a new error, but not any closer. Not sure what to say, not sure what to do. Try something else besides the docker setup.
*Poof* - 6 Months Later
So, after giving up on getting LakeKeeper running, I decided to return 6-8 months later and give it another shot to see if things had changed, improved, or become more stable.
Again, I set up a brand-new Linode and followed the exact same steps above to prep the server and install LakeKeeper. Basically, I restarted and am back at the step where I‚Äôm running Docker Compose with LakeKeeper.
>> git clone https://github.com/lakekeeper/lakekeeper
>> cd lakekeeper/examples/minimal/
>> docker compose up -d
This time, we get an all green on the first try. That‚Äôs encouraging.
In my case, going to http://172.238.167.32:8181/ui/ gives me above, same as before.
Again, I added a new Warehouse as above, and added S3 credentials.
Let‚Äôs try to connect and create an Iceberg table; we couldn't get it to work last time, no matter what we tried.
Now, a little code.
Will it work, no errors off the bat!!
Well, Gandalf‚Äôs Beard! It worked! Just to make sure it was indeed a Thanksgiving-day miracle, I checked the S3 bucket backing this LakeKeeper setup and saw the metadata file for the Iceberg table we just created.
Sure enough! I‚Äôm glad I gave LakeKeeper some time to marinate after my first attempt. I‚Äôve seen them active on different socials, so I figured they were working hard on improving LakeKeeper, and indeed they have.
The second time around, using docker compose, it took just a minute or two to get things running and a Warehouse added with S3 Credentails to have a fully functioning Iceberg REST Catalog.
Thoughts
It's good to see the Apache Iceberg community continue to grow and evolve to meet the rising demand for robust, easy-to-use catalog solutions for the Lake House world we live in today.
Historically, the catalog has been the Achilles heel of Iceberg; a fragmented and complicated relationship between open source and closed source options has created a lot of confusion and frustration with the general usage of Iceberg.
Clearly, LakeKeeper is trying to change that by offering very nice and easy-to-use options. I was super happy to return and see things working nicely, spells great things for the future of Iceberg.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
160908298.duckdb-pyiceberg-lambda,DuckDB + PyIceberg + Lambda,... with Cloudflare's R2 ...,2025-05-15 22:14:50.124-05,"Every once in a great while, I feel it‚Äôs good to pay some penance, to do a thing that isn‚Äôt fun at all, that you find appalling and horrible. I tell my children regularly that it‚Äôs a good thing to struggle; to do a ‚Äú
hard thing
,‚Äù makes ya‚Äô
stronger
.
We need all the strength we can muster with AI bearing down on us.
We all might soon be on the streets fighting each other for scraps of bread.
What are those hard things for me? I‚Äôm sure some of you could guess the answer outright.
Iceberg
DuckDB
Now, don‚Äôt get me wrong, there is nothing wrong with Apache Iceberg and DuckDB. Clearly, they are strong contenders in the Data World. I‚Äôve just had my issues with them in the past.
I‚Äôve got a bit of a hard edge, maybe a streak of
hard-to-forget
and
bitter at times
if you give me problems.
__________________________________________________________________
Please take a moment to check out this article‚Äôs sponsor,
OpenXData conference 2025
, without them this content would be possible.
Please click the links below to support this Newsletter.
The data infrastructure event of the season is here ‚Üí
OpenXData conference 2025
A free virtual event on open data architectures - Iceberg, Hudi, lakehouses, query engines, and more. Talks from Netflix, dbt Labs, Databricks, Microsoft, Google, Meta, Peloton, and other open data geeks.
May 21st. 9am - 3pm PDT
. No fluff. J
ust solid content, good vibes, and a live giveaway if you tune in!
__________________________________________________________________
Anyway, let‚Äôs pay some penance
by building an AWS Lambda that uses DuckDB and PyIceberg
to ingest and transform data, landing the results in an Iceberg table.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Really, this is about testing what everyone else talks about, but never does. People from all walks of life pontificate on the likes of DuckDB and Apache Iceberg, singing praises where they may, or may not, be due.
As per usual, I will leave the talking heads to talk about what they don‚Äôt know or have ever tried. For you and me, we shall plumb the actual depths of what can be done,
how these tools act in the real world, under real pressures.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Setting up the background and data.
First, since I like pain, but not that much, let‚Äôs get an Iceberg compliant table setup, and I‚Äôm going to use
Cloudflare‚Äôs R2 Data Catalog and Iceberg option
. Read more about that below.
So what we actually want to do with
AWS Lambda + DuckDB + PyIceberg
is see if we can actually ingest raw data, mung it with DuckDB, and write it with PyIceberg ‚Ä¶ all wrapped in an Lambda.
The idea is ‚Ä¶ instead of using something heavy like Spark, BigQuery, or Snowflake to ingest data into a Lake House, can we use this extremely light-weight architecture to do the incoming data processing and landing into our ‚ÄúStaging‚Äù or ‚ÄúBronze‚Äù layer.
I think we can. This could lead to big cost savings on a lot of Data Platforms.
Who needs to spin up a Spark Cluster when we can use a Lambda to ingest data into a Lake House???
The start of the matter.
Here is our Cloudflare R2 data bucket. If you are unfamiliar with
data storage on Cloudflare, read here.
Now all we have to do is ‚Ä¶ (
enable Iceberg Catalog on the bucket
)
>> npx wrangler r2 bucket catalog enable pickles
‚ú® Successfully enabled data catalog on bucket 'pickles'.

Catalog URI: 'https://catalog.cloudflarestorage.com/3ee64e77beb1e2c68a3ae7c1cd4d232e/pickles'
Warehouse: '3ee64e77beb1e2c68a3ae7c1cd4d232e_pickles'

Use this Catalog URI with Iceberg-compatible query engines (Spark, PyIceberg etc.) to query data as tables.
Note: You will need a Cloudflare API token with 'R2 Data Catalog' permission to authenticate your client with this catalog.
For more details, refer to: https://developers.cloudflare.com/r2/api/s3/tokens/
It‚Äôs never been so easy to spend money, just a few commands and you‚Äôre off to the races!!
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Creating an Apache Iceberg table.
First, we need to get an Iceberg table created in our R2 bucket so our DuckDB has something to work with. Let‚Äôs use Divvy Bike trip‚Äôs open-source dataset.
The data looks like so ‚Ä¶
Nothing fancy really,
just some CSV files that have a record for each bike ride taken.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Creating an Iceberg table with DuckDB.
Let‚Äôs use DuckDB to create our initial Iceberg table. This will be a little funky because
DuckDB DOESN‚ÄôT HAVE ICEBERG WRITE SUPPORT.
Ops, did I say that too loud?
Part of my job is to be the one that stands up in the midst of the lemmings walking over cliffs saying ‚Ä¶ ‚Äú
Use Iceberg it‚Äôs great, has lots of support ‚Ä¶ use DuckDB it‚Äôs great, the perfect tool for everything!
‚Äù
This is a perfect example solving problems on the ground in real life, let‚Äôs pretend we are a lowly Data Engineer and we have some ‚Äúsuper smart boss‚Äù who reads LinkedIn a lot all they know is that ‚Äú
DuckDB is awesome and great and so is Apache Iceberg.
‚Äù We simply have the demand to use these two technologies together, come what may.
We will probably have to use
pyiceberg
to create the table. If find it highly ridiculous that DuckDB cannot write to Iceberg.
This is one thing you have to love about Cloudflare‚Äôs R2 Iceberg setup ‚Ä¶ it‚Äôs unparalleled in the market in terms of ease of use and developer friendly implementation.
As you can see above we had to rely completely on pyiceberg to get the initial table setup. Since DuckDB has no write support.
As you might have noticed our schema for the Iceberg table doesn‚Äôt exactly match our raw trip data. This is where DuckDB will come into play in transforming our CSV data and calculating metrics that will need to be rolled up before being deposited in our
R2 Iceberg table.
The schema you saw above is for daily metrics.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Writing the code for the AWS Lambda.
First thing‚Äôs first, let‚Äôs create a basic Lambda that will hold our code and corresponding trigger.
Now we have a wonderful
iceiceBaby
Lambda with a trigger for any .CSV file that shows up in
s3://confessions-of-a-data-guy/iciceBaby/*.csv
So we have our data, we have or Iceberg table ready to go, our blank Lambda with a trigger.
Last, but not least, we need the code to ingest a CSV file with DuckDB and then use PyIceberg to write to our table.
Let‚Äôs start with the Python code using DuckDB and PyIceberg to do what we want. Let‚Äôs write it up and
give ‚Äòer the old kick in the pants.
Here‚Äôs the steps ‚Ä¶
make Dockerfile to hold AWS Lambda code
make ECR in AWS to hold image
make Lambda with trigger (already done)
Here is our simple Dockerfile.
#Dockerfile
FROM public.ecr.aws/lambda/python:3.8.2021.12.18.01-x86_64

COPY .  ./
RUN pip3 install duckdb pyiceberg pyarrow boto3
CMD [""main.lambda_handler""]
Easy enough. This is how we push it to the ECR in AWS.
#bash
aws --profile confessions ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 992921014520.dkr.ecr.us-east-1.amazonaws.com

docker build -t iceicebaby .

docker tag iceicebaby:latest 992921014520.dkr.ecr.us-east-1.amazonaws.com/iceicebaby:latest

docker push 992921014520.dkr.ecr.us-east-1.amazonaws.com/iceicebaby:latest
Ok, that‚Äôs done. Code next.
I know I‚Äôm throwing a lot at you, but that is
DuckDB
+
pyiceberg
for us to read a CSV file from s3. What we have going on is ‚Ä¶
read the file with DuckDB from s3
run the GROUP BY analytics with DuckDB
convert the result to Arrow
use PyIceberg to connect to the R2 Data Catalog
use PyIceberg to the Iceberg table via Arrow
Aren‚Äôt you excited to know what happened?
Apparently this is a think with DuckDB and Lambda runtimes.
Word is if we set `
HOME=/tmp
` or something of like in the environment, all will be well. I went ahead and added this ‚Ä¶
import os
os.environ['HOME']='/tmp'
Well, look at that. Success! If we go read out Iceberg table in Cloudflare‚Äôs R2, we have data!
Not going to lie, I didn‚Äôt think it was going to be that easy. I lay most of that ease at the feet of Cloudflare‚Äôs R2 Data Catalog setup.
It‚Äôs dead simple to use and setup, unlike most other options that are overly burdensome.
The DuckDB code itself is neither here not there, I find it perfectly acceptable although a little mundane. There is no doubt that the SQL god‚Äôs who have bewitched the minds of all Data Engineers for decades,
have served us this delicious DuckDB knowing that we could never keep our fingers from its little tricks.
The real pain here is dealing with pyiceberg, connecting to catalogs, connecting to tables, having to use that simpering middleware to write to our Iceberg table.
Simply unacceptable.
We need first class Iceberg READ and WRITE support in all major data tooling, no exceptions.
I need no catalog to read or write from a Delta Table if I so choose.
I will leave it here.
To put this to bed, for me the hour is getting late, I think that we‚Äôve proved a few things here.
It‚Äôs possible to build light weight Lake House data processing.
AWS Lambda is underutilized.
DuckDB is fast and versatile, easy to use.
Cloudflare‚Äôs R2 Data Catalog for Iceberg is an unsung hero.
pyiceberg and the Iceberg Catalog requirements are an achilles heel.
I‚Äôm old and crusty but can learn new tricks.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
160667214.review-of-data-orchestration-landscape,Review of Data Orchestration Landscape,... and honest one ...,2025-04-06 20:23:26.224-05,"Sometimes, I force my old caveman self to do things I do not want. Believe it or not, reviewing the landscape as it relates to data orchestration frameworks is not something on the top of my list. People, including me, have their own pet favorites when it comes to orchestration platforms, and it‚Äôs hard to be unbiased.
Does it matter what tool you choose? If it works, it works; if it doesn‚Äôt, it doesn‚Äôt; it‚Äôs not like we don‚Äôt have a plethora to choose from.
Either way, I suppose something like this could be helpful for those new to the Data Engineering world or those looking to build greenfield projects or migrations.
One of the most distasteful aspects of comparing tooling, in this case, orchestrators, is the desire of folks to simply get a side-by-side list of features.
Different tools are good at other things, and a simple side-by-side checkbox list is usually misleading.
Whatever, I‚Äôm not sure how to compare different orchestration tools, but I want to cover the high-level things. Then, I‚Äôm assuming you will ‚Äúgravitate‚Äù towards one or the other based on your needs. That should be good enough for both of us.
Mostly, I want to provide an unbiased comparison without any marketing drivel blocking the way.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Content like this would not be possible without their support. Check out
their website
below.
Now that we are done with that business let‚Äôs get down to it.
Orchestration Tooling Options
We should probably start at the top, simply listing our options, good and evil; I‚Äôm sure I will forget someone‚Äôs favorite, but so be it. Also, it‚Äôs hard to know where to draw the line, so to speak; some tools are pure orchestrators, and some are bent more toward data processing but act as orchestrators.
Apache Airflow
AWS MWAA
GCP Composer
Astronomer
Dagster
Prefect
Mage
Metaflow
Flyte
Databricks Workflows
AWS Step Functions
Luigi
Kestra
Argo
Orchestra
Another great way to make people angry and break up the playing field into something more manageable is to break the entire group up by open source vs. not ‚Ä¶
this is a crucial distinction when thinking about options
There are technically three different types of tools, like it or not. I expect to hear some pushback, and talking heads take me to task on this, but whatever.
Closed (
100% SaaS
)
Semi-Open Sourced (
Both Managed (paid) and some open-source
)
Open-Source
As Data Engineers and Data Platform builders, we must know whether any tool we use, including these orchestration tools, is a semi-open source
.
What I mean by that is ‚Ä¶ do the parties that control the ‚Äúopen-source‚Äù GitHub also offer it as a paid service?
This can be both a good AND a bad thing. We have to be realists; we live in a world filled with humans who do human things.
Will the best features be reserved for the paid versions? Will they pull the rug without warning?
Pretending like this stuff doesn‚Äôt happen is naive and dangerous. I don‚Äôt know if this is 100% correct, but here is my take on it. (
Everything under the Open/Closed category appears to be offered by the makers as a paid/hosted product
).
Sure, some companies take the open-source part more seriously than others, and you can run their tooling yourself and get the best features, but you can‚Äôt, or you get the rug pulled at some point.
You should just be aware of where things stand.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
How to compare Orchestration Tools
Before we can compare and contrast some of these tools, we should cross some off the list. Before you light your torches and grab the pitchforks, hear me out.
If we‚Äôre here to pick out a new tool, you start by finding out which tools won‚Äôt work for most folk.
How can we do this? Here are some criteria.
Small community and not widely used (
cross it off the list
)
unless you‚Äôre feeling spicy, we don't pick immature tooling.
We want full-feature tools, not those operating on a narrow fringe.
Anything else is fair game, although we will review other vital needs later. Based on the above, let‚Äôs cross a few off the list.
Why did I cross off metaflow, luigi, argo, and aws step functions? Because they are either too narrow or simply not widely used enough to have a thriving community.
metaflow
has been growing a lot, but it is more focused on data science and ML workflows and hasn‚Äôt been around long enough.
argo
is made to be run with Kubernetes; although popular, it‚Äôs too narrow in implementation.
Luigi
has been around for some and does get used, but has lost out to other tools for good reason.
AWS Step Functions
just aren‚Äôt fully featured enough, and it‚Äôs totally closed source.
if you think that‚Äôs not a problem, go read about what happened to AWS Data Pipeline.
Orchestra - just strange mix of open-source and paid.
Again, when picking between tools, we have to make cuts somewhere; these guys just didn‚Äôt make the cut.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Just because I‚Äôm curious I want to look at two stats on GitHub for all these tools so we can see them side by side, GitHub Stars (
which means less and less these days
), and Open Issues (
this is a stronger indication of usage
).
Ok, based on this I think we can cross some more off the list, it‚Äôs a brutal world we live in uh? Sorry if you‚Äôre favorite is getting the boot.
flyte appears to be low usage in comparison to our other options.
Kestra strangely has a lot of stars (??) but appears to have little usage based on issues. Also,
some recent notes on reddit
indicate the OSS version isn‚Äôt full featured and lacks community support.
This is the sort of thing we talked about in the beginning, and need to look out for.
That brings us down to the mighty 4.
We are left with ‚Ä¶
Prefect
Mage
Dagster
Airflow
Honestly, I‚Äôm not surprised, that is where I would expect to end up, if you work in the Data landscape, these tools are not unfamiliar to you. In my opinion, that means we are doing our review correctly.
If you think about it, this is really the single behemoth, Apache Airflow, vs the upstarts tying to get a piece of that action.
Now my intention isn‚Äôt to give you a how-to on each one of these tools, but let‚Äôs take a look at how to operate and their core concepts so YOU can get an idea of which tool resonates with YOU more.
In the end any of these tools you chose, if they work for you, is fine.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Concepts
Now, I‚Äôm no expert in all these tools, I just know how to read documentation and draw conclusions. I‚Äôve read the documentation for these tools and come up with some thoughts to help us understand
The Big Four
.
It‚Äôs clear that these tools approach Workflow Orchestration from different perspectives, which makes sense, they are different tools, no surprise there.
DAG-based approach
Code-centric based approach
Both Airflow and Dagster focus on the DAG as an abstraction to write and orchestrate your needs.
Mage and Prefect focus on more on code and the integration between code and the workflow is tight, not as much abstraction.
To help that idea sink in, here is an example, from their own docs, of a Workflow/DAG for each tool so you can see what I mean.
Airflow
DAG example.
Dagster
JOB example.
Just as a side note, Dagster sorta rides the rail between Airflow classic DAG setup and code centric approach. But, they look awful similar don‚Äôt they?
Mage
Pipeline example. (Mage uses Notebooks for code).
Prefect
Pipeline example.
What do you notice about Prefect and Mage? The are code centric and use decorators to build out pipelines.
Core concepts of Mage.ai
‚Ä¶
Pipeline
A pipeline contains references to all the blocks of code you want to run, charts for visualizing data, and organizes the dependency between each block of code.
Block
A block is a file with code that can be executed independently or within a pipeline. Together, blocks form a Directed Acyclic Graph (DAG), which we call pipelines.
There are 8 types of blocks ‚Ä¶
Data loader, Transformer, Data exporter, Scratchpad, Sensor, dbt, Extensions, Callbacks.
Core concepts of Prefect‚Ä¶
Flows
A Prefect workflow, modeled as a Python function.
Tasks
Discrete units of work in a Prefect workflow.
Results
The data returned by a flow or a task.
Similar concepts, but that is not a surprise, anyone building a workflow orchestration tool with Python would expect something similar. There is only so much abstraction that can be done.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What do people say about these tools in the wild?
Mage ‚Ä¶
Prefect ‚Ä¶
We should probably talk about Dagster.
We haven‚Äôt given Dagster much attention yet, but we probably should, since it‚Äôs one of The Big Four.
Dagster Concepts
Asset
An
asset
represents a logical unit of data such as a table, dataset, or machine learning model. Assets can have dependencies on other assets, forming the data lineage for your pipelines.
Graph
A
GraphDefinition
connects multiple
ops
together to form a DAG. If you are using
assets
, you will not need to use graphs directly.
IO Manager
An
IOManager
defines how data is stored and retrieved between the execution of
assets
and
ops
. This allows for a customizable storage and format at any interaction in a pipeline.
Job
A
job
is a subset of
assets
or the
GraphDefinition
of
ops
. Jobs are the main form of execution in Dagster.
OP
An
op
is a computational unit of work. Ops are arranged into a
GraphDefinition
to dictate their order. Ops have largely been replaced by
assets
.
What are people saying?
More Thoughts.
It‚Äôs my personal opinion that Apache Airflow is the go to for orchestration only,
in the most classic sense of the word
. It‚Äôs the 500lb gorilla in the room that has a massive community, lots of integrations and is ‚Äúeasy‚Äù to use.
Easy to use in the sense of a massive community, a tutorial covering anything you could dream up exists somewhere.
But, it‚Äôs just that. Orchestration.
Anytime you wander into use cases that actually call for the manipulation and processing of data ON that orchestration platform, as your primary use-case, you will find a better solution in Prefect, Dagster, or Mage.
Which should you choose, Prefect, Dagster, Mage?
How would I know?
You need to use them and test them against your main use case, only that will answer your question.
Speaking of integrations, we should poke at a minute, because that is one of the MAJOR selling points of Apache Airflow. Because of its wide usage and community, it has simply and to use integrations for everything under the sun, which makes life easy in the Modern Data Stack.
Dagster appears to have, on the surface every kind of integration one could image.
AWS and all its services (s3, EC2, etc, etc). Databricks, DBT, DuckDB, Kubernetes, Snowflake, Tableau, etc.
Mage integrations appear to be wide, but maybe not as all inclusive as Dagster?
In their own documentation in integrations they don‚Äôt even list Databricks.
It lists AWS s3 but not EC2, etc. Again, what do I know, just going off what they tell me.
What does
Prefect list
?
The Prefect list of integrations
isn‚Äôt huge, but it does appear to cover all the big stuff, Snowflake, AWS, Azure, Databricks etc.
We don‚Äôt need to go over Airflow integrations because they are as far and wide as the sea.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The end of it all.
To be honest you could simply list every feature of all these orchestration tools side by side, but you will most likely only find that in marketing content. The truth is that picking an workflow orchestration tool is a complex task and requires a lot of nuance.
That nuance usually involves the specific use case of a Data Platform. For example, I crossed of Argo early on during this discussion because it was solely focused on Kubernetes. But, I‚Äôm sure there are many teams (even mine a few years ago), that mostly work on Kubernetes and would benefit greatly from Argo.
At the end of the road we should NEED a reason to stray off the beaten path. The beaten path in this case being ‚Ä¶
Apache Airflow
Prefect
Dagster
Mage
One should probably also simply start with Airflow or one of it‚Äôs SaaS versions,
AWS MWAA
,
GCP Composer
, or
Astronomer
. It will most likely cover most use cases.
But, there are aways the brave few looking to push the edges and boundaries of what‚Äôs possible.
For these, something like Prefect, Dagster, or Mage are the order of the day.
Everyone is probably asking which one is ‚Äúbetter,‚Äù Mage, Prefect, Dagster. That‚Äôs like asking what is the best cloud platform like AWS, Azure, or GCP.
There is no REAL answer to that other than you can make each platform what you make of it.
Many successful and large Data Platforms are run on AWS, Azure, or GCP. Many data teams use Dagster, Prefect, or Mage to great effect. It‚Äôs more about the team‚Äôs needs and their execution rather than the specific tool.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
160452902.data-engineering-central-podcast-150,Data Engineering Central Podcast - 07,Episode 07,2025-04-02 16:08:25.486-05,"It‚Äôs time for another episode of the Data Engineering Central Podcast. In this episode, we cover ‚Ä¶
Rust-based tool called UV to replace pip and poetry etc
Apache X-Table and the Future of the Lake House
How is AI going to affect you?
Thanks for being a consumer of Data Engineering Central; your support means a lot. Please share this podcast with your friends; it helps the show a lot!
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
160284821.databricks-compute-thoughts-and-more,Databricks Compute. Thoughts and more.,dollar bill 'yall,2025-03-31 20:40:45.276-05,"There are two things certain in data engineering life: death and taxes, as well as S3 and EC2 (compute) costs. Both storage and compute costs probably make up 80% of most cloud bills for running a Data Platform, I imagine.
Regarding Databricks and compute, the ground shifts underneath you without anyone noticing it. Sometimes, we get stuck in a rut and simply ignore the goings on in the world of new features and ‚Äúthings‚Äù being released. We become blissfully unaware of our options and how we should use them to our advantage.
I don‚Äôt pretend to be a Databricks expert, and I need to remind myself to do my due diligence every year or so. Today, I will
review compute options inside Databricks
for the average semi-informed user.
It will probably form a semi-coherent blob of half-wrong/half-right information regarding options around Databricks compute,
and how we might avail ourselves of some cost savings.
Thanks to
Delta
for sponsoring this newsletter! I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering. Check out their website below.
If you and I come away with a clearer picture of compute options, when generally to use each, and possible cost savings or best practices ‚Ä¶
that‚Äôs a win in my book
.
Databricks Compute Options. (AWS point of view)
Let‚Äôs start with a simple, high-level overview of
general compute options for Databricks and their category
.
Serverless
SQL Warehouse
both serverless and classic ‚Ä¶
Job
All-Purpose
Instead of getting bogged down in the minute details of each compute type and agonizing over which one would save me three pennies over the other one (
context is always key
) ‚Ä¶ I generally stick to the big picture.
Job compute at all times because it‚Äôs cheap.
SQL Warehouse only if you need to support on-demand analytics with third-party apps (like Tableau).
All-purpose for short-lived Notebook research, etc.
Serverless for a quick question you want an instant answer to.
Try to keep it simple and keep to the obvious way of solving problems. A few nuances can either cost you or save you serious amounts of money.
Do not overuse All-Purpose compute
. If you are letting All-Purpose compute run for 8 hours a day so you can ‚Äúdo your job,‚Äù you are most likely wasting massive amounts of money.
If you have a Notebook that runs a query (or set of them), for more than 30 minutes.
Switch it over to a Job!
It‚Äôs not that hard. If you *NEED to run something in a Notebook, just go to t
he Workflows section of the Databricks UI and create a new Job
, select your Notebook, etc., and you‚Äôll save everyone money just like that. S
ee below
.
Avoid Serverless most of the time; use it for short, quick needs that last only a few minutes.
There‚Äôs a lot of changing, including pricing with Serverless; it‚Äôs a black box and unclear when you should and should not use it.
Be safe and stay away.
If you use a SQL Warehouse
, pay attention to what your Scaling looks like.
Do you really need three nodes running at all times?
Start as low as possible and work your way up.
Did you know you
can automate the starting and stopping of a SQL Warehouse
? (
what would happen if you shut it off between midnight and 5 am??? ‚Ä¶ you would probably save money, that‚Äôs it
)
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Besides the obvious, what can we do?
Ok, so what if you already know all about the different types of Databricks compute, and you generally use the right tool for the right job? Is there anything else we can do to lower compute usage and cost?
Again, I‚Äôm no Databricks savant, but here are a few ideas for you to think about.
Understand the difference between
SPOT
and
ON-DEMAND
instances, including
WITH FALLBACK
.
Almost 99% of Databricks Jobs should use
SPOT
or
SPOT WITH FALLBACK.
It‚Äôs not uncommon to save 80% when switching to SPOT clusters.
Use
FLEET
Clusters
when you can.
it
auto-selects the best pricing
and availability from a pool for you.
Cluster re-use with Databricks Workflows.
‚Äú
Databricks Workflows is a feature that allows for more efficient use of compute resources when running multiple tasks within a job leading to cost-savings.
‚Äù
Spend 5 minutes tuning your Clusters to the workload.
You should, on a regular schedule, simply analyze all your Jobs that run on a regular basis and see if you can tune them to fit the workload better.
Turn your auto-shut-off settings on All-Purpose clusters
to 5 minutes or less. It defaults to something like 45 minutes or something ridiculous.
Make people share Clusters or SQL Warehouses for compute.
Not everyone needs their own compute. Duh.
I think we can stop there for now. That is a good overview of the very basic and most simple things we can do to control and understand Databricks compute costs.
Probably in real life, the answers are a little more nuanced and depend on what your environment looks like and the use cases that most heavily define your workloads.
Like most things in life, it‚Äôs the 80/20 rule; the easy stuff probably makes up 80% of the cost overruns or wasted compute that is happening today on your Databricks setup. People getting trigger-happy with their All-Purpose clusters and running them all day, with too many nodes to boot.
Make people pretend they are back in kindergarten, and tell them they can all share a Cluster ‚Ä¶ wouldn‚Äôt kill ‚Äòem, and it will save some bucks.
Overuse Job Compute, if that‚Äôs even possible. Turn termination time limits way down. Use
SPOT
everywhere and
FLEET
everywhere. To be honest, it is when we get sloppy and stop paying attention that we get in trouble. That‚Äôs when your bill starts creeping the wrong way.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
159769340.apache-polaris-iceberg-catalog-with,Apache Polaris (Iceberg Catalog) ... with Daft,.. kicking tires ...,2025-03-26 16:09:44.748-05,"Much to my disappointment, that rapscallion upstart Apache Iceberg keeps getting more and more chatter. Whatever. Since when has everyone else been right? Either way, I refuse to give up my first love,
Delta Lake
, but I can be reasonable when I have to.
I mean, I have used
Delta Lake Uniform
to pretend like I‚Äôm reading and writing Iceberg. That counts for something.
It‚Äôs been a hot minute since I wrote something about Iceberg proper, and with all the lemmings talking about Iceberg nonstop, I figured,
why not drag myself begrudgingly onto the hype train
?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What should we do?
We will do something boring (or exciting depending on your view) like ‚Ä¶
Set up the Iceberg catalog on a remote instance.
Create an Iceberg table in s3.
Write some
pyiceberg
/polars/duckdb? that can be written to said Iceberg table in s3.
Wrap it up in an AWS lambda.
Put a file trigger on the s3 bucket for lambda to consume.
I want to do something that at least somewhat might resemble a production workflow. Honestly, when I see people doing something and it starts with ‚Äú
I setup x, y, or z on my machine and started using local host ‚Ä¶
‚Äù ‚Ä¶ I start to tune out. To truly understand a tool and how it behaves you have to put it in a production like environment.
Poking at Apache Polaris (incubating)
I plan to attempt to use the
Iceberg REST catalog that Polaris provides
, installing it on a remote server in the cloud so we can hit it via an AWS Lambda for processing CSV files into a ‚ÄúLake House.‚Äù
To do this
we will use Linode
, creating an Ubuntu server and installing Polaris on it.
As a side note, since we will be using an REST API via HTTP connections over the web, I‚Äôm going to use UFW on Ubuntu to block all incoming connections except IP addresses that I whitelist.
Now that we have the easy part done, let‚Äôs check the Documentation for Polars and see what are options are for install.
It appears the
docs only give us two options, local install and using Docker
. Docker is fine for us, hopefully they made it easy. If you‚Äôre running this code like me on a
fresh Ubuntu server, you can install docker and docker compose like this
.
Below is a screenshot from the docs.
You guessed it, the first error.
root@localhost:~/polaris# docker compose -f docker-compose.yml up --build
open /root/polaris/docker-compose.yml: no such file or directory
Apparently no docker-compose file in the base directory. Doing a `ls` confirms this is the case.
root@localhost:~/polaris# ls
aggregated-license-report  CODE_OF_CONDUCT.md  gradle             kind-registry.sh  regtests             spec
api                        codestyle           gradle.properties  LICENSE           run.sh               tools
bom                        CONTRIBUTING.md     gradlew            NOTICE            SECURITY.md          version.txt
build.gradle.kts           DISCLAIMER          helm               polaris           server-templates
build-logic                docs                ide-name.txt       polaris-core      service
CHAT_BYLAWS.md             extension           integration-tests  quarkus           settings.gradle.kts
client                     getting-started     k8                 README.md         site
Let‚Äôs search the
GitHub repo
for docker-compose.yaml and see if we can find where they stored it, since they lied to use in the docs.
Share
When looking at the Git repo notes I saw a little note about running it in Docker.
docker run -p 8181:8181 -p 8182:8182 apache/polaris:latest
Well, that doesn‚Äôt work, that Docker repository on DockerHub is empty. Lies. Fifthly lies.
Well, if you search the repo it turns out they have a lot of docker compose files, for some reason ‚Ä¶ trying to run a few of them with the following command I get the following error every time.
docker compose -f regtests/docker-compose.yml up --build
 ‚úò polaris Error manifest for apache/polaris:latest not found: manifest unknown: manifest unknown                           0.3s 
Error response from daemon: manifest for apache/polaris:latest not found: manifest unknown: manifest unknown
No surprise there, all the composer files reference the
image apache/polaris
which is non existent. Back to the docs.
apt install openjdk-21-jdk
./gradlew clean :polaris-quarkus-server:assemble -Dquarkus.container-image.build=true --no-build-cache
Important Note!
This command would not work and would die half way through, until I upgraded to a 4GB sized Ubuntu server ‚Ä¶ aka it needs some RAM to run. (I started with 2GB)
Next, they have a getting started docker compose that will start both Polaris and Spark ‚Ä¶ let‚Äôs just use that to make this easy, but we won‚Äôt use the Spark image that runs.
docker-compose -f getting-started/spark/docker-compose.yml up
Well, something worked, spitting stuff to the command line.
Also, since I whitelisted my IP address, I should be able to hit one of the ports on this server and get something back, good or bad,
and indeed, this is the case.
The Polaris quick start we used provided a Notebook that along with Spark can talk to itself.
I would like to NOT use this option, but this is a good sign that we have got Polaris up and running on our Ubuntu Server.
Next, let‚Äôs see if we can hit the Polaris Iceberg Catalog from something like PyIceberg or whatever Polars provides.
Side note, if you search
Polaris on PYPI
it appears there is only a single recent release.
Hurdles.
Well, before we set off fireworks and say we are done ‚Ä¶ we are not. The battle is only half fought. Now that we have Polaris up and running ‚Ä¶ we still need to create a Catalog.
At this point you might be thinking, like me, hey ‚Ä¶ we got Polaris up and running let‚Äôs connect to it with PyIceberg and do some things. Well, as you can see below from PyIceberg docs,
it assumes you already have a Catalog created ‚Ä¶ gives no options to create on from scratch.
So, we are going to have to rely on the Polaris Python package to create our first catalog. At this point, I just want the thing running, so instead of backing our first catalog up on s3,
let‚Äôs just use the local file system of the Ubuntu instance that is running Polaris.
Luckily,
the GitHub shows an example of this here
, we can modify it to fit our needs ‚Ä¶ and pray to the Tech Bro gods that it will work.
NOTE!
hitting this Polaris default endpoint appears to require a token, took me a minute to realize it, but I could see the token printed to the console logs ... see below. (have fun trying to hack my Polaris server ... it will be long gone by the time you read it)
What‚Äôs even better is that we can use the default Notebook I showed a screenshot of above,
as it already has the code in it to create a default Catalog in Polaris
.
If you are curious what the code looked like to do that, here it is in all it‚Äôs glory.
In theory now, we have a Iceberg Polaris Catalog called
‚Äúpolaris-demo.‚Äù
Now we should be able to switch to
pyiceberg
.
First ‚Ä¶ the Python/Iceberg Option.
I‚Äôm tired of hearing that ole‚Äô Matt Martin talking about
pyiceberg
non-stop. Since it‚Äôs all I ever hear about, we will be giving that Python package a try inside our AWS Lambda.
But, first let‚Äôs just see if we can get pyiceberg to talk to our Polaris Iceberg catalog from my local machine ‚Ä¶ this is where things get a little weird.
I kept getting 401 Not Authorized if I tried to hit the endpoint WITH or WITHOUT the token passed. I know that token is good, so is it HOW I am trying to pass it??
After piddling around I got a response that Polaris/Iceberg failed to verify the token ‚Ä¶ although I don‚Äôt know what that means.
According to the pyiceberg docs, I should be able to use '
token
‚Äô. The one thing I noticed below that I have not tried added is the ‚Äò
credential
‚Äô, and I believe we can see the ‚Äò
credential
‚Äô being used in the sample Notebook that we used to create the default catalog.
After many tears and much anger on my part, reading the pyiceberg docs and example Notebook, I finally got it to work. That is get pyiceberg to remotely connect to this Polaris Iceberg Catalog.
uri
credential
token type
scope
warehouse
access token
The above list is what you need. (
you have no idea how many combinations of guessing I tried to make this happen
).
Don‚Äôt worry, I‚Äôm sure I will recovery my sanity at some point. The things I put myself through for you people is unbelievable.
I could be sitting outside soaking up rays and touching grass.
Honestly, I‚Äôm ready to get down to it. Can I actually now write an AWS Lambda that will trigger on a CSV file loading into S3 and read it into an Iceberg Table?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
pyiceberg +
polars
+ Daft.
Now we can get to what we‚Äôve all been waiting for, actually writing some Python code to interact with Polaris Iceberg Rest catalog. First things first, let‚Äôs use
pyiceberg to create a namespace.
Using the code above to connect to our catalog, it‚Äôs as easy as ‚Ä¶
catalog.create_namespace('pickles')
Now we need an Iceberg table.
We will use pyarrow for this
. Also, we can use the
Divvy Bike Trip open source dataset for our data. Here is a sample file
.
Here‚Äôs our Python code to load a file with pyarrow and use it to create an Iceberg table.
You thought this was going to work??!!
Guess again you hobbit.
I told you this would be the hard part.
....
packages/pyiceberg/catalog/rest.py"", line 418, in _handle_non_200_response
    raise exception(response) from exc
pyiceberg.exceptions.ForbiddenError: ForbiddenException: Principal 'root' with activated PrincipalRoles '[]' and activated grants via '[service_admin, catalog_admin]' is not authorized for op CREATE_TABLE_DIRECT_WITH_WRITE_DELEGATION
Apparently my Polaris setup was not complete, I have no access to write tables. Think you want to cry yet?
Go read the flipping Access Control docs.
Reminds me of AWS IAM crap.
Good luck
.
The worst part is it doesn‚Äôt appear on the surface they have a UI on Polaris you can do this stuff through. Below is a screen shot of the Roles, Principals, Privilege concepts that Polaris uses.
Truth be told we shouldn‚Äôt be mad about this. This sort of fine grained control is what you would actually want in a Production environment,
access control is a critical part of any Catalog system.
That being said,
you can find an example of top-to-bottom walk through from Principal Creation down to Privileges in this Quickstart from Polaris
.
I‚Äôm going to try walking though it and creating one from scratch myself so I can actually write an Iceberg table.
We are so close.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I hate to do this to you, but this is what I had to run to get everything setup for myself. It‚Äôs important to print the results and save the output from those commands as it has the
client_id
and
client_secret
used for ‚Äòauthing via Iceberg Rest.
Once that was done I could update and re-run my command to create the Iceberg Table.
This command appeared to work ‚Ä¶ I didn‚Äôt get any errors out of the command line, just some text.
Iceberg does not yet support 'ns' timestamp precision. Downcasting to 'us'.
trip_data(
  1: ride_id: optional string,
  2: rideable_type: optional string,
  3: started_at: optional timestamp,
  4: ended_at: optional timestamp,
  5: start_station_name: optional string,
  6: start_station_id: optional string,
  7: end_station_name: optional string,
  8: end_station_id: optional string,
  9: start_lat: optional double,
  10: start_lng: optional double,
  11: end_lat: optional double,
  12: end_lng: optional double,
  13: member_casual: optional string
),
partition by: [],
sort order: [],
snapshot: null
I‚Äôm going to assume this means the table was created.
Let‚Äôs see if we can read it back with Polars.
Indeed it works, we have an empty Iceberg table that can be read by Polars. Fascinating. I feel as if I‚Äôve aged 50 years getting here.
I suppose we are this close, let‚Äôs
try to make an AWS Lambda
just write some Python that can actually write some data into this table.
I mean if I can just write the Python locally to write to this remote Polaris managed Iceberg catalog, there is no reason it would not work inside an AWS Lambda. I‚Äôve already spent too much time working on this, and I just want to be done.
Let‚Äôs switch it up and use
Daft to write a CSV file into an Iceberg Table.
(
Polars refused to work WRITING to an Iceberg table.
)
Easy enough. Let‚Äôs also use Daft to read it back and see if we have records now.
df = d.read_iceberg(table)
df.show()
Finally, an Iceberg Table we can managed through Polaris for read and write ‚Ä¶
use OSS all the way to the very bitter end.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The End.
Normally I would stick around and give you some grand overview of my thoughts about Apache Polaris and Iceberg REST catalogs in general. But since I‚Äôm now half dead after doing this all for you, (
I hid much of the pain and suffering from you
), I‚Äôm too tired to do so.
I will just bullet point my thoughts and then sign off.
Open Source Iceberg Catalogs suck.
Polaris is full featured, with things like access control which is impressive.
Polaris was one of the few Iceberg Catalogs I tried to setup (like LakeKeeper) and was the
ONLY
one I could actually get running.
Documentation and instructions suck (nothing new there).
Java based tools tend to be a pain.
My head hurts.
In all seriousness, Catalogs are the future and the KEY to managing Lake Houses now and in the future, at scale, in production. Also, Catalogs are NOT plug and play, they are too complicated for that.
They require setup and a good amount of time to understand and implement fully.
Once done, and the bugs and kinks worked out, the rest (the read/write) is easy with pretty much whatever you desire ‚Ä¶ plain Python, Spark, whatever.
Bye."
159549950.partitions-in-distributed-compute,Partitions in Distributed Compute.,finding out the unknown,2025-03-21 11:13:38.86-05,"The topic of partitions, both in memory and disk, in our distributed computing world seems to get little attention these days. With the rise of Vibe coding and t
he never-ending quest to abstract things away
, it will only get worse.
Yet, no matter what ‚Äúthey‚Äù do, understanding the fundamentals of distributed computing and how it relates to our datasets will separate the good, bad, and ugly Data Engineers.
Let me ask you a question: Staff Engineer, Senior Engineer, Junior Engineer, whoever you may be. Let me ask you a few questions.
What size of parquet files will your Spark jobs write into your Lake House?
What is the optimal parquet file size in your Lake House?
How do raw file types and sizes read by distributed compute look as partitions in memory?
How does the layout in memory of your distributed datasets affect the number and size of files at the end of the data when written?
The simple truth.
We should be honest with each other, dear reader. I have no desire to be a distributed compute or Spark master who commits code to the Spark codebase and spends my days reading execution plans ‚Ä¶ I decided a long time ago I don‚Äôt want to be a one-trick pony.
Most of you are of the same mind.
There is a limit to the minute details we can keep in your brains of every new AND changing tool in the Data Engineering landscape. You and I simply can‚Äôt always have an answer for everything, including those questions above.
At the same time, we learn and grow by poking at things and turning over rocks, and we indeed should think about obscure things like those questions mentioned above.
While we could spend time reading the Spark codebase and
Spark: The Definitive Guide
(
which I have done
), I prefer a more practical approach.
Finding out about partitions, files, and sizes ourselves.
I don‚Äôt mean this article to be an introduction to partitions. If you want to know more about partitions and their importance,
read this article
.
But, as builders and maintainers of Lake Houses, we must know that partitions matter. Fundamentally, the size and number of files stored in our data affect
cost and performance
.
But most Delta Lake and Iceberg users simply run
OPTIMIZE
every night on the Lake House and could care less about the details.
I can respect that, but let‚Äôs turn over a few rocks just for fun and see what happens.
Share
Turning over the Spark rock of CSV files ‚Üí Delta Lake files.
We are going to try some stuff today, and I am going to let you come to whatever conclusions you want. I hope to Spark (
pun-intended
) some interest in your mind about what is happening behind the scenes when ‚Ä¶
you have CSVs (or whatever) being ingested into a Lake House
aka Spark (or whatever) reads them
partitions show in memory
you write to a Lake House table
data shows up on disk in some size and number
Instead of talking about theory, let‚Äôs SEE what happens.
The Data
danielbeach@Daniels-MacBook-Pro data_Q3_2024 % ls
2024-07-01.csv	2024-07-09.csv	2024-07-17.csv	2024-07-25.csv	2024-08-02.csv	2024-08-10.csv	2024-08-18.csv	2024-08-26.csv	2024-09-03.csv	2024-09-11.csv	2024-09-19.csv	2024-09-27.csv
2024-07-02.csv	2024-07-10.csv	2024-07-18.csv	2024-07-26.csv	2024-08-03.csv	2024-08-11.csv	2024-08-19.csv	2024-08-27.csv	2024-09-04.csv	2024-09-12.csv	2024-09-20.csv	2024-09-28.csv
2024-07-03.csv	2024-07-11.csv	2024-07-19.csv	2024-07-27.csv	2024-08-04.csv	2024-08-12.csv	2024-08-20.csv	2024-08-28.csv	2024-09-05.csv	2024-09-13.csv	2024-09-21.csv	2024-09-29.csv
2024-07-04.csv	2024-07-12.csv	2024-07-20.csv	2024-07-28.csv	2024-08-05.csv	2024-08-13.csv	2024-08-21.csv	2024-08-29.csv	2024-09-06.csv	2024-09-14.csv	2024-09-22.csv	2024-09-30.csv
2024-07-05.csv	2024-07-13.csv	2024-07-21.csv	2024-07-29.csv	2024-08-06.csv	2024-08-14.csv	2024-08-22.csv	2024-08-30.csv	2024-09-07.csv	2024-09-15.csv	2024-09-23.csv
2024-07-06.csv	2024-07-14.csv	2024-07-22.csv	2024-07-30.csv	2024-08-07.csv	2024-08-15.csv	2024-08-23.csv	2024-08-31.csv	2024-09-08.csv	2024-09-16.csv	2024-09-24.csv
2024-07-07.csv	2024-07-15.csv	2024-07-23.csv	2024-07-31.csv	2024-08-08.csv	2024-08-16.csv	2024-08-24.csv	2024-09-01.csv	2024-09-09.csv	2024-09-17.csv	2024-09-25.csv
2024-07-08.csv	2024-07-16.csv	2024-07-24.csv	2024-08-01.csv	2024-08-09.csv	2024-08-17.csv	2024-08-25.csv	2024-09-02.csv	2024-09-10.csv	2024-09-18.csv	2024-09-26.csv
danielbeach@Daniels-MacBook-Pro data_Q3_2024 % aws s3 --profile confessions cp . s3://confessions-of-a-data-guy/partitions_test
upload failed: ./ to s3://confessions-of-a-data-guy/partitions_test [Errno 21] Is a directory: '/Users/danielbeach/Downloads/data_Q3_2024/'
danielbeach@Daniels-MacBook-Pro data_Q3_2024 % aws s3 --profile confessions cp . s3://confessions-of-a-data-guy/partitions_test --recursive
upload: ./.DS_Store to s3://confessions-of-a-data-guy/partitions_test/.DS_Store    
Completed 66.5 MiB/~10 GiB (8.2 MiB/s) with ~65 file(s) remaining (calculating...)
We have about 10 GB of data that we will put in an S3 bucket. It is the
Backblaze hard drive failure dataset data
.
Most of the CSV files are roughly
107MB
in size.
Let‚Äôs ask ChatGPT a question.
Let‚Äôs do something straightforward. Let‚Äôs use (Databricks) Spark to read these CSV files in s3, write them to a Unity Catalog Delta Lake table, and look at the ‚Ä¶
number of partitions in memory
the number of files and sizes created in our Lake House table
(
there were 63 input files and 92 partitions
)
Here, you can see the simple code. We are not partitioning or messing with the data at all; we are simply reading and writing about ~10GB of CSV data directly into a Delta Table.
Now let‚Äôs see what we ended up with on the backend (in s3) for the parquet files that make up that Delta Lake table.
aws s3 --profile confessions ls s3://confessions-of-a-data-guy/picklebob/__unitystorage/catalogs/4c1eb96a-264f-4a0d-bc1a-c80d9fbcdefa/tables/9fa942da-90e7-48b3-9833-a256faaaf18b/ --recursive | grep "".*.parquet.*"" | wc -l
      92
Here, we can see that the number of files created in the Delta Lake table is exactly the same as the number of partitions in memory that we had on our Dataframe.
How about file sizes?
aws s3 --profile confessions ls s3://confessions-of-a-data-guy/picklebob/__unitystorage/catalogs/4c1eb96a-264f-4a0d-bc1a-c80d9fbcdefa/tables/9fa942da-90e7-48b3-9833-a256faaaf18b/ --recursive --human-readable --summarize | grep '\.parquet'
‚Ä¶
...
2025-03-21 09:47:25   11.5 MiB picklebob/__unitystorage/catalogs/4c1eb96a-264f-4a0d-bc1a-c80d9fbcdefa/tables/9fa942da-90e7-48b3-9833-a256faaaf18b/part-00080-cbd125ea-7984-47ef-8ad1-083ef826d6a9.c000.snappy.parquet
2025-03-21 09:47:33   11.4 MiB picklebob/__unitystorage/catalogs/4c1eb96a-264f-4a0d-bc1a-c80d9fbcdefa/tables/9fa942da-90e7-48b3-9833-a256faaaf18b/part-00081-b01efa20-a9af-45dd-9417-c26c0343bfec.c000.snappy.parquet
2025-03-21 09:47:36   11.4 MiB ...
ALL the parquet files are
11.4 MiB
. Of course, you are brilliant and realize this is much smaller than the recommended file size we saw earlier.
This is why everyone runs OPTIMIZE as often as they can.
You, along with every other Lake House user, probably have periodic compaction jobs that run and, hopefully, rightsize the files behind your Lake House. Otherwise, we would have performance problems with the classic ‚Äúsmall file size‚Äù issues.
Also, remember, doing this OPTIMIZE (with Spark) costs you money in compute.
Should we, as astute data engineers, do more to reduce the overhead of having to OPTIMIZE so much? Are we leaving performance and money on the table by simply reading and writing data in our pipelines without a second thought?
I think food for thought is good.
Let‚Äôs
OPTIMIZE
this table in Databricks and see what happens.
object
numFilesAdded: 5
numFilesRemoved: 92
filesAdded: {""min"": 44872091, ""max"": 246150539, ""avg"": 205208873.2, ""totalFiles"": 5, ""totalSize"": 1026044366}
filesRemoved: {""min"": 11905740, ""max"": 12082116, ""avg"": 11992884.217391305, ""totalFiles"": 92, ""totalSize"": 1103345348}
partitionsOptimized: 0 
.....
And indeed, you can see the results of the OPTIMIZE command; it removed 92 files!
The average size of those five files now is around 205 MB
.
Again, remember that it costs you money to have this process happen. Nothing in this life is free.
What happens if we mess with the partitions in memory?
Let‚Äôs say we want to put a little more effort into our data transformations, so we add a
repartition()
, do our Spark code, and write a new table.
Well, of course, we get 10 files.
danielbeach@Daniels-MacBook-Pro data_Q3_2024 % aws s3 --profile confessions ls s3://confessions-of-a-data-guy/picklebob/__unitystorage/catalogs/4c1eb96a-264f-4a0d-bc1a-c80d9fbcdefa/tables/86fe017c-4e26-4919-a94d-aebca8dd5ed8/ --recursive | grep "".*.parquet.*"" | wc -l
      10
And these files, as we would expect ‚Ä¶ are around 100MB.
2025-03-21 10:57:28  110.8 MiB picklebob/__unitystorage/catalogs/4c1eb96a-264f-4a0d-bc1a-c80d9fbcdefa/tables/86fe017c-4e26-4919-a94d-aebca8dd5ed8/part-00006-c8d20255-a2d4-4ddf-a676-23aef5f49261.c000.snappy.parquet
2025-03-21 10:57:29  110.8 MiB
I mean, we either take the time while the data is ALREADY in memory and being transformed to repartition the dataset, or we pay to do it afterward.
Share
Stating the obvious.
Nothing earth-shattering there, uh? But you know the funny part? How many Data Engineers take the time to understand ‚Ä¶
how big the input dataset size is for a pipeline
figure out the correct partitions
make adjustments
My guess is probably about 1%.
But it‚Äôs not that simple. Probably not. When working with complex data pipelines, especially ones holding all the data in memory, we have to be careful with partition sizes; all those things affect downstream processing, shuffle, etc.
But, that doesn‚Äôt mean there isn‚Äôt a lesson in here for us hidden somewhere.
All I‚Äôm saying is that we should all work to understand more about our data sizes, both those incoming datasets and how they affect our data processing, including what ends up in our Lake Houses."
159081331.complicated-good,Complicated != Good,simplicity in the age of AI,2025-04-13 15:42:21.658-05,"In the age of obfuscation, when software runs on software that runs on software, and we are told that you need to terraform + SNS + SQS + Lambda + Lake House table to do a thing ‚Ä¶ I wonder if a person can ever return to simplicity without being labeled an
old curmudgeon
or worse.
Seems highly unlikely.
I see complexity raising its ugly and unwieldy head in two major places on data platforms.
Code (duh)
Infrastructure (more insidious)
I can tell you which is worse, but you tell me? Chicken or egg?
Where does the complexity come from?
Why do engineers feel compelled to design complex systems and write complex code? <<
yeah, yeah, I know there is always a subset of engineers who will say complexity is a good thing and can‚Äôt be undone
>>
The stream of new tools and ideas has reached a fever pitch.
It's almost impossible to keep up with. The rate at which SaaS tools release new features that you ‚Äú
simply must use
‚Äù is also never-ending.
I have some thoughts.
You will never reach nirvana (reliable platform(s)) if you're always changing.
There will ALWAYS be something newer and better.
Architectural complexity is worse than code complexity.
Most code complexity is self-inflicted, not inherent.
It is a delicate balance, to be sure. The need to solve tech debt, move a Data Platform and codebase forward while simultaneously providing ‚Ä¶
reliability
stability
ease of use and development
The most beautiful and complex system that seemed like a sunbeam sent down from heaven turns into a burning hell when something breaks in production, and it‚Äôs unclear how to solve or fix the problem.
It‚Äôs a delicate balance.
I want to review a few examples of complexity in the wild, talk about my experiences with complexity, and, most importantly, discuss
how to solve these challenges.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Architectural Complexity
When it comes to the type of complexity we deal with on Data Platforms, either building or maintaining them, architectural complexity is probably one of the hardest.
It is not because it is inherently problematic to solve but because humans are involved.
When engineers are put in charge of solving or working on architecture, i.e., tooling ‚Ä¶ a few things tend to happen.
they want to use the newest, shiniest tool and tool available
they tend to string together multiple tools instead of having a minimalist approach
there is a bias towards ‚Äúpopular‚Äù things
they fail to see the complexity behind choices until it‚Äôs too late
they equate ‚Äúfancy‚Äù and complicated as good
One of the biggest mistakes made in software, especially in architecture, is designing for things and problems that don‚Äôt exist and solving imaginary problems that are dreamed up.
Many Software Engineers, in the name of building something scalable and robust, throw caution to the wind and are ‚Äú
unable to read the room.
‚Äù
What do I mean?
If you work at a small-sized startup and company, and your volumes are X and your use case is Y, ignore this and build a solution to solve for Z.
Example Time.
Let‚Äôs take a second to examine a real-life use case of adding architecture complexity and over-engineering a solution and its downstream impacts.
Have files that hit an s3 bucket
Need to track files in a database
The use case is particular as related to Data Quality
Not very high volume, few hundred files MAX per day
Well, yes, this is one way to solve the problem. Overkill for the use case?
Most assuredly.
Context is key. We aren‚Äôt talking about 10‚Äôs of thousands of files.
I mean, if we are talking from 50-300 files per day, what exactly is wrong with this more simplified workflow?
Yes, we could argue for hours about making the perfect system that never fails and has zero edge cases and 100% uptime, but do the use case support and business needs require us to pretend this system is keeping the nuclear codes alive?
This is where the rubber meets the road. This is where a Senior+ Engineer can insert themselves into a project, raise their hand, and ask the simple question ‚Ä¶ ‚Äú
What about this super simple workflow that solves the same problem?
‚Äù
Less code
Less complexity
Less things to break
Easier to debug
Cheaper
This sort of thing happens every day in many places, and the danger isn‚Äôt in any particular spot. It‚Äôs what happens when 100 decisions like this are made over time,
and they converge together to produce an amorphous mass of complexity
that makes the system unapproachable.
We should strive to reduce architectural complexity in all its forms. It‚Äôs not that we are saying you CANNOT use technology X, Y, or Z. This is the classic take of unseasoned engineers who fail to see the consequences of their decisions. They boil such arguments down to, ‚Äú
So you‚Äôre saying I can‚Äôt use this technology?
‚Äù
What is being said is, ‚Äú
There are better ways to use this technology, or this technology might not fit this use case.
‚Äù
Complexity in Code.
Complexity in code is a ‚Äúless important‚Äù topic than complexity in architecture simply because code is generally easily fixed.
Code complexity often arises from overly intricate class hierarchies, extensive inheritance chains, or an overzealous application of design patterns.
While object-oriented programming (OOP) and robust frameworks can bring benefits, they can also introduce unnecessary layers that make code harder to follow, maintain, and test. When classes become too large or inheritance trees run too deep, tracing logic or isolating bugs can be challenging.
Another beneficial practice is to aim for small, reusable, testable functions. By focusing on writing clearly named functions that do one thing well, you can easily follow the flow of your application and quickly write meaningful tests.
Consistent naming conventions, thoughtful code reviews, and regular refactoring sessions are vital for catching complexity early.
The ultimate goal is to keep your codebase approachable, even as it grows, so you can adapt to evolving requirements without losing clarity or stability.
In the end, write code however you like. Just make it ‚Ä¶
clean
not long methods or functions
OOP only as necessary
small and reusable
testable
easy to debug and follow
In the end, we all know lousy code when we see it. When someone thinks they are clever, it comes through in the code. You write code for someone a year from now, not so the PR looks complex.
Down with complexity in all forms, God Save Simplicity!"
159040305.test-test-and-then-test-again,"Test, test, and then test again.",... or else ...,2025-03-17 08:44:26.633-05,"When it comes to testing, we are all our own worst enemies. It's the human in us coming out. We know we shouldn't eat potato chips on the couch and watch reruns,
but we do it anyway
. You know you shouldn't merge that PR,
but you do it anyway
. What's the worst that could happen?
In moments of weakness, I still fall into that trap, and I've foot-gunned myself doing crap like that for over a decade, but it still happens.
People talk about
TDD
like it will somehow prevent bugs from making it into production.
They are idiots. There is no such thing.
Bugs are to software what SQL is to
r/dataengineering
.
It could be a lost cause, after all. But if you look at why production gets broken, especially in the Data Platform context,
it often comes down to testing.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
We should ask ourselves when, why, and how untested code sneaks
its way into production and blows up our Slack channels at 10 p.m.
Missing unit tests.
It's just a ""small change,"" so I will push it.
In a time crunch, just getting &$##@ done.
No integration tests (end to end).
No development environment.
Poor dev lifecycles (the barrier to testing is too hard).
Poor engineering culture.
I'm sure we could continue forever, but what do you notice about this list of stuff?
It's not only technical; it's
human-centric
as well.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
These two things collide and often explode like fireworks, causing pain, suffering, and general mayhem. The combination of weak testing and unhelpful human systems surrounding a data platform and team is nothing short of apocalyptic.
How to stop breaking production.
It depends on the day as to whether I'm a glass üç∑ half full or half-empty kinda guy. I'm not under any allusions that one can simply stop all bugs from reaching production.
But, I will poke you in the eye with your Grandma's butter knife she's been using for 30 years if you tell me it's impossible to reduce the chance of bugs making it into production significantly.
I will be honest: the number one thing on my list that must change first is that you aren't going to like it. It's a buzzword. No one hates buzzwords more than me. üêù
Culture
.
No amount of pretend testing ideals can overcome an engineering culture that has consistently prioritized getting &@#$ done at any cost over a long period of time.
Be honest, don't lie; you know what kind of culture you are sitting in at this very moment.
Changing a culture takes time and some üèÄ, but it's probably one of the most important steps you and your team can take. No, it doesn't happen overnight. No, you can't tell your boss to go pound sand. ‚åõ
But, it does change a culture can start by simply ‚Ä¶
Asking for time to test
Planning for testing time in projects and tasks
Asking others in PRs if they have tested the change
Using language, both written and verbal, to bring up the idea of testing
Saying ""no"" more (no about XYZ until it's tested)
Culture will take time, months or a year(s), but I can be done, it must be done, you will not stop breaking production with catchable bugs until culture changes, no matter the tools in place.Data Engineering Central is a reader-supported publication.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Unit and Integration Testing
It doesn't need to be said, but in data engineering, there is an adverse effect on testing in the proper sense for some unknown reason.
It's amazing how many Data Teams I've talked to don't take this very simple first step and then complain about how unreliable things are.
It's not rocket science.
Every method should be unit-tested.
End-to-end integration tests must exist for every pipeline.
If you can't unit test your code, it means your code sucks, and it's no surprise things break. Unit testing across the board enforces clean, modular code. Automated unit tests catch lots of ""stupid"" errors and mistakes.
Integration Testing inside a Data Platform is the holy grail of stopping 90%+ of pipelines from ever breaking. Data Systems can differ from classic Software engineering in this way: the number and complexity of modules that ""data flows through"" is often complex and error-prone if not tested at this ""higher level.""
I don't care if you TDD or not;
leave that argument for the engineers who are obsessed with their own genius.
The idea is to create a
great dragnet that drapes over your entire codebase
. It catches unsuspecting junior engineers and occasional overconfident seniors who stray from the path and try to push something funny.
Unit
and
Integration
tests are this
dragnet
, catching all sorts of strange things from the depths of coding hell.
Share
It's just a small change + I‚Äôm in a time crunch.
I have to be honest, this is the one that occasionally gets me to this day. Doesn't matter how many times I swear to myself ‚Äú
This is the last time I will ever push a small change to production because it's so simple‚Ä¶ and promptly break everything.
‚Äù
This is the most classic bug of them all going into production.
What seems minor from your perspective might be deeply interconnected with other parts of the system.
A one-line code change can trigger unintended side effects, introduce subtle regressions, or break functionality in unexpected ways. Some of the most infamous production failures in tech history stem from seemingly insignificant changes that weren‚Äôt properly tested.
Case and point.
1.
The Knight Capital Group Trading Disaster (2012) ‚Äì $440 Million Lost in 45 Minutes
The Small Change:
A minor code deployment mistake in Knight Capital‚Äôs high-frequency trading system introduced an error that caused the system to buy high and sell low at an insane rate.
The Consequence:
Within 45 minutes, Knight Capital lost $440 million, wiping out the company and forcing a desperate sale to a competitor. The issue stemmed from a failure to test new deployment logic properly.
Subscribe now
The Time Crunch Fallacy
Skipping tests to save time is an illusion. A bug that slips through now will likely be far more expensive to fix later, especially if it makes it to production.
Debugging in a live environment is stressful, time-consuming, and can damage user trust. In reality, taking a few extra minutes to write or run tests now can save hours (or even days) of crisis management later.
Practical Approaches to Testing Under Pressure
If you‚Äôre truly in a time crunch, here are some pragmatic ways to balance speed and safety:
Prioritize Critical Tests
‚Äì If you can‚Äôt run everything, focus on the high-risk areas (e.g., core logic, authentication, payment processing).
Automate What You Can
‚Äì Invest in unit and integration tests beforehand so you can rely on quick, automated checks when you're short on time.
Leverage Feature Flags
‚Äì If the change is risky, consider deploying it behind a feature flag, allowing you to enable or disable it without redeploying.
Test in Production (Safely)
‚Äì Use canary deployments, observability tools, and error tracking to detect issues early if full pre-release testing isn‚Äôt possible.
Get a Second Set of Eyes
‚Äì Even a five-minute peer review can catch obvious mistakes before they cause damage.
A Mindset Shift
Instead of viewing testing as a blocker, see it as an insurance policy. The cost of a small delay to test is almost always lower than the cost of a major failure later. The next time you hear yourself thinking, ‚Äú
It‚Äôs just a small change
,‚Äù remember: the riskiest bugs often come from the things we assume are safe.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
No Dev Environment or Poor Dev Lifecycles.
Imagine trying to fix an engine while a car is speeding down the highway. That‚Äôs what developing software without a proper
development environment
feels like. Worse, when testing is so painful that engineers avoid it, mistakes inevitably slip through to production.
These two problems‚Äî
lack of a dev environment and poor testing lifecycles
‚Äî are common causes of broken deployments, costly outages, and endless firefighting.
No Development Environment = Testing in Production (Whether You Want to or Not)
A lack of a dedicated development environment means every change is essentially a gamble. If your only option is to deploy directly to production, you‚Äôre playing with fire. Without a
sandbox
to test changes safely, even a simple code tweak can cause massive failures.
Some consequences of skipping a proper dev environment:
Fear-driven development
‚Äì Engineers hesitate to make changes, slowing down innovation.
Hard-to-reproduce bugs
‚Äì Without an isolated test environment, debugging is chaotic.
Costly production failures
‚Äì Unverified changes lead to real-world customer impact.
When Testing is Too Hard, Engineers Skip It
Even with a dev environment, if your testing lifecycle is painful‚Äîrequiring long setup times, excessive approvals, or complex CI/CD configurations‚Äîdevelopers will naturally
find ways around it
. When the barrier to testing is too high, you get:
""YOLO"" Deployments
‚Äì Engineers bypass tests and push changes directly.
Slow feedback loops
‚Äì If running tests takes hours, no one will run them frequently.
Stale, broken test suites
‚Äì Outdated or flaky tests reduce confidence in automated testing.
I‚Äôve seen this many times before, an incredibly web of complex code and data, requiring hours to bring up, before a change could be properly tested. It wastes time and everyone stops testing eventually.
I get it.
Yeah, I‚Äôm still like you even after all that. Doing the right thing is hard when you want to move fast and build things. The balance and struggle is real. If you work in a fast-paced culture that expects you to get things done, you sometimes do what you must.
No one wants to work in the kaki-wearing corporate culture where every single code change is tag-teamed by two people. Then you have to wait for the Dev testers to do their thing, wade through a two-week PR process ‚Ä¶ etc. That‚Äôs soul-crushing.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Summary: Testing for Software Engineers‚ÄîWhy We Keep Breaking Production
Testing is often treated as an afterthought in software engineering, especially in fast-moving data teams. The reality is that bugs will always exist, but
how often they reach production is entirely in our control
.
At its core, the problem isn‚Äôt just technical‚Äîit‚Äôs human. Engineers cut corners, skip tests in a time crunch, and push ‚Äújust a small change‚Äù without considering the ripple effects. The result? Late-night Slack alerts, broken data pipelines, and production meltdowns.
Why Does Untested Code Reach Production?
Missing Unit & Integration Tests
‚Äì The foundation of reliable software.
""It's Just a Small Change"" Thinking
‚Äì Even one-liners can break everything.
No Development Environment
‚Äì Testing in production is a disaster waiting to happen.
Poor Dev Lifecycles
‚Äì If testing is too hard, engineers will find ways to skip it.
Toxic Engineering Culture
‚Äì Prioritizing speed over quality leads to long-term pain.
How to Stop Breaking Production
Shift Left on Testing
‚Äì Make testing an early and natural part of development.
Automate Where Possible
‚Äì Faster tests mean engineers are more likely to run them.
Make Testing Easy
‚Äì Reduce friction in CI/CD pipelines, and support local testing.
Encourage a Testing Culture
‚Äì Testing isn't just a process; it's a mindset shift.
The Hard Truth: Culture is the First Thing That Needs to Change
No tool, framework, or process can overcome an engineering culture that treats testing as an afterthought. Fixing this takes time, but small steps‚Äîasking for time to test, planning for testing in project roadmaps, and holding each other accountable‚Äîcan shift the balance toward quality.
At the end of the day,
testing is an insurance policy against production disasters
. The choice isn‚Äôt whether to test‚Äîit's whether you'd rather catch problems early or scramble to fix them in the middle of the night"
158299599.smallpond-distributed-duckdb,smallpond ... distributed DuckDB?,... courtesy of deepseek-ai ,2025-03-06 15:25:53.841-06,"When it comes to the AI hype, I pretty much have tried to ignore the constant roll of never ending models and other hoopla that is mostly meaningless. Better to let the dust settle.
Also, one of the mantras I live by is ‚Äú
You‚Äôre not the boss of me.
‚Äù But I did have someone request I review this topic, and for once I will be nice, even though it
pains
me
deeply
.
That being said, I‚Äôve spent plenty of time both professionally and
personally fine-tuning LLMs
, building RAGs, and the like. It‚Äôs lost its luster for me, no longer magic. It‚Äôs just 90% Data Engineering, prepping datasets and the like.
I‚Äôve found that most of the time spent in working with AI is simply building datasets and a lot of DevOps-y stuff, like setting up the workflows and clusters to train etc.
This leads to today‚Äôs topic. Because ‚Äú
building
and
processing
datasets‚Äù is most of the work, and based on request(s) from readers, I thought I should take a look at
deepseek-ai‚Äôs new data processing tool
built on/with DuckDB.
As always, what we will do isn‚Äôt rocket science, we will simply take a stick and try to poke
smallpond
in the eye.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
smallpond - A lightweight data processing framework built on
DuckDB
and
3FS
.
Ok, where do we start, I don‚Äôt know, let‚Äôs just get after it.
So, other than
smallpond
being built on top of
DuckDB
and 3FS, AND that it is a data processing tool, what else can we add that means anything?
‚Äú
High-performance
data processing powered by
DuckDB
‚Äù
‚Äú
Scalable
to handle PB-scale datasets
‚Äù
It is interesting,
but not surprising
, off the starting line, that
deepseek-ai
needed to build yet another ‚Äú
high performance and scalable
‚Äù data processing framework.
Why?
People seem to be tired of Spark and want other options. We do have options now, things like
Polars
,
Daft
, etc that are built with
Rust
‚Ä¶ and even
DuckDB
itself ‚Ä¶ but I think the key need in
deepseek-ai
‚Äôs case, and probably many others, is
scalability
.
And,
scalability
not in just in terms of single node fast processing, but massive datasets that are typically only tackled with Spark.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I‚Äôm going to pop some bubbles here.
Before we get into writing code and testing things, I am going to rain on a few parades here. With all the hype around
deepseek-ai
and now
smallpond
, we should toss a little cold water on ourselves to and make sure we understand what is going on.
There are going to be two users out in the world for smallpond
regular Data Engineering users on small datasets (
90% of the users
)
advanced users trying to use smallpond on TBs+ datasets (
10% of the users
)
The truth is that
smallpond
was made for massive scale, and there is little chance people start using it in place of
duckdb
or Spark for that matter.
As well, it doesn‚Äôt appear, to a layperson like me, that
deepseek-ai
investing some new amazing distributed thingy. They are using
Ray
for distributed processing.
‚ÄúThe High-level API currently uses Ray as the backend, supporting dynamic construction and execution of data flow graphs.‚Äù -
docs
I used
Ray
recently to distributed some
huggingface transformers
LLM fine tuning, it‚Äôs a nice tool and understandable that
smallpond
didn‚Äôt re-invent that wheel.
I would like to start learning about
smallpond
(
I‚Äôm under no allusions that it takes lots of time and usage to learn something well
), so I will try it with two approaches.
locally with some files in s3 on a few small CSVs
just to learn the tool
try to take advantage of a Ray cluster with smallpond
just to see how hard it is and how much it breaks
One of questions I‚Äôm asking myself, and so should you, is why should I use smallpond vs little old DuckDB.
The answer is probably never, but smallpond will lead to further ground breaking tools with DuckDB.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
smallpond on your laptop
As always,
I will throw this stuff up into GitHub
for your copy and pasting pleasure. (
if you haven‚Äôt
started using uv
, what the crap is the matter with you???
)
Look ma, this is all you need to do, even your grandma could probably figure it out.
uv init try-smallpond
cd try-smallpond
uv add duckdb smallpond
smallpond seems to be very ‚Ä¶ hmmm ‚Ä¶ limited on the surface? Not much too it. But that‚Äôs a good thing. First off, the documentation sucks and is non-existent, unless you count the
half-baked github stuff
.
But no worries, I‚Äôm used to writing Rust, aka the code is the documentation.
import smallpond

sp = smallpond.init()
Ok, well, what next?
The two main questions I have, since this is a data transformation tool, are ‚Ä¶
what kind of data can I load, and how do I load it?
how do I apply transformations.
Neither of these questions are clearly answered in the README of the GitHub. So, basically
I did a code search on smallpond for anything with ‚Äú
read_
‚Äù
to so what kind of reads they support. Based on that it appears we have two options.
read_parquet
read_csv
read_json
(
also, reading some of the code, it appears
smallpond uses arrow for reads
? This begs the question is smallpond using DuckDB for only in memory processing and arrow for IO?
)
FYI.
I could only find a single option to WRITE. write_parquet on a DataFrame is all I could see.
Look, take what I say with a grain of salt, I‚Äôm just reading the code while I sit in a school pickup carline trying to decipher what smallpond can do, since they apparently thought it was a good idea not to just tell us.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Ok, so we know how to load data, what about transform? As far as the GitHub goes the only thing it says about transform is ‚Ä¶ ‚Äú
Apply python functions or SQL expressions to transform data.
‚Äù Although it doesn‚Äôt show you how to do said SQL, for example.
After digging though the
DataFrame code some more, it looks like you can do things like filter with sql OR python expression
. Aka write sql or a lambda expression, guess which one everyone will chose?
So we can ‚Ä¶
filter
df = df.filter('a > 1')
map
df = df.map('a + b as c')
flatmap
df = df.flat_map('unnest(array[a, b]) as c')
partial_sql
(Execute a SQL query on each partition of the input DataFrames.)
c = sp.partial_sql(""select * from {0} join {1} on a.id = b.id"", a, b)
Again, who know‚Äôs what I‚Äôm missing, just a dude trying to figure it out. Those are all I could find. Let‚Äôs expand on our code, just try reading some s3 CSV data out of the box. I‚Äôm curious how it deals with s3 URIs.
# hello.py
import smallpond

def main():
    sp = smallpond.init()
    df = sp.read_csv(""s3://confessions-of-a-data-guy/harddrivedata/2024-07-01.csv"")
    print(df)

if __name__ == ""__main__"":
    main()
uv run -- hello.py
Oh lordy.
Traceback (most recent call last):
  File ""/Users/danielbeach/code/try-smallpond/hello.py"", line 12, in <module>
    main()
  File ""/Users/danielbeach/code/try-smallpond/hello.py"", line 7, in main
    df = sp.read_csv(""s3://confessions-of-a-data-guy/harddrivedata/2024-07-01.csv"")
TypeError: read_csv() missing 1 required positional argument: 'schema'
I honestly don‚Äôt have time for that crap, no way I‚Äôm bothering writing up the schema for that CSV file.
Not promising
,
I think I will just convert the dataset to parquet and move on with life.
Ok, we simply updated this line, and re-ran.
df = sp.read_parquet(""s3://confessions-of-a-data-guy/harddrivedata/sample.parquet"")
Ok, that time my command line filled up with crap, but no error. So maybe let‚Äôs try a simple SQL filter and then write the results to a CSV file.
At this point I realized, duh, that smallpond is lazy and
won‚Äôt trigger execution until something obvious happens, like a write
.
import smallpond

def main():
    sp = smallpond.init()

    df = sp.read_parquet(""s3://confessions-of-a-data-guy/harddrivedata/sample.parquet"")
    failures = df.filter(""failure = 1"")
    failures.write_parquet(""hobbits.parquet"")

if __name__ == ""__main__"":
    main()
Opp. That time we got an error.
File ""pyarrow/error.pxi"", line 91, in pyarrow.lib.check_status
    raise convert_status(status)
FileNotFoundError: [Errno 2] Failed to open local file '/Users/danielbeach/code/try-smallpond/s3://confessions-of-a-data-guy/harddrivedata/sample.parquet'. Detail: [errno 2] No such file or directory
Looks like smallpond can‚Äôt interpret raw s3 URIs. I neither have the time or inclination to do any more research to use
3FS
or make S3 work with smallpond somehow. Looks like we will fall back to local files.
This time with the parquet file local, it worked fine. You can see the
hobbits.parquet
created fine below.
Can we read the parquet file?
>>> import pandas as pd
>>> df = pd.read_parquet('hobbits.parquet')
>>> print(df)
         date   serial_number                 model  capacity_bytes  ...  smart_254_normalized smart_254_raw  smart_255_normalized  smart_255_raw
0  2024-07-01  PL1331LAGXN3AH  HGST HDS5C4040ALE630   4000787030016  ...                   NaN           NaN                   NaN            NaN

[1 rows x 197 columns]
That‚Äôs a good sign, we got something to work uh.
FYI, when running smallpond code, a Ray Cluster UI is served up.
I also tried the lambda (
Python
) filter just to see ‚Ä¶
failures = df.filter(lambda r: r['failure'] == 1)
Worked fine.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I‚Äôm not sure I need to see anymore.
As weird as this sounds, I couldn‚Äôt figure out how I could run a simple groupBy with smallpond, strange, as it is based on DuckDB?? Maybe this old dog can‚Äôt learn new tricks, but I read through the code on the DataFrame API and could only find methods that applied to each row.
This sort of makes sense in the AI context this was created, taking in datasets, applying transformations at scale, and writing out. This is NOT an analytics tool from all appearances.
Again, be like the
Bereans
and figure this out for yourself, maybe I‚Äôm totally wrong.
High Level Thoughts
What do I think of smallpond? I think it‚Äôs another great addition to the Data community that continues to push forward the edges of what is possible today.
The fact that it is built on DuckDB and not the JVM is yet another win for Data Teams who are looking to escape Spark as the ONLY real option for Big Data. Those days are numbered smallpond is the proof of that, sooner or later it‚Äôs all going to change.
It seems very early stage to me, r
eminds of a recent incubating Apache Project we looked at
. Tons of work to do in the areas of ‚Ä¶
documentation
expanded functionality (read/write/transform)
increased usability
first class cloud integration (like s3)
If you want an even more in depth analysis of under the hood of
smallpond, read this blog post
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
158077935.dbt-on-databricks,dbt on Databricks,... thoughts and more,2025-03-03 07:08:49.13-06,"I usually like poking at things that are new and strange, and sometimes things that I am not familiar with. Dbt is one of those things for me. Sure, I‚Äôve played around with it over so slighly in the past, but as someone who tries to write as little SQL as possible, I‚Äôve pretty much avoided it.
It‚Äôs not that I have anything against SQL, it‚Äôs just when you do something for well over a decade, it becomes old hat and you just don‚Äôt want to do it anymore.
Either way, dbt is clearly here to stay for the long-haul, and I do have enough SparkSQL rattling around in Databricks Jobs that I‚Äôve been curious about a
dbt
+Databricks combo. You know, what would it look like, is it worth it, etc.
So, not going to lie, today is going to be more for me than you, I‚Äôm curious what my reaction will be to a
dbt
+Databricks setup, seeing how all the Reddit rapscallions seem to worship at it‚Äôs feet (
like they do with all things SQL related
).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
dbt for the uninitiated.
I can‚Äôt imagine there are very many people reading this who are unaware of
dbt
in all its glory, but here is the obligatory overview.
It‚Äôs simply impossible to escape the marketing drivel isn‚Äôt it.
dbt (Data Build Tool)
is an
open-source
analytics engineering tool that helps teams
transform, model, and manage
data within their data warehouse using
SQL-based workflows
. It is widely used in modern data stacks to ensure
reproducible, version-controlled, and testable
data transformations.
Dbt saves us from unruly SQL, that‚Äôs what it comes down to. It also gives us testable and reusable SQL, brings some order to the chaos so to speak.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
dbt + Databricks
I‚Äôm honestly just interested about this dbt+Databricks+SparkSQL thing and I don‚Äôt want to wait around anymore, prefer to just jump in with both feet. My mother still says I‚Äôm too much of a hot-head.
When it comes to doing local development with dbt + Databricks, we apparently needs the following things ‚Ä¶
‚Äúdbt Core enabled as an OAuth application in your account. This is enabled by default.‚Äù - databricks
A personal access token - databricks
Ok, that‚Äôs a good sign, I don‚Äôt want to be throwing salt over my left shoulder and singing
Stairway to Heaven
backwards, like most other tools. Straight and to the point is how we like it.
If you are following along in your hymnal like your Grandma wants, it would be something like this ‚Ä¶
uv init dbtbricks
cd dbtbricks
uv venv dbtbricks
source dbtbricks/bin/activate
uv pip install dbt-core dbt-databricks
After twiddling your thumbs for a minute, that will finish up. Next we can setup a new dbt project.
(dbtbricks) danielbeach@Daniels-MacBook-Pro dbtbricks % dbt init --profiles-dir . bricks
05:54:37  Running with dbt=1.9.2
05:54:37  
Your new dbt project ""bricks"" was created!

05:54:37  Setting up your profile.
Which database would you like to use?
[1] databricks
[2] spark

(Don't see the one you want? https://docs.getdbt.com/docs/available-adapters)

Enter a number: 1

host (yourorg.databricks.com): https://dbc-9a64f31c-25b9.cloud.databricks.com/
http_path (HTTP Path): sql/protocolv1/o/319592733000122/0125-214456-xrlnwni0
[1] use access token
Desired access token option (enter a number): 1
token (dapiXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX): 
[1] use Unity Catalog
[2] not use Unity Catalog
Desired unity catalog option (enter a number): 1
catalog (initial catalog): confessions
schema (default schema that dbt will build objects in): default
threads (1 or more) [1]: 1
Note what I needed to be able to complete this.
Host uri of Databricks account
Personal Access Token
HTTP URI of cluster via Advanced Cluster settings under ODBC/JDBC
Catalog name we are using for Unity Catalog
You can
read more about the dbt-databricks adaptor and connection options here
,(
In Production you would most likely use an SQL Warehouse for the endpoint at which to point dbt, I‚Äôm using a cluster I created for simplicity of testing
).
If you want to test your new dbt setup is good-to-go, including connecting to Databricks, run this ‚Ä¶
>>dbt debug
19:28:52  Running with dbt=1.9.2
19:28:52  dbt version: 1.9.2
19:28:52  python version: 3.9.21
19:28:52  python path: /Users/danielbeach/code/dbtbricks/dbtbricks/bin/python3
19:28:52  os info: macOS-15.3.1-x86_64-i386-64bit
/Users/danielbeach/code/dbtbricks/dbtbricks/lib/python3.9/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:
* 'allow_population_by_field_name' has been renamed to 'populate_by_name'
  warnings.warn(message, UserWarning)
19:28:54  Using profiles dir at /Users/danielbeach/code/dbtbricks/bricks
19:28:54  Using profiles.yml file at /Users/danielbeach/code/dbtbricks/bricks/profiles.yml
19:28:54  Using dbt_project.yml file at /Users/danielbeach/code/dbtbricks/bricks/dbt_project.yml
19:28:54  adapter type: databricks
19:28:54  adapter version: 1.9.7
19:28:54  Configuration:
19:28:54    profiles.yml file [OK found and valid]
19:28:54    dbt_project.yml file [OK found and valid]
19:28:54  Required dependencies:
19:28:55   - git [OK found]
19:28:55  Connection:
19:28:55    host: https://dbc-9a64f31c-25b9.cloud.databricks.com/
19:28:55    http_path: sql/protocolv1/o/319592733000122/0125-214456-xrlnwni0
19:28:55    catalog: confessions
19:28:55    schema: default
19:28:55  Registered adapter: databricks=1.9.7
19:29:08    Connection test: [OK connection ok]
19:29:08  All checks passed!
Running that will command will tell you if you have any problems with your new setup.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Writing dbt models on Databricks.
This isn‚Äôt supposed to be a tutorial about dbt itself, although if you are new to it, it may serve the purpose of giving you a high-level overview. I did want to mention some ‚Äúthings‚Äù about how dbt works, in the context of Databricks, because I think there are some topics worth talking about.
A word on how your models are materialized ‚Ä¶ or not.
Materializations are how one would
persist
dbt models in a warehouse. There are five types of materializations built into dbt. By default, dbt models are materialized as ""views"".
They are ‚Ä¶
table
your model is rebuilt as a table on each run.
view
your model is rebuilt as a view on each run.
incremental
insert or update records into a table since the last time that model was run.
ephemeral
not directly built into the database, aka CTE.
materialized view
materialized views are a combination of a view and a table.
In my humble opinion this is a very important distinction when running dbt, especially in a Databricks context where you may be working within a Lake House that has many 100s+TBs of data.
Do you really want every single query written as a ‚Äútable‚Äù and persisted between models simply because someone wasn‚Äôt paying attention? Your storage costs would explode.
We should move on to actually writing a model on Databricks.
Note
:
there are many ways to run dbt jobs on Databricks, we can talk about them later.
Let‚Äôs get inside our models directory (for dbt) and make something.
(dbtbricks) danielbeach@Daniels-MacBook-Pro bricks % ls
README.md	dbt_project.yml	macros		profiles.yml	snapshots
analyses	logs		models		seeds		tests
I‚Äôm going to skip a lot of best practices here, we can talk about them later, but I just want to see generally what a dbt model looks like for Databricks. I have a pre-existing Delta Lake Unity Catalog table, with Backblaze failure information about harddrives in a data center.
Let‚Äôs just write a simple model that will rollup failures by date and model.
{{
 config(
 materialized = ""table"",
 file_format = ""delta""
 )
}}

with source_data as (

    select date, model, count(failure) as failure_count
    from confessions.default.hard_drive_failures
    group by date, model

)

select *
from source_data
Doing a simple run ‚Ä¶
(dbtbricks) danielbeach@Daniels-MacBook-Pro bricks % dbt run --model models/hardware.sql
22:14:32  Running with dbt=1.9.2
22:14:34  Registered adapter: databricks=1.9.7
22:14:35  Found 3 models, 4 data tests, 607 macros
22:14:35  
22:14:35  Concurrency: 1 threads (target='dev')
22:14:35  
22:14:37  1 of 1 START sql table model default.hardware .................................. [RUN]
22:14:44  1 of 1 OK created sql table model default.hardware ............................. [OK in 7.63s]
22:14:45  
22:14:45  Finished running 1 table model in 0 hours 0 minutes and 9.57 seconds (9.57s).
22:14:45  
22:14:45  Completed successfully
22:14:45  
22:14:45  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
We can see below that a new Unity Catalog Delta Table has been created below that matches our model. Pretty much works as expected.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Yeah, but what about production workloads?
Like most popular tools with plenty of usage, actually putting dbt into production in Databricks would would look a little different, but you have enough options for whatever suits you.
You can easily ‚Ä¶
put your dbt models in a git repo, which is cloned and sync‚Äôd to Databricks
run Databricks Jobs (Notebooks, or other code)
There is nothing special about this, most people who are using Databricks know how, and already have systems in place, to run Databricks jobs. Integrating dbt models into this workflow would be very straight forward.
If you want to see examples of this,
Databricks has a few you can see here
, again, nothing special if you‚Äôre used to being in the Databricks environment.
Thinking about dbt on Databricks at a high level.
While dbt itself is popular, I want to spend some time thinking about the combination of
dbt
with Databricks, and why a Data Team might choose to go down this path, because personally, I find this the more interesting question.
Firstly, from everything I‚Äôve seen,
dbt
and Databricks
have first class support for each other
. You will have zero problems, on the surface it seems, integrating the two.
But again, why would you use it (
dbt
) that is.
you get modular SQL
you get easy testing
???
What I‚Äôm trying to say is that you can achieve these things without dbt easily enough, in fact, I would say the majority of serious Databricks users DO indeed get any number of ‚Äúadvantages‚Äù you might list for dbt, while NOT using dbt.
I‚Äôm not bashing dbt, I‚Äôm just making a point. Why would a Data Team choose to use dbt as their primary data transformation tool on Databricks?
Because they are a SQL-only based team.
I‚Äôve mostly seen two types of Databricks users, those that do more advanced things with Spark like Machine Learning etc, and those who use it as a simple and basic Lake House tooling, to do basic transformations.
advanced users, like Machine Learning pipelines etc.
basic data transformation (
Lake House, Data Warehouse type models
)
This first group probably uses mostly PySpark or Scala DataFrame based API code. Write modular, reusable, and unit-tested methods and libraries. This is very common, in this is actually the type of work I do on a daily basis.
The second group are teams that do most tabular type data transformations and are simply more comfortable writing SQL rather than being more ‚Äúprogramming oriented.‚Äù Also, as far as SQL goes, the bar is lower and it‚Äôs easier to build, recruit, and maintain teams that primarily use SQL.
Dbt has an almost irresistable appeal for SQL based teams. It provides what SQL hasn‚Äôt given for decades and what teams of struggled with forever.
A modular system, including testing, that brings Software Engineering level best practices to Data Teams that have been void of them.
Should you use dbt on Databricks? If you are a SQL based team and 50%+ of your pipelines are written in SQL, than you are doing yourself a disservice by NOT using SQL.
In my case I already have 90%+ DataFrame code that is modular and unit tested etc, I would gain no benefit from adopting dbt and re-writing everything in SQL. Much of the Machine Learning pipelines I work on simply don‚Äôt fit the SQL paradim well.
I can say I am pleased with the simplicity of using dbt on Databricks, it honestly couldn‚Äôt be any easier, like at all.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
157927136.data-analysis-is-hell,Data Analysis is Hell,... nothing you can do about it.,2025-02-27 14:23:11.273-06,"I‚Äôve been in and around data analysis of one kind of another, in a professional capacity since about 2014. This one thing as stayed true and steady. From giant corporations to tiny startups, and everything in between.
Data Analysis is Hell.
Let me explain.
Why Data Analysis will eat your soul.
Working in and around tech, starting in the Data Warehouse space and what we used to call ‚ÄúBusiness Intelligence‚Äù before Data Engineering was thing, I accumulated my fair share of battle scars. Most all those scars could be attributed to Data Analysis in some form or other.
Sure, it can be hard to see the dividing line between Analytics and Data Engineering these days, but there is a difference. To this day it appears to me that Analytics eats up and spits people out on a regular basis, like clock work.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I have my theories as to why. They are numerous and varied, but I think we can find veins of truth, like gold, buried in there, and it might be possible to mine out some precious truths and solutions to whey
Data Analysis is Hell
.
I‚Äôm going to just start listing things, because who doesn‚Äôt love lists. Then at the end, if I‚Äôm feeling spicy, I will give you my two cents on how to address said problems.
I should also state an assumption before I start, I see ‚ÄúAnalytics‚Äù as a broad stroke, but generally assume the following tasks and people usually fall into this bucket.
Data Analysts, Data Scientist, and any sort of Report or Dashboard development.
Target audience is usually end users or the business.
End result of make Data Warehouse or Lake House platforms.
Anyways, onto the main topic for today.
Why Data Analysis is Hell.
Most* data quality is simply poor
Poor data modeling
‚ÄúThe Business‚Äù doesn‚Äôt communicate needs or requirements well
Every business has ‚Äúspecial needs‚Äù that increase complexity
Analytics tasks are ALWAYS under scoped in terms of time and complexity
Analytics systems are typically less best (
developer
) practice driven
Operating close the business comes with a certain amount of chaos
One-off and priority requests out of nowhere are normal operating procedure.
It‚Äôs hard to get bunches of people to agree on *anything
I could go on but I should probably stop here. If you‚Äôre an astute reader I‚Äôm assuming you can find some veins of continuity buried in there.
Nothing earth shattering just real life down in the data trenches. I count myself blessed that I managed to un-entangle myself from *
most
analytics, besides for the odd task that comes up in which I pat myself on the back and say ‚Ä¶ ‚Äú
There, there, it‚Äôs ok. It‚Äôs just an hour two, you can do it.
‚Äù
From my perspective some of these issues can be lessened, some alleviated, but most of them are simply par for the course. Sometimes things are what they are and you must accept a certain level of chaos.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Attempting to escape the inner rings of analytics hell.
Let‚Äôs see if we can come up with a few approaches that might be able to make the life of those moving in the Analytics world a little less burdensome and less chaotic.
I think this is the key to most things in life, including Analytics, we should try to reduce any sort of chaos that is inserting itself in the process. Reduce chaos.
Most* data quality is simply poor
This is probably the most easily addressed problem that plagues many Data Platforms and causing nightmares for downstream consumers including Analytics.
Look into tools like Soda, Great Expectations, etc.
Poor data modeling
Lack of forethought and insight into how the data will be consumed downstream. This is very simple to fix in theory. If you are will to simply spend the time doing some redesign and refactor
‚Äú
The Business
‚Äù doesn‚Äôt communicate needs or requirements well
This has and always will be the case. You can learn to do a few things better to help alleviate and deal with this issue, not solve it.
Have more requirements meetings
Have regular progress checkins and reviews
Work on cross team communication
Every business has ‚Äú
special needs
‚Äù that increase complexity
This is part of life and Analytics. The best you can do is simply increase your estimates to account for this complexity.
Don‚Äôt pretend like stuff is easy when it‚Äôs not. Don‚Äôt say it will take a day when it will take three.
Analytics tasks are ALWAYS under scoped in terms of time and complexity
Related closed to above, this can be easy fixed. Stop saying ‚Äú
Oh, no problem, that‚Äôs easy, I know how to do that.
‚Äù You know there will be ‚Äúthings‚Äù that come up.
If the task involves analytics, add a few days to deal with the people and complexity issues.
Analytics systems are typically less best (
developer
) practice driven
This is 90% the case most of the time. Many folk working in and around Analytics simply don‚Äôt come from a classic Software Engineering background.
This leads to not following well-known best practices. Testing, good development life cycles, clean code, etc.
It‚Äôs easy enough to teach these things, simply take the time to do it.
Operating close the business comes with a certain amount of chaos
You can‚Äôt escape this you can only mitigate it by planning ahead and being firm.
Have more meetings, request clarity, request documentation or tickets for work. Learn emotional intelligence, learn to be kind but firm.
One-off and priority requests out of nowhere are normal operating procedure
This will always be the case when interfacing with the business and working with analytics.
Again, be firm by kind, request they follow due process, request tickets, bring up these problems in team meetings.
If you teach the business that you won‚Äôt drop everything at all times without some sort of process, the chaos will never end.
It‚Äôs hard to get bunches of people to agree on *anything
This is really just about learning to bring people together, make hard decisions, make tradeoffs, communicate well, and meeting in the middle.
This are soft skills that can be learned and taught.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
There you have it, the solutions to all your analytics problems ‚Ä¶ well, not really, but maybe a little.
Control the controlables, and there are plenty of them that you can get your hands around that will at least bring a semblance of balance to the constant chaos you might be experiencing.
To boil it down.
Enforce Software Engineering best practices in the Analytics context.
Increasing communication and expectations around processes.
Be realistic when planning and estimating projects.
Have people formally learn to up-skill themselves in the area of ‚Äúsoft skills‚Äù like communication etc.
From the technical side you can reduce a lot of analytics problems by implementing Data Quality tools, bringing DevOps, CI/CD, Testing, and other such things into the Analytics process. This will disappear a whole host of issues.
From the human side you simple need to inject a little rigor into the process, the business will balk, like they always due, but if you get %50 of your wants in place, you will all be the happier.
Be firm but kind about one-off and chaotic requests, increase the amount of inter-team communication (
via Slack, face-to-face meetings, etc
)."
157162490.lord-have-mercy-apache-xtable,Lord have mercy. Apache XTable.,... add another one to the list,2025-02-17 07:07:22.166-06,"I feel like we‚Äôve seen this story before, haven‚Äôt we? I swear we‚Äôve been over this a few times now. Yet here we are again, doomed to live in the eternal cycle of the newest ‚Äú
god table format
‚Äù on the scene.
It‚Äôs like the Data World is on the quest for the One Ring of Power, except we are unsure who Gandalf and Saruman are. So here we are, the Lake House Table Format War rages on in earnest with new soldiers being called on to fight the good fight and take up arms.
Either way, here we are, and I‚Äôve been hearing rumors that
Apache XTable
is the long-awaited one-table to rule them all. We shall see.
I‚Äôm not sure how long or short this will be; I know nothing about
XTable
, so it will be a journey of surprises for us all. Let‚Äôs do what we do and kick the tires.
What is Apache XTable, and why do we need it?
Apache XTable‚Ñ¢ provides cross-table omni-directional interop between lakehouse table formats
Apache XTable‚Ñ¢ is NOT a new or separate format, Apache XTable‚Ñ¢ provides abstractions and tools for the translation of lakehouse table format metadata""
-
XTable
Well, this is interesting. Honestly, knowing nothing about it, I pretty much assumed that Apache XTable was yet another format for our Lake Houses.
However, the documentation/website specifically states that it is NOT another table format but an ‚Äúomni
directional interop
‚Äù between formats.
Well, now I am interested.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
They
XTable websitehttps://xtable.apache.org/
has a nice diagram that give us an conceptual overview of what the idea is.
I personally have not found the need (working on a small team) to have multiple copies/versions/whatever of my Lake House tables in different formats. But, based on a lot of the comments I‚Äôve seen online, this appears to be more and more common.
I can‚Äôt claim to understand all the reasons for this, but I suppose in large organizations of size when many thousands of datasets, this sort of inter‚Äôopting of formats could indeed be handy.
Apache XTable
simplifies this process by
reading your table‚Äôs existing metadata
and generating compatible metadata for other table formats.
Instead of rewriting or duplicating data, XTable‚Ñ¢ leverages each table format‚Äôs
native APIs
to make your existing data
instantly readable
as if it were originally written in Delta, Hudi, or Iceberg. The converted metadata is stored in the respective format‚Äôs default location:
Delta Lake
‚Üí
_delta_log/
Apache Iceberg
‚Üí
metadata/
Apache Hudi
‚Üí
.hoodie/
This means you can query the same data using different engines without complex migrations. For example, in
Apache Spark
, you can seamlessly read your data using:
spark.read.format(""delta | hudi | iceberg"").load(""path/to/data"")
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Backup a second.
Before we get ahead of ourselves, I think what we greatly feared is true, based on the example above of Apache Spark. If you have any stars in your eyes we can quickly expel them by heading over to the
GitHub page for XTable
and reading about ‚Äúinstalling‚Äù and ‚Äúusing‚Äù it.
it requires Java (
hence using Spark in most cases probably
)
the example shows a large YAML file to setup different formats
you have to run a JAR command to do the work.
Here is an example YAML configuration.
sourceFormat: DELTA
targetFormats:
  - HUDI
  - ICEBERG
datasets:
  -
    tableBasePath: s3://confessions-of-a-data-guy/hobbits
    tableDataPath: s3://confessions-of-a-data-guy/hudi/hobbits/
    tableName: hobbits
    namespace: my.db
And the Java command.
java -jar xtable-utilities/target/xtable-utilities_2.12-0.2.0-SNAPSHOT-bundled.jar --datasetConfig my_config.yaml [--hadoopConfig hdfs-site.xml] [--convertersConfig converters.yaml] [--icebergCatalogConfig catalog.yaml]
Well ‚Ä¶ this is an ‚Äúincubating‚Äù Apache Project so we can‚Äôt pull out the pitchforks and torches and go for em‚Äô, even if want to. I‚Äôm trying not to ole‚Äô Gandalf Stormcrow, but most of hate doing Java crud.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Can we make it work on Databricks?
Anywho, I think the best thing to do here is try to see if we can make this work inside a Databricks environment. Sure, we could do the
whole Docker thing
, but honestly, that simply is just playing around a laptop.
I would prefer to see if we can make this work out in the wild, I think it will gives us more of a a sense of at what stage of life we find XTable.
So here is what I propose ‚Ä¶
clone XTable git repo
build the JAR
put JAR into Databricks DBFS
Point JAR / command line at existing Delta Lake (Unity Catalog table in S3) ‚Ä¶ ask for Iceberg and Hudi versions.
Try to read the XTable Iceberg and Hudi tables with Spark.
Decide if this thing is what it says it is. Here goes nothing.
git clone git@github.com:apache/incubator-xtable.git
cd incubator-xtable

## if you're running God's OS run something like this if you need Maven
export HOMEBREW_NO_AUTO_UPDATE=1
brew install maven
##

mvn install -DskipTests
I came upon the fist error, which is no surprise because I didn‚Äôt listen to what they were telling me ‚Ä¶ build on Java 11 or use the Docker image.
Downloaded from central: https://repo.maven.apache.org/maven2/com/squareup/okhttp3/okhttp/4.12.0/okhttp-4.12.0.jar (790 kB at 1.4 MB/s)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for XTable Project Parent POM 0.2.0-SNAPSHOT:
[INFO] 
[INFO] XTable Project Parent POM .......................... FAILURE [ 16.886 s]
[INFO] XTable Project API ................................. SKIPPED
[INFO] XTable Project Hudi Support Parent POM ............. SKIPPED
[INFO] XTable Project Hudi Support Utils .................. SKIPPED
[INFO] XTable Project Core ................................ SKIPPED
[INFO] XTable Project Hudi Support Extensions ............. SKIPPED
[INFO] XTable AWS ......................................... SKIPPED
[INFO] XTable HMS ......................................... SKIPPED
[INFO] XTable Project Utilities ........................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  17.622 s
[INFO] Finished at: 2025-02-16T14:20:29-06:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.diffplug.spotless:spotless-maven-plugin:2.43.0:check (default) on project xtable: Execution default of goal com.diffplug.spotless:spotless-maven-plugin:2.43.0:check failed: You are running Spotless on JVM 23. This requires google-java-format of at least 1.17.0 (you are using 1.8).
Let‚Äôs save time and switch over the building the JAR with the provided Dockerfile.
docker build . -t xtable
Well, that gave us another error.
danielbeach@Daniels-MacBook-Pro incubator-xtable % docker build . -t xtable
[+] Building 0.1s (1/1) FINISHED                                                                                                                docker:desktop-linux
 => [internal] load build definition from Dockerfile                                                                                                            0.0s
 => => transferring dockerfile: 2.37kB                                                                                                                          0.0s
 => WARN: FromAsCasing: 'as' and 'FROM' keywords' casing do not match (line 19)                                                                                 0.0s

 1 warning found (use docker --debug to expand):
 - FromAsCasing: 'as' and 'FROM' keywords' casing do not match (line 19)
Dockerfile:49
--------------------
  48 |     ENTRYPOINT [ 
  49 | >>>             ""java"", \
  50 | >>>             ""--add-opens=java.base/sun.nio.hb=ALL-UNNAMED"", \
  51 | >>>             ""--add-opens=java.base/sun.nio.ch=ALL-UNNAMED"", \      
  52 | >>>             ""--add-opens=java.base/java.nio=ALL-UNNAMED"", \     
  53 | >>>             ""--add-opens=java.base/java.lang.invoke=ALL-UNNAMED"", \
  54 | >>>             ""--add-opens=java.base/java.util=ALL-UNNAMED"", \
  55 | >>>             ""--add-opens=java.base/java.lang=ALL-UNNAMED"", \
  56 | >>>             ""-jar"", \
  57 | >>>             ""./app.jar"" \ 
  58 | >>>         ]
  59 |     
--------------------
ERROR: failed to solve: dockerfile parse error on line 49: unknown instruction: ""java"",

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/g8na5c2io68ocgfikmuw7hr4x
I am getting somewhat annoyed at this point, although not surprising for a Java project. I‚Äôm thinking to try to get myself to the end point (
having a XTable built JAR as quickly as possible
), I will just make my own Dockerfile that actually works.
FROM ubuntu:24.04

ENV TZ=America/Chicago
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

RUN apt-get update && \
    apt-get install -y --no-install-recommends openjdk-11-jre-headless wget vim software-properties-common maven git-all
docker run -it -v ""$(pwd):/app"" xtable /bin/bash
>> git clone https://github.com/apache/incubator-xtable.git
>> cd incubator-xtable
>> mvn clean package -DskipTests
Now why Apache XTable couldn‚Äôt provide us with a working Dockerfile is beyond me. Looks like our little Dockerfile that could worked.
[INFO] Reactor Summary for XTable Project Parent POM 0.2.0-SNAPSHOT:
[INFO] 
[INFO] XTable Project Parent POM .......................... SUCCESS [ 19.439 s]
.....
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  06:48 min
[INFO] Finished at: 2025-02-16T15:35:58-06:00
[INFO] -----------------------------------------------------------------
Now at this point we should have the JAR we need to run commands that will create our ‚Äúinteropt‚Äù tables for Delta, Hudi, and Iceberg.
I was initially thinking we should do this all inside Databricks, but now that I understand more of what is going on here, that probably isn‚Äôt really necessary.
if we really wanted to do this in a Production environment we would probably ‚Ä¶
write a script to automate this task over many tables
as long as we have creds for whatever environment (aws, etc)
I still want to see if this Apache XTable command will work on top of an
EXISTING
Delta Lake (Unity Catalog) table and make us a Hudi and Iceberg version.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Brass Tacks.
First thing to do is go find us a Unity Catalog Delta Table and then make us a YAML config file to match. Luckily I have a few spare tables rattling around my personal Databricks account, this should work fine.
What I need is to grab the
s3
URI of the table and use it for our config file we will run the XTable command with.
sourceFormat: DELTA
targetFormats:
  - HUDI
  - ICEBERG
datasets:
  -
    tableBasePath: s3://confessions-of-a-data-guy/picklebob/__unitystorage/catalogs/4c1eb96a-264f-4a0d-bc1a-c80d9fbcdefa/tables/688f96c3-a44d-409e-96db-2a3d075cf7bc 
    tableName: trip_data
What is also not clear to me at this point is where XTable will store these Hudi and Iceberg ‚Äútables.‚Äù I could not sus out in the documentation or configs of the YAML if you can specify them or not.
Does it just create them in the same location?
I‚Äôm nervous. Saved the above as
config.yaml
. What do you think is going to happen?? (
run this in the same place you built and ran the maven command
)
java -jar xtable-utilities/target/xtable-utilities_2.12-0.2.0-SNAPSHOT-bundled.jar --datasetConfig config.yaml
My command line started to crazy, which is a good thing probably, means it‚Äôs doing something. (
If you look closely you can see it talking about Hudi for example and referencing our s3 Table.
)
All done.
2025-02-16 15:56:33 INFO  org.apache.xtable.conversion.ConversionController:213 - Sync is successful for the following formats HUDI,ICEBERG
I was unable to find where this thing spit out where my new XTable (
Hudi
and
Iceberg
) formats, and it didn't tell me where they could be read from. I guess we should go dig around that Delta Table s3 spot? (
annoying ‚Ä¶
am I supposed to guess??
)
Here is the S3 location on AWS for that Delta Table. I‚Äôm sorta assuming some of the NON _delta_log stuff is what we just created?
I confirmed this by opening the ‚Äú.hoodie/‚Äù location and seeing the created timestamps were current.
So ‚Ä¶ can I go into my Databricks account and read this Delta Lake Unity Catalog table as a Hudi?
it appears the
metadata
folder is the Iceberg table
the
.hoodie
folder is the Hudi stuff
So, let‚Äôs try to display this table ‚Ä¶ with NOT Spark ‚Ä¶ but something else ‚Ä¶ like Daft. This will ensure that XTable is creating legit tables.
Here is our base Delta Lake table.
Can we read it as Hudi now? No?? Just returns an empty Dataframe, not even and error thrown. Strange eh?
How about Spark with Hudi?
No such luck, screaming about paths and overlapping crap. (
I tried with Iceberg got same errors
).
Didn‚Äôt matter if I read the base path (
like the docs show
) or the actual folders it created etc, always the same Spark error.
Try and try again.
Not sure if this has something to do with these Unity Catalog tables messing with things? I want to at least do my best to make this work, so how about let‚Äôs create a new NON-managed and NON-Unity Delta Table and try this all again?
It‚Äôs not like I have anything better to do on a Sunday. Here is our fresh start.
trips = spark.read.csv('s3://confessions-of-a-data-guy/trips/202412-divvy-tripdata.csv', header=True)
trips.write.format('delta').save('s3://confessions-of-a-data-guy/buggerall')
I went ahead and updated that XTable config YAML file and re-ran the command on this new raw and unmanaged Delta Table. It did it‚Äôs thing again writing out the same directories in S3 as before.
Note the two new table locations, metadata and .hoodie (
Iceberg
,
Hudi
)
So, let‚Äôs try it again, read Hudi with Daft. No luck, same error.
Ok, what about using Spark with the Iceberg version? I‚Äôm
desperate
to get some part of
XTable
working here.
This was a bomb as well, it appeared Spark was picking up the Delta log in the same spot and yelling when trying to read it as Iceberg. So much for seamless uh?
At this point I have no idea of this is a Databricks thing or not. (
I‚Äôve used UniForm tables with no problems on Databricks
)
What if we point Spark right at the Iceberg ‚Äúmeta‚Äù folder we know exists in that location?
No juice.
Bloody buggers. Could it be Databricks environment somehow messing with this all?
I decided to switch back to just a local environment and see if locally I could use Polars or something to read the Iceberg table.
import polars as pl

storage_options = {
    ""s3.region"": ""us-east-1"",
    ""s3.access-key-id"": ""XXXXXXXXXXXXXXXX"",
    ""s3.secret-access-key"": ""XXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",
}

df = pl.scan_iceberg('s3://confessions-of-a-data-guy/buggerall/metadata/v2.metadata.json', storage_options=storage_options)
print(df.collect())
Well finally.
I did have to directly give it the location of the Iceberg
meta.json
file, otherwise it was still throwing
errors
.
in the docs they make it appears if you put in the base table location it will work, it does not, from my experience.
Ugg ‚Ä¶ I‚Äôm done for now. That
IS
what you would call incubating.
To be honest I didn‚Äôt share even half the errors and crud I had to go through even to get to this point.
I probably won‚Äôt be returning to
Apache XTable
anytime soon, not until it gets a little more stable and has some more time under it‚Äôs belt and improvements made.
An important question.
When reading the documentation for
XTable
and important question came up. It provides both ‚Äúincremental‚Äù and ‚Äúfull‚Äù sync modes between the table formats.
Does this mean to keep say a Delta Lake table in sync with Hudi and Iceberg we would have to run this on a daily+ basis? This is unclear to me.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Signing Off.
Well, that does it for kicking the tires on
XTable
.
My foot hurts
. I guess I can see the reason why they made this, although
Databricks UniForm
seems light years ahead to me (
worked out the box
).
Trying not to be too hard on an incubating project, probably lots of work to come I would think."
157105917.data-engineering-central-podcast-cd0, Data Engineering Central Podcast - 06,Episode 06,2025-02-13 16:37:19.347-06,"It‚Äôs time for another episode of the Data Engineering Central Podcast. In this episode, we cover ‚Ä¶
AWS Lambda + DuckDB and Delta Lake (Polars, Daft, etc).
IAC - Long Live Terraform.
Databricks Data Quality with DQX.
Unity Catalog releases for DuckDB and Polars
Bespoke vs Managed Data Platforms
Delta Lake vs. Iceberg and UinFORM for a single table.
Thanks for being consumers of Data Engineering Central; your support means a lot; please share this Podcast with your friends; it helps the show a lot!
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
157070999.ai-code-revolution-embrace-or-deny,AI Code Revolution. Embrace or Deny?,for Software Development,2025-02-24 07:01:46.993-06,"I simply can‚Äôt resist the urge to put my ring in the hat on this one. It's mostly because I see the glut of AI nonsense put out by talking heads who‚Äôve never written a single line of usable code in their lives.
But what‚Äôs also intriguing is the takes by well-seasoned Software Engineers who‚Äôve been writing code for decades and who are apparently taking different sides in this battle. What battle, you ask?
The computers vs humans battles.
We‚Äôve finally arrived. After all this wondering when Skynet was going to take over and ruin everything, are we there? What is a Software Engineer to do when the AI they created is able to write their code for them?
This must have been what Dr. Victor Frankenstein felt when he raised that infamous beast to life. I feel most coders are at a great crossroads in their lives and times. What should they do? Embrace the AI or deny it?
I want to figure that out today. I want to find an answer to this question and make your life easier. I want to reach a reasonable conclusion to this important question:
Should you embrace AI as part of Software Development?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The intersection of AI and Code.
What‚Äôs a programmer to do? Run off into the sunset angry and mad at the AI that can write better code than them? Put your head in the sand and just ignore it all.
As a writer of code you have a few choices.
Embrace AI in it‚Äôs totality for everything Software Development related
Let it do everything for you at all times
Totally reject AI and any of it‚Äôs tentacles it‚Äôs trying to stick into your coding experience.
Never touch the stuff
Try to find a happy medium that doesn‚Äôt hurt you but don‚Äôt leave you behind.
Try to walk the line like Johnny Cash
Now, I will be the first to admit there is a lot of grey areas running around between those three points above.
Some of it bad, some of it good.
AI in your life and development lifecycle, the decisions you make to use or avoid it, are mostly likely going to have serious long-term ramifications over the totality of your career.
How could it not?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
It depends on who you are.
I hate to say it depends, but it really does. Should you ignore AI and never use it for coding? It depends. Should you use it everyday (like I do)? It depends.
I don't think the following statements are controversial or rocket science ‚Ä¶ they are fact.
Let me say what I want without reading between the lines, we will get to the grey areas later.
Junior or New Dev's skills will be greatly stunted if using AI to write code.
Mid-level Devs can fall either way and it depends on their personality.
Senior Devs are most likely to benefit from AI in their journey.
Like I said, not rocket science. If you have not been writing code long and lack experience, letting anyone or anything write most of your new code, will make you a shallow Dev who is easily replaceable with the AI you embrace.
One the other end of the spectrum, Senior Engineers are most likely to benefits as they have mastered technical skills and simply use AI to enable them to deliver value at greater speeds.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The grey areas. Daily AI usage in Software.
The talking heads can and will publish all sorts of arguments over the coming years about how AI is affecting the writing of code, jobs, and software in general. But, we should have some introspection after making the above bold claims about how AI could affect you on your day to day job.
If the landscape has fundamentally shifted under our feet (in regards to AI generating code), does it MATTER if your ‚Äúknowledge and growth‚Äù is stunted by using AI too much?
This is a question that needs to be asked.
If AI can write much of your code, good or bad (
because somedays you write bad code too
), and the world collectively decides to walk down that path, why chain yourself to the old ways of learning soon-to-be archaic skills?
Clearly large tech companies (go look at the data) are betting and moving towards a future where AI will in part write some of the code. You can‚Äôt fight against the current of progress ‚Ä¶ you will simply drown.
My answer to that question(s).
In YOUR working lifetime things will shift, but probably take longer than you think.
folk in the middle to end of their careers have less to worry about than those who are new
If your in the middle-end of your career and are considered Senior+ level ‚Ä¶ you have the benefit of either choosing to embrace AI or not.
the DOWNSIDE of making the wrong choice will be less pronounced.
Many current Senior+ devs have the opportunity to use AI for their benefit.
Using AI to increase production and value, while minimizing personal growth and learning downsides.
What do I mean by all this? Think about it. AI is here to stay, the longer it is here the more it will impact the overall and day-to-day of Software Development.
What else has something to do with time? We do. We are mortal humans eeking our existence on this desolate rock by typing on a keyboard.
You want to know if YOU, personally, should embrace AI in your Software creation? Well, it depends on where YOU are at in your journey on this dusty rock, how far along you are.
But, let‚Äôs say you are just a few years into your career as a Software Developer. You have a long ways to go and a lot to learn before you reach what we would call ‚ÄúSenior+‚Äù level. This is a problem.
Why?
Because we are all still somewhat uncertain about the long time horizon and what the LONG TERM impacts of AI will be on Software. We can see the ground is shifting fundamentally and fast, we can see companies embracing AI for Software.
But, we are living in the transition period!
I truly feel for those folks just getting into Software Development of whatever kind. I can‚Äôt imagine the unknowns and questions facing those new comers today.
Will my job look the same? How much AI should I use? What should I learn? What don‚Äôt I need to learn?
Good questions all.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Keeping it real.
So what‚Äôs a developer to do? Seems like we are between a rock and a hard place. If we lean on AI too much in our daily Software writing, we stunt our learning and become hobbled and un-needed, providing little to no value. On the other hand if we ignore completely the AI revolution, we leave a clear amount of increased capacity on the table.
There are Senior+ Engineers who reject AI.
They may have the luxury to do so, take that advice at your own risk.
They will probably quietly drift into the long-term background like old cowboys who refused to drive a car.
There are Senior+ Engineers who use AI wisely to extend their reach, knowledge, and ability to deliver more value.
They will continue to grow and extend their reach and be promoted higher and higher.
There are Junior and Mid-Level Engineers who will rely on AI way to much.
They will never master fundamentals, become script kitties, never gain experience to build and lead large projects.
Becoming servants of their AI masters
There are Junior and Mid-Level Engineers who will use AI wisely, much like their Senior+ counterparts, to extend what value they can deliver.
While at the same time driving growth in technical and non-technical skillsets.
I encourage you to tread your AI path forward very carefully and intentionally. Be critical with yourself and your daily usage of AI, is it leading you down a path towards your longterm goals?
Control the controllables is the old saying, and I think it applies here.
If you can‚Äôt beat em‚Äô ‚Ä¶ join em‚Äô.
Look, at this early point in the game we don‚Äôt know the answers to where the AI will lead us as writers of Software. We can sort of lick our fingers and stick our hand in the air like our grandfather and suss out which way the wind blows, that‚Äôs about it.
Times are changing for sure, we shouldn‚Äôt pretend like AI isn‚Äôt going to change how we do our jobs.
But, the rest of it, where we go from here is up to the individual person, their goals, and where they reside in their journey through professional life.
Never think it‚Äôs ok to not understand fundamentals to AI.
Do not stop learning and growing technically.
Non-coding skills are now more important than ever.
AI is here to stay, use it to your advantage (because others will)
If you‚Äôre starting in your career, be more nuanced in how you approach AI.
If you‚Äôre seasoned, you probably have more flexibility to wing it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What do I do?
I‚Äôm not sure if it matters, but I will tell you I use AI on a daily basis and let you make your decisions.
These is how I generally interact with AI (
being someone in the second half of my career who has attained that ‚ÄúSenior+‚Äù title
).
I use it (
carefully
) to do non-value added tasks (or
mundane
tasks).
Writing unit tests, boiler plate code, configurations, sample files, project structures, etc.
The idea is to spend as much time as possible doing the most value work possible.
Double check the approaches or designs I HAVE ALREADY decided on.
Trying to catch any gaps or get a different point of view on the problem that I might miss.
Increase the velocity at which I can understand and learn a new topic.
I make a point of learning a new thing for myself, but AI can accelerate that path (via examples and summaries etc).
What it really boils down to is that I use AI everyday to make me more efficient at what I already know. I‚Äôm not worried about AI doing some ‚Äú
Data Engineering thing
‚Äù for me and becoming overly relient on it for X thing. I am aware of this pitfall and avoid it at all costs.
I‚Äôve long been a position where it isn‚Äôt the code I write (
although I write a lot of code and try to do it well
) that makes me valuable as an Engineer. It‚Äôs the combination of technical and NON-technical skills that have gotten me to where I sit today.
Guess what?
AI is great at belching out a never ending stream of code that isn‚Äôt that bad. But you know what it‚Äôs not good at yet?
Working as part of a large team in a collaborative manner.
Making tradeoff‚Äôs and decisions in the wider context of a business.
Taking the learnings and nuances of decades of writing code and turning that into ideas that can be applied to problems.
Planning and managing large scale and complex problems and feature requests.
Working with the business. Talking with the business. Listening to the business.
Never forget, code is only half the battle. The other half of the battle is human-centric. That‚Äôs where we still hold the power as the crafters of Software.
AI, at least right now, appears to be taking some of the coding off our hands, but the most important parts, the human parts, are still in dire need of people with empathy, understanding, and good soft skills to step in and make things work day-to-day, or plan that next big thing.
If you are an Engineer who only cares about writing code, the perfect code, that‚Äôs all you think about and that‚Äôs all you want to do ‚Ä¶ well, yeah, you probably got some soul searching to do over the next 5 years as AI writes more and more code.
Yeah, I know there will always been a need for the insanley smart and nuanced programming genius who‚Äôs going to build the next Apache Spark or Redis. I get it.
But that‚Äôs not the majority of us. Embrace your human side, make yourself valuable to the team and business you work in. Bring beautifully crafted Software to the table, but also bring nuisance. Step beyond the code and bring people together, teach others, up-skill, work to solve complex problems that cross team boundaries.
Do those things and AI will be serving you instead (
while getting paid a lot I might add
)."
156554431.your-limiting-factor-in-software,Your Limiting Factor (in Software), ... is you ...,2025-02-05 15:46:33.346-06,"I‚Äôve never been one who‚Äôs much for self-help, you know, all those books you walk past in the airport or at Barnes and Noble. ‚Äú
10 Ways to Transform Yourself
‚Ä¶‚Äù Yeah, right, I usually snort to myself and walk on.
Maybe I was wrong all along, or maybe it was because I was raised in a cornfield, and nothing was handed to me besides a fist full of dirt. Either way, the only kind of self-help I believe in is ‚Äú
Put The Work In
, "" which is the title of my self-help book.
Today‚Äôs article is less about software and more about the art of becoming a ‚Ä¶
better software engineer
more senior engineer
smarter engineer
expert in <
insert technology
>
someone to be reckoned with
Maybe this is my version of self-help for Software Engineers 101. What do you think? Would you like to buy a copy for 9.99 on late-night TV?
I was inspired to write this while lying flat on my back in my unfinished basement, pumping a set of dumbbells up and down for the millionth time.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I hate to break the bad news to you.
I'm in a somewhat undesirable position in that I‚Äôve been writing to all of you for years. I get many inquiries from various people about everything related to data engineering and programming. I‚Äôve also had the unfortunate or fortunate position of being able to work at various startups over a wide distribution of teams, giving me a first-hand view of the foibles of the software masses.
There are always outliers in every dataset, but generally, I find the following to be true.
You are your worst enemy, and your limiting factor is yourself.
At the risk of simplifying too much, in the age of AI and ChatGPT that can write all your code for you ‚Ä¶ you are at risk of simply not putting in the work.
Let me ask you a question. Think of the one technology you work on a lot ‚Ä¶ such as Spark, Databricks, Snowflake, Kubernetes, AWS, GCP, Postgres, Rust, or Python‚Ä¶.
Next, think about someone you ‚Äúknow‚Äù or are aware of who seems to be ‚Äú
an expert
‚Äù or ‚Äú
really good at that thing.
‚Äù
Do you really think that knowledge simply fell from the sky into their ears and slowly made its way to their brains, making them experts overnight?
You, my friend, breathe the same air as everyone else. Sure, some people are ‚Äúsmarter‚Äù than others, but generally speaking, when we look across the mass of Software Engineers pumping out code on a daily basis ‚Ä¶ there is a relatively small set of things that set the ‚Äúgood ones‚Äù or ‚Äúexperts‚Äù apart from the mediocre.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I fairly regularly get contacted by someone(s) about getting into creating content, especially this Substack, folks looking for advice, tips, secrets, and encouragement to do it themselves. I always let them in on some incredible insights that are mind-blowing (
insert sarcasm
). I think they apply to software as well. Here, I‚Äôm giving them to you for free.
Consistency over long periods of time is #1
Find what you‚Äôre good at and double down
Learn the things you don‚Äôt want to (marketing, etc, aka soft skills)
Doing something is better than nothing or perfection.
If you want to be a good writer ‚Ä¶ then write. If you want to be better at Rust ‚Ä¶ then write more Rust. If you want to become a master at Spark, then write more Spark and read and study more about Spark.
Do hard things and grow yourself.
Life is like that inescapable bell curve from college or high school, good old data distribution. Why do you think everyone hangs in the middle, just doing their thing?
Because it‚Äôs easy, that‚Äôs why. You can just show up every day and do your job, your little pipelines, your
Python and SQL
.
But, if you want to move over to the right side of the bell curve, well that takes a little more effort.
Surprise, surprise
.
How many things have you started and stopped in life? Wanted to start a business, build that cool project, write that book, start a YouTube channel, get in shape ‚Ä¶ hmmm?
Not everyone does those things or moves over to the right because that is a hard path. A path filled with extra work, less free time, filled with blood, sweat, and tears.
I‚Äôm not saying you can or should be good at everything, that isn‚Äôt possible. It is possible to be good at some things, nay, better than most at some things.
Learn some new things, be comfortable being uncomfortable
Be ok with being you
Put in extra work and time into your craft OUTSIDE the 9-5
Set realistic goals for the year
Don‚Äôt have a classic victim mentality. There is always a reason why you can‚Äôt do something. Or you could just do that hard thing anyway and reap the benefits.
It‚Äôs only rarely that hard work doesn‚Äôt pay off.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Special note for Software Engineers
I want to end this little pep talk with a special note for all you Software Engineers pumping out tech debt every day. By all means, get better at writing code. You must.
Learn new languages, tools, you name it, shoot for the technical stars.
But ‚Ä¶ never forget to work on your soft skills. This is the single largest limiting factor I see on a regular basis.
I could point you to a myriad of brilliant Software Engineers who languish in obscurity, always unsure why noone recognizes their talent for what it is.
It doesn‚Äôt matter how nice your code is if you are an
ass
to work with.
Business value matters, so do communication skills.
Be able to see beyond the code to the bigger picture.
We all provide software and solutions in the larger context of a business that needs to operate at profit and make tradeoffs based on supporting internal and external customers.
The code you write isn‚Äôt the most important thing in the world.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
156171315.delta-lake-vs-iceberg-uniform-and,Delta Lake vs Iceberg. UniForm and Unity Catalog.,... will there ever be harmony?,2025-02-10 07:44:08.863-06,"I don‚Äôt know what to think. I imagine both
Apache Iceberg
and
Delta Lake
will be with us for a long time to come. At this point, I don‚Äôt think it‚Äôs possible for one format to totally defeat the other. Each is ingrained into different sections of the market, with Delta Lake in the large and growing Databricks Community and Iceberg in the open-source and AWS land.
I much prefer Delta Lake myself; the developer experience is far superior. But to be a good Data Engineer, we should explore everything we can find. Having some experience with everything is better than being a one-trick pony.
I‚Äôve heard rumblings about
UniForm
and
Unity Catalog
solving the issue of having two table formats to choose from.
Marketing folks make it seem like you can use
UniForm
and/or
Unity Catalog
to seamlessly integrate Apache Iceberg and Delta Lake on the same platform. This sounds too good to be true. My guess is that key features are missing, and they get glossed over. But
hey, let‚Äôs find out, shall we?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Trying out UniForm and Unity Catalog to deal with Iceberg AND Delta Lake
First off, I‚Äôm not sure I totally understand the desire to run both Delta Lake and Iceberg on the same platform, but I guess with larger companies at scale they will end up in a situation like this.
That‚Äôs just life.
Either way, I‚Äôm curious about a few questions that I want to see if we can answer.
Can we create Iceberg tables on Databricks and use UniForm to read them?
Can we register Iceberg tables in Unity Catalog?
What does the code look like to read both Delta Lake AND Apache Iceberg.
Can we WRITE to Iceberg tables with UniForm/Unity Catalog?
Got any guesses? I don‚Äôt even know if these are the correct questions to ask, or if they are possible. Let‚Äôs just play with the tools and find what we find.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Where to start?
I‚Äôm not totally sure where to start, but I think with a flip of a coin we will start with
(Delta) Uniform
. Now if you‚Äôre like me and you‚Äôve only heard the name, or never heard the name, what is it and what does it have to do with Delta Lake and Apache Iceberg???
‚ÄúDelta UniForm takes advantage of the fact that Delta Lake, Iceberg, and Hudi are all built on
Apache Parquet
data files. Under the hood, Delta UniForm works by automatically generating the metadata for Iceberg and Hudi alongside Delta Lake - all against a single copy of the Parquet data.‚Äù - Databricks
Well that makes sense on the surface don‚Äôt it? One way to beat your opponent is to put on a mask and make yourself look like their friend? Classic trojan horse. Just saying.
We will use the
Backblaze Hard Drive failure dataset
.
Let‚Äôs see how easy and simple it is to create a UniForm Delta Lake table.
Not bad eh? Just a few extra Table Properties indicating we want Iceberg compatibly. So can we read this table as both Iceberg AND Delta from Spark?
Let‚Äôs insert some data.
Now let‚Äôs see if we read the table two ways.
I mean on the surface it appears to work.
I suppose the real test would be if we can read the Delta UniForm table as Iceberg with something like Polars maybe. From everything I‚Äôve read we need to find the meta.json file for Iceberg, which we can get by running DESCRIBE TABLE EXTENDED on our UniForm Delta table.
Below you can see the s3 path of the Iceberg meta.json
Well ‚Ä¶ look at that. Didn‚Äôt expect that to work the first time that‚Äôs for sure.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
All I had to do was install
polars
and
pyicberg
, set the location of our Delta Uniform Iceberg meta.json file and we are off to the races with any sort of Apache Iceberg reader thinking it‚Äôs eating a Iceberg table when in fact it‚Äôs just been tricked.
This does make me wonder something important.
If you were an Apache Iceberg fanatic, for whatever reason, why wouldn‚Äôt use just use Delta Uniform to pretend store Iceberg tables as Delta Uniform???
You get the best of both world out of the box. You can, depending on your use case and client, read Delta or Iceberg. I mean it seems like a no brainer to me.
I‚Äôm curious, can we WRITE to our UniForm table like it‚Äôs an Iceberg table with Spark?
This would make Uniform truly a nice slice of bread with a slab of butter on it, would it not?
Well, look at that!
You can see I read the table as Iceberg before and after appending more records and the count went up. Must be working!
Note on Unity Catalog.
It‚Äôs probably because I don‚Äôt know what I‚Äôm doing, but I tried various ways to connect an Iceberg client (pyiceberg) to my Unity Catalog endpoint for Iceberg for WRITE support, but I couldn‚Äôt make it work.
Unity Catalog does indeed provide a Icerberg endpoint, so in theory it should work, mine did not. I imagine it has something to do with credentials.
I honestly didn‚Äôt feel like fighting it. As long as I have read and write support with Spark, and read support for Iceberg via Uniform for things like Polars ‚Ä¶ than I don‚Äôt really need to pyiceberg (which would give me write support in theory for NON-Spark workloads).
Also, when reading the docs for UniForm, I ran across a line that said
Iceberg client support for write is not available.
Which, again, we say we have write support with ‚ÄúIceberg‚Äù via Spark, so whatever.
Thoughts
I do find Delta Uniform very enticing indeed, it‚Äôs wonderful that you may be able to persuade those filthy Iceberg lovers just to slide on over to Databricks and eat their fill of Iceberg when they are really swallowing Delta Lake.
I do enjoy seeing this sort of work done by the Databricks and Delta Lake folk, they are really trying to make the storage format wars obsolete with this type of development. I mean they didn‚Äôt HAVE to put the time and effort into doing this.
Yet another reason I choose Delta Lake, clearly the folks behind the development of this are engaged and thinking ahead about how to make our lives easier.
Ingenious, if you ask me."
155796796.polars-and-duckdb-release-unity-catalog,Polars and DuckDB release Unity Catalog (Delta Lake) integrations. Who lied? Who didn't?,... say it ain't so ...,2025-01-26 18:57:25.874-06,"You know, just when you think things have finally settled down and our poor ears won‚Äôt have to hear another thing about The Great Catalog War, or The Great Lake House Format War ‚Ä¶ it just keeps rolling in.
As much as I hate to admit it, it appears Catalogs are indeed becoming essential and core to most modern, and future, Data Stack being built and used.
For example, if you are using Databricks as your main Platform for data storage and processing <
insert Iceberg and Snowflake if you please
> , if you are uncreative in nature it means you have a one track mind that consists of you using Spark inside Databricks to do
EVERYTHING
with your Delta Lake tables.
This can be a hard road if you have anything approaching a normal Data Platform that churns and burns data from a variety of sources and sends data back out to variety of downstream processes and systems. Rarely is the data world we move in black and white. So, with that being the reality, there are many use-cases that force us to get creative when we THINK our only option is to use a Databricks Job or heaven forbid a notebook.
In the past, choosing a particular platform ‚Äúlocked you in‚Äù into that vendor.
With some gyrations and throwing salt over your left shoulder at midnight could you manage to extract yourself and your data for some specific use case that didn‚Äôt work with that big old beast Spark.
I think most vendors, after too many years, finally came to realize that customers demanded more flexibility.
We have multiple tools
We have multiple clouds
We have multiple use cases
We have multiple data sets
So, our loving vendor overlords gave us a solution. Catalogs. One of those titians vying for supremacy is
Unity Catalog
.
I‚Äôm not going to spend time getting into the discussion of WHAT exactly is a Catalog, it‚Äôs pretty much self explanatory. If you want it explained like a 5 year old, it‚Äôs a one stop storefront where you can get all your data.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The announcements start rolling.
Of course with all this development around catalogs, like Unity, gaining so my attraction and attention (
you can‚Äôt simply write off the fact that most ALL Databricks users have massive amounts of data stored
in
Unity Catalog Delta Tables
), general open source tools are starting to adopt and provide interfaces for catalogs, like Unity.
Both
DuckDB
and
Polars
have fairy recently provided first, unstable versions of this integration.
Now, before we get into testing this new features. I want to mention that you could still typically use these tools to interact with ‚Äú
Unity Catalog based Delta Lake
‚Äù tables ‚Ä¶ it just took a little ingenuity.
Below is a snippet of code that is actually used in Production , for over a year, to interact with a Unity Catalog Delta Lake table, long before any Unity Catalog integration was released.
Basically if you knew the ‚Äúbackend‚Äù storage location of a certain Delta Lake table in Unity Catalog, you could do what you needed today. This sort of workflow is sorta a pain, espeically when table locations could change, and needing to find the physical location beforehand.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
It‚Äôs time now to test if this new experimental integrations from DuckDB and Polars are real or not. Did they lie to us? You know they better have at least tested them before you know I will take them to task if they don‚Äôt at least minimally work.
Testing Polars new Unity Catalog integration for Delta Lake tables.
So, let‚Äôs see if this code can be much simplified, let‚Äôs write a simple data pipeline using Polars + Unity Catalog and see what happens.
First thing to note is that the documentation sucks for Polars and Unity Catalog. I‚Äôm not sure if that is because things are unstable and they are waiting for things to solidify? Either way you can find
minimal documentation of the code here
.
I was able to get a simple data pipeline running. I have to admit, it did make the interaction with a Unity Catalog Delta Lake table much more simple and straight forward.
Very seamless indeed.
Basically all I needed to use this new Polars Unity Catalog feature was ‚Ä¶
a call to Catalog()
URL of the Unity Catalog (
Databricks or Open Source
)
Databricks access token
understanding of
Catalog
Schema
Table names
For example once you call
pl.Catalog(""https://xxx-xxxxxxxxxxx.cloud.databricks.com/"",bearer_token=BEARER_TOKEN)
You can scan any Delta Table into a Lazy Frame if you know the FULL name ‚Ä¶ catalog.schema.table
df = catalog.scan_table(""confessions"", ""default"", ""trip_data"")
Of course as a side note, with Polars you can ran SQL, like I did, on your LazyFrame representing your Delta Lake data in Unity Catalog, do as you like, and push the results somewhere.
It looks there isn‚Äôt much for functionality yet inside the Polars Unity Catalog integration, but at this point it seems to be pretty early on. The
scan_table
seems to be about the only real useful thing provided yet ‚Ä¶ what we really need is a
write_table
.
I tested my code both inside a Databricks Notebook AND inside an AWS lambda, both worked great. Kudos to Polars to actually building something that works.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Testing DuckDB‚Äôs new Unity Catalog integration for Delta Lake tables.
Now, let‚Äôs head on over to DuckDB and see if they can also live up to their word in the same fashion as Polars. The DuckDB extension for Unity Catalog seems to have been around a lot longer, and actually has more documentation, from both DuckDB and Unity, so I‚Äôm expecting a lot out of them.
Let‚Äôs try it on a Databricks Notebook first, something running inside the Databricks Environment will hopefully make things less complicated and more likely to work?
Surprise surprise, no luck with this.
This is a strange error indeed. My catalog doesn‚Äôt exist? Well of course it does, we used that catalog to run the Polars code just fine.
After looking through more documentation a little harder, it appears I might have to manually attached my ‚Äúcatalog‚Äù to DuckDB? Strange.
I tried this, but still got a different error.
It was now saying my table doesn‚Äôt exist, which it does, which makes me wonder if I went down the right road with the Catalog attach thingy.
After some research it appears that I‚Äôm not the only one having this sort of problem.
There appears to be a open GitHub issue about this sort of thing open for like half a year
. Surprise surprise. We ran into this same sort of thing with DuckDB and OOM issues.
I tried everything I could, simply couldn‚Äôt make DuckDB read a Unity Catalog Delta Lake table. Liars.
I‚Äôve come to like DuckDB more and more lately, it‚Äôs ability to read remote JSON files for example and flatten them into a tabular structure is sliced-bread. But, I do frequently run into these issues (like large than memory datasets) with DuckDB where it‚Äôs like ‚Äú
it works on my machine, why not yours??
‚Äù
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Catalogs are the future, get with it people.
Honestly, tools need to get with it, like Polars did. Heck, Daft has had the GOAT integration with Unity Catalog Delta Lake tables for a long, long time.
The more time that goes past, the more people migrate and start to use tools like Databricks and Unity Catalog, more and more data is being stored behind these Catalogs ‚Ä¶ there are workaround, but many times they aren‚Äôt that fun and people are lazy.
If your tool doesn‚Äôt stay up-to-date with the times, people will simply use other tools to do the work, in this case use Polars instead of DuckDB.
Sure, you can fix a thing, but by the time you have fixed it people have moved on and simply settled on other ways to solve the problem.
And you are going to get up on a stage and title a YouTube video ‚Äú
Announcing DuckDB support for Delta Lake and the Unity Catalog extension
‚Äù ‚Ä¶ then you should probably make sure that the Unity Catalog extension at least nominally works for the most basic use case INSIDE A DATABRICKS NOTEBOOK.
Anywho. I‚Äôm sure the internet is full of people who will shortly tell me I‚Äôm and idiot and what I did wrong. Thank the Lord for them people eh?
Either way I just approached the problem simply as any other engineer would and took what information and documentation they provided at face value and gave it a shot.
Polars worked. DuckDB didn‚Äôt.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
155361416.bespoke-vs-managed-data-platforms,Bespoke vs Managed Data Platforms, ... choose your poison ...,2025-02-03 07:52:50.394-06,"A few days ago, I was having a conversation with someone about their Data Platform; they were looking for advice and feedback on their ‚Äú
architecture
.‚Äù This happens a lot, and here is how the conversation goes.
Our company is X size, and our data team consists of roughly Y people. Our tech stack consists of A, B, and C. We‚Äôve been having problems with scalability and reliability, and D, F, and G break frequently. We aren‚Äôt happy with our setup and want something better. What should we do?
This is a tale as old as time, my friend. It is the same story spun by so many companies of all different shapes and sizes. Although complaints are various and wide-ranging, they all have some common themes.
performance
scalability
complexity
debugging and monitoring
data quality
They are simply unhappy with their Data Platform as a whole (
if they even call it or think about it like a Data Platform.
) They list numerous problems, and it‚Äôs hard for them to put their finger on a single thing ‚Ä¶ it's usually just whatever has broken lately and is on their mind.
Everyone is looking for a magic bullet, pill, technology, or anything to fix their problems.
If only it were that easy.
You will find plenty of snake oil salespersons out there shilling you their solutions to solve all your problems and sending you glassy-eyed into the land of your forefathers to live in eternal data peace forever. Lies.
All of them ought to be burned at the stake.
If we lived in a plug-and-play world, there would be a single Data Platform, and everyone would do the same thing. But there isn‚Äôt, and we don‚Äôt.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Bespoke vs Buy It.
You simply can‚Äôt have your cake and eat it too. Also, when a Data Platform is generally viewed as ‚Äúnot working well‚Äù or ‚Äúbreaking a lot,‚Äù there is rarely one thing you can point your finger to and say, ‚ÄúThis is the problem.‚Äù
I‚Äôm sure it happens, but typically, the problems are at more of a philosophical level. It‚Äôs not the tools themselves that are junk; it‚Äôs the implementation ‚Ä¶ aka it was the human that took those tools and stuck them into a certain spot and asked the tool to do a certain thing for them.
Let me be clear, some tools are crap, but not enough to make them your sole problem. It's how you've decided to use them.
An example of going down the wrong road.
I want to use an example that was specifically mentioned to me recently. It involved Kubernetes. Kubernetes is great. I've used it many times over the years, and it is amazing at solving custom and bespoke Data Engineering use cases.
Kubernetes is not great for a small data team on a new Data Platform trying to build simple and reliable data pipelines. It‚Äôs a matter of asking what you want to do with your time.
Setting up and managing a Kubernetes Cluster
Deploying and managing services on Kubernetes
Or ‚Ä¶ actually building data pipelines??
The answer should be obvious. But it might not be obvious to talented Data Engineers who want something fun to do.
If you are so smart and convinced it‚Äôs not that big a deal, go read this article
I wrote years ago about deploying and running Apache Airflow on Kubernetes
.
This is exactly what was happening on the Data Platform that was ‚Äú
having problems.
‚Äù They were not only managing their own Kubernetes cluster but also deploying and managing their own Orchestrator (
that will remain unnamed
) on it. This was a small team ‚Ä¶ tiny.
Basically, they were having technological and business problems with data. They were ingesting various data sets from many different sources, and it sounded like a mess.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The problem is obvious.
As an outsider, I realized after listening to them talk for an hour about the minute details of their problems‚Ä¶ data quality, Kubernetes PODs OOM, unhappiness with their Orchestrator choice, and lots of custom Python code‚Ä¶ that their problem was not the tools themselves. But they kept coming back to this point.
‚ÄúWhat tools should we be using? Should we replace X with Y? What would you change?‚Äù
They had succumbed to a hap-hazard approach to their Data Platform; there was zero rhyme or reason as to why they had chosen certain tools, like Kubernetes, in fact when I asked this exact question ‚Ä¶ ‚Äú
Why are you managing your own Kubernetes cluster and deploying applications on it ‚Ä¶ you are doing normal Data Engineering work with normal data sets that are extremely common?
‚Äù
The person in charge of the day-to-day data operations answered, ‚ÄúBecause I like learning new and cool things.‚Äù
Excited but misguided.
While this might be commendable as an attribute of a Data Engineer, being excited and wanting to work on new things, I can‚Äôt imagine anything worse when it comes to building and making decisions about how the entire Data Platform is going to be built.
That was the problem: they didn‚Äôt view the pieces as a whole and didn‚Äôt even consider their data platform as a Data Platform.
Building reliable Data Platforms that work well is based on at least some of the following things.
Reducing complexity
both logical and technological
Sizing and matching tooling to requirements
Understanding data team(s) size and skills
matching this to the tooling
This is more human-related than technology because, again, the tools are the tools, and pretty much anything you choose that is done with some forethought and done well will work.
A haphazard approach that is more concerned with using something ‚Äúfun‚Äù or ‚Äúcool,‚Äù where each decision is made in a vacuum in the heat of the moment, will lead to a poorly built puzzle where nothing works as it should.
It simply isn‚Äôt true to say, ‚ÄúIf I just replaced X tools with Y, all my problems would go away, and suddenly, my Data Platform would run like a well-oiled machine.‚Äù
Back to Bespoke vs Buy It.
I figure I should finally get back to the matter at hand and to the story of woe that I was listening to with someone who just wasn‚Äôt happy with the failing Data Platform.
It is hard, but one must find a way to purge one's mind and heart of insensibilities and preconceived solutions and tools before setting out on the path to enlightenment and either planning a greenfield or data migration project.
Many times, making this sort of decision in the context of building or changing a Data Platform leads to a fork in the road.
Build
Buy
We have an endless number of decisions to make every single day when coming up with solutions to problems. Do we build our own solution for X, Y, and Z? Or do we use this tool open source or from a vendor for X, Y, and Z?
This is the proverbial million-dollar question, is it not?
It is discussed on a large scale when planning big projects and migrations, and it is also discussed every day in small decisions about how to solve a certain problem in the next two weeks.
To make these decisions well depends on a multitude of factors.
What SHOULD you be spending your time doing?
How large is your team? What are they capable of?
What is the timeline or deadline for said ‚Äúthing‚Äù or ‚Äúfeature.‚Äù
What other high-priority things are coming down the pipeline?
etc, etc.
For example, in the case I‚Äôve been discussing, this extremely small team didn‚Äôt have time for the one thing they were doing: building and maintaining their own Kubernetes cluster and deploying Orchestration platforms onto it.
It‚Äôs almost laughable. Instead of spending time only actually providing high-quality value in the form of generic data ingestions, they were playing around with PODs on Kubernetes.
They would never have had the time to properly consider and work on the rest of the Data Platform and pipelines ‚Ä¶ they were too busy with
infrastructure
.
In their case, they needed to make a decision, albeit a hard one, one that requires a mindset shift.
Pay for a managed Orchestration platform.
Delete all the current work and code required to keep their own managed version of X tool running.
The simple removal of the mental burden of such a thing is worth its weight in gold.
Delete Kubernetes and choose something managed.
They are spending more ‚Äúmoney‚Äù (
and time, effort, maintenance
) managing Kubernetes than paying for some compute on a managed platform.
Remember, this isn‚Äôt some 500 person Engineering Team that needs to support a variety of complex workloads. This is two dudes trying to ingest data from an API.
What I‚Äôm not saying.
I‚Äôm telling everyone to stop building things and go buy something off the shelf. What I am saying is that people need to be more thoughtful in their approach to solving problems.
They shouldn‚Äôt get caught up in doing things because they want to. Stepping outside the situation and applying some rigor to the decision and design process is all that is needed to avoid Data Platforms ‚Äúthat don‚Äôt work well.‚Äù
Instead you should ‚Ä¶
Be realistic when making decisions about tools
Understand the full consequences of decisions you make
Understand your own abilities and your teams size and prior
ities
Do your best to find the simplest solution to your problem
Typically it can seem like a hard decision when you start, should I build, should I buy, what‚Äôs the best answer? I think taking a step back and viewing your Data Platform as a whole, thinking about your teams abilities and free time to execute projects, will usually bring the best decisions into focus."
154914837.data-quality-with-databricks-labs,Data Quality with Databricks Labs new DQX tool. , ... better be good ...,2025-01-16 17:15:30.314-06,"I recently saw something pop into
my LinkedIn feed
that made me pee and scream simultaneously. An announcement from
Databricks Labs
about a Data Quality tool specifically designed for Spark PySpark Dataframes.
Dang. Been a long time coming if you ask me.
Data Quality has been one of those topics that has come and gone within the Data Community like clockwork over the last few years. Yet, it has suffered from a lack of various easy-to-use and open-source tooling. Of course, there is
Soda Core
and
Great Expectations
, which I have checked out before.
Anywho, back to the exciting news at hand.
‚ÄúWe‚Äôre thrilled to announce the release of DQX, Databricks new Python-based Data Quality framework designed for validating the quality of PySpark DataFrames.
DQX is your go-to tool for proactive data validation, ensuring bad data never enters the system. Whether you‚Äôre working with batch or streaming pipelines, DQX helps maintain the integrity and reliability of your data.‚Äù -
Databricks Labs
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Data Quality + PySpark + Databricks.
I‚Äôm not sure why Data Quality hasn‚Äôt caught on more inside the Data Engineering community, oh wait, it‚Äôs because there just isn‚Äôt easy and seamless solutions. Funny, you would think withs something as popular as Spark that there would be about 15 solutions to choose from.
I mean how in the world could people not love things like
pydeequ
?
Or Great Expectations? I mean who doesn‚Äôt want to write JSON like this as just one tiny piece of greater system of code trying to implement DQ?
I hope you can hear the sarcasm dripping off my fingers, because it is.
‚Äú
The problem with most Data Quality tools up this point has been the overhead of complexity on all levels to implement those tools in a production worthy tools. If the thing helping you is more complicated than the thing its helping ‚Ä¶ adoption will always be abysmal.
‚Äù - Me
We are going to find out of Databricks Labs has finally delivered something simple, elegant, and easy to use. Here we sit like little birdies with our mouth open waiting for our benevolent Mother Databricks to drop a juicy Data Quality worm in our mouth.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Diving into DQX from Databricks Labs.
Sorry it took me so long to get this point, had to get that all off my chest. Definitely not bitter or anything. Right now all have to go off of is the
DQX GitHub pag
e, so we will start there and see if we can implement some Data Quality checks inside a Databricks Workspace etc, and see if their promises are true.
What does DQX have to say about itself, hopefully we can get a sense of what we are in for.
Invalid data can be quarantined to make sure bad data is never written to the output.
Support for Spark Batch and Streaming including DLT (Delta Live Tables).
Different reactions on failed checks, e.g. drop, mark, or quarantine invalid data.
Support for check levels: warning (mark) or errors (mark and don't propagate the rows).
Support for quality rules at row and column level.
Profiling and generation of data quality rules candidates.
Checks definition as code or config.
Validation summary and data quality dashboard for identifying and tracking data quality issues.
I see both things I expect in that list, that are pretty much the status quo of any DQ tool, things that make me nervous (
definition as code or config
), and new things (
Data Quality Dashboard
).
We can only eat an elephant one bit a time, so let‚Äôs just try to simply use
DQX
to do some Data Quality checks on a PySpark pipeline and learn and try things as we go.
Trying out DQX for PySpark Data Quality.
Let‚Äôs get cracking. We have multiple ways of installing dqx as a tool, both from pip or inside a Databricks Workspace etc.
Note: if you decide to install
DQX
inside a Databricks Workspace with the CLI the following things will happen. Do this at your own risk and watch your wallet.
The cli command will install the following components in the workspace:

- A Python wheel file with the library packaged.
- DQX configuration file ('config.yml').
- Profiling workflow for generating quality rule candidates.
- Quality dashboard for monitoring to display information about the data quality issues.
The one that has me wondering is the Dashboard, sounds nice but possible expensive? I wish more detail would have been provided about that.
Let‚Äôs just pip install DQX in a Notebook inside my Databricks account and play around ourselves. What I want to do is sorta treat DQX like just another standalone Data Quality tool (
similar to GE or Soda), and not really take the approach of ‚Äúfull on Workspace install 100% Databricks tool.
)
I tried to visualize how
DQX
works generally and what it provides.
Generally DQX will do these following things for you ‚Ä¶
1. Data profiling and generate quality rule candidates with stats
2. Ability to define more checks and validations as code or config
3. Set criticality level and quarantine bad data
4. Works in a batch of streaming system
5. Provide a Dashboard
It honestly reminds me a lot of the tools that have come in the past, except it seems less complicated, less overhead, less config and more integrated with Spark (
duh, since it was designed for PySpark
).
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
DQX in Code.
Code time. We are going to use the open source
Divvy Bike Trip dataset
as our sample data to mess around with. It looks like this. FYI,
all this code I wrote for this testing is on GitHub
.
Here is the basic code Databricks gives to load up a Dataframe and get the basic default stats.
Here are the ‚Äúsummary stats‚Äù
Here is the ‚Äúprofiles‚Äù
The next part is actually pretty cool. We can take the ‚Äúprofiles‚Äù that describe our data (DataFrame), and generate default validation checks based on that.
Here is a sample of the checks that were produced.
- check:
    arguments:
      col_name: _c0
    function: is_not_null
  criticality: error
  name: _c0_is_null
- check:
    arguments:
      col_name: _c1
    function: is_not_null
  criticality: error
  name: _c1_is_null
- check:
    arguments:
      allowed:
      - electric_bike
      - classic_bike
      - rideable_type
      col_name: _c1
    function: value_is_in_list
  criticality: error
  name: _c1_other_value
- check:
    arguments:
      col_name: _c2
    function: is_not_null
  criticality: error
  name: _c2_is_null
Not bad! If you look close it knows if a column should be null or not, it also for example if a column should have values in a pre-defined list!
Not bad for doing no work out of the box. This might be a great first option for a Lake House architecture that has a ton of data tables (sets) and you need to get some basic DQ checks off the ground.
Also, of course we could write our own checks in a `YAML` file and then load that file in Python.
But, something I like even better is the ability to define checks ‚Äúin-code.‚Äù
What‚Äôs going on here? Well, an easy way to define PySpark DataFrame data quality checks is what is happening! You can see different kind of checks going on.
checking multiple columns at one for ‚Ä¶. ‚Äúis_not_null‚Äù
aka we could check whatever we want
defining a specific check for a single column
defining checks to see if a value is in a list, etc
you can split out the results of the check into a different DataFrame
this would allow flexible approach to quarantine bad data
just add the check results in a column on the DataFrame
Dang, pretty easy and nice to use.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Wait ‚Ä¶ like late night TV ‚Ä¶ there is MORE!
Just like that like night preacher on the tube screaming at you, there is always more. I couldn‚Äôt believe some of the genius additions they made to
DQX
.
SQL for data quality checks!!! Say I wanted to make sure the ride start times were before the ride end times.
- criticality: ""error""
  check:
    function: ""sql_expression""
    arguments:
      expression: ""ended_at > started_at""
      msg: ""ended_at is greater than started_at""
I mean that right there is butter on the bread. I know all your SQL junkies are running for you keyboards as I type. Addicts.
As well, you can define custom checks with plain old Python. For example we might want to check that our rideable_bike column ends with ‚Äú_bike‚Äù per our expected data outcomes.
import pyspark.sql.functions as F
from pyspark.sql import Column
from databricks.labs.dqx.col_functions import make_condition

def ends_with_foo(col_name: str) -> Column:
    column = F.col(col_name)
    return make_condition(column.endswith(""_bike""), f""Column {col_name} ends with _bike"", f""{col_name}_ends_with_bike"")
The sky is the limit for
DQX
checks on PySpark DataFrames in our Lake House, if you can imagine it, you can write it ‚Ä¶ even SQL.
You know what I like about
DQX
‚Ä¶ it‚Äôs flipping simple to use. I really hope Databricks puts some effort into marketing this tool, it‚Äôs easy of use and implementation would pay back dividends for a lot of users.
But, they need to know it exists. I can imagine it‚Äôs only going to get better in the future.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I hope this quick preview of
Databricks Labs DQX
was enough to whet your whistle and get your mind and fingers moving. It did mine. Excited to dig into it more and put this little bugger to work!"
154825391.replace-pythons-pip-with-uv,Replace Python's pip with uv?, Anonymous Rust Dev,2025-02-19 16:47:25.161-06,"It's your friendly neighborhood Anonymous Rust Dev, back to offend your sensibilities once again...
Among its strengths, Python includes a language that is so ""easy"" to learn that it's given to children as their introduction to programming. Its ease of writing and a comprehensive ecosystem of third-party libraries allow it to handle almost any conceivable need.
But you have to get those libraries somehow. Some options are available, including the perennial favorite pip (
which you generally get for free these days just by having Python
). So why would we want to have a conversation about the cool new kid on the block,
uv
?
Well, I
am
the resident Rust evangelist, so of course, I'm going to gravitate toward the Rust-based option, right? Well, before Rust, I came from a NodeJS environment and have seen something similar in that world‚ÄîNPM, the out-of-the-box king, has been threatened by
Yarn
and
PNPM
.
And guess what? At the end of the day, the original ruler keeps his throne, though not without some attrition.
So, for all my desire to see a Rust-based project succeed, I know in my heart of hearts that the incumbent option tends to hold its position tightly. A morbid curiosity demands that I find out for myself whether I have yet another feather in my cap that I can brandish at the Python crowd to woo them over, or if my hopes in the Rust-based option are misplaced.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What is UV?
Actually, I don't even know where it gets the name. Does it cause skin cancer? Are we just
taking more random characters from the alphabet
?
For the curious, the authors
mostly just ran out of good names
.
More importantly, though, what is it, and what problem does it solve? Well, if you're here, you probably already know, but let's consider what the
official docs have to say on the matter
:
üöÄ A single tool to replace pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv, and more.
‚ö° 10-100x faster than pip.
üóÇÔ∏è Provides comprehensive project management, with a universal lockfile.
‚ùáÔ∏è Runs scripts, with support for inline dependency metadata.
üêç Installs and manages Python versions.
üõ†Ô∏è Runs and installs tools published as Python packages.
üî© Includes a pip-compatible interface for a performance boost with a familiar CLI.
üè¢ Supports Cargo-style workspaces for scalable projects.
üíæ Disk-space efficient, with a global cache for dependency deduplication.
‚è¨ Installable without Rust or Python via curl or pip.
üñ•Ô∏è Supports macOS, Linux, and Windows.
In that first point:
That's a lot of stuff for a single tool to carry. And, as someone who has to dust his Python knowledge off maybe once or twice a year, some of the stuff you take for granted ‚Ä¶
managing virtual environments
ensuring the correct versions of dependencies
committing the right files
crafting the perfect .gitignore
‚Ä¶ is a pain point for me that I have to stumble my way through.
So I'm wondering, as someone who hasn't already learned how to just suck it up and deal with the status quo, does UV give me an easier go of it than an out-of-the-box Python installation? Is this the tool that takes me from a Python casual to a regular user?
Starting a new project
I'll aim to showcase a handful of things about UV from that list in one go. My high-level objectives in this will be to demonstrate how I can achieve:
A self-contained project that can easily be committed to Git (meaning, no bundling of actual libraries themselves, only references through either a package manager or Git submodules) and spooled back up from a fresh checkout
A demonstration of how a single tool can effectively scaffold a new project, manage its dependencies, and somehow keep it all contained in a local scope (ala virtualenv)
A chance to show off some of the selling points of this tool over its established alternatives
To get me there, let's take a trip to what is probably a familiar trove of data to you:
divvy-tripdata
. In particular, let's focus on one file,
https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2018_Q3.zip
. Shown below is the header row and the first few rows of data:
trip_id,start_time,end_time,bikeid,tripduration,from_station_id,from_station_name,to_station_id,to_station_name,usertype,gender,birthyear
19244622,2018-07-01 00:00:03,2018-07-01 23:56:11,5429,""86,168.0"",140,Dearborn Pkwy & Delaware Pl,106,State St & Pearson St,Customer,,
19244623,2018-07-01 00:00:13,2018-07-01 00:06:39,93,386.0,153,Southport Ave & Wellington Ave,250,Ashland Ave & Wellington Ave,Subscriber,Male,1986
19244624,2018-07-01 00:00:15,2018-07-01 00:23:26,2461,""1,391.0"",76,Lake Shore Dr & Monroe St,301,Clark St & Schiller St,Subscriber,Female,1987
Let's pretend that your pointy-haired boss comes to you in a frenzy because he heard on the golf course that there's an untapped market of cyclists born in 1991.
He insists on knowing, relative to the average of cyclists
, how much more time they spend on their rides.
Sticking UV into the process.
First, it goes without saying that we need to
install UV
. If you're following along, feel free to hit that page up and find your installation method of choice (
me, I went with the Cargo route, because Rust
).
$ uv --version
uv 0.5.29 (d22a2dfd1 2025-02-09)
As of writing this, that puts us at v0.5.29.
So, my back-of-the-napkin approach to this is:
Pull down and extract the dataset
Ingest the resulting .CSV file using Polars
Run a fancy query to give us some aggregates
Format the results and render it
But before that, we need a project. I'm looking now at the
readme
and following along with its basic steps:
$ cd /media/development/sandbox

# Scaffold new project:
$ uv init phb_really_needs_this
Initialized project `phb-really-needs-this` at `/media/development/sandbox/phb_really_needs_this`

$ cd phb_really_needs_this

$ uv add polars
Using CPython 3.12.3 interpreter at: /usr/bin/python3.12
Creating virtual environment at: .venv
Resolved 2 packages in 202ms
Prepared 1 package in 436ms
‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] Installing wheels...                                                                                                                                                                                                                                                                                                                                     warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
         If the cache and target directories are on different filesystems, hardlinking may not be supported.
         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.
Installed 1 package in 36ms
 + polars==1.22.0

$ uv add dash
Resolved 29 packages in 222ms
Prepared 26 packages in 660ms
‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/26] Installing wheels...                                                                                                                                                                                                                                                                                                                                    warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
         If the cache and target directories are on different filesystems, hardlinking may not be supported.
         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.
Installed 26 packages in 174ms
 + blinker==1.9.0
 + certifi==2025.1.31
 + charset-normalizer==3.4.1
 + click==8.1.8
 + dash==2.18.2
 + dash-core-components==2.0.0
 + dash-html-components==2.0.0
 + dash-table==5.0.0
 + flask==3.0.3
 + idna==3.10
 + importlib-metadata==8.6.1
 + itsdangerous==2.2.0
 + jinja2==3.1.5
 + markupsafe==3.0.2
 + narwhals==1.25.2
 + nest-asyncio==1.6.0
 + packaging==24.2
 + plotly==6.0.0
 + requests==2.32.3
 + retrying==1.3.4
 + setuptools==75.8.0
 + six==1.17.0
 + typing-extensions==4.12.2
 + urllib3==2.3.0
 + werkzeug==3.0.6
 + zipp==3.21.0

$ uv add unzip-http
Resolved 30 packages in 317ms
Prepared 1 package in 69ms
‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] Installing wheels...                                                                                                                                                                                                                                                                                                                                     warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
         If the cache and target directories are on different filesystems, hardlinking may not be supported.
         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.
Installed 1 package in 13ms
 + unzip-http==0.6
 
# Pin our dependencies for future generations:
$ uv lock
Resolved 30 packages in 0.49ms
Okay, so it's hard for me as a non-Pythonista to express just how
clean
this felt compared to my previous forays into the language.
A few
uv
commands netted me everything I would dread having to craft myself:
.gitignore
file
project file (
pyproject.toml
) that feels straight out of a Cargo project
lock file (
uv.lock
) that pins my dependencies to ensure a reproducible build.
There's other stuff in my folder as seen below, all thanks to UV:
Since it gave me
hello.py
, I'll be fleshing it out with my app:
from io import StringIO

import dash
import plotly.express as px
import polars as pl
import unzip_http
from dash import dcc, html

def main():
    my_zip = unzip_http.RemoteZipFile('https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2018_Q3.zip')
    divvy_csv_io = my_zip.open_text('Divvy_Trips_2018_Q3.csv')
    divvy_csv = divvy_csv_io.read()

    df = pl.read_csv(StringIO(divvy_csv)).with_columns(
        pl.col(""start_time"").str.to_datetime(""%Y-%m-%d %H:%M:%S""),
        pl.col(""end_time"").str.to_datetime(""%Y-%m-%d %H:%M:%S""),
    )
    ctx = pl.SQLContext(Divvy_Trips_2018_Q3=df, eager=True)

    results = ctx.execute(""""""
        WITH
            younguns AS (SELECT AVG(end_time - start_time) as avg_time FROM Divvy_Trips_2018_Q3 WHERE birthyear = 1991),
            everyone AS (SELECT AVG(end_time - start_time) as avg_time FROM Divvy_Trips_2018_Q3)
        SELECT younguns.avg_time, 'For 1991' AS cohort FROM younguns
        UNION
        SELECT everyone.avg_time, 'Everyone' AS cohort FROM everyone
    """""").with_columns(avg_time=pl.col(""avg_time"").dt.total_minutes())

    dash_app = dash.Dash(__name__)
    the_big_reveal = px.histogram(results, x='cohort', y='avg_time')
    dash_app.layout = html.Div(children=[
        dcc.Graph(figure=the_big_reveal),
    ])

    dash_app.run_server(debug=True)

if __name__ == ""__main__"":
    main()
And now I can walk into the boardroom, head held high, as I reveal:
Okay, so I'm not super proud of this, but I did get it done before the standup, so give me just a little credit. Anyway, PHB gets his report, and I get a few spare minutes of foosball before doing actual work.
Ultimately, I've made this a self-contained project that can probably be revisited in the future.
$ git add .
$ git commit -m ""Saving all the things""
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Round-tripping
With my lovely commit in place, let's try cloning a fresh copy of my repo and seeing if it all still works.
# Confirm that we're still in our project folder:
$ pwd
/media/development/sandbox/phb_really_needs_this

$ cd ..
$ git clone ./phb_really_needs_this ./phb_spike
Cloning into './phb_spike'...
done.

$ cd phb_spike
$ uv sync
Using CPython 3.12.3 interpreter at: /usr/bin/python3.12
Creating virtual environment at: .venv
Resolved 31 packages in 1ms
‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/29] Installing wheels...                                                                                                                                                                                                                                            warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.
         If the cache and target directories are on different filesystems, hardlinking may not be supported.
         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.
Installed 29 packages in 162ms
 + blinker==1.9.0
 + certifi==2025.1.31
 + charset-normalizer==3.4.1
 + click==8.1.8
 + dash==2.18.2
 + dash-core-components==2.0.0
 + dash-html-components==2.0.0
 + dash-table==5.0.0
 + flask==3.0.3
 + idna==3.10
 + importlib-metadata==8.6.1
 + itsdangerous==2.2.0
 + jinja2==3.1.5
 + markupsafe==3.0.2
 + narwhals==1.25.2
 + nest-asyncio==1.6.0
 + numpy==2.2.2
 + packaging==24.2
 + plotly==6.0.0
 + polars==1.22.0
 + requests==2.32.3
 + retrying==1.3.4
 + setuptools==75.8.0
 + six==1.17.0
 + typing-extensions==4.12.2
 + unzip-http==0.6
 + urllib3==2.3.0
 + werkzeug==3.0.6
 + zipp==3.21.0
 
$ uv run hello.py
Dash is running on http://127.0.0.1:8050/

 * Serving Flask app 'hello'
 * Debug mode: on
...and everything just works.
Working exclusively off intuition and a quickstart, I was not only up and running with a new project but also had a sane configuration that made committing and restoring my work painless in a way I'm not used to when I use Python.
I would absolutely recommend this tool to anyone who is coming to Python for the first time and just wants to write some code.
Managing Python versions with UV
So, a disclaimer: in my above example, I already had a version of Python installed, so I didn't think to check whether UV works without it. For the sake of CI/CD or other automations (or simply guaranteeing the reproducibility of a build) I'll fire up a new Docker instance to see what happens:
$ docker run --rm -it debian

root@f3d8a7310689:/# apt update && apt install -y curl
root@f3d8a7310689:/# curl -LsSf https://astral.sh/uv/install.sh | sh

# This is necessary after running the install script:
root@f3d8a7310689:/# source $HOME/.local/bin/env

# So, what did it install up to this point?
root@f3d8a7310689:/# uv python list
cpython-3.14.0a4+freethreaded-linux-x86_64-gnu    <download available>
cpython-3.14.0a4-linux-x86_64-gnu                 <download available>
cpython-3.13.2+freethreaded-linux-x86_64-gnu      <download available>
cpython-3.13.2-linux-x86_64-gnu                   <download available>
cpython-3.12.9-linux-x86_64-gnu                   <download available>
cpython-3.11.11-linux-x86_64-gnu                  <download available>
cpython-3.10.16-linux-x86_64-gnu                  <download available>
cpython-3.9.21-linux-x86_64-gnu                   <download available>
cpython-3.8.20-linux-x86_64-gnu                   <download available>
cpython-3.7.9-linux-x86_64-gnu                    <download available>
pypy-3.10.14-linux-x86_64-gnu                     <download available>
pypy-3.9.19-linux-x86_64-gnu                      <download available>
pypy-3.8.16-linux-x86_64-gnu                      <download available>
pypy-3.7.13-linux-x86_64-gnu                      <download available>

# And now, for the magic:
root@f3d8a7310689:/# uv init my_first_project
Initialized project `my-first-project` at `/my_first_project`

# I'm surprised it worked!  Did anything happen in that process?
root@f3d8a7310689:/# uv python list
cpython-3.14.0a4+freethreaded-linux-x86_64-gnu    <download available>
cpython-3.14.0a4-linux-x86_64-gnu                 <download available>
cpython-3.13.2+freethreaded-linux-x86_64-gnu      <download available>
cpython-3.13.2-linux-x86_64-gnu                   /root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/bin/python3.13
cpython-3.12.9-linux-x86_64-gnu                   <download available>
cpython-3.11.11-linux-x86_64-gnu                  <download available>
cpython-3.10.16-linux-x86_64-gnu                  <download available>
cpython-3.9.21-linux-x86_64-gnu                   <download available>
cpython-3.8.20-linux-x86_64-gnu                   <download available>
cpython-3.7.9-linux-x86_64-gnu                    <download available>
pypy-3.10.14-linux-x86_64-gnu                     <download available>
pypy-3.9.19-linux-x86_64-gnu                      <download available>
pypy-3.8.16-linux-x86_64-gnu                      <download available>
pypy-3.7.13-linux-x86_64-gnu                      <download available>
So there you have it - an out-of-the-box UV installation dealt with the boring details of managing my Python install. The README also
explains how to install other versions
as appropriate, as well as how to manage your versions within the scope of a project.
Working with an existing pip-based project in uv
Let's briefly look again at the README, specifically the section on
""The pip interface""
:
uv provides a drop-in replacement for common pip, pip-tools, and virtualenv commands.
...Migrate to uv without changing your existing workflows ‚Äî and experience a 10-100x speedup ‚Äî with the
uv pip
interface.
It then goes on to illustrate how you can run your commands by ""piping"" them through
uv
first, giving us illustrations with
uv pip
and
uv venv
.
In a nutshell, whatever you're doing today, keep doing it, but through the uv workflow, and apparently, you get good backward compatibility with legacy projects with many of the benefits it touts.
And, that ellipsis in my quote above is also worth discussing:
uv extends their interfaces with advanced features, such as dependency version overrides, platform-independent resolutions, reproducible resolutions, alternative resolution strategies, and more.
So how does that ""extends"" business work in practice? The interesting part is when you get into the docs and find
this little gem
:
uv does not rely on or invoke pip. The pip interface is named as such to highlight its dedicated purpose of providing low-level commands that match pip's interface and to separate it from the rest of UV's commands, which operate at a higher level of abstraction.
So, I take back what I said above about ""piping"" commands - it is a complete reimplementation of the tool. Above that is some additional text of note:
Please note these commands do not exactly implement the interfaces and behavior of the tools they are based on. The further you stray from common workflows, the more likely you are to encounter differences.
So, you run the risk of things not working if you're running some kind of Mickey Mouse operation. Consider yourself warned - or, better yet, just dump the old way of doing things and make the switch to UV.
In closing
UV abstracts away the messiness of managing Python-based projects and installations. Any performance benefits are fringe benefits to me, but they may be huge selling points to you. And, for all the ease of use that the tool offers, like any other paradigm shift, you'll maybe get some pushback from your peers or teammates who aren't yet ready to explore other options.
If I have learned anything about programmers, it's that they know what they know and hate what they don't.
So, while I personally think this is my go-to tool for all things Python, I also recognize that it's a departure from what many teams are already doing, and getting buy-in is likely a challenge. It is dual-licensed under both MIT and Apache 2.0, so it should at least pass most governance teams without much grief.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
154278517.duckdb-processing-remote-s3-json,DuckDB processing remote (s3) JSON files.,... just because we can,2025-01-08 07:05:00.331-06,"In a quest not to get too bored in the Data Engineering world, with Databricks and Snowflake being old hat these days ‚Äî it‚Äôs always good to go looking for interesting things to do. Interesting things yet obvious things, the simple things, things other people forget about with all the stars in their eyes.
I‚Äôve been enjoying poking at DuckDB these days when I get the chance; it‚Äôs a simple tool that exemplifies the best of Engineering ‚Ä¶
simple
fast
lightweight
Today, I was reading something on Reddit about crunching JSON files with PySpark. I‚Äôve been there. It was actually one of the first big Data Engineering projects I worked on. Processing millions of new JSON files showing up daily in an S3 bucket with AWS Glue and Athena. Ahhh ‚Ä¶ the good ole‚Äô days.
It made me wonder how DuckDB would handle this problem ‚Äî
JSON files in S3
. DuckDB is an SQL tool known for being extremely versatile in its use cases. Can we add JSON to the list?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
DuckDB with JSON files (in s3)
So, first things first, we need some sort of reasonably sized JSON dataset to work with for our little experiment, when in doubt, I always head over to GitHub and
peruse through the open-source dataset section
. I decided to poke at
Toronto open data
.
What could be more exciting
Toronto Island Ferry Ticket Counts
‚Ä¶ woohoo.
Once we get that data into S3, now we can get the meat and taters of what we want to do.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Reading JSON with DuckDB
So, I wasn‚Äôt sure what was going to happen when the two words DuckDB and JSON came into my mind, I figured as a SQL tool it might not have first class support for JSON files, but oh boy was I wrong.
Note: DuckDB is the GOAT for integrations of all shapes and sizes.
‚ÄúThe DuckDB JSON reader can automatically infer which configuration flags to use by analyzing the JSON file.‚Äù What a wonderful tool, a simple
READ_JSON()
is all we need??!!
Let‚Äôs giver er‚Äô the old one-two punch-a-roo.
All code is available on GitHub
.
And what happened?
Of course it worked you hobbit ‚Ä¶ ‚Äúoh ye of little faith.‚Äù I‚Äôve learned that DuckDB rarely disappoints with it‚Äôs integrations and features (
unless that something is OOM
).
Now before you just scroll on past and move on with life because the above code was so simple and tiny and the result so ho-hum, I beg you to stop a minute.
I remember the days when we had to unwind JSON files with code both ways uphill in the rain and snow, throw salt over our left shoulder and spit some blood on a dead crow at midnight to be able to run SQL straight on top of a JSON file(s).
I don‚Äôt know, it just seems incredibly helpful to be able to RUN SQL ON JSON files. In the past I would have written code to convert the CSV files to something else ‚Ä¶ CSV, Parquet, etc ‚Ä¶ and then after writing that code/pipeline ‚Ä¶ finally run some SQL on it with Spark or something else.
I mean isn‚Äôt there something just beautiful about that?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Not every tool can do something so easily. Take Polars for example. It can‚Äôt seem to understand the S3 file path out of the box.
I mean it‚Äôs good to know that apparently DuckDB is the class of it‚Äôs own when it comes to processing remote JSON files in cloud storage, which by the way, is a super common usecase.
Do you think it can handle multiple JSON files automatically? This is sort of important for a Production use case, there will be a lot of times when you have a whole bucket of JSON files needing to be processed.
I went ahead cloned that JSON file we were using into the same bucket again, let‚Äôs try to read both files at once.
All I did was add the ‚Äú*‚Äù wild card to the URI and it appeared to work, you can see our numbers for the same query doubled, just like we would expect. Wow, powerful!
All code is available on GitHub
.
Thoughts
Next time you have to deal with a bunch of JSON files, instead of writing a pipeline to transform that data and massage and unpack all that data into another format, why not reach for DuckDB?
It can cleanly read multiple JSON files in remote storage and is able to run SQL directly on those files with only a few lines of code.
It‚Äôs hard not to love DuckDB for this sort of incredible use case, even for someone who‚Äôs gotten tired of SQL after a few decades of writing it.
In my opinion, its stuff like this, the little things, that will make DuckDB a tool that is here to stay and a powerhouse of data processing for years to come.
What cool stuff have you done with DuckDB, have you used it all yet??"
153591412.aws-lambda-duckdb-and-delta-lake,AWS Lambda + DuckDB (and Delta Lake),simplicity in Data Engineering,2024-12-29 10:05:08.856-06,"I‚Äôm well aware volumes of words have been written about simplicity in the context of Software Engineering, ye ole‚Äô KISS concept that your grandmother taught you is as old as time. Yet the siren call of the modern data stack has lured many a poor soul down the proverbial Davey Jones locker of the data sea.
Let that not be us.
The idea of
simplicity
in the design of Data Platforms, Pipelines, ETLs, and data in general is incredibly under-appreciated. It is simply given lip service but not taught or followed in any real manner.
In fact, in the name of building resumes and being seen as smart, overly complicated designs are the common fare of the day, served up on Linkedin, Reddit, and
YouTube
to fill the bellies and itch the ears of the restless data practitioners.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
AWS Lambda + DuckDB. The Ultimate Data Pipeline?
I‚Äôve spent 15 years tinkering with data, and I would like to think I‚Äôve learned a thing or two in that time, although there are always a fair number of internet trolls who tell me otherwise.
When building and maintaining Data Platforms at scale and in production, one of the most overlooked pieces of the puzzle is the compute software and infrastructure.
We tend to get caught up in the shiny and popular and are decidedly
UNcreative
in our thinking about how to solve problems. We reach for what we‚Äôve always done and what has been done.
Overall, the Software and Data community seems to be starting to appreciate the possibilities of a more straightforward approach to data processing.
The simplicity of DuckDB.
If I were in the business of making predictions, which strangely, I do find myself in that position often, I would say one of the hottest rising stars for the next 5 years in Data Engineering will be
DuckDB
.
When it comes to simplifying a complex data world, DuckDB is the clear winner. This is probably one of the most under-appreciated aspects of DuckDB and needs to be discussed more.
For those of us who have fought complex and challenging to deal with architectures for years, trying to solve installs and integrations, DuckDB is uniquely powerful AND, at the same time, one of the most simple tools to manage and use.
pip install anywhere, anytime
no centralized configurations or metadata
powerful integrations inside DuckDB
cloud storage, databases, etc (read-writes)
The flexibility and extensibility of DuckDB is rare in the data world, and it‚Äôs capable of combining multiple tasks into a single tool.
Analytics
General ETL and Data Pipelining
SQL
Data Connector via Integrations
(ex, Postgres, Delta Lake, S3, etc, etc)
Instead of using various tools and Python packages to bring together disparate data sources and gain insights or further processing, you could simply use DuckDB for ALL that stuff.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Combining DuckDB with Lambda for the perfect data processing tool.
When on the topic of simplicity and DuckDB I think a natural companion to create the ultimate lightweight Data Stack is AWS Lambdas. I mean, can you imagine?
No heavy infrastructure to manage, no clusters to setup, no complex tooling to install and configure, talk about being able to concentrate on what really matters ‚Ä¶
providing business value
.
As someone who manages and has built many Data Platforms the appeal of this sort of lightweight setup is ‚Ä¶
reduced infrastructure costs
reduced infrastructure management
reduced complexity at all levels
Yes, there is always a tradeoff, like everything in life, there is no free lunch. But, the AWS Lambda and DuckDB combination is about as low cost of a lunch as you are going eat in this Data Engineering life.
On top of it all, the number of provided DuckDB integrations does not make it feel like a simple solution.
The number of DuckDB extensions is out of this world. Arrow, AWS, Delta, Excel, Iceberg, MySQL, Postgres, SQLite, etc, etc.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Enough chitter chatter, time to get to it.
So, without further ado, let‚Äôs show to the world the simplicity of AWS Lambda + DuckDB with a simple data pipeline. Let‚Äôs start with a ‚Äúraw‚Äù CSV hitting an S3 bucket, then using Lambda+ DuckDB to process the data into two different Delta Tables, one for general storage, another for summary analytics.
This type of data pipeline flow would typically be done with Airflow + Snowflake/Databricks/BigQuery etc + a lot of code.
Below is a conceptual data flow that we will be building.
We have a few tasks ahead of us ‚Ä¶
create AWS Lambda
create Lambda trigger for CSV file hitting S3
create two Delta Lake‚Äôs in S3
write the DuckDB code
ALL this code is available on GitHub.
I‚Äôm thinking we start working on these things in reverse, let‚Äôs start with writing the DuckDB code for the Lambda. Essentially when a file hits our S3 bucket, it will trigger the lambda, one of the arguments of automatically sent to our triggered lambda will be the s3 URI of the file that triggered. We will want to do two things ‚Ä¶
transform the data as needed with DuckDB and insert into a basic accumulating Delta Table.
Do some analytics to update a cumulative Delta Table
be reading the current state of the accumulating Delta Table and adding to those records the current records coming in, calculate some metrics for that day, and write the results back the cumulative Delta Table.
Let‚Äôs just get too it, you will see it in action if I lost you there.
Subscribe now
Building Dockerfile for AWS ECR to use with the Lambda.
First, we need to build a simple Docker image we can upload to ECR (
Elastic Container Registry on AWS
). I find this an easy and great option with it comes to deploying Lambdas on AWS when you want a simple architecture.
The reason for this is that building and deploying Docker images, even when you submit new code to a repository and can easily be automated with something like GitHub actions etc.
We can then build the image as follows ‚Ä¶
In case you were wondering what was in the
requirements.txt
file, it‚Äôs nothing special.
duckdb
deltalake
getdaft[deltalake]
numpy
pandas
We need some toy data that we can play around with for our little experiment. I often turn to the
Backblaze Hard Drive Dataset
that is free, it will work perfect for this.
The data is large and dirty, making it more real life than some classic ‚Äú
taxi cab
‚Äù dataset.
Creating Delta Lake tables on S3.
The next part of our project involves having two Delta Lake tables available in S3 that our AWS Lambda will interact with. Let‚Äôs go ahead and write some simple code to create those two tables.
I‚Äôm going to use Daft to create the Delta Lake tables, because it‚Äôs simple and easy. Here is the code to do that, remember we are building two tables ‚Ä¶
general ‚Äúaccumulating‚Äù table in a ‚ÄúFact‚Äù style.
analytics type daily ‚Äúcumulative‚Äù table in a ‚ÄúData Mart‚Äù style.
Luckily, Daft does a good job at picking up and sensing the data types on this massively wide dataset, keeping me from doing too much work. (
I‚Äôm going to select a subset of these columns just to speed this all up
).
Let‚Äôs just save this out as our accumulating style Delta Table in s3. Next, let‚Äôs make an analytics ‚Äú
cumulative‚Äù style table, it will basically hold a daily rollup.
Let‚Äôs a decent amount of work before we even get to our Lambda and DuckDB isn‚Äôt it? Oh well, we have our Delta Tables and data now, so we can finally get to it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Creating an AWS Lambda with DuckDB.
Ok, so now let‚Äôs write the core our Lambda code that will use DuckDB to process incoming CSV files that land in S3, so we can process them downstream into ‚Ä¶
accumulating table
cumulative table
Now I‚Äôm going to just throw all the code at you, and after you look it over we can talk about it.
ALL this code is available on GitHub.
Ok, so let‚Äôs just talk about of a few things, first, I think for the work we are doing, what we have here is a fairly simple and straight forward solution.
I mean sure, there is a little complexity with ‚Ä¶
messing with partitions
back and forth with arrow()
getting configs correct to write Delta Lake tables
getting data types correct to Delta and Dataframes play nice
But, considering the complexity of what we are doing ‚Ä¶ ingesting raw CSV files from S3 into a accumulating table, and calculating daily cumulative Delta Table ‚Ä¶ the solution is simple.
Most people doing ingestion of flat-files, CSV is this case, in a remote storage bucket, pushing that data into multiple Delta Lake tables would probably see that as a ‚Äúcomplex‚Äù problem requiring a ‚Äúheavy solution,‚Äù and come up with some serious architecture to support it ‚Ä¶ aka Apache Spark, Databricks, etc.
Clearly, if one thinks outside the box, there are much more light weight options ‚Ä¶ namely DuckDB and AWS Lambda.
Subscribe now
Things to note about DuckDB code with Lambda for Delta Lake tables (all in S3).
There is a lot to talk about here, almost too much, so I will just put together a list of benefits that I see here.
Lambda‚Äôs are a ‚Äúlight‚Äù solution in context of architecture.
also simple to maintain and automate
DuckDB makes it simplistic to read remote files in S3.
DuckDB‚Äôs use of default credential chain is nice
it could pickup creds from .aws or the ENV etc
The ability to materialize queries as Arrow datasets is awesome
Arrow datasets are universally integrated into many tools
DuckDB‚Äôs ability to read remote Delta Lake tables is seemless.
DuckDB‚Äôs ability to support complex SQL statements to manipulate data is first-class.
A little creativity is all that is needed to create easy, cheap, and lightweight solutions.
Finishing the project.
Ok, before we get carried away and pretend like we are done, let‚Äôs finish the work surrounding creating the AWS Lambda since we have our code finished up.
First, we need to create an
ECR Registry
that will hold our Docker image.
Next, we simply need to build our previous Docker image, and the push it our new ECR Repository.
Below is our simple Docker build statement.
Next we can login to our ECR.
And now we can
tag
and
push
our newly built image.
Next we need to create our AWS Lambda that will use this image, attach a trigger on s3 for when we get a new file in a certain bucket and prefix so our Lambda can process that data into our Delta Tables.
We could use the CLI to create the lambda, or Terraform, or Cloud Formation, or whatever, but let‚Äôs just use the UI so we can get his article finished before we all fall asleep.
Make sure the role you give the AWS Lambda either has, or afterwards gets,
access to read/write from S3
. Also, make sure the ‚Äú
timeout
‚Äù setting on your Lambda is something reasonable (default is 3 seconds, which isn‚Äôt much). Don‚Äôt forget about
memory size limits
as well.
Easy enough. Next, let‚Äôs add a Trigger.
Always good to double check the
Prefix
and
Suffix
for when to trigger the Lambda.
Next, to test our Lambda and Trigger, let‚Äôs upload a file to that location in S3!
Let‚Äôs go check the Logs on our Lambda and see what happened.
Looking good! All is well according to the logs.
To confirm we can go look at that S3 Delta Lake table for our ‚Äú
cumulative
‚Äù daily table and inspect the the files themselves and see that new partitions were actually written out, per below.
Also, let‚Äôs read and look at the table in code.
Looks just as we expected and wanted!!
ALL this code is available on GitHub.
Impressive and minimalist data stack if I do say so myself. Nothing is easier to work with than Lambda‚Äôs, and you don‚Äôt have to worry or think about them. They just run.
DuckDB? Well, you have to admit, it‚Äôs pretty slick for running any sort of simple or complex data transformations. Did you note how we were able to use DuckDB to read a CSV file into memory, read a Delta Lake table into memory, combine those datasets with a UNION and do calculations? All in just a few lines of code??
If Lake Houses are truly the future for us all, which they probably are, it‚Äôs clear that tools like AWS Lambda and DuckDB can play an essential role in building out any data stack.
NOTE
: If you need concurrency with Delta Lake and multiple writes,
check out the DynamoDB model
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks for your support of this Newsletter, it takes me a lot of time, blood, sweat, and tears. If you would consider becoming a paid subscriber, that helps me out a ton and allows me to pay for things like AWS services to bring you this content.
Subscribe now"
153392602.data-engineering-central-podcast-bbc,Data Engineering Central Podcast - 05,Episode 05,2024-12-20 07:20:14.104-06,"It‚Äôs time for another episode of the Data Engineering Central Podcast. In this episode we cover ‚Ä¶
AWS S3 Tables - What‚Äôs going on??
Databricks raises more money!!! When is IPO??
Are Data Contracts Dead? Were they ever alive??
The Lake House Storage Format Battle is heating up!
Thanks for being consumers of Data Engineering Central, your support means a lot, please share this Podcast with your friends, it helps the show a lot!
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
153329123.amazon-s3-tables,Amazon (S3) Tables, ... the truth of the matter,2024-12-19 07:06:53.132-06,"Vendor Lock in and ‚ÄúClosed‚Äù Technology.
S3 Tables are not a Lake House, they are also not commodity file storage on which you can build your Lake House. T
hey are an Amazon table lock-in.
Let me explain:
We live in a Lake House world don‚Äôt we? It‚Äôs fair to say that the death of the Data Warehouse happened slowly over time, but that the rise of storage format technologies like Delta Lake, Iceberg, and Hudi put the final nail in the proverbial coffin.
I know we all wish we lived in a hand-hewn cabin in the woods and drank our water from a clear bubbling stream, but alas, here we all are stuck in Data Land, and one of the hottest topics today is
AWS S3 Tables
, which were recently announced at reinvent.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
As soon as that new shiny toy was released, I did a deep dive, mostly technical, into the how and what of this addition to the data landscape. Y
ou can read about me setting up a brand new AWS S3 Table from scratch here
.
I will give you the bullet points from that initial poking around at S3 Tables ‚Ä¶
AWS S3 Tables are made to be exclusively used with AWS Products like Glue, Athena, EMR, etc.
It‚Äôs not made to be ‚Äú
open
‚Äù in any sense of the word.
S3 Tables lack
general
query engine and interaction support outside Apache Spark (
and favor EMR heavily
).
It‚Äôs a little bit of a black box.
Don‚Äôt be tricked by the ‚Äúwe used Apache Iceberg format‚Äù comments.
Because of ‚Äúauto-maintenance‚Äù and other factors, S3 Tables is a VERY expensive way to attempt to build a Lake House.
Lest you think I have an axe to grind, I do not. I examine tools from both an Engineering AND high-level Architecture viewpoint. I ask myself, ‚Äú
Could one, and SHOULD one, adopt this tool and build a Lake House on top of it
?
I‚Äôm going to save you some trouble and say No.
Why, you ask? Because it clearly lacks technical depth and support for general-purpose data engineering, which is required to build a Lake House.
Go back and read my technical review
, or play around with them yourself, you will come to the same conclusion.
Also, there is another
more philosophical and arguably more important reason.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The Lake House is distinguishable from a proprietary Data Warehouse because the
Lake House is open
. It is built on open storage, and since this storage is open, you can choose which query engine to use. Crucially, the Lake House requires both an open storage system with open table formats. Without both, you are locked into a proprietary platform.
Take a minute to think about that carefully.
Part of our job as Data Platform builders and engineers is to see the big picture. We are critical of the tools and systems we use and build on  top of.
The Modern Data Stack by definition is made up of a variety of tools that can seamlessly talk and work together to move, store, and transform data as needed.
Amazon S3 Tables aren‚Äôt S3 or a data lake
, they are in fact a proprietary table
API
for reading and writing tables; not reading and writing files.
Amazon S3 Tables can only be accessed by AWS services
. Anyone wanting to integrate with them must adopt their proprietary
API
through a 3rd party connector (
here
). They are not following open source Apache Iceberg standards.
Consequently, there is no support for Databricks, Snowflake, Starburst, ‚Ä¶, and I doubt there ever will be
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
But what about their built in maintenance and performance claims? Nonsense! The maintenance is nothing more than the same basic, unintelligent
automatic compaction
and
file deletion / clean-up
they already offer with Glue Catalog. You can turn it off, but you can‚Äôt use anything better‚Ä¶ because Amazon S3 Tables are not open to other engines.
As for the performance, there is nothing here that you can‚Äôt get, and more, with other engines with OSS Iceberg + S3. Everyone can 10x their S3 performance with prefixes (see the S3
user guide
). Look, I am all for competition in the Lake House market. May the best open Lake House win.
My ask to all the vendors is: Keep to the Lake House architecture,
keep the storage open
!
Don‚Äôt lock me into your proprietary tables!
I don‚Äôt want to go back to data warehouse lock-ins and price hikes!
I want the Lake House to stay open and competitive.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
153216139.how-to-write-better-pyspark-code,How to write better PySpark code.,... do it or else,2025-01-06 07:05:27.936-06,"I hate to say it because it makes me feel old, but I‚Äôve been writing PySpark code for ‚Ä¶ hmmm ‚Ä¶ I don't know, a long time. I mean, I was
writing blogs about Spark back in 2018
, it‚Äôs fair to say I was playing with it well before then.
Say, maybe 7-10 years or something? That‚Äôs long enough, and I still don‚Äôt consider myself a Spark expert by any stretch of the imagination, even if I have been writing it full-time for years and at the 100TB+ level.
That should tell you something about yours truly.
Anywho, as much as I would love to teach you how to write Spark‚Äôs Execution Plans and how to solve every slow pipeline problem with one or two tweaks, that‚Äôs not what the real world is like.
I have nothing against the savants of our age who wave their Spark wands, throw a little dust on their terminal, and can peer into the deeps of the Catalyst Optimizer and make those bits scream.
Me, I‚Äôm a simple man.
I have found that the key to Spark happiness is simply to follow the well-worn and boring path of clean, concise, and well-thought-out code and data.
Today, that will be my job: to lead you down the path of true Spark righteousness and
help you find salvation for our data soul in the simplest place possible.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
I have a number of random tips that have helped me over the years write good, clean Spark code that rarely breaks or grinds to a grinding halt. Although that does happen on occasion, that‚Äôs just life.
The following ideas are an unordered assortment of things that I have found generally helpful. I will give them to you upfront because I‚Äôm no snake salesman, and we can discuss them one by one.
Write modular and clean code
Break up complicated and long pipelines
Spend time data modeling (learn partitions, clustering, skew, etc)
Know when to cache() and Filter early and often
Share
Starting with modular clean Spark code.
This first tip is the most straightforward and obvious approach to writing better PySpark, and sometimes, I feel funny even saying it. Yet, time and time again, I see the exact opposite, so I never take it for granted.
Most of the Spark I‚Äôve run across in my life looks something like we see here below.
Just a giant Python file with a random collection of transformations directly in a large line that goes on and on until it‚Äôs impossible to make sense of what is happening.
hard coded crap
nothing is reusable or modular
it‚Äôs hard to read and follow after awhile
you lose track of what is happening where
Instead, we should strive to wring ALL PySpark code inside reusable and testable functions.
We can abstract away business logic, pass configurations in, and use the same method in multiple pipelines. This makes everything more readable and less error‚Äîand bug-prone.
Break up large pipelines into pieces.
Honestly, I can‚Äôt even count the number of times an oversized and way too complicated Spark pipeline has ground to a halt for no apparent reason. All of a sudden, the runtime goes from 40 minutes to 3 hours.
What do most people do? Start reading that nasty old Spark plan and waving wet Dogwood branches over a fire and saying incantations.
Do you know what the first thing you should do is? Give Spark a fighting chance to figure out what it actually needs to do (
believe it or not, in my experience, the Spark internals gets buggered up easily
). That chance is given by simply breaking up long and complicated pipelines into separate pieces were possible.
Even if it doesn‚Äôt solve your long runtimes (which it usually does), it makes the eventual feature additions, debugging, and general understanding of the codebase increase tenfold.
use multiple utility files broken up by logical units
break the pipeline physically from one into three different pipelines
Data Modeling is key.
This tip doesn‚Äôt immediately come to mind when people think about Spark, but it‚Äôs arguably the most important part of the code ‚Ä¶ even though it‚Äôs not the code. The code you write, how complicated it is, how well it works, what it looks like ‚Ä¶ it all depends on how the data is modeled.
Working with good datasets that are well modeled for the problems being solved reduces the computational burden of your Spark code, both mentally and physically.
You should become the master of ‚Ä¶
Partitions and Data Clustering.
Data Skew and Shuffle
Joins and Data Types
etc
If you have crappy data models, then your code will most likely be slow and crappy as well. Here are some articles on the topic of data modeling in the context of Big Data.
The Data World Revolves around Partitioning
Delta Lake Liquid Clustering vs Partitions
Data Skew, a Gentle Introduction
Data Modeling 101 ‚Ä¶ Part 1
Data Modeling 101 ‚Ä¶ Part 2
Data Modeling at Scale
Diving into Data Modeling
Learn to model your data well, and your Spark pipelines will fly.
Cache() and Filtering
These last two tips are probably the simplest things you can do to improve Spark's performance. Just so you know, it is usually the simple things that work.
cache()
filter early and often
I feel like cache() is the secret superpower that 80% of Spark pipelines need, something that would save countless amounts of money and runtimes. The problem is that when introduced to Spark for the first time, many newcomers fail to realize the Lazy nature of Spark‚Äôs execution.
What is cache()?
cache() is a transformation in Apache Spark that persists the DataFrame, Dataset, or RDD in memory. It stores the data the first time an action is performed, avoiding re-computation for subsequent actions.
When it comes to filtering, I can hear all the pundits telling me that Spark will figure it ‚Ä¶ ‚Äú
I promise you, it will know ‚Ä¶ it will ALWAYS figure it out for you.
‚Äù
Never take anything for granted, never assume that someone or something (Spark) will indeed do it perfectly and filter you data for you based on some piece of code 100 lines later.
The first thing you should always do when starting to write a new pipeline ‚Ä¶
SELECT only the columns you want
add as much to the WHERE clause as possible
filter EVERY SINGLE data source
Never underestimate the power of cache() with SELECT and WHERE clauses to clean up your code. Not only will your pipelines run fast, but there will be no ‚Äúhidden‚Äù logic that is just assumed. It will be obvious to everyone what the intent was.
Writing better Spark code.
I know these are the most boring Spark tips that anyone could possibly tell you. Sure, I could have told you to turn your
spark.shuffle.partitions
to be 1.5 times the number of cores on your cluster. But that‚Äôs not what I‚Äôve seen day to day all these years.
You could spend your time trying to read and learn all the Spark internals, get confused, mess with things that shouldn‚Äôt be messed with, put in a PR, and break the pipeline.
I‚Äôm not saying never do those things; I think you should get there, but I think you should start out nice and easy, with the basics.
If your Spark code is not modular, reusable, and has no unit tests, you‚Äôre not ready for the Big Time. If you don‚Äôt have ANY cache() going on anywhere, than you probably need to start there. If you aren‚Äôt not filtering EVERY SINGLE data source with a WHERE and specific SELECT clause ‚Ä¶ then you need to start there.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
153125113.data-skew-a-gentle-introduction,"Data Skew, a Gentle Introduction.",... just a little lopsided,2024-12-23 19:47:22.951-06,"Sometimes when then the cold rain is falling outside and leaves have long withered and fallen brown and dead from the trees, I look out the window on that cloudy and dreary day wondering if all things have changed since I first started my life in the Data World.
Maybe everything is doomed to grow old and decay, a new spring comes bringing forth bright green and exciting technologies, leaving the old wizened proverbs to die in ignominy, to be scorned by the throng of Data Engineering newcomers springing up from the eternal ground like grass.
One of those old teachings that I cut my teeth on, of which I nary hear a word spoken of, is the idea of ‚Äú
Data Skew.
‚Äù
I‚Äôm not sure what caused the death of this once great idea, maybe it‚Äôs the continued growth and smartness brought forth by Databricks and Delta Lake, could it be the dark magic called Liquid Clustering? Who knows.
I simply don‚Äôt know if Data Skew is something that is dead or alive, but either way, let‚Äôs pull ourselves up to a nice warm winter‚Äôs fire, and tell stories of the old days, days when
Data Skew
mattered. Maybe there is a lesson to be learned after all these years.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Introduction to Data Skew.
This is one of those topics that has probably slide into obscurity the minute that the Data Lake was replaced by the Lake House, I‚Äôm sure of it. Although I doubt the problem has gone away, it‚Äôs been hidden by yet another layer of abstraction upon abstraction, and half-fixed by features like
Liquid Clustering
.
Also, it‚Äôs clear that ‚Äú
Data Practitioners
‚Äù we may be called, yet a large number among our ranks still get too obsessed and drawn away to the bright and shiny lights of the next new toy, like
S3 Tables
, that get released like steady clockwork.
This causes folks to forget about the actual DATA itself, what it looks like, its volumes, frequency, and makeup, etc.
Data Skew is a very simple yet basic and fundamental concept.
Data Skew is simply the idea of one ‚Äúentity‚Äù having an outsized and large quantity of data, compared to its peers.
This is a broad concept and can encompass data stored physically on a disk or in a cloud storage bucket, even data distributed across memory in a cluster, like Spark.
So, maybe a more academic answer would be ‚Ä¶
In a
data engineering context
,
data skew
refers to an imbalance in the distribution of data across partitions or nodes in a distributed system. This imbalance can result in certain nodes or partitions handling significantly more data or processing work than others, leading to inefficiencies such as:
Increased Processing Time
: Overloaded nodes take longer to complete their tasks, slowing down the overall workflow.
Resource Inefficiency
: Underutilized nodes sit idle while overloaded nodes are bottlenecked.
System Instability
: The uneven workload can cause failures or degraded performance in the system.
Data Skew itself is half art and half science. Some of it happens because of poor or ‚Äúbad‚Äù Data Modeling ‚Ä¶ some of it is the way ‚Äúqueries‚Äù are constructed, and some of it could just be a real part of the way the ‚Äú
business operates
.‚Äù
Data Skew becomes very apparent ‚Äúat scale,‚Äù it‚Äôs hard to see the effects of data skew when working on small datasets, maybe even 10s of TBs of data, once you get into the 100s of TB+, it can become a real problem.
This is probably the reason many people are unfamiliar with Data Skew in real life ‚Ä¶ most people don‚Äôt work on Big Data.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Data Skew in code.
Let‚Äôs take a more hardened and code-oriented look at an extremely basic example of Data Skew. Let‚Äôs use Delta Lake and some Daft as an example.
Let‚Äôs use the open-source
Divvy Bike Trips dataset
as our guide.
This dataset is really just a list of all the bike rides that have taken place for some time series, with a bunch of metadata about those rides.
Doing without thinking.
Let‚Äôs say a Junior Data Engineer is given this dataset, told a new file will produced every month, and that someone in Product wants a Dashboard eventually to answer some questions about the most popular Stations being used, so more bikes can be staged there.
In a moment of genius, that junior Engineer decides to setup a new Delta Table as follows.
I‚Äôm not sure how many of you have been around data of size, data that requires partitioning or clustering. This is a very common pattern in Big Data systems.
In practice a data set has one or more ‚Äúaccess patterns,‚Äù something that could be described as the ‚Äúcommon‚Äù way that data in a system is analyzed, this will vary widely based on the different business use cases.
To hammer this idea home, here are some simple examples.
1.
Time-Based Partitioning
Example:
Partition by
year
,
month
,
day
, or
hour
.
Use Case:
Log data, IoT sensor data, ride-sharing data.
Benefit:
Queries involving date ranges (e.g., last 7 days, monthly reports) are accelerated by scanning only relevant partitions.
2.
Geographical Partitioning
Example:
Partition by
region
,
country
, or
city
.
Use Case:
Sales data, weather data, or location-based services.
Benefit:
Limits query scope to specific geographic areas, reducing unnecessary reads.
3.
Category or Type-Based Partitioning
Example:
Partition by
product_category
,
device_type
, or
event_type
.
Use Case:
E-commerce product data, clickstream data, or event logs.
Benefit:
Accelerates queries by narrowing down to specific categories.
You are smart folk, you get the idea. The query access patterns will mostly likely drive how the data is stored so as to provide efficient access to queries.
The idea of partitioning data physically, as per the old Hive-style format is slowly dying a death by a thousand cuts given to it by Clustering. But, Clustering still has a ways to go in many places, Partitioning it still the lord of the day.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Going back to our example.
Let‚Äôs circle back to our Delta Lake table we setup before we went down the rabbit hole. We can open some of that
Divvy Bike Trips dataset
and store it into our table, based on it‚Äôs partitioning and than inspect what is going on behind the scenes in relation to Data Skew.
To do this we will use the unsung GOAT of Dataframes called
Daft
. If you would like to follow along with the generic Python Docker image, run the following ‚Ä¶
Now we can drop into Python and read our CSV file from our bike trip data and write to a Delta Lake table.
Easy enough, notice the partition columns we specified, using the
start_station_id
because that‚Äôs what our Junior Engineer thought of when they heard that someone might want a report based on what stations people ride from often.
Physical nature of data skew.
Now if we go look at the physical location of our Delta Table, we will see something that gives us a hint something bad might be happening with our data.
There is a plethora of folders ‚Ä¶ because large number of partition values ‚Ä¶ remember we picked
start_station_id
.
Let‚Äôs do a little Bash script that will get us the list and sizes of all the sub folders in our Delta directory to see what‚Äôs going on behind the scenes.
And the results ‚Ä¶
As one would expect, in our simplistic example, we can see a serious range of values in the
SIZE
of the data distributed across our partitions (
start_station_id
). Aka ‚Ä¶ some files/partitions have a large amount of data, some have very little.
What we have just seen is essentially Data Skew, and imagine instead of our little play dataset we were talking Delta Table(s) with hundreds of terabytes of data.
Put yourself in the place of Spark. Yes you. You are a PySpark script and you are given a set of work to do, maybe a good old
GroupBy(‚Äòstart_station_id‚Äò).
Why is this a problem?
How Skew Affects GroupBy Operations:
Partition Overload:
PySpark's
groupBy
involves shuffling data across the Spark cluster made up of many machines. If one key (
partition
) has significantly more records, the corresponding partition will receive a massive amount of data, leading to:
Memory overflow
(
causing OOM problems
).
Task Straggling
‚Äì One partition takes much longer to process while others finish quickly, slowing down the entire job. ‚Üê
very common
Wasted Resources:
Nodes processing small partitions remain idle, while nodes handling skewed partitions are overloaded.
Increased Shuffle Size:
More data is shuffled across the network for skewed keys, increasing I/O and slowing down performance
We can see the same ‚Äúskew‚Äù in memory if we inspect the Daft Dataframe.
This isn‚Äôt a lesson about distributed systems like Spark, but it really helps us understand the
fundamentally simple problem of Data Skew can cause serious complication problems downstream.
And it is a simple problem at it‚Äôs core isn‚Äôt it? It‚Äôs really about correct Data Modeling, understanding query access patterns, and thinking ahead to understand data before simply doing a thing.
Generally speaking, in a perfect world, we want all our data split ‚Äúevenly‚Äù across whatever storage system we are using.
That could be in the form of ‚Ä¶
files in s3 buckets
data inside parquet files
etc
Again, this problem doesn‚Äôt ‚Äúshow up‚Äù a lot in small data, say anything a few hundred GBs and under, our modern fast (
Rust?
) processing systems can over come a reasonable amount of Data Skew without us even noticing. But, it is still a critical and fundamental competency that every Data Practitioner should try to understand.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
In a future post we will dive into Liquid Clustering, the newcomer on the block that promises to reduce Data Skew problems, and we shall find the truth will we not?"
152810219.why-is-everybody-so-big-on-zig,Why Is Everybody So Big on Zig?,Guest Post by Anonymous Rust Dev,2024-12-09 07:51:46.856-06,"Anonymous Rust Dev back with an exploration of Zig... Maybe I've been watching too much
Primagen
, or maybe it's just because
r/rust
can't ditch its fascination with the language. Whatever the driving force, I find it increasingly harder to ignore Zig. But, I have to admit, I don't know it well enough to really say whether I should care about it (or be jumping ship, for that matter) in the first place.
So today, I'll be getting my own feet wet. I'm going in largely blind - I've heard people talk about it, and encountered some stuff on the internet, but never really explored or considered it before now. Now, with bated breath...
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
First off, what's Zig's deal?
I mean, where do you get the name Zig in the first place? Does your code make you zig-zag your way through a solution? Are we building a ziggurat to ascend the heavens (or encountering
robots
)? It wasn't clearly spelled out on their
webpage
, but
here
is what Wikipedia has to say:
The name 'Zig' was reportedly chosen through a process involving a Python script that randomly combined letters, starting with the letter 'Z' and followed by a vowel or 'Y', in order to generate four-letter words. Despite the intended length, 'Zig', a three-letter word, was ultimately selected from the various combinations produced by the script.
That's... anticlimactic. I guess it's better than just picking a letter (looking at you, C, R, and S languages), and it's nice that it also doesn't have a lot of baggage behind the name to cause problems down the road.
Well, what does Zig have to say about itself? Straight away in the
documentation
, it lists some goals:
Robust
- Behavior is correct even for edge cases such as out of memory.
Optimal
- Write programs the best way they can behave and perform.
Reusable
- The same code works in many environments which have different constraints.
Maintainable
- Precisely communicate intent to the compiler and other programmers. The language imposes a low overhead to reading code and is resilient to changing requirements and environments.
Well, shucks; they don't specifically mention wanting to be the new Rust. I came here looking for a fight and all I got was this reasonable statement. Actually, those four points sound like a fantastic rationale for a new language.
Let's do the needful and tell the world hello
It's practically mandatory to do a Hello, World app. I mean, it's literally at the top of Zig's documentation, so I'm obligated to go through the motions. But, before that, I'm pretty sure my computer doesn't even have a Zig compiler yet. How do I get there?
Well, doing to their
download
page, they have some .zx and .zip files for releases. Or, as they also suggest, it's possible to use package managers to install. I'm lazy and on Ubuntu, so I'll take the latter route.
$ snap install zig --classic --beta
And, as of the latest documentation, here's the recommended hello code (calling it
hello.zig
and sticking it in some random folder):
const std = @import(""std"");

pub fn main() !void {
    const stdout = std.io.getStdOut().writer();
    try stdout.print(""Hello, {s}!\n"", .{""world""});
}
...and from the CLI:
$ time zig build-exe hello.zig
# zig build-exe hello.zig  0.62s user 0.13s system 101% cpu 0.743 total
Some messages showed up and subsequently disappeared. This is already starkly different from Rust's compiler, which writes a book and gives you its life story in its output. Looks like it created a
hello.o
and
hello
executable:
-rwxrwxr-x  1 rustdev rustdev  2241544 Dec  7 06:18  hello
-rw-rw-r--  1 rustdev rustdev  2029136 Dec  7 06:18  hello.o
-rw-rw-r--  1 rustdev rustdev      152 Dec  7 06:17  hello.zig
Okay, that's got me curious how the equivalent Rust app might go:
fn main() {
    println!(""Hello, {}!\n"", ""world"");
}
time rustc hello.rs
# rustc hello.rs  0.04s user 0.04s system 102% cpu 0.080 total
-rwxrwxr-x 1 rustdev rustdev 3832696 Dec  7 06:24 hello
Oof, my bones! Maybe if I try a ""release"" build:
$ time rustc -C debuginfo=0 -C opt-level=3 hello.rs
# rustc -C debuginfo=0 -C opt-level=3 hello.rs  0.05s user 0.04s system 100% cpu 0.092 total
-rwxrwxr-x 1 rustdev rustdev 3828160 Dec  7 06:29 hello
Well, looks like Zig wins the file size award. Oh, and in case you're wondering, the output is predictably correct in both the Zig and Rust apps, no shockers there.
So what did I just write?
Or...
:cough:
copy-and-paste, I should say. I didn't really write anything myself, but I should probably at least take a brief look at the structure of our Hello, World app.
const std = @import(""std"");
First, the
std
import has some notable things going on:
I see the
const
keyword - right out of the gate, the docs want me to prefer immutability
Imports (at least, so far) are top-level assignments, not relying on a compiler keyword as with many other languages (Rust, Python).
The
@import
doesn't look like a normal function call; I'll bet there's some kind of magic with it (e.g. built-in?)
Reading
further into the docs
, I see that we can also pass a relative path to another .zig file.
Semicolon-delimited statements. As is proper.
Writing this feels natural after years of CommonJS imports.
pub fn main() !void { .. }
As with Rust, we're writing top-level functions; contrast this with something like C# or Java, which either doesn't allow it or uses syntactic sugar to hide your code in a class
Odd seeing a visibility modifier on
main
- is this explicitly required?
(Yes, I tried removing it and got an error)
Very interesting seeing the
!void
return type. Also, in the docs' subsequent hello_again.zig example, they drop the
!
prefix on the return, which leads me to believe this is some kind of error-handling mechanism.
(Correct, confirmed by the footnote under that example code)
Okay, my hat's off to this function signature - we know it's public, has no (void) return, and is fallible. Also, where Rust explicitly handles errors via its type system (Option, Result), I'm also inferring that this language is going to do something more traditional like a try/catch - feels like a move backward, but let's reserve judgment for now.
Scanning
further down
the documentation, the language feels like a blend of several others I've used in the past. As noted above, I'm getting some strong JS vibes from the
const
and
var
assignments that I'm encountering, the Rust-like syntax of function and variable declarations (among other idiosyncrasies), some C#/TypeScript feels for the optional types...
And then, jumping down to
structs
, looks like types themselves are effectively variables (e.g.
const Point = struct { .. }
). Everything's either a variable or a function.
First opinions
Okay, I've gone far enough to develop some opinions. This language is trying to learn from others, and is basically a wishlist of features or things that people have been burned by. It's seemingly guided by some intentional choices that are surely driven by people who themselves have some strong opinions.
First, it's easy to understand the code with either Rust or C chops. Just scanning the docs sidebar for the assorted topics in play, this language feels like Rust in ""easy mode."" Jumping down to
Lifetime and Ownership
:
It is the Zig programmer's responsibility to ensure that a pointer is not accessed when the memory pointed to is no longer available.
FULL STOP.
It occurs to me that I have yet to figure out how memory management works in this language. I'm getting a bit scared, but now I'll take a moment to
read
, and...
Oh my, what have I stepped into? I take back what I said about ""easy mode"":
Zig has a general purpose allocator available to be imported with std.heap.GeneralPurposeAllocator. However,
it is still recommended to follow the Choosing an Allocator guide
.
Wait, so now I am supposed to know how to choose an allocator? Reading between the lines through this section of the docs, I get the impression that I
could
just use the out-of-the-box allocator for dynamic memory needs, but it's highly discouraged for reasons. If you're coming from C, then
malloc
(which is also exposed in Zig) is almost certainly what you're using - Zig, for whatever reason, instead wants you to be more deliberate about your choice.
Also, it was tough to piece together how memory is disposed of. In Rust, for instance, drops happen automatically when something goes out of scope (assuming you didn't leak it). This is aided by smart containers (e.g.
Vec
,
Box
) which handle allocations and releases through the magic of RAII.
Zig instead leans more towards C's manual memory management model, where you explicitly request and release memory - and, this is further complicated with their
defer
clause that assists in handling cleanup operations. In short, the language wants you as the developer to own the responsibility of requesting and releasing memory, whereas GC-based languages or Rust instead want you to avoid manual memory management as much as possible.
Or, to put it another way, where many languages depend on the language itself to handle memory needs, Zig prefers that you stick to established conventions. To me, this is where C++'s slow-motion pending doom comes from - it trusts the developer to remember to do the right thing.
And, at least from what I've seen so far, there are no ""memory police"" knocking on your door to let you know when you've screwed up. If you're comfortable with spinning plates, all the more power to you, but I think I won't be jumping on this particular bandwagon in my day job - the safety of Rust has spoiled me.
What about something more useful?
I'm not reasonably going to learn a language in the span of a couple of hours - at least, not comprehensively, or to an extent that would let me put it on a resume. That said, I think I know enough now to at least attempt to solve a real-world problem with Zig.
In particular, the muscles I want to flex here include:
Writing and calling functions
Working with dynamic memory in some capacity
Working with structured data and intrinsic types
Using code from a community ecosystem (e.g. library repositories)
Considering my audience, I think maybe a JSON -> XML conversion would be a good exercise. This
""Phones 2024"" Kaggle dataset
should serve as a good starting point for this task.
The Rust version
First, I'll solve this problem in Rust, both to let me think through the problem as well as to establish a performance baseline against which to compare our Zig-based approach.
My dependencies:
[dependencies]
serde = { version = ""1.0.215"", features = [""derive""] }
serde_json = ""1.0.133""
xml = ""0.8.20""
The
serde
and its JSON component should be familiar to Rust developers at this point. The
xml crate
, on the other hand, might be a bit more niche and perhaps requires some explanation: it's a streaming writer (meaning we write open and close tags as necessary, rather than creating a hierarchical data structure). It's performant, but writing documents is a bit tedious since we don't have the same serde magic available to us and instead need to hand-roll a lot of writing logic. It took a bit of effort to write this one, since the examples stream directly to stdout while I capture the results in a string and couldn't find good examples of this. That said, Rust crates like this one typically do a good job of explaining how to use their code through example applications.
To see my code, it's at the bottom of this article. As for performance:
$ cargo build --release
$ time ./target/release/untitled > out.xml  # yes, I named my project ""untitled""
# ./target/release/untitled > out.xml  0.14s user 0.04s system 99% cpu 0.179 total
$ stat out.xml
#   File: out.xml
#   Size: 26373577        Blocks: 51512      IO Block: 4096   regular file
# Device: 252,0   Inode: 40007657    Links: 1
# Access: (0664/-rw-rw-r--)  Uid: ( 1000/  rustdev)   Gid: ( 1000/  rustdev)
# Access: 2024-12-07 09:23:15.909131926 -0600
# Modify: 2024-12-07 09:23:15.908131894 -0600
# Change: 2024-12-07 09:23:15.908131894 -0600
#  Birth: 2024-12-07 09:03:02.375187495 -0600
The Zig version
My naive plan is to first read the input file, parse it in-memory, process it into XML somehow, and as with the Rust version I'll echo out the results to stdout (allowing for piping to a file at the CLI level).
For the first part, I found a great writeup
here
explaining how to do the read and parse steps. And, after reading it, I decided to keep looking for another solution.
Then, I found
this article
, which described how to parse typed JSON. I read this, went back to the Zig docs a few times, and struggled for a bit. Basically, I figured out that the Zig standard libs have some ability to parse typed JSON, so I started trying to model my data.
const Phone = struct {
    // ???
};
What are my types? First, here's the structure of my data in the Rust version:
type Spec = HashMap<String, String>;
type VendorPriceDetail = HashMap<String, Spec>;
type Pricing = HashMap<String, VendorPriceDetail>;

#[derive(Clone, Debug, Deserialize)]
struct Phone {
    phone_brand: String,
    phone_model: Option<String>,
    price: Option<String>,
    specs: HashMap<String, Spec>,
    pricing: Option<Pricing>,
}
Here, I aliased some types just to make my life a bit easier, but in a nutshell the
.specs
and
.pricing
are dictionaries of dictionaries, with the added caveat that
.pricing
is also not always present on every object. I think the Rust model does a so-so job of explaining itself, though the data model itself was gnarly and dictated a lot of the complexity.
To get the same thing in Zig, I first had to find an example of how dynamic strings are stored. That last article I linked to shows a
Foo
struct with a
.c
property for reference, so I borrowed its approach (
[]u8
) to represent what I assume is UTF8-based text of indeterminate length.
const Phone = struct {
    phone_brand: []u8,
    // ...
};
I took a stab in the dark and assumed that the
.phone_model
(an optional string) would work similarly but with the
?
prefix on the type, and the compiler was okay with this. Similarly, the
.price
field:
const Phone = struct {
    phone_brand: []u8,
    phone_model: ?[]u8,
    price: ?[]u8,
    // ...
};
Now, I'm at a crossroads. In Rust, I had a data structure in the standard library (HashMap) that could represent key-value sets. Did some more googling and sleuthing to discover that
std.json.Value
could function as a catch-all, and so I stubbed out the remaining fields in this way:
const Phone = struct {
    phone_brand: []u8,
    phone_model: ?[]u8,
    price: ?[]u8,
    specs: json.Value,
    pricing: ?json.Value,
};
Next, let's read a file and parse it. This was a headache; I bounced all over the internet trying to scrape these bits:
pub fn main() anyerror!void {
    const stdout = std.io.getStdOut().writer();
    var gp = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gp.deinit();
    const allocator = gp.allocator();

    // Using hard-coded path for now; looks like there's a bit of work to use relative paths.
    const file = try std.fs.openFileAbsolute(""/home/rustdev/Downloads/phones_2024.json"", .{});
    defer file.close();

    const stat = try file.stat();
    const json_text = try file.reader().readAllAlloc(allocator, stat.size);
    defer allocator.free(json_text);

    try stdout.print(""{s}"", .{json_text});
}
I've gotta say, the few lines of code you see above you were the product of a LOT of work. Most of it was sleuthing, but there was one implementation you won't see that had streaming buffered reads that caused a memory leak somewhere. In the process, I did learn a lot about both the compile-time and run-time checks that Zig performs - it's not the same kind of meaningful feedback I'm used to from Rust, but there are some guardrails in place.
Of particular interest is the use of
defer
to ensure that, after scope closes, the behavior that follows is run. This of course means that you can not only dictate if your resources are cleaned up, but also when it happens - in Rust or GC languages, you would typically try to not dip your toes into these waters, but here it's all but mandatory.
The Zig convention is to immediately follow an allocation with its corresponding teardown, but you're free to decide if this is the way you'll choose to go (and there may be reasons, like return values, where you don't want to tear things down).
The other thing to note is the explicit use of the allocator in the
readAllAlloc
function. Basically, you either use the allocator to directly summon some memory, or you give it to functions or containers (e.g.
ArrayList
initializer). Technically, in Rust there's a global allocator already wired up for you by default, so this isn't a novel idea, but as with resource teardown the language also expects you to own the allocator itself as a resource.
Well, we're here now, with that
json_text
just waiting to be deserialized. I add the following to my
main()
body:
const parsed = try std.json.parseFromSlice([]Phone, allocator, json_text, .{});
defer parsed.deinit();
I then get some ""error: MissingField"" notifications, which it turns out means I have to go back to my struct and provide default values for optional fields (in my case,
null
):
const Phone = struct {
    phone_brand: []u8,
    phone_model: ?[]u8 = null,
    price: ?[]u8 = null,
    specs: json.Value,
    pricing: ?json.Value = null,
};
Now, it compiles and runs without error.
(I really want to quit at this point, but I'm pushing on...)
Only, now I don't know how to write an XML file in Zig. Unlike JSON, it doesn't seem like there's a first-class citizen here, so it's off to the third-party ecosystem. And here is where my struggles
really
begin...
I found one library (
https://ianjohnson.dev/zig-xml
) that looked like it would fit the bill. Only, keep in mind my Zig experience can be measured in the single digits of hours, so it's hard for me to slog through autogenerated documentation. I've thus far figured out that I can create a
Writer
by calling
.init
with a sink parameter. Except, WTF even
is
a sink? Well, I bust out the source code for the library and find a test that shows me I don't even need to worry about that, there's a better way to instantiate stuff.
So, that's an important point: like Rust, you can't rely on docs in Zig to tell you everything - inevitably, the only place that has the answers you need is the source code itself.
In the end, I managed to fumble my way through the XML API, and my source actually ended up closely resembling the Rust version. It turns out that both XML libraries follow a similar streaming philosophy, and so while it's not an exact 1:1 I did find myself using the Rust implementation as a template for how to conquer the Zig attempt.
As with before, I'm putting my source code at the bottom.
$ zig build-exe convert.zig
$ time ./convert > out.xml
# ./convert > out.xml  0.88s user 0.08s system 99% cpu 0.960 total
...Err, that doesn't look so good. Let's try this again with a compiler optimization flag:
$ zig build-exe -O ReleaseSafe convert.zig
$ time ./convert > out.xml
# ./convert > out.xml  0.14s user 0.06s system 99% cpu 0.203 total
Hey, much better! And the file size:
$ stat out.xml
#   File: out.xml
#   Size: 27593583        Blocks: 53896      IO Block: 4096   regular file
# Device: 252,0   Inode: 36582675    Links: 1
# Access: (0664/-rw-rw-r--)  Uid: ( 1000/  rustdev)   Gid: ( 1000/  rustdev)
# Access: 2024-12-07 15:05:14.294084261 -0600
# Modify: 2024-12-07 15:14:00.368668175 -0600
# Change: 2024-12-07 15:14:00.368668175 -0600
#  Birth: 2024-12-07 15:05:14.294084261 -0600
The Rust-based version had more padding (4 spaces, to Zig's 2), hence the slight size difference.
Lessons learned
Okay, first, this was painful. Zig is deceptively difficult - it looks easy at first glance, but once you understand what the ask is (you own the pain), you can see the language for what it is. At least, what I
think
it is: a streamlined alternative to Rust and C that gives the high-level language benefits of the former with the control and finesse of the latter. It feels less general-purpose to me than either language, and probably takes a special kind of person to really get the most out of it, but it's never a bad thing to have another gladiator in the ring.
It does some things the hard way (e.g. the allocator business). Other things, such as its optional value handling, are executed in a way I really appreciated but at the same time disliked -
as you'll see in my code, there is an alarming arrow antipattern emerging from my handling of optional values
(actually, this is just as much a problem in Rust)
; this might be something that goes away with more education on my part, or better overall design.
The type system had me somewhat stumped at first - trying to pass a slice by reference to a function, for instance, had me pulling my hair out, and I opted not to get bogged down in that detail. I didn't spend much time on getting my tooling configured, so I didn't have the usual LSP helpers to give developer insight.
I never got to play with tuples, and my amateurish fumbling only just barely got me through the union type that was
std.json.Value
. I'm still not happy with the fact that I used that type for my data modeling and suspect there was a better alternative; it would have surely made my code a lot cleaner to go with something else.
I'm not sure about modules - from what I could tell, you're expected to just drop libraries into a
lib
folder and reference them with
@import
- I wasn't able to find much evidence of a unified repository or cohesive ecosystem for third-party code, which given the language's age is somewhat to be expected. This is probably where git modules will become helpful for many.
If you're already using Rust, I don't see you jumping ship any time soon. If you're doing modern C++, I suspect you'll pick Rust over Zig for the RAII constructs. If you're a C developer, though, I think Zig is absolutely worth the exploration.
As for myself, I'm probably done with Zig. This was a worthwhile venture, if only to better understand alternative ways of accomplishing the stuff I do daily, but I don't personally see a case for what seems to be a lateral move.
(Appendix: Rust source)
use std::collections::HashMap;
use serde::Deserialize;
use xml::EmitterConfig;
use xml::writer::XmlEvent;

type Spec = HashMap<String, String>;
type VendorPriceDetail = HashMap<String, Spec>;
type Pricing = HashMap<String, VendorPriceDetail>;

#[derive(Clone, Debug, Deserialize)]
struct Phone {
    phone_brand: String,
    phone_model: Option<String>,
    price: Option<String>,
    specs: HashMap<String, Spec>,
    pricing: Option<Pricing>,
}

fn main() {
    let phone_json = std::fs::read_to_string(""/home/rustdev/Downloads/phones_2024.json"").unwrap();
    let phones: Vec<Phone> = serde_json::from_str(&phone_json).unwrap();

    let mut writer = EmitterConfig::new().perform_indent(true).create_writer(Vec::new());

    writer.write(XmlEvent::start_element(""phones"")).unwrap();

    for phone in phones {
        for event in write_phone(&phone) {
            writer.write(event).unwrap();
        }
    }

    writer.write(XmlEvent::end_element()).unwrap();

    let xml = String::from_utf8(writer.into_inner()).unwrap();

    println!(""{}"", xml);
}

fn write_phone(phone: &Phone) -> Vec<XmlEvent> {
    let mut phone_events = vec![];
    let mut event = XmlEvent::start_element(""phone"")
        .attr(""phone_brand"", phone.phone_brand.as_str());

    if let Some(phone_model) = phone.phone_model.as_deref() {
        event = event.attr(""phone_model"", phone_model);
    }

    if let Some(price) = phone.price.as_deref() {
        event = event.attr(""price"", price);
    }
    phone_events.push(event.into());

    phone_events.extend(write_specs(phone));
    phone_events.extend(write_pricing(phone));

    phone_events.push(XmlEvent::end_element().into());

    phone_events
}

fn write_specs(phone: &Phone) -> Vec<XmlEvent> {
    let mut spec_events = vec![];

    spec_events.push(XmlEvent::start_element(""specs"").into());
    for spec in &phone.specs {
        spec_events.push(XmlEvent::start_element(""spec"").attr(""name"", spec.0.as_str()).into());

        for detail in spec.1 {
            spec_events.push(XmlEvent::start_element(""item"")
                .attr(""name"", detail.0.as_str())
                .attr(""value"", detail.1.as_str())
                .into()
            );
            spec_events.push(XmlEvent::end_element().into());
        }

        spec_events.push(XmlEvent::end_element().into());
    }
    spec_events.push(XmlEvent::end_element().into());

    spec_events
}

fn write_pricing(phone: &Phone) -> Vec<XmlEvent> {
    let mut spec_events = vec![];

    if let Some(pricing) = &phone.pricing {
        spec_events.push(XmlEvent::start_element(""pricings"").into());
        for region in pricing {
            spec_events.push(XmlEvent::start_element(""pricing_region"").attr(""name"", region.0.as_str()).into());

            for vendor in region.1 {
                spec_events.push(XmlEvent::start_element(""vendor"")
                    .attr(""name"", vendor.0.as_str())
                    .into()
                );

                for item in vendor.1 {
                    spec_events.push(XmlEvent::start_element(""item"")
                        .attr(""name"", item.0.as_str())
                        .attr(""value"", item.1.as_str())
                        .into()
                    );
                    spec_events.push(XmlEvent::end_element().into());
                }

                spec_events.push(XmlEvent::end_element().into());
            }

            spec_events.push(XmlEvent::end_element().into());
        }
        spec_events.push(XmlEvent::end_element().into());
    }

    spec_events
}
(Appendix: Zig source)
const std = @import(""std"");
const xml = @import(""./zig-xml-main/src/xml.zig"");  // https://github.com/ianprime0509/zig-xml git archive
const json = std.json;

const Phone = struct {
    phone_brand: []u8,
    phone_model: ?[]u8 = null,
    price: ?[]u8 = null,
    specs: json.Value,
    pricing: ?json.Value = null,
};

pub fn main() anyerror!void {
    const stdout = std.io.getStdOut().writer();
    var gp = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gp.deinit(); // Haven't seen this before, but looks smart to me
    const allocator = gp.allocator();

    // Using hard-coded path for now; looks like there's a bit of work to use relative paths.
    const file = try std.fs.openFileAbsolute(""/home/rustdev/Downloads/phones_2024.json"", .{});
    defer file.close();

    const stat = try file.stat();
    const json_text = try file.reader().readAllAlloc(allocator, stat.size);
    defer allocator.free(json_text);

    const parsed = try std.json.parseFromSlice([]Phone, allocator, json_text, .{});
    defer parsed.deinit();

    var xml_out = std.ArrayList(u8).init(allocator);
    defer xml_out.deinit();
    const out = xml.streamingOutput(xml_out.writer());
    var writer = out.writer(allocator, .{ .indent = "" "" });
    defer writer.deinit();

    try writer.elementStart(""phones"");

    for (parsed.value) |phone| {
        try write_phone(&writer.writer, &phone);
    }

    try writer.elementEnd();

    try stdout.print(""{s}"", .{xml_out.items});
}

fn write_phone(writer: *xml.Writer, phone: *const Phone) !void {
    try writer.elementStart(""phone"");
    try writer.attribute(""phone_brand"", phone.phone_brand);

    if (phone.phone_model) |phone_model| {
        try writer.attribute(""phone_model"", phone_model);
    }
    if (phone.price) |price| {
        try writer.attribute(""price"", price);
    }

    try write_specs(writer, &phone.specs);

    if (phone.pricing) |pricing| {
        try write_pricing(writer, &pricing);
    }

    try writer.elementEnd(); // end phone
}

fn write_specs(writer: *xml.Writer, specs: *const json.Value) !void {
    try writer.elementStart(""specs"");

    for (specs.object.keys()) |key| {
        const maybe_spec = specs.object.get(key);

        if (maybe_spec) |spec| {
            try writer.elementStart(""spec"");
            try writer.attribute(""name"", key);

            // Too lazy to name the key2 better, and the compiler hates shadowing
            for (spec.object.keys()) |key2| {
                const maybe_item = spec.object.get(key2);

                if (maybe_item) |item| {
                    try writer.elementStart(""item"");
                    try writer.attribute(""name"", key2);
                    try writer.attribute(""value"", item.string);
                    try writer.elementEnd(); // end item
                }
            }

            try writer.elementEnd(); // end spec
        }
    }

    try writer.elementEnd(); // end specs
}

fn write_pricing(writer: *xml.Writer, pricing: *const json.Value) !void {
    try writer.elementStart(""pricings"");

    for (pricing.object.keys()) |key| {
        try writer.elementStart(""pricing_region"");
        try writer.attribute(""name"", key);

        const maybe_vendors = pricing.object.get(key);

        if (maybe_vendors) |vendors| {
            for (vendors.object.keys()) |vendor| {
                try writer.elementStart(""vendor"");
                try writer.attribute(""name"", vendor);

                const maybe_items = vendors.object.get(vendor);

                if (maybe_items) |items| {
                    for (items.object.keys()) |item_key| {
                        const maybe_item_value = items.object.get(item_key);

                        if (maybe_item_value) |item_value| {
                            try writer.elementStart(""item"");
                            try writer.attribute(""name"", item_key);
                            try writer.attribute(""value"", item_value.string);

                            try writer.elementEnd(); // end item
                        }
                    }
                }

                try writer.elementEnd(); // end vendor
            }
        }

        try writer.elementEnd(); // end pricing_region
    }

    try writer.elementEnd(); // end pricings
}
Leave a comment"
152773121.delta-lake-vs-apache-iceberg-the,Delta Lake vs Apache Iceberg. The Lake House Squabble. ,... the real deal.,2024-12-11 14:00:10.93-06,"I learned something important early on in my career, I can still remember the conversation, which funnily enough, happened not at work, but at a Bed and Breakfast of all places.
Sitting inside a huge and hand-hewn log cabin, nay, mansion, talking to the owner who had just come in from cutting and bailing hay, he was telling me about a book he had just written on ‚Äú
emotional intelligence
.‚Äù
He had retired early and wealthy from consulting, he summed up what he had done for a living that was so lucrative.
‚Äú
I spent my career teaching adults how to share and get a long, kinda like a glorified kindergarten teacher.
‚Äù
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
No solving world hunger, not building some fancy SaaS tool, nope, he was happily retired living inside a giant bespoke log mansion because there is apparently enough fighting that goes on in high places of power that kept him busy and rich, and probably still so.
This story is to illustrate a point, a sort of table setting to prepare us for the discussion to come. What is that saying? Something about man‚Äôs inhumanity to man?
That finally brings us, unsurprisingly to today‚Äôs topic, which should be a fun one.
Let‚Äôs grind our axes, polish our armor, and kiss our loved ones goodbye. It's time to head into battle.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Yes. There indeed is a Lake House battle raging.
First, anyone who says there is
NOT
a Lake House battle brewing and bubbling most likely has an agenda tied to one or other of those dueling sides. It‚Äôs so obvious with all the saber-rattling that has happened in 2024 that it‚Äôs hardly worth my time, or yours, to even defend the position that there is indeed a Lake House battle raging.
I‚Äôm sure
Hudi
might want to interject itself, but we all know that the two clear contenders are
Delta Lake
and
Apache Iceberg
.
Note:
Full disclosure, I will be as fair as possible, although I prefer Delta Lake simply because it‚Äôs better than Apache Iceberg.
I think for our younger generation, and because I‚Äôm old and like to tell the same stories, like your Grandma, that we should do a quick little visual history lesson on Lake Houses, and what led to where we are today.
More or less, the Lake House was the natural progression from the classic Data Warehouse of the Kimball years with SQL Server, Oracle, etc etc. Those systems struggled to scale with the increasing size and velocity of incoming data.
This led to the increasing adoption of ‚Ä¶
Tools like Apache Spark, etc.
Storage like Parquet files in cloud storage (s3 etc).
While this solved the scalability issues, this sort of Data Platform had ALOT of shortcomings that were solved with the classic Data Warehouse.
Schemas
Constraints
Governance
ACID
CRUD
etc.
Enter the Lake House.
‚ÄúIn the context of data, a
Lake House
refers to a modern data architecture that combines the best features of a
data lake
and a
data warehouse
to create a unified platform for managing both structured and unstructured data. It is designed to provide the scalability and flexibility of a data lake, with the performance and management features of a data warehouse.‚Äù
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Back to the Lake House battle.
Ok, I told you I was going to tell you a story, didn‚Äôt I? I think it‚Äôs important to understand the past to help us hone into why exactly there even is a Lake House battle.
There is another saying about ‚Äúfollow the money,‚Äù or something along those lines right?
This is the ‚Äú
why
‚Äù of the Lake House battle.
Cloud Computing Market:

In 2023, the global cloud computing market was valued at approximately USD 602.31 billion. It is expected to grow at a compound annual growth rate (CAGR) of 21.2%, reaching around USD 1,266.4 billion by 2028.

Another estimate places the market at USD 626.4 billion in 2023, with projections to reach USD 1,266.4 billion by 2028, growing at a CAGR of 15.1%.

-----

Data Analytics Market:

The global data analytics market was valued at USD 41.05 billion in 2022 and is projected to grow from USD 51.55 billion in 2023 to USD 279.31 billion by 2030, exhibiting a CAGR of 27.3% during the forecast period.

Another report indicates that the data analytics market size was USD 49.03 billion in 2022, with an expected CAGR of 26.7% from 2023 to 2030.
In other words, there is a lot of money at stake. People have built tools and invested many millions and billions into developing their products to integrate into these tools.
Of course, they are going to care what you decide to use.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
We are talking about the storage layer ‚Ä¶ but ‚Ä¶
When we talk about Delta Lake vs Apache Iceberg, yes we are talking about the storage layer of the Lake House, but it‚Äôs not that simple, it never really is.
These systems built by Databricks and Snowflake, for example, sell themselves as Data & AI Platforms, your late-night TV, bundle it all into one, get it while it lasts tool set.
You don‚Äôt
typically
, for example, migrate to Databricks in a vacuum, when move to Databricks, you are deciding that you are a Spark + Delta Lake shop, that is going to be the foundation of your entire system.
Sure, I know you CAN do other things, but the CAN and the .01% of people shouldn‚Äôt really shape our conversation on this topic.
I mean ask yourself, why
did Databricks by Tabular for a billion flipping dollars
?
Why is UniForm now a thing for Databricks/Delta Lake?
Why do you think
AWS chose Apache Iceberg for its S3 Tables
? Because it‚Äôs a battle, my friend.
What if the data market solidifies on a single tool, Iceberg, or Delta Lake? Well, that leaves the people who‚Äôve invested a majority amount of their time and energy into the ‚Äúlosing‚Äù side are now going to have a lot of work and explaining to do.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Note:
At this point, my personal opinion is that both Delta Lake and Iceberg are going to be with us for the long term. Both tools are so deeply entrenched into so many ecosystems and downstream SaaS that at this point, neither can simply disappear.
Realistic comparison of Apache Iceberg and Delta Lake.
Like always, I will do a super high-level technical overview of both tools and thereby send many different acolytes into screaming and foaming rage of madness. It‚Äôs why I do what I do.
I always like to start with GitHub profiles of both tools, I know it can be a flawed thing, but it‚Äôs not like that‚Äôs our sole deciding point, we are just going to use it give us an idea of what *might* be going on in the community.
- delta
7.7k
stars
284 open pull requests
589 open issues
- iceberg
6.6k
stars
128 open pull requests
458 open issues
Take from that what you will, you‚Äôre smart people. What else should we know about these two projects.?
Iceberg was created in 2017, 2018 it went to the Apache Foundation.
Delta Lake was created in 2017, and open-sourced in 2019.
What is Apache Iceberg?
Iceberg is a high-performance format for huge analytic tables. Iceberg brings the reliability and simplicity of SQL tables to big data while making it possible for engines like Spark, Trino, Flink, Presto, Hive, and Impala to safely work with the same tables, at the same time.
What is Delta Lake?
Delta Lake is an open-source storage framework that enables building a format-agnostic Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, Hive, Snowflake, Google BigQuery, Athena, Redshift, Databricks, Azure Fabric and APIs for Scala, Java, Rust, and Python. With Delta Universal Format aka UniForm, you can now read Delta tables with Iceberg and Hudi clients.
This really isn‚Äôt a post about the technical underpinnings of HOW both Delta Lake and Apache Iceberg work, despite whatever the pundits may tell you, these two tools generaly work the same way.
parquet files store the data in the background
some sort of ‚Äúmanifest‚Äù or ‚Äútransaction log‚Äù file to hold metadata.
Of course, the differences keep going deeper and
books
have been written about them, but that‚Äôs not why we are here today.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Delta Lake and Apache Iceberg from a Python perspective.
Like with any good data tool, we should inspect that tool from a Python perspective.
Iceberg
pyiceberg
Delta Lake
deltalake
Let‚Äôs be real, 90%+ of Iceberg and Delta Lake interactions happen with PySpark, so we need to check out that as well. So we should test that out as well.
We should also look at the download stats for both pyiceberg and deltalake. This can give us some sort of indication of adoption in the wild.
If we look at downloads last week AND last month, the
deltalake
Python bindings blow
pyiceberg
out of the water. (the last day numbers for Iceberg are probably skewed because this was when S3 Tables were released)
Again, take from that what you will, you are smart folk, I think, there are some things you can read between the lines in those numbers,
like it or not
.
Share
A side note about Catalogs.
The whole Catalog
dealo
is another rabbit hole that we must TRY not to go down during this discussion, yet at the same time, it is also a cornerstone of most Data Platforms, and the integration between a Catalog and say a Delta or Iceberg tabe is incredibly important.
Unity Catalog
Iceberg Catalog
REST
Hive
JDBC store
As a whole, Unity Catalog has come a lot farther and is more straight forward to work with than the fragmented Apache Iceberg Catalog landscape.
Maybe we will come back to Catalogs later on, not sure.
Share
General Python support for Iceberg and Delta Lake.
Another aspect of these two dualing storage systems is their general adoption of
NON-Spark Python packages
.
Again, I find this sort of stuff tells a story.
It cuts past all the marketing fluff from either side of the aisle and proves almost to the point of putting the other to bed, that ‚Äú
one tool overcomes the other.
‚Äù
For example, if one side claims to be the winner of the Lake House storage format wars, yet lacks adoption in the general end-user Python packages being used by the masses on a daily basis, I can assure you that you have found a
liar, liar, pants on fire
.
We could also extend it past just Python packages, to just programming tools in general.
As you can see below, unfortunately for Iceberg there is a serious lack of FULL support for
read
and
write
from many major tools that already include Delta Lake full support.
Note: An astute reader told me DuckDB doesn‚Äôt fully support Delta.
It‚Äôs always good to be like those Bereans and search out the truth for yourself. I‚Äôm only human.
I will let that sink into your giant brains and come up with a reasonable explanation for the above. Also, there is another BIG REASON that Iceberg falls behind on these tools along with local development.
It explains the above. We will cover that below.
And so, the show goes on.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Technically, is there any difference between Iceberg and Delta Lake?
Although this seems like an important question on the surface, one could surmise based on all the data we have looked at so far, that these two behemoths of Lake House storage formats are pretty much neck and neck, although one seems to be behind the other in terms of
overall adoption
.
If one tool
clearly
was far superior to the other in features and performance offered, then the race would not be so tight in the first place.
Of course with different implementations of the same feature, they are indeed different tools, but in the end your usage of either Delta Lake or Iceberg via say, Apache Spark, is going to get you to the exact same spot.
As I mentioned above there probably IS a reason that Iceberg lacks adoption across a wide range of tools like Delta Lake. I have a feeling we can sus out the reason why by simply doing a experiment.
Let‚Äôs create a 10 million row Iceberg and Delta Lake table and simply
FEEL
what the developer experience is like, and how much work it takes.
Up first, Apache Iceberg.
The easiest way to do all this, and for you to follow along, would be to use Docker, we will use a basic Python image to do the work.
#bash
docker run -it python:latest /bin/bash
Once inside we can pip install the below tools we
might want to play with.
#bash
pip install pyiceberg polars duckdb getdaft[deltalake] pyarrow deltalake
We are also going to use the open-source tool
datahobbit
to generate a test dataset we can use to create Iceberg tables. To do that, we need rust.
#bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
export PATH=""$PATH:$HOME/.cargo/bin""
git clone https://github.com/danielbeach/datahobbit.git
Let‚Äôs generate the initial dataset we can read with a tool and write to an Iceberg table.
#bash
cd datahobbit
cargo run -- schema.json output.parquet --records 10000000 --format parquet
We can see our parquet files now ‚Ä¶
#bash
root@6197c5643da2:/datahobbit# ls
Cargo.lock  output.parquet_0.parquet   output.parquet_2.parquet  output.parquet_5.parquet  output.parquet_8.parquet  python	  target
Cargo.toml  output.parquet_1.parquet   output.parquet_3.parquet  output.parquet_6.parquet  output.parquet_9.parquet  schema.json
README.md   output.parquet_10.parquet  output.parquet_4.parquet  output.parquet_7.parquet  pyproject.toml	     src
Let‚Äôs use pyarrow to read these parquet files and write an Iceberg table. If you are curious, this is what the data looks like.
Note
: this will
NOT
be as easy as Delta Lake, we
HAVE
to have a Catalog configured to Iceberg to work, and we will have to include pyiceberg+ in to get the job done.
Let‚Äôs continue with our Catalog work that is REQUIRED to play with Iceberg locally, including another pip install that is required ‚Ä¶
#bash
mkdir /tmp/warehouse
pip install sqlalchemy
Once this is done in Bash, we can switch over to Python.
# python
import pyarrow.parquet as pq
from pyiceberg.catalog.sql import SqlCatalog
from glob import glob

warehouse_path = ""/tmp/warehouse""
catalog = SqlCatalog(
    ""default"",
    **{
        ""uri"": f""sqlite:///{warehouse_path}/pyiceberg_catalog.db"",
        ""warehouse"": f""file://{warehouse_path}"",
    },
)

catalog.create_namespace(""default"")
files = glob('/datahobbit/*.parquet')
df = pq.read_table(files)
table = catalog.create_table(
    ""default.test"",
    schema=df.schema,
)
table.append(df)
You might be asking me a question, a question that I asked myself. Why can‚Äôt I simply use something like Daft to read and write? Well, for example, to write an Iceberg table in Daft you need an actual pyiceberg table to pass to Daft, if you go to pyiceberg you NEED to have Catalog configured to write a table as far as I can tell.
It‚Äôs a daisy chain of crap.
I mean I went through all the learning steps just to prove the point, but others will just stop half way when they figure out they need to configure a catalog and get
lost in the detritus of the internet figuring out how to do that.
The simple truth is we humans are lazy, Data Engineers are no different. For a tool to gain widespread adoption among users AND gain support for tooling integrations ‚Ä¶ it must be ‚ÄúEasy‚Äù to use.
Take for example the following Delta Lake setup if I so desire, and want the ability to locally develop with Delta Lake, LIKE ANY GOOD ENGINEER would want while building our larger data platforms.
The same thing for Delta Lake.
Here it goes, hold on.
import daft as dft
df = dft.read_parquet('datahobbit/*.parquet*')
df.write_deltalake('test')
Before you say I‚Äôm cheating and not being fair to Iceberg, I would have you note the above in Daft for Delta Lake is just as easy in DuckDB, Polars, etc.
Remember, I can‚Äôt turn to Polars, DuckDB, etc to write Iceberg tables ‚Ä¶ they don‚Äôt have support.
Details matter because Engineers have to worry about them. If you‚Äôre trying to decide between Delta Lake and Iceberg for your Lake House, including local development systems, etc, what are you going to choose?
I would have to have a REALLY good reason to choose Iceberg over Delta Lake at the moment.
Simply based on tooling integrations and ease of development.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Let‚Äôs be honest.
I think we should stop and just be honest with each other, the whole reason the
Battle For the Lake House Storage
Format
even exists is because we have our own pet favorites for various and sundry reasons.
If you‚Äôve been using Databricks for years, you would look at what Iceberg users have to go through to even piddle around with it locally and ask yourself why in the hell they would put themselves through such nonsense when something better exists.
On the other hand, if you‚Äôve used open-source Apache Iceberg, or one of its many closed-source variants, it‚Äôs just what you are used to, and your human, you aren‚Äôt going to switch.
Plus, seeing something like the
AWS S3 Tables announcement
is going to make you stand up and pat yourself on the back, ‚Äú
I was on the right side.
‚Äù
Clearly that is just a biased take, any moderate person who simply goes down the list of exploring the two projects side by side, and comparing them for what they are, including with code, is going to come to the obvious conclusion you can see above.
Delta Lake is better.
Iceberg is mid at best.
Ha Ha! Don‚Äôt forget, I‚Äôm just another biased user who loves Delta Lake because its better than Iceberg. But, you can use whatever you want, don‚Äôt let anyone tell you otherwise.
At the end of the day a Lake House platform built with Iceberg is going to give you the same analytics as one built with Delta Lake. If we get to our goal, the middle doesn‚Äôt matter that much.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
152763814.maniacal-focus,Maniacal Focus, ... the missing skill,2024-12-16 07:46:45.475-06,"Every so often, as the fancy takes me, I like to stray from ole‚Äô bread and butter of technical content in data, to other more general topics that indeed are important, but more esoteric.
Today is one of those days.
I‚Äôm in that sad part of my career in the Data World where I‚Äôve been doing the same thing way too long, and eventually, you end up in the spot where you spend mostof your days ‚Ä¶
project planning
putting out fires
helping and upskilling others
etc, etc, for the good of humanity
I‚Äôve spent a decent amount of time these last many years observing what makes a Senior+ Engineer that very thing. I see the ‚Äú
mistakes
,‚Äù if we can call them that, of those eternally optimistic and Leetcode-hungry mid-level Engineers, ever eager to pull us down from our high towers.
So this brings me to the topic for today.
Manical Focus
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The Missing Skill of Most Software Engineers
I can think of many reasons why mid-level Engineers seem to toil and waste away in the middle of the pack, the list is unending that is for sure. But, there are few bad habits, or lack of good habits if you will,
that cause an otherwise capable Engineer or Software Hobbit of some kind to forever waste away in the bustling hallways of the interwebs like some forgotten Oliver Twist
.
One of the top reasons is the inability to have
Maniacal Focus
. A failure in this category leads to a plethora of unfortunate side effects.
Unfinished or half-finished projects
Overwhelmed by too many things going on at once
Mediocre solutions to every problem presented
Everything takes longer than it should to complete
Unable to plan well for projects
Hard time growing in career to next level
Frequently introduced bugs
Sad, but true.
For whatever reason, the idea of Maniacal Focus seems to be one of those famous, but under appreciated and implemented truths.
Steve Jobs:
‚ÄúFocusing is about saying no. And you've got to say no, no, no. And when you say no, you focus on the few things that are really important.‚Äù

Gary Keller (The ONE Thing):
‚ÄúSuccess demands singleness of purpose.‚Äù

Confucius:
‚ÄúThe man who chases two rabbits catches neither.‚Äù

Peter Drucker:
‚ÄúEfficiency is doing things right; effectiveness is doing the right things.‚Äù

Cal Newport (Deep Work):
""To produce at your peak level, you need to work for extended periods with full concentration on a single task free from distraction.""
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What is the power of focus in software?
Focus comes in many forms, the opposite of focus, the distracted or diffused person also comes in as many forms.
It‚Äôs easy to spot those distracted folks, especially in the work place.
Never sure what they will be working on tomorrow
Standups and checking-ins are hard for them
Can‚Äôt talk high level, only tiny details
Appears flustered and rushed constantly
Always stuck in the same spot
Most all of these problems could be solved with just a little focus. Stop working on so many things! Pick one thing and work on it until you are done or are totally blocked.
There will always be the constant humdrum of little flies and bugs poking and biting at you, that‚Äôs life. You have to say NO!
Maniacal
Focus
is required.
In any project, Software or otherwise,
even at the task level
, there is going to be a very specific set of problems and work that MUST get done to complete the task.
There will also be a plethora of other little ‚Äú
ankle biters
‚Äù that come up, little time demons that will try to creep into your mind and through your fingertips, drawing you away from the ‚Äú
line of attack
,‚Äù that is the most direct path between you and completion of said task.
You must spurn the call of ‚Ä¶
fixing other bugs you find when poking around
deciding to do ‚Äúformatting‚Äù or other nice-to-haves
little quick fixes and requests that are coming in
other lower priority projects
working on multiple projects at once
The problem is that going from task to task throughout a single day will steal the ability for you to gain real traction on a single thing. You will get nothing done while looking busy, and it WILL catch up with you.
When you think about a task or project, there is typically a most ‚Äúoptimal‚Äù line of traversal between the start and finish. But, the problem is that there is more than one path to get to that finish line. Sure, you will get there, but that path will be longer and more laborious because those individual steps get swapped out for steps from other projects.
Share
Tips for keeping on track.
There is a significant amount of staying focused that simply just depends on a person‚Äôs ability to be disciplined. It‚Äôs kinda like going to the gym, in the beginning, you‚Äôre just going to have to force yourself to stay focused, and over time that skill will just turn into a habit.
But, for those of you who still struggle, here are some tried and true tips to keep you on track.
Plan your month, week, and days ahead of time.
You should have known goals at the beginning of each month, week, and day.
Focus on planning the most direct path to your goals.
You should track your progress toward these daily, weekly, and monthly goals.
So you know when things are going off track.
Purposely
avoid rabbit holes that your own mind or other people bring up.
Break Down Tasks
: Divide large tasks into smaller, manageable chunks.
Prioritize
: Use tools like the Eisenhower Matrix to focus on what's urgent and important.
Set Clear Goals
: Define what ""done"" looks like for each task.
Turn Off Notifications
: Silence your phone and close unnecessary tabs.
Time Blocking
: Allocate specific time slots for tasks on your calendar.
Set Deadlines
: Even self-imposed deadlines can help you stay on track.
Start with Easy Wins
: Complete a simple task first to build momentum.
Use the Two-Minute Rule
: If a task takes less than two minutes, do it immediately.
Stay Hydrated
: Drink water regularly to keep your brain functioning at its best.
Take Breaks
: Step away from work periodically to recharge.
Avoid Multitasking
: Focus on one task at a time for higher productivity.
In the end, it‚Äôs going to come down to you. You will have to make the changes to ensure you‚Äôre staying focused. You will simply have to force yourself to work on that one thing, even when you don‚Äôt want to until that task is done.
Working in the field of Software and Tech is an especially hard place to have that super focused work. Always new tools and solutions, bugs to fix, features to add, and people to bother you.
Plan ahead, know priorities, and stick to them at all cost."
152565348.aws-s3-tables-the-iceberg-cometh,AWS S3 Tables?! The Iceberg Cometh. ,"weep moan, wail all ye Databricks and Snowflake worshipers",2024-12-05 09:36:00.724-06,"Well, what is that old saying? Better late than never? Something like that. Weep, howl, and moan all ye Databricks and Snowflake padiwans, what you have greatly feared has come down upon you with a heavy hand.
I can just see all the meetings being called and pats on the back with ‚Äú
It‚Äôs going to be ok
,‚Äù happening right now inside Databricks and Snowflake conference rooms. I can almost taste the tears running down the faces of those angry Product Managers, those wicked Sales people asking the hyped-up Marketing group how to respond to this most expected, or unexpected, entry into the Lake House world.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I‚Äôm pretty sure a few overworked Databricks and Snowflake employees woke up with sweats and nightmares last night, with this headline running like a raging lion through their minds.
Amazon S3 Tables
Ok, enough storytelling, I can‚Äôt help it. At the most recent
aws:reinvent
, aka a few days ago, there was an unsurprising announcement that will most likely have a large impact on the Data World at large ‚Ä¶
maybe
.
What is the hoopla about Amazon‚Äôs release of S3 Tables? What are they? What do we care??
‚ÄúAmazon S3 Tables deliver the first cloud object store with built-in Apache Iceberg support, and the easiest way to store tabular data at scale. S3 Tables are specifically optimized for analytics workloads
‚Ä¶
Table buckets are the third type of S3 bucket, taking their place alongside the existing general purpose and directory buckets. You can think of a table bucket as an analytics warehouse that can store Iceberg tables‚Ä¶‚Äù
Ok, so this might require a little background for those uninitiated Lake House virg‚Ä¶ I‚Äôm mean newbies. A little background.
Let‚Äôs talk some backstory here, and set the stage.
We will indeed take time today to give these new S3 Tables a try, with code, but let‚Äôs first just give ourselves a chance to think theoretically about these new S3 Tables, and try to cut through all the hype and marketing buzz to the truth of the matter.
Databricks and Snowflake dominate the Lake House space.
Databricks and Snowflake struck the almost fatal blow to AWS tools like EMR, Glue, etc.
There is a TON of money to be made in the Lake House space ‚Ä¶ after all, it is the new Data Warehouse.
Only a fool wouldn‚Äôt try to dominate and have their fingers in the as much of the Lake House space as possible.
We cannot think of AWS S3 Tables in a vacuum, because they don‚Äôt exist in a vacuum, nor were the designed without context. The Lake House as we know it today is seen as a conglomeration of tools and tech that fit together like a puzzle to provide end users a one-stop shop for their Data Platform needs.
When you read the marketing material put out by AWS on these new S3 Tables you will notice what is mentioned at the very top, in the same or proceeding sentences. Other services of course.
They (AWS) were clearly falling behind in the data race, or at least in ‚Äúcontrolling‚Äù the race to perfect Lake House system. Sure, they still had a piece of the underlying pie, with much of the compute and storage still running inside AWS, but the more time was going by, the more irrelevant they were becoming.
What would stop AWS or Snowflake from providing cheaper compute and storage seamlessly to their customers if they so choose? Nothing.
In fact, besides the Platform Engineers and System Admins, the day-to-day user of Snowflake or Databricks playing around in a Notebook, God forbid, or SQL, wouldn‚Äôt even know AWS was all those layers down the stack, back there in the shadows.
The truth is AWS needs, and wants, to get in on the Lake House action. They want you to use AWS EMR. But why would you when you can go over to Databricks and clickety click have Delta Lake and Compute at your finger tips?
This is where AWS S3 Tables come into play. In theory, AWS is now providing you the ability to have a single one-stop shop Lake House experience using their tools, keyword, in theory.
Is it too late? Can AWS even begin to compete with Snowflake and Databricks and their beautifully and well-crafted Lake House offerings?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
A word on Iceberg vs Delta Lake.
I know a lot of people in the Data World, who have a vested interest of course, have been trying to downplay the silent war being ragged behind the scenes to be THEE Lake House format winner.
Those are Apache Iceberg and Delta Lake. Now, we don‚Äôt have time to dive too much into this, but it was probably a surprise to some that AWS S3 Tables chose Iceberg as their default format.
This is a confusing and complicated war. Databricks more or less drives Delta Lake and it‚Äôs at the core of their product.
They recently bought Tabular
and moved to try and take over Apache Iceberg, more or less.
Also, they‚Äôve been pushing
UniForm
and other tools to make the storage format (between Iceberg and Delta Lake) sorta pointless, kinda a way of hedging their bets.
Clearly AWS using Iceberg was a sort of jab in the side of Databricks, showing them that hey, AWS still has some fight left in them, and they are willing to play dirty.
Don‚Äôt think that AWS S3 Tables being Iceberg-ish is a death knell for Delta Lake, that battle is far from over. Also, there IS a Lake House storage format war going on no matter people tell you. People like one or the other, and most of the tools either integrate better with one or the other.
It is a war.
IMPORTANT NOTE
- while new news is always exciting, remember that S3 Tables are late to the game. Databricks and Snowflake have been building Lake House platforms, and getting very good at it, for some time. Being the underdog at the back of the pack comes with its own set of problems, even if you are AWS.
Introduction to S3 Tables on AWS.
Ok, we are getting to the point where we need to turn the ship back to the star of the show,
AWS S3 Tables
. Now that we have set the historical table, with both sides beating their plowshares into swords, let‚Äôs try to actually look at this new Lake House format in code.
The idea here isn‚Äôt to become experts, but to simply kick at the tires and poke at the holes, and in so doing, be able to better understand for ourselves the
REAL LIFE
implications of S3 Tables.
First, let‚Äôs simply start with the basics of AWS S3 Tables that we can get from all the blogs and docs on AWS. We will move quick, strap in.
S3 Tables
are stored in a new bucket type: a table bucket.
‚Äú
stores tables as subresources
‚Äù - one would surmise from this that you can store multiple
S3 Tables
in a single ‚ÄúTable Bucket.‚Äù
Table buckets support storing tables in the Apache Iceberg format.
do you think this will include other options in the future?
You can use SQL statements to query your tables.
of course, you rabble of SQL junkies.
You can use any tool that has a ‚Äúquery engines that support Iceberg.‚Äù
which is a good number of tools, both distributed and not.
To optimize your tables for querying, S3 continuously performs automatic maintenance operations,I mean really ‚Ä¶ that‚Äôs it.
You can manage access for both table buckets and individual tables with AWS Identity and Access Management (IAM).
This is what people must realize with this
S3 Tables
announcement, it is cool, yes, but all it is, is AWS joining the ranks of what Databricks and Snowflake have been offering you for years on end.
They are simply adding the missing Lake House piece to their stack, and an integrated storage layer that works seamlessly inside their environment, targeting that one-stop Lake House platform idea.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
S3 Tables in Code.
Ok, let‚Äôs learn a little more about S3 Tables by trying them out. I think we should do the following.
create Table Bucket with CLI
create S3 Table
INSERT data into our S3 Table
Query our S3 Table with ‚Ä¶
Polars
Daft
Spark
DuckDB
Then we can review what we saw, learned, and maybe make some more conjectures about the future of S3 Tables based on the results.
On with the show.
First, we must upgrade our
aws CLI
. If you have an older version if you try to run any commands, you get the following error ‚Ä¶
% aws s3tables 
>> aws: error: argument command: Invalid choice, valid choices are:
...
Run the following to upgrade.
% curl ""https://awscli.amazonaws.com/AWSCLIV2.pkg"" -o ""AWSCLIV2.pkg""
sudo installer -pkg AWSCLIV2.pkg -target /
>> installer: The upgrade was successful.
Creating a Table Bucket in S3.
First, we must create what AWS calls a
Table Bucket
, this is a new kind of S3 bucket specifically for S3 Tables.
‚ÄúThe console automatically integrates your table buckets with AWS analytics services. If you create your first table bucket programatically by using the AWS Command Line Interface, AWS SDKs, or REST API, you must manually complete the AWS analytics services integration.‚Äù ‚Üê T
his is called $money$ people, pay attention!
Let‚Äôs do this thing.
Ok, easy enough I think. We should now be able to call and get the details of the Table Bucket.
Next, we should try to create an empty S3 Table in this bucket.
This is where things get funny ‚Ä¶ and disappointing.
We knew there would be a few hitches in the workflow, no surprises there, and when you are a S3 Table newbie, and are at that point of, ‚Äú
Hey, I would like to read and write to a new S3 Table, how do I do it?
‚Äù It isn‚Äôt going to be easy.
If you read the official blog,
they use Spark,
EMR that is
, with a bunch of configs. See below ‚Ä¶ they sort of ‚Äú
gloss over
‚Äù the Spark work required to connect and create the table.
If you poke around in the docs, you see this example that shows you what you must do. It isn‚Äôt fun or ‚Äú
seamless
,‚Äù a
nd exemplifies the entire reason EMR lost the game to Databricks and Snowflake.
It‚Äôs a bunch of crap that MOST average Databricks or Snowflake users NEVER have to do to work with their Lake House data.
This is the achilles heel of S3 Tables.
Here are some snippets from AWS Docs.
create Table Bucket, setup security etc.
create EMR cluster
pass a bunch of configs
set up config file
SSH into machine
pass more configs to start Spark etc.
I mean honestly, these AWS people never learn their lesson apparently. They want us to create an EMR cluster, make a config, SSH into a machine, run Spark shell with a bunch of configs.
Look, I‚Äôve spent my fair share of time 14 years ago doing this kind of crap
!
The world has simply moved on. You‚Äôre average Data person has no desire to jump through these hoops
. They will not.
They will simply use Databricks or Snowflake.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The saga of creating a S3 Table continues.
I was so sure they had made a CLI command, ANYTHING, that would help us easily make an S3 Table without using a flipping EMR cluster or Spark, but as I dug into the docs, I found this page.
To the casual observer, this seems to indicate that to use ‚Äúopen source query engines‚Äù ‚Ä¶ aka NOT Glue, EMR, Athena, and the rest ‚Ä¶ we must use ‚Äú
Amazon S3 Table Catalog for Apache Iceberg client.
‚Äù
And it lists the following engines that are supported ‚Ä¶
Apache Spark
Great, does this mean I cannot use Daft, Polars, DuckDB, or anything else that already supports Iceberg to create and write to AWS S3 buckets?? I don‚Äôt know, on the surface, it seems so.
Look, I‚Äôm not an expert. I‚Äôm just an average engineer trying to understand the tool and use it using the available documentation within a reasonable timeframe.
NOTE:
This seems to go well with my earlier theme that AWS is doing this whole S3 Tables thing because they are trying to compete in the world of a single-stop shop Lake House data platform ‚Ä¶ that doesn‚Äôt really exist with their products today.
They WANT you to use S3 Tables in the context of Glue, Athena, EMR, etc.
Ok, if that is the case, I don‚Äôt want to spend EMR money, let‚Äôs see if we can get this to work on a tiny EC2 instance. I need those AWS hobbits to send this poor by a bunch of credits so I can keep doing the Lord‚Äôs work out here in the hinterlands.
We are going to use a t2.small, a beast, running Ubuntu for the OS. After the setup and waiting, we can easily SSH into our little train that could.
Next, we need to get Apache Spark installed.
And it‚Äôs working.
Now the fun part, let‚Äôs see if we can get the commands right to start Spark with the ability to talk to
AWS S3 Tables
. Note in the below table the
AWS ARN
pointing to our previously created Table Bucket.
It appears Spark started fine, now, following their documentation let‚Äôs try to create a namespace for our table.
It‚Äôs looking like it almost worked, but couldn‚Äôt find credentials, in fact, I didn‚Äôt attach a policy or put keys on this machine, so let me fix that by adding credentials to the environment.
Ok, no errors this time after putting creds on the EC2. Let‚Äôs go ahead and create a S3 Table.
Now, if we slide on over to my AWS account and check the Table Bucket, we can actually see that new S3 Table we just created.
So it working well. Let‚Äôs just insert some sample data to say we did.
And of course, let‚Äôs do a simple query to ensure we can access the data we just inserted.
I guess that was surprisingly easy, aside from the fact I had to set up my own Spark EC2 Instance and piddle with the settings and configs to get things talking, but nothing we haven‚Äôt been doing with Spark for the last 10 years.
I‚Äôm glad we are done with that, let‚Äôs circle back in a little bit and talk about what we learned in relation to S3 Tables and the impact and comparison to current tooling, but first, let‚Äôs mention pricing.
Pricing.
I think it‚Äôs important to at least mention the
pricing of S3 Tables
in passing, it‚Äôs not going to be cheap, s3 really isn‚Äôt, so that‚Äôs no surprise.
As far as I can tell it‚Äôs broken down 3 ways.
S3 Tables storage pricing
S3 Tables requests pricing
S3 Tables maintenance pricing
You can go ahead and do the math yourselves, but just note, that most real-life Lake Houses storing hundreds of terabytes of data+, these costs will add up quickly.
Boiling it all down to something we can take a bite out of.
Yikes, that was a lot to take in for what we are trying to cover. I hope you at least now can cut through the hype that will be pouring out into the interwebs over the next weeks and months over this release.
But, we need to try to bring it all together here at the end, now that we‚Äôve tried to understand S3 Tables at both a theoretical level, and actually wrote some code!
I might just resort to a good old-fashioned list of things we‚Äôve learned.
AWS S3 Tables are made to be used and integrated with AWS Products like Glue, Athena, EMR, etc.
S3 Tables are just an AWS answer to what Databricks has done with Delta Lake etc.
S3 Tables lack general query engine and interaction support outside Apache Spark.
S3 Tables have a higher learning curve than just ‚ÄúS3,‚Äù this will throw a lot of people off and surprise them.
Lake Houses are taking over the Data World and are becoming the defacto standard, replacing the Data Warehouse, and AWS making S3 Tables prove that.
This will deepen the already large divide in Lake House storage format wars, between Apache Iceberg and Delta Lake.
Managed S3 Tables with auto maintenance (
per the pricing
) appears to be a very expensive way to run a Lake House.
AWS still has some serious continuity and seamless integration problems with the one-stop-shop Lake House platform they are trying to build with the addition of S3 tables.
I‚Äôm sure this is a very happy day for all those organizations who are still building their Lake House on AWS without Databricks and Snowflake, this is a real third option for those still using ‚Ä¶
EMR
Athena
Glue
‚Ä¶ on a daily basis in their environments.
My hope would be that S3 Tables from AWS would be a net positive to the Data Community at large. Having another big player enter this Lake House space will force Databricks and Snowflake to work even harder and provide best-in-class solutions.
My biggest fear for AWS is that they will leave S3 Tables in their current state, and because of that, they will slowly die the same death as EMR and Glue.
They feel a little raw and not so user-friendly yet. It would be a different story if they had released a bunch of docs showing how to build and load data to S3 Tables with Python, boto3, the aws CLI etc.
There is only a small cadre of users willing to dive into EMR full bore to take advantage of S3 Tables, it‚Äôs going to be the people already using EMR in the first place.
Who‚Äôs going to use Databricks and write to a S3 Table? No one. Doesn‚Äôt make sense in 99% of use cases. Also, I‚Äôm not looking forward to the continued fighting between Iceberg and Delta Lake zealots (of which I am one).
It would be nice for everyone to converge onto a single approach, but that clearly isn‚Äôt going to happen anytime soon, even with all the promises of abstracting Iceberg and Delta Lake away altogether.
I think at this point we‚Äôve said enough, since S3 Tables are so new, I‚Äôm sure we can expect to hear and see a lot more about them in the coming weeks and probably all of 2025. Hopefully, you leave us here with a more robust and realistic understanding of S3 Tables.
Please, let us know in the comments below your thoughts on all this! What do you think of them? Iceberg vs Delta Lake, what‚Äôs going to happen? What are your thoughts on AWS taking on Databricks and Snowflake with this move?
Leave a comment"
152463289.databricks-raises-money-55-billion,Databricks Raises Money - 55 Billion Dollar Valuation, ... to IPO or not IPO ...,2024-12-03 09:25:36.384-06,"Follow the money, isn‚Äôt that what the wise old sages always say? There is indeed a big pile o‚Äô money at the end of the rainbow, and that rainbow is Databricks.
I‚Äôm not sure if there has been any doubt, but Databricks is the AI and Data behemoth that simply can‚Äôt be defeated. It‚Äôs pretty obvious that a company in the ‚Äú
niche
‚Äù of data, that has been able to become a household tech name, like
Databricks
, has pretty much made it to the pinnacle of the known world.
To prove it, Databricks just raised a massive amount of funding.
Recently, Databricks reportedly raised 5 billion in cash money, leading to a new valuation of a whopping 55 billion.
The obvious next question that has been rattling around in everyone‚Äôs mind, and probably all those soon-to-be millionaire employees of Databricks, is, when oh when is Databricks going to IPO and go public?
Well, apparently not right now!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Let‚Äôs put this in perspective, shall we? Let‚Äôs compare Databricks latest numbers to OpenAI, the hottest slice of bread on the block this year.
OpenAI in 2024 - raising $6.6 billion in October at a $157 billion valuation.
Databricks in 2024 - raising $5 billion at $55 billion.
I mean the fact the two are anywhere in the same vicinity is a feat by Databricks that should not be ignored. It makes you wonder what kind of money those two companies (
Databricks and OpenAI
) are making.
I mean who knows if these numbers are correct, the fickle internet is what it is. But, it‚Äôs looking pretty good for
Databricks
if you ask me. The question we don‚Äôt know is ‚Äú
How much are they spending to get that revenue.
‚Äù
The word on the street is that Databricks is making some 1.6 billion, and OpenAI is around $3.6 billion. That‚Äôs not too far apart and look at the valuations. Interesting.
Wondering why Databricks decided to raise funds and not IPO? The press is saying ‚Ä¶
‚ÄúThe latest funding is designed to help Databricks employees sell shares, one of the people said. Reducing pressure from employees to cash out also reduces the need for a liquidity event such as an IPO.‚Äù
-
news
That brings up an interesting topic: Databricks employees. Not sure if you are aware, but there are some smart folk they have hired over there and they‚Äôve been slowly building that A+ team over the years.
Having interacted with them (Databricks employees) on various fronts over the years, I can say, it‚Äôs painfully obvious to me they only hire the best and brightest for the most part.
I can think of only one idiot off the top of my head that I had to deal with there (
I‚Äôm not bitter or anything
).
All you have to do is use their product to be parted from the theory that Databricks employees are nothing special. They‚Äôve built a best-in-class Data and AI Platform that is light years ahead of anyone else. It‚Äôs the employees that did that.
So apparently this funding round was to let some of the employees get rich, as they should, and not leave for greener pastures.
I mean if that silly site Glassdoor can be trusted at all anymore, seems like Databricks is loved by it‚Äôs employees, which is pretty good for a company that size.
Funny enough, a few months ago on Reddit you could find stuff like this. ‚Äú
Why hasn‚Äôt Databricks IPO‚Äôs?
‚Äù ‚Üí Employee ‚Üí ‚Äú
I wish they would.
‚Äù Now, whomp whomp, they purpodedly raised a ton of money to help some employees hit the jackpot.
Make‚Äôs you wonder if Databricks will do something else with all that money. They have been known to drop some coin acquiring different businesses.
‚Äú
In the summer of 2023, the company spent $1.3 billion
to acquire MosaicML
, which developed an AI factory that creates GenAI models. Earlier this year, it shelled out between $1 billion and $2 billion
to acquire Tabular
, the commercial venture behind the open source Apache Iceberg project
.‚Äù -
news
Either way, it seems like Databricks continues to execute in all aspects at high levels. It shows a kind of ‚Äú
bravado
‚Äù to continue to not IPO when everyone, including employees, continues to expect and and call for it.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
152374116.datafusion-from-a-python-perspective,Datafusion ... from a Python perspective,"... let's be real, that's what matters",2024-12-13 13:31:18.652-06,"Datafusion
. Probably one of the least-known yet most prolific Data Engineering tools on the market today. I have my thoughts on why, and that is what we are going to talk about today.
I want to do both technical and theoretical overview of Datafusion, that underdog, and talk about ‚Ä¶
Who‚Äôs using it
For what ‚Ä¶
Where it‚Äôs used ‚Ä¶
Why it‚Äôs used ‚Ä¶
It‚Äôs
lack
of general adoption by the Python hoard.
This is a topic that interests me to no end, the rise of an incredible tool, yet a lot of ‚Äú
failures
,‚Äù if I may be so bold to call them that, in what we could call the DevRel (
Developer Relations and Marketing
) part of the technology.
It‚Äôs something good, no, great, Software Engineers and builders forget. Your wonderful tool may be the new sliced bread, but if no one knows about it, and you don‚Äôt market yourself well to the right crowd, you‚Äôre doomed to live on in the shadow of other giants, *cough* (
Polars
).
I‚Äôve written a few scattered and random polemics on Datafusion. You can read them here.
Datafusion SQL CLI ‚Äì Look Ma, I made a new ETL tool.
Dataframe Showdown ‚Äì Polars vs Spark vs Pandas vs DataFusion. Guess who wins?
DataFusion courtesy of Rust, vs Spark. Performance and other thoughts.
DataFusion, My Swiss Army Knife
Apache Datafusion Comet
Anyways, with that long-winded introduction, let‚Äôs settle down on this cold winter day, grab a cup of tea or acky coffee, and dive into
Datafusion
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Datafusion from the top.
So, if you are some sort of cave dweller who hasn‚Äôt even heard the word Datafusion, let‚Äôs do the 10,000-foot view.
‚ÄúDataFusion is an extensible query engine written in
Rust
that uses
Apache Arrow
as its in-memory format ‚Ä¶ Out of the box DataFusion offers
SQL
and
Dataframe
APIs, excellent
performance
, built-in support for CSV, Parquet, JSON, and Avro, extensive customization, and a great community.
Python Bindings
are also available.‚Äù -
Datafusion
Sound like pretty much every other data tool on the market? Yes and no.
There is a reason why you‚Äôve probably not heard much about Datafusion and even less so, seen it used out in the wild.
Strange
.
You would think such a data-centric tool would be popping up everywhere.
It can be found ‚Äú
behind
‚Äù the scenes, Datafusion appears to be a more popular tool to ‚Äú
build on top of
‚Äù. It‚Äôs a tool used to build other tools.
horaedb
-
a high-performance, distributed, cloud-native time-series database.
ballista
-
is a distributed execution engine which makes Apache DataFusion applications distributed.
arroyo
-
a distributed stream processing engine.
comet
-
Apache Datafusion Comet, a high-performance accelerator for Apache Spark.
dasksql
-
a distributed SQL query engine in Python.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.You
Share
Good tools lacking adoption.
You hate to see it, I‚Äôve been writing pieces on Datafusion for years now, from when no one had heard of it, to now there are like 15 people. Even if you find someone who‚Äôs heard of Datafusion, the chances they‚Äôve ever considered using it, is about zero.
If you want to have real adoption of a tool in Python Data World, you must have good Python support, and that, datafusion does not have.
In fact, read this quote from the
Python Datafusion Python Bindings GitHub repo
.
They literally tell you not to use them, instead pointing you toward DuckDB or Polars.
At this point it‚Äôs pretty clear that Datafusion
DOESN‚ÄôT WANT
to be a ‚Äúfront-end‚Äù/‚Äùend-user‚Äù data tool at all.
They want to be a tool used to build other tools apparently.
The problem with this approach is clear.
people won‚Äôt use you for anything if they don‚Äôt know you exist as an option.
many of you have probably heard of maybe 1 tool on the above list that uses Datafusion in the background.
again if you‚Äôre unknown, for whatever reason, then you won‚Äôt get picked up as an option, even to build other tools.
most projects want to be popular, so they may decide to use something more well-known on the backend, to inherit that street cred.
if you have no street cred it‚Äôs hard to be seen as useful.
The truth is, if someone(s) is going to put that much effort into developing a tool like Datafusion, it‚Äôs in your best interest to make sure you care about marketing and developer relations.
This is the classic smart engineer mistake, thinking that because you build something complicated and fast, that people will flock to you and worship the thing you built, and use it ad infinitum.
Any Senior+ Engineer who‚Äôs been writing code for any amount of time knows this is not true.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Sometimes they don‚Äôt know what they got.
The funniest part of the whole thing is, sometimes people don‚Äôt even know what they got. Have you ever used Pandas to work on some CSV file data for example?
It ain‚Äôt that fun.
Shall we try it in Python Datafusion (
what they literally tell you not to do
), and see if we like it??
We can pip-install it easily enough.
Let‚Äôs download one of those
classic Divvy Bike Trip datasets
. We have a file called `
202411-divvy-tripdata.csv
`, so let‚Äôs try some Datafusion on it.
We shall run a simple SQL query.
No riddle me this ‚Ä¶ why would Datafusion hide this under a bush? This is extremely nice and easy Python to write, including the fact we have a simple-to-use SQL for our data?
Most tools, even Dataframe tools in Python rarely ever reach this stage.
If it were me, I wouldn‚Äôt hide this sort of thing, I would ‚Ä¶
Make some nice looking Python Docs and Quickstarts
Put a little time into brushing up on the simple Python Bindings
Start marketing it
More people will start using Datafusion, more people will contribute, more likely to get picked up by other tools.
I mean on the surface, the above approach is no worse, and in fact, might be just as simple or simpler than DuckDB or Polars, both of which the
official Python Bindings in GitHub tell you to use instead!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I mean honestly, if Datafusion put as much effort into the development of their Python Bindings as their somewhat
useless CLI
there would be
10‚Äôs of thousands of Datafusion users out there in the wild as we speak
.
Some commonsense would have gone a long way when making the decision(s) about what to work on.
But again, this is not an uncommon problem in the Software world, if you let Engineers run amuck to their hearts‚Äô desire, you might get what you expect %50 of the time, the rest is just people just doing what they think is cool, or want to work on, without regards to the real world.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
152329112.what-is-a-healthy-lake-house-delta,"What is a ""healthy"" Lake House (Delta Lake style)?", ... building ...,2025-03-10 07:32:56.347-05,"Sometimes, late at night, when the moon is shining bright, my brain is still running fast from watching The Day of the Jackle, and my belly is full of popcorn, I think about strange things.
Like ‚Ä¶ ‚Äú
What makes my Lake House healthy, what makes it unhealthy, with all those hundreds of tables splashing around in my Lake House, they‚Äôve become invisible to me
.‚Äù
It‚Äôs funny. The Lake House has become the new Data Warehouse, the de facto way to store data in this new era. But you never hear much, if anything, about the health of the Lake House itself. Maybe you hear something about
Data Quality
here and there, about
the data internal to the system
‚Ä¶ but never about the Lake House in its own right.
Why is that?
If we are all dumping data into Delta Lake, Iceberg, Hudi, or whatever, you would think we would be concerned about keeping those systems running at peak efficiency.
The most any of us have heard in relation to this is usually‚Ä¶
Compaction via
OPTIMIZE
(small file problem)
VACUUM
(deleting dead files)
partition/clustering or data skew
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Maybe we don‚Äôt need to worry about anything else? This probably isn‚Äôt true, but the other things probably wander from very technical topics (
files too big or small, maybe unreferenced files
) to more ephemeral things like ‚Ä¶
data models
naming conventions
schemas
data types
constraints
etc.
I mean, if every column is specified as a
STRING
, then we can surmise that our Lake House might have a crack in the corner. Or, if we have hundreds of tables in a schema called ‚Äú
default
,‚Äù then we can also assume not much thought was put into our data model.
All these questions also bring up the problem inside the large Lake House:
how do we, or can we, know that the basics of our Lake House are functioning correctly?
What if we have thousands of tables? How do we know they are all being compacted on a regular basis? How do we know dead files are being removed? What about data skew and partition or clustering health?
I don‚Äôt know if a such a tool exists surprisingly, you would think in the Lake House world we live in we would have all sorts of Lake House health dashboards or packages to use.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
With that in mind.
Since that is the case, no easy one stop shop to say ‚ÄúIs this Lake House table healthy?‚Äù There is nothing left to do but take matters in our own hand. I‚Äôve given in a start, but I‚Äôm hoping others will help finish getting things over the line.
In what little free time I have, I‚Äôve started hacking away on a
new Python tool
(
hopefully it can be ported to Rust at some point ‚Ä¶ but I find it easier to iterate and POC in Python
.)
My hope is that the data community will help add ideas and finish development.
What is
BoggleyWollah
? (
first you should go read Vanity Fair
).
BoggleyWollah
is made to help you ...
Understand the size(s) of the files in your
Lake House
... are they too big or small?
Understand if you have ""dead files"" (unreferenced files).
Data skew via inspecting your partitioning or clustering strategy.
‚Ä¶ hopefully more in the future.
Right now the POC will do the following ‚Ä¶
danielbeach@Daniels-MacBook-Pro BoggleyWollah % python3 src/boggley.py 
INFO:botocore.credentials:Found credentials in environment variables.

INFO:root:Analyzing table: some/table/one

INFO:root:Table metrics: {'total_size_mb': 1238.24, 'total_parquet_files': 176, 'average_file_size_mb': 7.04}

INFO:root:some/table/one Average file size 7.04 MB is less than 256 MB, you should run a compaction job
INFO:root:Dead files found 175 

INFO:root:Analyzing table: some/table/two
INFO:root:Table metrics: {'total_size_mb': 6.09, 'total_parquet_files': 2, 'average_file_size_mb': 3.04}

INFO:root:For table some/table/two Average file size 3.04 MB is less than 256 MB, you should run a compaction job
INFO:root:Dead files found 2
This is all done by ‚Ä¶
boggley = BoggleyWollah(""delta"", ""confessions-of-a-data-guy"", [""some/table/two"", ""some/table/one""])

boggley.analyze_tables()
I‚Äôm only as far as getting `
BoggleyWollah
` to go read a list of tables and find the ‚Ä¶
number of files that make up the table
size of each file
average file size
find dead files
This sets the groundwork to do a few things ‚Ä¶
It starts to answer the question based on the total table size ‚Ä¶
Are your Lake House tables COMPACT enough
too big, too small, etc.
Do we have unreferenced and dead files.
aka, do you need a VACUUM
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Listing all said files for a table.
Calculate table metrics ‚Ä¶
Strip down a s3 key to the file, get dead files.
Checking average file sizes, analyzing tables.
One of the major pieces left is to figure out how to look at data skew via partitions and counting. We will probably use Daft for these method(s).
Once that is done I feel like that is a good first POC to finding out if we have a healthy Delta Lake or not‚Ä¶ we can start to have something worth it‚Äôs weight in salt.
The partial answer to is my Delta Lake or Lake House health?
(
I plan to add Iceberg support
) ‚Ä¶
What is the average size of parquet files in my table(s)
Compared to my table size, do I need to OPTIMIZE or not?
Do I have lots of dead files, do I need to VACUUM?
What else makes a Lake House healthy? I think one question that can maybe answered without too much problem would be the data skew and partition/clustering question.
It should be fairly easy to find out the clustering or partitioning keys for a table and to do a simple groupBY and count with Daft or something else.
This could answer the skew question fairly well.
But what else do you look at for Lake House health?
How do we understand data models and data types, what about NULL values etc? I feel that wanders more into the Data Quality world, but it‚Äôs hard to draw the line.
Either way, if you are bored head over to
https://github.com/danielbeach/BoggleyWollah/tree/main
and help me work on this tool, the hope is to get both Delta Lake and Iceberg support for the following questions.
OPTIMIZE or not (file sizes)
VACUUM or not (dead files)
Skew (by looking at partition or clustering counts)
Once that is done it would be great to get the tool onto PYPI."
152213888.turkey-day-is-here-black-friday-sale,Turkey Day Is Here - Black Friday Sale - %50 Off,... come one come all ...,2024-11-26 16:39:03.881-06,"Well, another turkey day has come upon us all. I trust you are getting at least a day or two off from your overlords from writing code and taking names.
While the rest of you will be slicing up that turkey with your friends and family, clinking your glasses and giving toasts to each other, I will be stuck behind my glowing screen taping away at my keyboard producing juicy content for you all.
So, from now through this weekend, in a Black Friday sale extravaganza,
%50 off memberships are available.
Get 50% off for 1 year
What do you get?
Exclusive paid-only content (4-5 paid-only pieces per month)
Full archives
Data Engineering Central Podcast full access
A hand in helping me bring the hammer down on unsuspecting SaaS victims.
I appreciate your support in helping me to continue to bring interesting and sometimes controversial content into the Data Engineering space.
We recently reached over 14k subscribers to the newsletter and about 17k followers on Substack.
Get 50% off for 1 year
Looking forward to bringing you more hard-hitting content and doubling down on the Podcast! As always, send me a message if you have an idea or topic you want covered."
151949596.data-engineering-central-podcast-1dc,Data Engineering Central Podcast - 04,Episode 04,2024-11-20 17:17:22.226-06,"It‚Äôs time for another episode of the Data Engineering Central Podcast. In this episode we cover ‚Ä¶
Apache Airflow vs Databricks Workflows
End-of-Year Engineering Planning for 2025
10 Billion Row Challenge with DuckDB vs Daft vs Polars
Raw Data Ingestion.
As usual, the full episode is available to paid subscribers, and a shortened version to you free loaders out there, don‚Äôt worry, I still love you though."
151935510.duckdb-delta-lake,DuckDB + Delta Lake.,... just checking for rough edges.,2024-11-25 07:23:11.165-06,"I‚Äôve done a lot of playing around with Delta Lake in my day, enough to have fallen in and out of love a few different times. I think for those of us who grew up in the land of Kimball and SQL Server, before the demi-gods of Snowflake and Databricks came along, we still can‚Äôt believe such a thing as a Lake House exists. What a wonder indeed.
Anywho, I‚Äôve been meaning to give Delta Lake + DuckDB a try and see what‚Äôs cracking.
I‚Äôm not sure I have a particular purpose in doing so, other than that I‚Äôve done similar things with Non-Spark tools like Daft and found great success. We‚Äôve all heard how fast DuckDB is, so it should be interesting to unleash it on Delta Lake and see what happens.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
DuckDB + Delta Lake (open source / non-Databricks)
I think we should take a dual approach to this little DuckDB experiment. It will be a two-pronged attack.
Laptop - DuckDB + local open/source Delta Lake.
Laptop - DuckDB + local open/source Delta Lake (s3 backend).
I‚Äôm not really sure if there will be much, or any difference between these two approaches. I am curious generally if when we move onto testing DuckDB with s3, what happens with credentials and such things, but let‚Äôs not get ahead of ourselves.
Let‚Äôs just try out a local setup of DuckDB + open-source Delta Lake.
Share
Setting up a local environment for DuckDB + Delta Lake.
Let‚Äôs start with the easy one, just a local laptop setup for DuckDB and Delta Lake, we will wrap it up with Docker so you can do the same yourself.
As far as I can tell, at the moment, there isn‚Äôt any WRITE support with DuckDB + Delta Lake, so we will have to resort to some other package to get that part done (
the creation of the Delta Lake that is
).
Read support for Delta Lake is nice, but it would be even better to have write support ‚Ä¶ the full deal that is. At this point, the idea would be to use DuckDB as a pure analytics crunching machine on top of Delta Lake.
Let‚Äôs use the following basic Dockerfile.
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

VOLUME [""/app""]

RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y \
    && . ""$HOME/.cargo/env"" \
    && cargo --version

RUN git clone https://github.com/danielbeach/datahobbit.git

ENV PATH=""/root/.cargo/bin:${PATH}""

CMD [""python""]
Here is our
requirements.txt
duckdb
getdaft[deltalake,sql]
deltalake
To build the image, simply use ‚Ä¶
docker build . --tag=ducky
To drop into the container to mess around you can ‚Ä¶
docker run -it ducky /bin/bash
Now let‚Äôs make ourselves a dataset with
my wonderful datahobbit tool
.
git clone https://github.com/danielbeach/datahobbit.git
cd datahobbit && cargo run -- schema.json output.csv --records 10000000
Now that I‚Äôm about to convert this 10 million line CSV file over to a Delta Lake table format, with some tool,
I just remembered I recently wrote about a tool called Daft
, using Delta Lake. Let‚Äôs use that project/tool to convert our CSV file to a Delta Lake we can read with DuckDB.
Let‚Äôs get that CSV into Delta Lake format.
import daft

df = daft.read_csv('output.csv')
df.write_deltalake(""example_deltalake"", mode=""overwrite"")
Well, it worked, that‚Äôs something.
root@dcd6ec0c818d:/app/datahobbit# ls example_deltalake/
0-06d5b7fb-757b-47d0-9061-78f2bbb63b5c-0.parquet  _delta_log
We now have a Delta Table with about 10 million records, enough to play around with DuckDB finally!
>>> df.count().show()
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ                                                                                                                                       
‚îÇ count    ‚îÇ                                                                                                                                       
‚îÇ ---      ‚îÇ
‚îÇ UInt64   ‚îÇ
‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°
‚îÇ 10000000 ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
DuckDB + Delta Lake ‚Ä¶ locally.
Well, let‚Äôs hope this is blissfully boring, shall we? We don‚Äôt want any surprises and we want it to be simple and fast.
import duckdb
df = duckdb.delta_scan('delta_example');

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: module 'duckdb' has no attribute 'delta_scan'
Ah yes, I‚Äôm a DuckDB padawan, so I forget things easily.
INSTALL delta;
LOAD delta;
So, to back up a bit.
import duckdb

duckdb.sql(""""""
INSTALL delta;
LOAD delta;
"""""")
duckdb.sql(""""""
SELECT * 
FROM delta_scan('delta_example');
"""""").show()
Ok, so that‚Äôs always a great sign when things work out of the box like that, no funny stuff going on, no buttons to push or secret switches to flip.
Let‚Äôs try running and aggregation query, and writing the results to a CSV file.
Let‚Äôs do some query like this.
from datetime import datetime
import duckdb

t1 = datetime.now()
duckdb.sql(""""""
INSTALL delta;
LOAD delta;
"""""")

duckdb.sql(""""""
SELECT age, is_active, AVG(age) as avg_age
FROM delta_scan('example_deltalake')
GROUP BY age, is_active
"""""").write_csv(""results.csv"");
t2 = datetime.now()
total = t2-t1
print(f""it took {total} to run this query."")
Well, I didn‚Äôt expect that.
it took 0:00:00.462964 to run this query.
So, DuckDB did the work, very fast, less than half a second for 10 million rows of aggregationon Delta Lake. That‚Äôs pretty fast!
Should we try it with Daft just to see how fast DuckDB is so we can get an idea??
Of course. Let‚Äôs try this same aggregation with Daft and get an idea of just how fast DuckDB is since we really don‚Äôt have a baseline.
import daft
from daft.sql import SQLCatalog
from datetime import datetime

t1 = datetime.now()
df = daft.read_deltalake('example_deltalake')
catalog = SQLCatalog({'data': df})

results = daft.sql(""""""
SELECT age, is_active, AVG(age) as avg_age
FROM df
GROUP BY age, is_active
"""""", catalog=catalog)
results.write_csv('daft_results')
t2 = datetime.now()
total = t2-t1
print(f'it took {total} to run the query')
Very interesting indeed.
it took 0:00:01.145941 to run the query
I think we can safely say that DuckDB is VERY fast, compared to Daft at least ‚Ä¶ and Rust based Daft is very fast at just about everything. Amazing, DuckDB is 3x faster than Daft at running an aggregation on Delta Lake.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
But what about an AWS s3 based Delta Lake inside + DuckDB.
This will be very interesting won‚Äôt it? Will DuckDB be able to handle a remote s3 Delta Lake well, and will it still be fast and easy??
There is our same Delta Lake, except up in the cloud.
from datetime import datetime
import duckdb

t1 = datetime.now()
duckdb.sql(""""""
INSTALL delta;
LOAD delta;
CREATE SECRET delta_s1 (
    TYPE S3,
    PROVIDER CREDENTIAL_CHAIN
);
"""""")

duckdb.sql(""""""
SELECT age, is_active, AVG(age) as avg_age
FROM delta_scan('s3://confessions-of-a-data-guy/duckdbtest/')
GROUP BY age, is_active
"""""").write_csv(""results.csv"");
t2 = datetime.now()
total = t2-t1
print(f""it took {total} to run this query."")
And the results.
it took 0:00:07.065167 to run this query.
I mean it ran, but took way longer for sure, than local, this is not a surprise I would suppose.
Let‚Äôs try Daft to see what happens here.
import daft
from daft.sql import SQLCatalog
from datetime import datetime

t1 = datetime.now()
df = daft.read_deltalake('s3://confessions-of-a-data-guy/duckdbtest/')
catalog = SQLCatalog({'data': df})

results = daft.sql(""""""
SELECT age, is_active, AVG(age) as avg_age
FROM df
GROUP BY age, is_active
"""""", catalog=catalog)
results.write_csv('daft_results')
t2 = datetime.now()
total = t2-t1
print(f'it took {total} to run the query')
Interesting indeed! Daft took the lead.
it took 0:00:03.706798 to run the query
Daft is more than twice as fast as DuckDB when it comes to remote s3 execution of a Delta Lake query!!
Strange indeed are the times we live in. But, you know, the real world of data tools never ends up being what we think it will be like.
Who would have saw that coming??
These results actually make the most sense to me. Rarely is one tool the golden child that does us no ill. Those we love most usually hurt us the most.
DuckDB, like our children, went from being the quickest tool and brining us rainbows and unicorns to the slowest of the slow, making us wonder where we went wrong. This is life.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
151333948.10-billion-row-challenge-duckdb-vs,10 billion row challenge. DuckDB vs Polars vs Daft.,... just for fun.,2024-11-14 14:34:55.913-06,"Sometimes I find myself lying in my sunroom, staring out the window in the blue sky above me while the sun plays on the maple tree, empty of most all but a few red leaves ‚Ä¶ wondering what else I can do to make the already angry readers of my babbling even more angry.
There is an art to it, I swear.
I am periodically overcome with esoteric questions about why anyone would read what I have to say in the first place, but then I remind myself of the truth.
There is a massive number of marketing machines that night and day puke forth a glut of data drivel that falls right onto the heads of the (mostly) unsuspecting data engineering victims who happen to be my audience.
I‚Äôm here to do the Lord‚Äôs work.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Someone has to keep them in line, someone has to be willing to stand up and fight back and rage against the machine. Today, we will do just that.
We will do that 10 billion times.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Take the survey.
Honestly, this is just for fun, and of course, it will be interesting. I have something I want you to do. Don‚Äôt lie. Don‚Äôt cheat,
vote for who you think is going to be fastest
.
Performance on 10 billion records. DuckDB. Polars. Daft.
So, just to keep the talking heads off my back as long as possible when it comes to this non-TCP benchmark, finger in the wind test, I will use an AWS EC2 Instance for the compute and store the data as Parquet files in s3.
2 vCPU and 8GB RAM should be enough to see which one of these tools has the gumption to get the job done without puking. Running things on commodity hardware is important.
Next, we need to generate a dataset with
10 billion records
. I‚Äôm going to use my recently built Rust-based tool
datahobbit
. The problem is it only generates CSV files, so I updated it to pump out the results into Parquet format optionally.
Also, we need to do a few basic commands on our new EC2 instance, just a get a few tools that will come in handy.
Side Note
: if you are a Data Engineer who is unfamiliar with the command line, Ubuntu/Linux, ssh‚Äôing to machines, etc. It‚Äôs an extremely useful set of skills that will make you very valuable. Strike while the iron is hot.
Now that we are in our newly acquired EC2 instance, let‚Äôs update some apts, install git, and pull down our
datahobbit
so we can generate or 10 billion row parquet dataset.
Let‚Äôs make our dataset shall we?
Easy enough ‚Ä¶ you‚Äôre welcome. Let‚Äôs push these files out to s3, shall we?
Looks like we ended up with about 173 parquet files totaling 16.5 GBs of data. This should be very easily ingested by all the tools we want to test.
In case you are wondering, our data looks like this.
I think this test will be interesting for a few reasons.
How well the tool can read lots of files on s3
How well it can run a basic aggregation
How good the tool is at looking for default AWS creds (in .aws )
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Polars vs DuckDB vs Daft ‚Ä¶ 10 billion records.
Now that the hard part is behind us, let‚Äôs do what we came here to do. See if these data tools that each claim to be the GOAT, are just sheep or the real deal.
Let‚Äôs start with DuckDB.
DuckDB with 10 billion records in s3 parquet files.
So, I‚Äôm not a DuckDB expert, I don‚Äôt use it much, but this is my best guess based on the documentation of the best way to read a bunch of parquet files in s3 and write the results out to a CSV on s3.
I am using a depreciated way to grab local AWS credentials, it won‚Äôt work magically (unlike other tools), and this depreciated load_aws_credentials() seemed like the path of least pain.
Besides the performance of this DuckDB code, what else do we note about what we had to write?
clean and simple code
doesn‚Äôt appear to automatically sense we are doing s3 stuff and pickup default credentials from the system
clearly SQL-based
The first run resulted in OOM (which is nothing new to DuckDB).
This isn‚Äôt really a surprise if the work isn‚Äôt done in a lazy manner as our dataset is slightly bigger than our machine‚Äôs memory size (16.5GB vs 16GB).
Let‚Äôs add three lines to our DuckDB code.
I‚Äôm throwing mud at the wall and hoping one, or the combination of these settings will allow DuckDB not to choke.
SET temp_direcotry = ‚Äò/tmp/duckdb_swap‚Äô;
SET memory_limit = ‚Äò15GB‚Äô;
SET max_temp_directory_size= '25GB';
still no juice!
I have no idea, maybe I should just remove the
max_temp_directory_size
setting, as the above error says ‚Ä¶. ‚ÄúBy default, this setting utilizes the available disk space on the drive where the `temp_directory` is located.
That was a no go, still the same error. I simply cannot make DuckDB not puke with OOM.
Moving on.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Daft with 10 billion records in s3 parquet files.
Daft is one of my new favorite Dataframe (and now SQL) tools to use. It‚Äôs fast as crud and easy to use, it has very straightforward and simple APIs to work with.
You don‚Äôt have to throw salt over your left shoulder to get it to work ‚Ä¶ cough ‚Ä¶ unlike other tools.
I mean look at this code. Seriously.
I don‚Äôt have to tell it about credentials or that I‚Äôm doing AWS stuff, I don‚Äôt have to tell it to do memory antics at all. It just flipping works.
Now finally some results. Simple clean code with no boilerplate, worked on the first try,
2:25 minute runtime
. 10 billion records in s3, not bad Sunny Jim.
Crazy simple code.
It‚Äôs smart enough to take away boilerplate like AWS creds etc.
It‚Äôs fast.
It has SQL.
All bow before the power of Daft.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Polars with 10 billion records in s3 parquet files.
And now we come to the self-proclaimed GOAT of the new and rising Rust-based Dataframe tools. Shall we see if it deserves this recognition or not?
Well, not hard to beat DuckDB that simply won‚Äôt work without some sort of black magic of which I have not been endowed.
I‚Äôm interested to see not the Polars code itself compared to Daft, but the performance. Being both Rust-based gives them each a distinct advantage to be fast, but when we start dealing with files in the cloud, s3, this adds a layer of complexity that is really going to show who put the time in.
What do we note about this code?
Didn‚Äôt require any AWS or credential funnies, it picked up the defaults of the machine fine.
We had to use scan_parquet etc to force lazy evaluation.
You have to be in the know about lazy vs eager and things like collect() when using Polars on large datasets.
Offers SQL, and fairly clean code.
First error with Polars.
Apparently, Polars is smart enough to scan_parquet on an s3 folder without trouble, but not smart enough to simply write_csv to an s3 location. Annoying as crud.
We must do some more Python that we are used to, unfortunately. That old s3fs must be used, the little blighter.
And the performance.
Not bad, pretty fast 2:30, only 5 seconds slower than Daft (
which is at 2:25
).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Summing it all up.
Well, not sure I saw all that coming.
I thought DuckDB had fixed its OOM issues
, but apparently parquets in s3 were enough to throw it off. Not sure what else I could have done, I‚Äôm simply only going to spend so much time trying to make it work before I move on.
I thought Daft would blow Polars out of the water, but they both came in around the same execution time.
It appears the Daft code is a little cleaner, less nuances and things to deal with, more straightforward compared to the Polars code, although Polars wasn‚Äôt that bad.
I accomplished what I wanted today.
In the end I just try to take the best tools available and put them through some sort of normal workflow that would mirror some production pipeline.
Production Pipelines don‚Äôt run on people‚Äôs laptops. They are on commodity hardware like EC2, they read and write data to s3. This is where the difference is.
Many of the posts I see are people doing things locally, which is all fine and dandy, but honestly, it‚Äôs the small things like the ability to work in the cloud that make the difference between a fun play-toy on a laptop, and running the real deal in production.
What say you?"
151163389.raw-data-ingestion,Raw Data Ingestion ..., ... in the Lake House world ...,2024-11-18 07:18:20.985-06,"This is an interesting topic, is it not? I feel like in our rat race of Data Engineering, in our quest to do the fancy and new, to impress ‚Ä¶ we miss the forest for the trees.
While we are busy worrying about the newest Databricks or Snowflake feature, the basics go ignored and lonely, forgotten and beaten down by time.
When was the last time you heard someone talk about staging raw data strategies?
It's not ML or AI, it's boring on the surface.
Raw Data Staging - Surprisingly a diverse set of approaches.
While I was pondering the supposedly simple problem of staging raw data, I decided to query
my LinkedIn followers
to see what they had to say on the subject. It seems to be one of those subjects you just assume an answer ‚Ä¶ ‚Äú
I just do it this way.
‚Äù Without any rhyme or reason.
Surprisingly or not, I got about 32 comments to my question ‚Äú
How do you stage raw data.
‚Äù
The answers were as varied as you would expect, and all over the board.
I could go on, but you get the picture. It‚Äôs clear to me reading through them, and thinking about the problem of staging or raw data tables, that we are pulled between two opposing forces in our Data Engineering minds.
the need to have an untouched and as close to the exact raw copy of data as possible.
the need to minimize data copies, have efficient partitions, clustering, and enforce some type of schema.
Of course, this is where the arguments and mud-throwing ensue in full force. Some argue that touching raw data on ingest is to introduce bugs and defeat the entire purpose of raw staging.
Others say ‚Äú
Baaahumbug, we know the data we need and want, let‚Äôs enforce things off the bat and partition our data efficiently.
‚Äù
Hmm ‚Ä¶ what do I think??? I think they are both correct. This is the crux and difficulty of Data Engineering at scale. We must make hard decisions and design resilient systems that can lead us to glory.
I think most likely the ‚Äúcorrect answer‚Äù is the one that works well, without problems, without sliding to one extreme or the other.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Typical raw data staging strategies.
Probably one thee most famous approaches to modeling data, including raw data, is the
Medallion Architecture brought to us by Databricks
. It is most specifically used for Lake House data modeling, although in truth, it doesn‚Äôt differ much from the Kimball-style Data Warehouses that have been built for decades.
What I find interesting, is the above screenshot from the Databricks website is a particular sentence ‚Ä¶ ‚Äú
Landing zone for raw data, no schema needed.
‚Äù
I find this a bit cheeky because is the Databricks and Delta Lake world of the Lake House ‚Ä¶ there is no such thing as ‚Äúno schema‚Äù in that world. Every single Iceberg, Delta, or Hudi table in and of itself has a schema.
But, I get the drift, I guess we don‚Äôt have to squabble over words to this extent. Here is what Databricks says about their suggested raw layer.
‚ÄúThe
Bronze layer
is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures ""as-is,"" along with any additional metadata columns that capture the load date/time, process ID, etc.‚Äù - Databricks
I actually agree with this assessment and I believe it sums of the Best Practice of loading and staging raw data.
Let the data flow in ‚Äúas-is.‚Äù
YOU add additional columns and metadata to allow for your own needs (partitioning, clustering, etc.)
What it boils down to, for obvious reasons, is that we do need some sort of raw copy of our data in the real world we live in. Not being able to trace back a record to its source and raw state is the ultimate recipe for trouble.
But, that need must be balanced by the best practices as Engineers and managers of data. We need correctly partitioned and clustered data at a minimum, otherwise our designs won‚Äôt scale.
What does that look like in real life? Usually something like this.
I‚Äôm not sure why it has to be so complicated, but I guess the devil is in the details. What I found interesting in many of the responses related to how people should handle raw data, is that in fact, many engineers decided to move some of those Green ‚Äútransformations‚Äù above back into the Blue layer.
I would at the outset say this is incorrect, but it‚Äôs hard to know without looking at each individual problem space. There are a few things that I would NOT yell at someone for doing.
transforming
DATE
or
TIMESTAMP
columns that might be used as Partitioning or Clustering keys.
Enforcing some extremely basic schema to reject bad files/data
It‚Äôs hard to know where to draw the line though. I would say we should err on the side of caution. Land the data as raw as possible, touch it as little as possible, and move on to the rest of the pipeline.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
A moment of your time about Meta Data.
I think there is one last subject we should touch on briefly when it comes to raw staging of data, one that is mostly ignored until it is too late. This is the addition of critical metadata information appended to the raw data.
‚ÄúIn a data engineering context,
metadata
refers to ""data about data."" It provides descriptive information about datasets, enabling better understanding, management, and utilization of the data. Metadata is essential for organizing, searching, and governing data within a data ecosystem.‚Äù - ChatGPT
In a good Data Platform, this includes ‚Ä¶
insert/creation timestamps and dates
raw data file/system source information
aka ‚Ä¶ this record came from s3://some-bucket/some_file.csv
etl or other job id‚Äôs that loaded the data
Of course, there can be more, but that, at a bare minimum should be included in most staging and raw datasets. We are really talking about the ability to unwind and work backward to follow the path a particular dataset or record took to come into existence in our world.
Let me know your thoughts, should it be more complicated, less???
Leave a comment"
151136439.end-of-year-engineering-planning,End of Year Engineering Planning for 2025, ... how to do it ...,2024-11-06 11:22:34.852-06,"How many of you little hobbits are thinking more than 5 minutes from now? You‚Äôre probably just thinking about what new Snowflake or Databricks feature you want to play with. What to post about on Reddit. When should you finally call your mom to check in? That you should finally go to the gym. What you might get for Christmas.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
The end of the year is nye upon us as the leaves change and the cold wind and rains start blowing in. The end of the year means one thing ‚Ä¶ you should have already started your Engineering planning for 2025.
Whether you‚Äôve ever even thought about this process probably depends on where you are at in your career, or your position at work. But, even though ye may be a lowly sniveling weevil digging little holes in mountains of code, I dare you to stick your head up into the sunshine and do yourself a favor. Start planning, or learning how to do Engineering planning for the upcoming year.
If you are old or new, young or old, whatever, Engineering planning can be an esoteric sort of process that seems to be like staring into a crystal ball and saying strange incantations at midnight.
I think we can unravel that mystery for you. It‚Äôs not that bad, it‚Äôs just dreaming and thinking big, high level, and then bringing it all back down to earth.
Let‚Äôs jump into what that process looks like.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
How to do Engineering Planning.
I‚Äôm going to make this as straightforward and concrete as possible, I really don‚Äôt think it needs to be that hard. Not doing Engineering Planning for the new year ahead is a grave mistake that too many Data Teams make.
It‚Äôs too easy to wander around in the dark lands of data, stubbing your toe and bumping your head against the rocks hidden on the wayside. You‚Äôll never ‚Äúget ahead‚Äù or ‚Äúget on top‚Äù of things if you don‚Äôt plan.
Engineering Planning for the year ahead will ‚Ä¶
Help you keep a clear ahead when you‚Äôre in the pits next year
Hold yourself and your team accountable
Enable you to grow stronger technically as a team and individual
Enable you to implement new features and technologies
Keep ahead of tech debt
Deliver value to the business
You are lost without a map, just wandering, doing a lot, and never getting anything done. The Engineering Plan, at a high level, will solve those issues.
This process outlined below is somewhat simplified, there is much nuance, but this is as good a place to start as any.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Mapping out the Engineering Planning Process.
Let‚Äôs break this process down a little more, from the above image, into actionable steps.
Business Priorities
The number one most important thing to do, even if it requires you kidnapping someone, is to get the C-suite/Executives to lay down on paper the Product Roadmap and their desires and priorities for the coming year.
This stuff will drive 80%+ of what Engineering works on in the next cycle.
The timing of the desired outcomes and products/features is key, we need to be able to lay them out over a calendar.
Engineering Wishlist
Second, after Business Priories, are the wants and needs of Engineering as a whole. What upgrades, tooling changes, infrastructure additions, or whatever that we ‚Äúwant‚Äù or ‚Äúwish‚Äù to happen?
This is not tech debt, this is a wishlist of where ‚ÄúEngineering wants to drive towards in the future.‚Äù
This keeps people happy and engaged and the long-term viability of the Data Platform in play.
This will be a much smaller list than Business Priorities, probably 1-5 things might get picked from this list and done.
Engineering Technical Debt
It‚Äôs critical for the long-term success of any Data Platform that dealing Technical Debt is planned for, and put into the normal work cycle.
If you don‚Äôt PLAN for tackling some of your serious tech debt, your Platform is doomed to fail over time.
The rest will follow. It‚Äôs really about gathering all available plans and desires from Product first, then Engineering, laying them out. This gives a good idea of what the next year will generally look like.
It will force Product and C-Suite to communicate their plans and priorities (that you can hold them to).
It will give Engineering a general idea of what coming at them AND what they need to do to prepare for that work
.
Engineering will have to prioritize their own wishlist and pick a few things from the list.
Engineering can hold themselves accountable for pushing the Data Platform forward.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
It‚Äôs a give and take, you can‚Äôt have your cake and eat it too.
I view Engineering Planning for the new year as mostly an art, and less of a science. The thing is, you just need to do it. Some plan and some idea of what is coming next year is better than just winging it like most people.
Sure, you can just juggle stuff as it comes, work randomly on this and that when you have time, but to be a next level Data Engineering team you have to put expected actions and outcomes down on paper, you have to look yourself and the work in the mirror and make hard decisions.
It‚Äôs one thing to generally wave your hands in the air and say ‚Ä¶
we want to adopt x, y, and z new tech stack.
we want to address this pain point.
it sounds like Product has some new things to work on.
It‚Äôs another to actually talk to real-life people, get stakeholder input, combine it with Engineering‚Äôs desired plans, and start making hard decisions and mapping out a year‚Äôs worth of work.
I mean it will probably only be 70% right if you‚Äôre lucky, but it will help you get the right things done in the right order.
Engineering planning can be summed up as making a giant list of TODOs, Must-Dos, Wish-Lists, and the like, and spending time thinking about ‚Ä¶
How complex and long each project generally is.
What needs to be in what order.
Understand blockers and dependencies ahead of time
The ability to be forced to pick the few most important things that need to get done.
The ability to be held accountable to others and yourself.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Sharing and Verbalizing the Plan.
So once you‚Äôve done the leg work, made a nice Google Drive doc, talked to Engineers, talked to Product, sat around staring at the trees and dreaming about your wishlist for Engineering ‚Ä¶ you have it all in one spot, the good, the bad, the ugly.
The next part is actually the hard one.
You must verbalize and disseminate this plan.
Putting a plan on paper forces people to be involved.
Expect pushback and changes, that is a GOOD thing.
The reality on paper with a planned timeline will always be different than what‚Äôs been in people‚Äôs minds.
So, be brave, be different. Make a plan. Write it down. Think about it. Talk to people about it. Make decisions. Decide what‚Äôs important, and what‚Äôs not.
How do you plan for Engineering in 2025?
Leave a comment"
151041505.duckdb-inside-postgres-pg_duckdb,DuckDB inside Postgres (pg_duckdb) Exposed!,performance tests and more,2024-11-01 13:56:29.964-05,"I recently dove into the new DuckDB INSIDE Postgres extension pg_duckdb ‚Ä¶ performance and the truth, more truth than you can handle. Doing the Lord‚Äôs work.
You know me, I love nothing more than to poke the ole‚Äô tires on anything new and exciting in Data Engineering. Running DuckDB inside Postgres? Yes please."
150667883.duckdb-inside-postgres,DuckDB inside Postgres!!??, ... black magic,2024-10-27 07:45:41.056-05,"Maybe it‚Äôs my age, I‚Äôm not totally sure, but it‚Äôs getting harder and harder for this old guy to keep up with all the new things that come trickling out of the quagmire that is the Data Engineering space these days.
Some of the ideas that come into being simply boggle my mind, so obvious once you see them, so unintuitive that it would be a thing in someone‚Äôs mind and then come to be. Amazing.
One example of that is DuckDB INSIDE Postgres. I had to read that post on Linkedin twice to make sure I wasn‚Äôt hallucinating from whatever I ate for breakfast.
What a genius idea if I do say so myself.
This amalgamation of DuckDB and Postgres caught my eye for a few obvious reasons to anyone who‚Äôs working in Data Engineering for more than a year.
Postgres is ubiquitous across most Data Platforms
DuckDB is blazingly fast
Postgres starts to choke when supporting OLAP on datasets that are ‚Äúlarge‚Äù
Don‚Äôt get me wrong, I love Postgres, it‚Äôs the GOAT of the RDMBS (relational database management system) world. It‚Äôs popular for a reason.
In the Lake House world, I find myself living in low these long and languid years, and even before that, I‚Äôve frequently found myself annoyed with Postgres when it comes to query performance.
I get it.
Postgres shines in the OLTP world, OLAP these days requires either the scale of Databricks or Snowflake, or the speed of Polars, Duckdb, Daft etc. It was genius for someone to take DuckDB and put it INSIDE Postgres.
What does DuckDB say about itself, when they think it should be used in a Postgres context??
‚Äú
PostgreSQL is often used for analytics, even though it's not specifically designed for that purpose. This is because the data is readily available, making it easy to start. However, as the data volume grows and more complex analytical queries involving aggregation and grouping are needed, users often encounter limitations. This is where an analytical database engine like DuckDB comes to the rescue.‚Äù
- DuckDB
You can‚Äôt argue with them, we all know it‚Äôs true.
But, you know me, seeing is believing.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Testing out pg_duckdb ‚Ä¶ aka DuckDB inside Postgres for ourselves.
Either this is going to be wonderfully fun, or wonderfully boring, we will find out. But, we must test this out ourselves. Let‚Äôs do the obvious thing any Data Engineer would do when first finding out about pg_duckdb ‚Ä¶
give er‚Äô the good old kick in the tires.
We will need two things only, well, maybe three.
Postgres with the DuckDB extension installed (
pg_duckdb
)
The ability to generate 50 million records and get that inside Postgres
An OLAP query to run against that dataset to test
old-school Postgres speed
DuckDB inside Postgres speed
Luckily, DuckDB has made this easy by providing us with a p
re-built Docker image
.(
This is a breath of fresh air for a newish project, most people forget this important part, makes everyone‚Äôs life easier in the beginning
)
So, that‚Äôs one problem we don‚Äôt have to solve. Next, we need to generate about 50 million records in a CSV format that we can dump into Postgres.
I found a Rust-based CSV and data file-generating repo on GitHub, but it was so slow and locked my computer up, that I decided to write my own.
Enter
datahobbit
.
Feel free to help contribute to this
open-source codebase
. I think there is of course a lot of room to improve it, turn it into a Python package etc, but that will be for another day.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Putting it all together.
So, as mentioned before, you can obtain the Postgres+DuckDB Docker image by running something like ‚Ä¶
docker run -d -e POSTGRES_PASSWORD=duckdb pgduckdb/pgduckdb:17-v0.1.0
This command will download the image if you don‚Äôt have it already. Next, we need to generate 50 million records for some sort of sample dataset we can push into a Postgres table to use as a test.
We are going to use my Rust-based CLI tool called
datahobbit
. This makes it easily to generate our sample CSV file. (
of course you must clone the repo and and build the project first
)
cargo build --release
./target/release/csv_generator schema.json output.csv --records 50000000
50 million records is barley enough to put Postgres and DuckDB through the paces for a single query, but we shall see.
The schema I‚Äôm setting looks like this (
datahobbdit
requires a schema in JSON format to build the CS
V) ‚Ä¶
{
    ""columns"": [
      { ""name"": ""id"", ""type"": ""integer"" },
      { ""name"": ""first_name"", ""type"": ""first_name"" },
      { ""name"": ""last_name"", ""type"": ""last_name"" },
      { ""name"": ""email"", ""type"": ""email"" },
      { ""name"": ""phone_number"", ""type"": ""phone_number"" },
      { ""name"": ""age"", ""type"": ""integer"" },
      { ""name"": ""bio"", ""type"": ""sentence"" },
      { ""name"": ""is_active"", ""type"": ""boolean"" }
    ]
  }
Getting the data into Postgres.
Next, we need to get all this data in CSV format, dumped into our Postgres+DuckDB image.
Create the container
docker run -it -d --name pg_duckdb -p 5432:5432 -e POSTGRES_HOST_AUTH_METHOD=trust pgduckdb/pgduckdb:17-v0.1.0
Copy file(s) to the container (
run this in the directory that has
our CSV
)
docker cp . pg_duckdb:/tmp/postgres
Connect using psql to Postgres
docker start pg_duckdb
docker exec -it pg_duckdb psql
Create SQL Postgres table to match our schema. (
the index will be to support our analytics query later that we will run for the benchmark
)
CREATE TABLE data (
    id INTEGER,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    email VARCHAR(100),
    phone_number VARCHAR(50),
    age INTEGER,
    bio TEXT,
    is_active BOOLEAN
);
CREATE INDEX idx_data_age_is_active ON data (age, is_active);
COPY that data (CSV) into the Postgres table.
COPY data FROM '/tmp/postgres/output.csv' DELIMITER ',' CSV HEADER;
Run this query
WITHOUT
DuckDB and
JUST
Postgres.
EXPLAIN ANALYZE
SELECT
    age,
    is_active,
    COUNT(*) AS user_count
FROM
    data
GROUP BY
    age,
    is_active;
Results of plain ole‚Äô Postgres
Planning Time: 0.617 ms
 JIT:
   Functions: 21
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 2.204 ms (Deform 0.786 ms), Inlining 101.451 ms, Optimization 108.949 ms, Emission 64.002 ms, Total 276.607 ms
Execution Time: 3971.350 ms (3.97135 seconds)
(22 rows)
force Postgres to use DuckDB to execute SQL
SET duckdb.force_execution = true;
Results of DuckDB
Custom Scan (DuckDBScan)  (cost=0.00..0.00 rows=0 width=0) (actual time=7878.442..7878.541 rows=1 loops=1)
   DuckDB Execution Plan: 
 
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
 ‚îÇ‚îÇ    Query Profiling Information    ‚îÇ‚îÇ
 ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 EXPLAIN ANALYZE SELECT age, is_active, count(*) AS user_count FROM pgduckdb.public.data GROUP BY age, is_active
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
 ‚îÇ‚îÇ               Total Time: 5.85s              ‚îÇ‚îÇ
 ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ           QUERY           ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ           0 Rows          ‚îÇ
 ‚îÇ          (0.00s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ      EXPLAIN_ANALYZE      ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ           0 Rows          ‚îÇ
 ‚îÇ          (0.00s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ       HASH_GROUP_BY       ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ          Groups:          ‚îÇ
 ‚îÇ             #0            ‚îÇ
 ‚îÇ             #1            ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ        Aggregates:        ‚îÇ
 ‚îÇ        count_star()       ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ         2000 Rows         ‚îÇ
 ‚îÇ          (0.60s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ         PROJECTION        ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ            age            ‚îÇ
 ‚îÇ         is_active         ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ       50000000 Rows       ‚îÇ
 ‚îÇ          (0.01s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ         TABLE_SCAN        ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ         Function:         ‚îÇ
 ‚îÇ     POSTGRES_SEQ_SCAN     ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ        Projections:       ‚îÇ
 ‚îÇ            age            ‚îÇ
 ‚îÇ         is_active         ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ       50000000 Rows       ‚îÇ
 ‚îÇ          (5.22s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 
 
 Planning Time: 113.380 ms
 JIT:
   Functions: 1
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 0.270 ms (Deform 0.000 ms), Inlining 0.000 ms, Optimization 0.000 ms, Emission 0.000 ms, Total 0.270 ms
Execution Time: 7884.068 ms (7.884068 seconds)
(69 rows)
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
That‚Äôs not what I expected.
Interesting indeed, I honestly did not expect that, I thought DuckDB would be much quicker, I mean it is known for its quickness,
but apparently the GOAT of databases called Postgres isn‚Äôt ready to be retired yet.
What‚Äôs going on here?
I wonder if we can close the gap with 100 million records maybe?? I went ahead and cleaned up the resources and started from scratch again, this time generating a 100 million record file with
datahobbit
.
Here we go again.
Raw Postgres.
Finalize GroupAggregate  (cost=1912472.21..1912575.55 rows=400 width=13) (actual time=45861.045..45925.503 rows=2000 loops=1)
   Group Key: age, is_active
   ->  Gather Merge  (cost=1912472.21..1912565.55 rows=800 width=13) (actual time=45860.769..45924.410 rows=6000 loops=1)
         Workers Planned: 2
         Workers Launched: 2
         ->  Sort  (cost=1911472.19..1911473.19 rows=400 width=13) (actual time=45838.693..45838.920 rows=2000 loops=3)
               Sort Key: age, is_active
               Sort Method: quicksort  Memory: 111kB
               Worker 0:  Sort Method: quicksort  Memory: 111kB
               Worker 1:  Sort Method: quicksort  Memory: 111kB
               ->  Partial HashAggregate  (cost=1911450.90..1911454.90 rows=400 width=13) (actual time=45837.022..45837.267 rows=2000 loops=3)
                     Group Key: age, is_active
                     Batches: 1  Memory Usage: 241kB
                     Worker 0:  Batches: 1  Memory Usage: 241kB
                     Worker 1:  Batches: 1  Memory Usage: 241kB
                     ->  Parallel Seq Scan on data  (cost=0.00..1845538.80 rows=8788280 width=5) (actual time=0.232..40643.594 rows=33333333 loops=3)
 Planning Time: 6.428 ms
 JIT:
   Functions: 21
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 1.616 ms (Deform 0.444 ms), Inlining 266.226 ms, Optimization 89.928 ms, Emission 102.221 ms, Total 459.990 ms
Execution Time: 46156.029 ms (46.156029 seconds)
(22 rows)
And here is pg_duckdb.
Custom Scan (DuckDBScan)  (cost=0.00..0.00 rows=0 width=0) (actual time=64438.844..64440.117 rows=1 loops=1)
   DuckDB Execution Plan: 
 
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
 ‚îÇ‚îÇ    Query Profiling Information    ‚îÇ‚îÇ
 ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 EXPLAIN ANALYZE SELECT age, is_active, count(*) AS user_count FROM pgduckdb.public.data GROUP BY age, is_active
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
 ‚îÇ‚îÇ              Total Time: 75.96s              ‚îÇ‚îÇ
 ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ           QUERY           ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ           0 Rows          ‚îÇ
 ‚îÇ          (0.00s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ      EXPLAIN_ANALYZE      ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ           0 Rows          ‚îÇ
 ‚îÇ          (0.00s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ       HASH_GROUP_BY       ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ          Groups:          ‚îÇ
 ‚îÇ             #0            ‚îÇ
 ‚îÇ             #1            ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ        Aggregates:        ‚îÇ
 ‚îÇ        count_star()       ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ         2000 Rows         ‚îÇ
 ‚îÇ          (9.36s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ         PROJECTION        ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ            age            ‚îÇ
 ‚îÇ         is_active         ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ       100000000 Rows      ‚îÇ
 ‚îÇ          (0.06s)          ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚îÇ         TABLE_SCAN        ‚îÇ
 ‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ
 ‚îÇ         Function:         ‚îÇ
 ‚îÇ     POSTGRES_SEQ_SCAN     ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ        Projections:       ‚îÇ
 ‚îÇ            age            ‚îÇ
 ‚îÇ         is_active         ‚îÇ
 ‚îÇ                           ‚îÇ
 ‚îÇ       100000000 Rows      ‚îÇ
 ‚îÇ          (66.46s)         ‚îÇ
 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 
 
 Planning Time: 33.035 ms
 JIT:
   Functions: 1
   Options: Inlining true, Optimization true, Expressions true, Deforming true
   Timing: Generation 2.204 ms (Deform 0.000 ms), Inlining 0.000 ms, Optimization 0.000 ms, Emission 0.000 ms, Total 2.204 ms
Execution Time: 72970.699 ms (72.970699 seconds)
Well, I guess that results were consistent anyway, even though it is surprising. I thought maybe the more records I added, the closer DuckDB would get to Postgres analytics query performance, not farther.
What‚Äôs going on here?
Well, I think before we poo poo on
pg_duckdb
too much, let‚Äôs step back a moment. I personally don‚Äôt think that simply running DuckDB queries inside Postgres is the most useful thing that pg_duckdb provides.
According to
DuckDB blog announcing this release
we can do all sorts of interesting DuckDB ‚Äúthings‚Äù INSIDE Postgres.
Install extensions and read datasets like Iceberg, for example.
Write back out to a Lake House or some other cloud storage location from inside DuckDB
Think about this for a moment. How do people normally run some query inside Postgres and then pump the results to some CSV file in s3?
Most likely add something like Python with a bunch of logic to do this.
This sort of feature,
pg_duckdb
that is, is about innovation. It allows Data Engineers to design novel new ways of combining and processing data.
This is a big deal because ALOT of people use Postgres.
Why is pg_duckdb so much slower than raw Postgres?
This is a good question, and you can
find the answer on Reddit
.
What I find unsurprisingly obvious is that in the
blog announcement by DuckDB
, seen below, seems to indicate, with a contrived TPC benchmark, the opposite of what we saw, that pg_duckdb is much faster than just Postgres.
‚ÄúWith
pg_duckdb
, you can use the DuckDB execution engine within PostgreSQL to work with data already stored there, and for some queries, this can result in a dramatic performance improvement ‚Ä¶‚Äù -
DuckDB
Notice something they mentioned? ‚Äú
‚Ä¶ without indexes ‚Ä¶
‚Äù  Who the flip runs a TPC benchmark AGAINST Postgres and decides to do it with ZERO Postgres indexes?
Someone trying to cook the books, that‚Äôs who.
Don‚Äôt forget, DuckDB promised us something at the very beginning of their announcement.
‚Äú
PostgreSQL is often used for analytics, even though it's not specifically designed for that purpose. This is because the data is readily available, making it easy to start. However, as the data volume grows and more complex analytical queries involving aggregation and grouping are needed, users often encounter limitations. This is where an analytical database engine like DuckDB comes to the rescue.
‚Äù -
MotherDuck
They literally told us that ‚Äúas data volume grows‚Äù and ‚Äú
more complex analytical queries involving aggregation and grouping are needed
‚Äù ‚Ä¶ ‚Äú‚Ä¶
this is where DuckDB comes to rescue
.‚Äù
We saw the opposite. Sure, we could just have used DuckDB directly on our CSV dataset and got maybe a faster answer than Postgres. But remember, this commentary from DuckDB is INSIDE their announcement about pg_duckdb AND it
gives the casual observer the impression
that using DuckDB to query your Postgres data is MUCH FASTER than raw Postgres ‚Ä¶ which is clearly not the case.
What comes next?
Well, I can tell you what comes next, just like every other time I try to preach the truth to the misinformed and moldering masses. I can hear the hissing and spitting from here.
Doesn‚Äôt bother me. Been doing this long enough.
I see it as my calling to take the new and noteworthy, take them at their word, and then put their word through the wringer. See if they are telling us the WHOLE truth
or maybe a few half-truths sprinkled in there.
Don‚Äôt get me wrong, pg_duckdb is an amazing creation that is going to breed some cool innovations and solutions in the future, no doubt about that.
But, you know, when everything is rainbows and glitter unicorns frolicking in the pastures, you can count on me to rain on that parade.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
150642582.are-data-contracts-dead,Are Data Contracts Dead?,... were they ever alive?,2024-12-02 07:05:52.939-06,"The truth of the matter is that there is always a plethora of new ideas and technologies rattling around in the Data World, the ever-present humdrum of advertisements that relentlessly work to part us from our dollars.
For some reason, I feel people forget this important fact. While there are lots of good people building good things for the good of humanity, there are also a lot of people building things with the sole purpose of selling a thing, and for a pretty penny at that.
It‚Äôs become commonplace today to see some new tool on GitHub, some veneer of ‚Äúopen-source,‚Äù this or that, only to be the worm on the hook of a Company bent on selling you their hosted SaaS or whatever.
Call me jaded if you want, for indeed, jaded I am.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Suppose you try to tell me about Data Contracts.  I have one word for you. Data Mesh.
Of course, there are the ever-present religious type zealots who will often die on their hill of whatever. Data Mesh. Data Contracts. What is it now? SQLMesh?
You name it, you can find someone who has a financial incentive for something to succeed and will do almost anything to raise a host of acolytes to spread their word across the known universe.
That doesn‚Äôt change the fact that for many years, we‚Äôve been sitting around looking at the sky, waiting for Data Contracts to save us from our collective Data Sins.
Our Data Messiah has yet to arrive, and now we are starting to question its existence.
I couldn‚Äôt resist this quick experiment of saying something similar to this on Linkedin to see what hobbits I could bring out of the woodwork.
Probably the best comment could be summed up as follows.
I mean some things are just so painfully obvious it hurts, doesn‚Äôt it?
At some point, we have to come to the conclusion that yes, someone tried hard to create a thing and maybe believed it was going to solve problems, save the world, etc.
But, sometimes those dreams and ideas simply don‚Äôt pan out. The world is fickle, and the data world is fickle, some things catch on and fly (like DuckDB or Polars), and others try, try, try, and try some more.
It‚Äôs like they can‚Äôt see the writing on the wall.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The truth is I have nothing in particular against Data Contracts at all. Most ideas on the surface seem fine and dandy. I want to ask the question ‚Ä¶ after so many years of
hearing
about Data Contacts, why has it still not ‚Äú
made it
‚Äù into the mainstream conversation in Data Engineering?
Why is there not a plethora of Data Contract tools being used daily by Data Engineers?
Who knows, maybe it will happen someday. Never say never.
What are Data Contracts anyway?
If you go read on Reddit or Linkedin you will get many different types of answers and guesses. Maybe we should go get the answer from the horse‚Äôs mouth, let‚Äôs start with
gable.ai
They have a blog post called ‚Äú
What is a Data Contract? 3 Types and Examples.
‚Äù Let‚Äôs see what we can learn, and what we think about it.
They list three pillars that make up a good data contract.
schema (
we all know what this is
)
semantics (
business logic
)
metadata (
we all know what this is
)
These are all familiar to us as Data Engineers, they are nothing new. I find this extremely interesting, is this a possible reason why Data Contracts have seemed to stumble of late??
The idea of a Data Contact must be about wrapping these three different parts together into a single ‚Äúproduct,‚Äù that is schema, semantics, and metadata.
Two of these things already exist as part of the most well-known Lake House and other modern Data Platforms.
Things like Iceberg, Delta Lake, Hudi, and the like have made Schemas an expected and solved problem (without even talking about Postgres etc.)
Data Governance, Data Dictionaries, and MDM tools to centralize and manage meta-data have also been in existence for some time.
The only new idea to me is the ‚Äúsemantics,‚Äù the wrapping up of business logic traditionally captured in ETL inside this product, aka Data Contract.
I *
feel*
like it might be an uphill battle trying to convince someone with a large Data Platform that already has schema enforcement with Delta Lake and Data Governance with Databricks, that they need to buy and implement yet another tool into their stack ‚Ä¶ with a promise of solving data quality issues ‚Ä¶ especially if they already have bespoke or other DQ tools like Great Expectations.
I mean to drop more coin on more tools you are going to need some good persuasion. Also, there is probably a better chance of making the pitch for some new Data Contact tools on Greenfield projects, or Data Platform overhauls.
Data Contracts in the wild.
I was sorta curious what a Data Contract might look like in real life, in the wild that is. I found
an example on GitHub
.
dataContractSpecification: 1.1.0
id: urn:datacontract:checkout:orders-latest
info:
  title: Orders Latest
  version: 2.0.0
  description: |
    Successful customer orders in the webshop. 
    All orders since 2020-01-01. 
    Orders with their line items are in their current state (no history included).
  owner: Checkout Team
  contact:
    name: John Doe (Data Product Owner)
    url: https://teams.microsoft.com/l/channel/example/checkout

...

models:
  orders:
    description: One record per order. Includes cancelled and deleted orders.
    type: table
    fields:
      order_id:
        $ref: '#/definitions/order_id'
        required: true
        unique: true
        primaryKey: true
      order_timestamp:
        description: The business timestamp in UTC when the order was successfully registered in the source system and the payment was successful.
        type: timestamp
        required: true
        examples:
          - ""2024-09-09T08:30:00Z""
        tags: [""business-timestamp""]

....
You can go look for yourself, but it just appears to be glorified ‚Ä¶ mmm ‚Ä¶ codified way of explaining a data set.
I‚Äôm not saying this is good, bad, or whatever. It just is.
What is missing from both Gable.ai and GitHub as the moment, at least in an obvious way,
is the implementation details on the delivery
.
How are these Data Contracts being run or checked, with what system or tool, when and how often, etc etc?
I suppose this is what the future
Gable.ai
is, for example. It will be the HOW of Data Contracts. How to implement and enforce these inside a Data Platform.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Life-changing tools will ‚Ä¶ change your life.
Again, I can see the benefits on the surface of a Data Contract, IF you don‚Äôt already have these things in pieces and parts existing already inside a well oiled machine.
But, it does seem to be a lot of overhead IF you are running something that is working well. It‚Äôs not a simple ‚ÄúYes‚Äù when it comes to adding ‚Ä¶
complexity
overhead
more tools
‚Ä¶ to any Data Stack. It‚Äôs easy for any minimal benefits added to be overshadowed by yet another system to deploy, manage, learn, and implement. That‚Äôs just real life.
What is it about something like DuckDB or Polars that seems to take the Data World by storm and leave no trace or shadow of a doubt that they are here to stay?
They change the way Data Engineers do things in a real way. The rise of Rust based Dataframe tooling like Polars, Daft, and Datafusion are offering real and obvious benefits in speed, cost savings, etc. DuckDB offers good ole‚Äô SQL in a new and easy to use interface and way that is novel and has obvious benefits.
Data Contracts? Maybe?
I‚Äôm not sure. They have to fight against the existing DQ tools, MDM, product offerings by Snowflake, Databricks, etc. The obvious benefits aren‚Äôt so immediately seeable, nor is it an easy and quick solution to implement.
Time will tell.
Who knows, maybe time will show me wrong, it‚Äôs happened before. The more years and time drags on and goes by, with even less and less conversation happening around the topics of Data Contracts ‚Ä¶ makes me question their future.
Maybe it‚Äôs a slow burn, maybe we will wake up and everyone will be using them someday, maybe they are working in the shadows like Batman."
150636206.clustering-vs-partitions-pick-your,Clustering vs Partitions - Pick your poison.,... for the Lake House,2025-04-24 07:32:13.873-05,"Now that the Lake House has taken over the known world and become the new SQL Server of the old (
I lived them
) days of the Data Warehouse ‚Ä¶ things have changed. One topic that is mainly ignored yet at the core of our data processing is
partitioning
‚Ä¶ and, more recently,
clustering
.
Today, I plan to ramble on about these topics in the context of the Lake House ‚Ä¶ specifically Delta Lake and Apache Iceberg.
It isn‚Äôt a deep dive under the hood of
partitioning
and
clustering
in these two tools but more of a discussion about where we have come from (
Hive-style partitioning
), where we are today (
confused
),
and what matters at the end of the day.
__________________________________________________________________
Please take a moment to check out this article‚Äôs sponsor,
OpenXData conference 2025
, without them this content would be possible.
Please click the links below to support this Newsletter.
The data infrastructure event of the season is here ‚Üí
OpenXData conference 2025
A free virtual event on open data architectures - Iceberg, Hudi, lakehouses, query engines and more. Talks from Netflix, dbt Labs, Databricks, Microsoft, Google, Meta, Peloton and other open data geeks.
May 21st. 9am - 3pm PDT
. No fluff. J
ust solid content, good vibes, and a live giveaway if you tune in!
__________________________________________________________________
What
IS
data partitioning, and why does it matter?
We should probably start at the dawn of time when dinosaurs roamed the earth. What is called ‚Äú
Hive-style
‚Äù partitioning was, and in many places still is, the go-to way to store big data.
This may be obvious to some, but if you are new to the data world and big data, the idea is that you can‚Äôt, or shouldn‚Äôt,
randomly store data in files (at scale)
‚Ä¶ without context (
liquid clustering or invisible partitioning
).
If you need to read a data set and find where date = ‚Äò
2025-04-16
‚Äò, you have to‚Ä¶
read the entire dataset (files) to find all instances of date = ‚Äò2025-04-16‚Äò
or use some partitioning and jump to the files where that data is stored.
Before we continue, I want to make a point.
No matter what new fancy technology is used or created in the future, understanding at a fundamental level
WHY data layout and partitioning are important
is key to being a good Data Engineer.
This concept is at the core of data layouts and partitioning,
the idea of physically colocating similar data together in files.
For example, like above, files with today‚Äôs date are stored together.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Context is key.
Never forget that in all situations of data layout, from old-school Hive-style partitions to the new fancy liquid clustering (
we will talk about that later
), the
query patterns
on the data itself should and do drive how data is referenced and/or stored.
It‚Äôs sorta a catch-22, so to speak. You want your data partition strategy to cover 80%+ of your use cases, but it‚Äôs virtually impossible to cover all use cases.
As you can probably already tell from the examples, this sort of partitioning is very ‚Äú
rigid
‚Äù in nature. Once data has been written into files and partitioned ‚Ä¶ the ship has sailed. This is fine if you are smart and query patterns don‚Äôt change ‚Ä¶ which does exist, but if things changed, you could be in trouble (
need to re-write data, etc
.)
Also, such rigid ‚ÄúHive-style‚Äù partitioning is highly susceptible to ‚Äúdata skew‚Äù and partition imbalance.
some partitions have lots of data
some have very little data
Things started to change.
Today, ‚ÄúHive-style‚Äù partitioning is still in full swing, and it is used in many places. Heck, I use it.
Why?
it‚Äôs conceptually simple
easy to use (via queries)
works well for data and query patterns that never change
You can still use this partitioning style if you know what you are doing, and it will bring you great success. But, of course, things started to change in the Lake House world with more complex queries, requirements, and business needs.
Features
like Z-Order and OPTIMIZE
(compaction) with Delta Lake and Databricks started to become integral to running a Lake House.
‚ÄúIn Databricks with
Delta Lake
, Z-ordering is
a technique that helps to colocate related data within the same set of files, enabling efficient data skipping during queries
.‚Äù
Again, I‚Äôm not trying to teach you these concepts from scratch; go read about them yourself if you are unfamiliar with them, but what I‚Äôm trying to show is how technology and engineers have changed how partitioning is done to meet the ever-increasing demand for faster and better performance.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Liquid Clustering and Invisible Partitions ‚Ä¶ a new Era!
All this demand for ever faster and more flexible data layouts to support high-performance has lead to a new era of data layouts that are now more ‚Äú
automatic
‚Äù and ‚Äú
flexible
,‚Äù relying less on humans and more on computers.
I am far from an expert in these new technologies, and I‚Äôm sure someone can spot something wrong below, but here is a high level take on this new style you will see across the Lake House landscape.
üßä Apache Iceberg ‚Äì Hidden Partitioning
Traditional partitioning (like Hive)
exposes partition columns in the table schema (e.g.
year=2024/month=04
).
Iceberg uses
hidden partitioning
:
Partition columns are defined
at the metadata level
, not exposed in the schema.
Partition transforms like
bucket(id, 16)
,
truncate(name, 4)
, or
day(timestamp)
are common.
Benefits:
Cleaner table schemas.
More flexibility for optimizers to choose how to layout the data without leaking internals.
You can change partitioning without changing the schema or breaking queries.
üåä Delta Lake ‚Äì Liquid Clustering (Databricks proprietary)
Liquid Clustering replaces static partition columns with
dynamic clustering keys
(e.g.
user_id
,
event_type
).
Files are
physically laid out
(and Z-Ordered) based on clustering keys, but
no directory structure or partition columns
are exposed.
Think of it as ""continuous optimization"" without static partitions.
OPTIMIZE TABLE ... CLUSTER BY (...)
is used to maintain clustering.
now available is clustering on ingestion.
Benefits:
No need to pick rigid partitions.
Avoids the cost of re-writing partitions.
More granular, Z-order-style layout and skipping.
Where does that put us today? I don‚Äôt know, somewhere in the middle I suppose. Partitioning, like everything else in tech, is starting to be abstracted away from the average Lake House user.
This is both a good and a bad thing.
In one sense, a newcomer to the Lake House world may have never heard of or used Hive-style or rigid partitions. Maybe us old dogs just have a hard time letting go.
I‚Äôve seen
major speedups in query speeds in Delta Lake moving to Liquid Clustering
. Even an old dog has to admit that the new dogs have beat them at their own game.
I still enjoy to teach about partitions, and encourage those newer to the Lake House world to learn about them, try them out, get familiar. Why?
It‚Äôs always better to know
HOW
and
WHY
something works
Sure, liquid clustering and hidden partitions are the new norm, and I‚Äôm sure those will be replaced and updated over time.
At the end of the day understanding how data is laid out inside files, how query patterns are affected by such things ‚Ä¶ this sort of knowledge is a golden key that unlocks all sorts of things.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
150360628.datafusion-my-swiss-army-knife,"DataFusion, My Swiss Army Knife",guest post from Anonymous Rust Dev ,2024-11-11 08:55:54.417-06,"Anonymous Rust Dev checking in for another guest segment. Having been using DataFusion in production for a few years, I wanted to give a practical retrospective.
The app I currently use it (
Datafusion
) in is a wild one. It started its life many years back as a homegrown Rust-based ETL tool to ingest BIM models and feed various shop activities (robotic saws, marking lasers) with CSV deliverables.
Over time, and as various stakeholders became involved, we saw the value in the accumulated wealth of data we got from processing the models, and not only expanded shop operations to keep pace with the insights but also started producing reports to capture and predict operations.
Turns out that the reports ended up being quite popular; and walking over collections of Rust-basd data structures, as it turns out, wasn't the most pleasant way to collate and aggregate diverse data.
That was when my quest for a better way to handle my data drove me to DataFusion, and it's been a core part of my analytics needs since.
A quick primer on DataFusion
In case you don't have your thumb on the pulse of Rust-based tools,
DataFusion
(DF) is an Apache-maintained library built atop the Rust Arrow implementation. It is the lesser-known
alternative
to the more infamous
Polars library
, and covers some of the same bases ‚Ä¶
dataframes
optional SQL interface
lazy evaluation
and runtime performance derived from being written in a systems language.
While Polars has its day, it's worth remembering that it's not the only kid on the block; and while I won't try to make the case for one tool over the other, I am personally a fan of Datafusion.
Like Polars, it offers an API for both Rust and
Python
, as well as the aforementioned SQL parsing functionality. The SQL support is pretty well fleshed-out for both SELECT and CREATE TABLE statements, and for the curious the
roadmap
shows which parts of the language are complete or on the way.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
An in-memory database?
At the beginning, thinking I'd progressively migrate my application data structures over to dataframes, I chose to create digital twins of my Rust collections in DataFusion.
Well, I never actually found a good way to migrate off of my original datasets - while the BIM model itself was static and unchanging, I would periodically annotate that data with runtime information. I never figured out how to efficiently perform row-level CRUD on my DF datasets, and even now any time I make any changes I find myself having to tear down and recreate that data.
So what gives? Was it a mistake to start pumping all my data into dataframes if I'm going to constantly build it from scratch any time my app sneezes? Well, as for my original plan to replace my original data with DataFusion, let's just say that's not happening.
Trying to think of DataFusion as an in-app RDBMS was the wrong approach.
What I quickly discovered, though, is that all those datasets I mirrored in DF turned out to be an incredible cache layer, particularly for runtime analysis. Whatever penalties I'm paying for having to rebuild my data is repaid many times over when, in my scripting layer, I can belt out arbitrary SQL queries at runtime to rapidly iterate on reports.
In practice, business logic operates directly on the raw collections of data, which when updated are mirrored in their latest state as frames in the DF context.
Afterward, my scripting layer (
Rhai
, for the curious) has access to not only assorted business logic and app state, but also has the ability to execute DataFusion SQL queries and get the results back in serialized tabular data that can then be easily worked with.
This took a bit of work to wire up - for instance, the effort needed to populate the DF context with my data can be gnarly, particularly for hierarchical data structures, and so the ""digital twin"" sometimes doesn't look exactly like the source data it backs. Another challenge that I faced was how to use columnar data in a row-based application (yeah, I know I instantly lost cred among some of you data scientist types with that statement), so I had to hand-roll a lot of logic that lets me easily go back and forth between DF, Rhai/JSON, and raw Rust collections.
A ""trivial"" example of what it takes for me to feed my data into the DF context, for instance, can be seen below (keep in mind that I need to do this for any collection that needs to be synced):
While I'm very happy with the result, my initial goal wasn't met. Incidentally, the thing I was looking for at the time may now exist (see:
Tonbo
), but I'm in no rush to chase down the new shiny.
ETL
While my scripting layer gives me some ""back-door"" functionality to get app state out and into different formats, I've also found great utility in directly exporting my DF data into Parquet. For instance, I've submitted bug reports for libraries where having example data was critical, and the ability to
dump my context out to disk
via DataFusion has been invaluable.
It works both ways, too. Arbitrary data can be directly sourced from parquet or CSV sources from any query with the
CREATE EXTERNAL TABLE
clause. With default support for CSV and Parquet, and optional support for Avro or JSON, many tabular formats are already supported; and, for those file types that aren't already supported, you have the opportunity to
write your own provider
.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
And, on the ETL topic, we also have:
Datafusion-CLI
Maybe you're curious about DataFusion but don't want to pull it into your application. If so, the
datafusion-cli
utility might be what you're looking for. This standalone executable serves as a self-contained SQL interface capable of working with the aforementioned file formats and can be integrated into your favorite shell scripts, including CI/CD pipelines.
In my own use, it's been a great tool for exploring CSV datasets. I would also consider reaching for it if I'm trying to convert between file formats (e.g. CSV to JSON). And, these files can be pulled in from
remote sources
like S3 or HTTP. I think it might be time to consider
revisiting an older article
to include using DF:
To start, I'll pick on the
Divvy_Trips_2019_Q3.csv
file, which contains 1,640,718 rows - reasonably close to the size of the ~1.8M records from the original article.
> CREATE EXTERNAL TABLE divvy STORED AS CSV LOCATION '/Users/me/Downloads/Divvy_Trips_2019_Q3.csv';
0 row(s) fetched. 
Elapsed 0.005 seconds.

> COPY divvy TO '/tmp/test.parquet';
+---------+
| count   |
+---------+
| 1640718 |
+---------+
1 row(s) fetched. 
Elapsed 0.466 seconds.
Just so we're clear, this is NOT a scientific test - I made no effort to test this process using the older scripts, am doing this on a different computer than the original article, we have a smidge less data to process, and who knows what other factors I failed to consider. That said, the DF CLI tool offered a highly performant tool requiring only two lines of ""code"" to do the same work as the original scripts.
Would I recommend DataFusion to you?
It's hard to say whether this is the right pick for you. If I were being lazy, I'd tell you to just use Polars if for no other reason than that it has a broader user base and corresponding resources to go with. That said, since it's what I'm familiar with, I'll personally gravitate toward DataFusion any time I write an app with query-intensive needs.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
150325872.data-engineering-central-podcast-1ed,Data Engineering Central Podcast - 03 ,Episode 03,2024-10-16 15:56:35.611-05,"It‚Äôs time for another episode of Data Engineering Central Podcast, our third one! Topics in this episode ‚Ä¶
Should you use DuckDB or Polars?
Small Engineering Changes (PR Reviews)
Daft vs Spark on Databricks with Unity Catalog (Delta Lake)
Primary and Foreign keys in the Lake House
Enjoy!"
150313525.primary-and-foreign-keys-in-the-lake,The Death of Primary and Foreign Keys?,... in the Lake House?,2024-10-23 15:14:19.031-05,"Suppose you‚Äôre an old-school data person like me, one of those Kimball acolytes who spat on the graves of non-
Data Warehouse Toolkit
ignoramuses. In that case, you‚Äôve probably sat around in the late hours of the night, staring up at the stars asking yourself this very question‚Ä¶
‚Äú
If primary and foreign keys were at the heart of the Data Warehouse, and the Lake House has replaced the Data Warehouse, without primary and foreign keys ‚Ä¶ does that mean we were all misled and drank that Kool-Aid for nothing???
‚Äù
I feel duped, like a ninny.
All those years, nay, more than a decade spent minutely examining database tables for the perfect primary key, that combination of columns that would lead me to the Promised Land, those foreign keys that would keep the wolf from the door.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What did I do that for? I mean when is the last time you heard a Lake House zealot raise themself from their golden throne built on the bones of Data Warehouse soldiers and pontificate about a primary or foreign key?
That‚Äôs right.
NEVER
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What are Primary and Foreign Keys
Let‚Äôs start at the beginning, the easy stuff.
Primary Key:
A
primary key
is a column (
or a subset of columns
) in a table that uniquely identifies each row in that table or dataset.
It must contain unique values, and it cannot contain
NULL
values.
There can only be one primary key per table, but it can be made up of multiple columns (
composite key
).
Example:
I
n a table of
Employees
, the
EmployeeID
might be the primary key because it uniquely identifies each employee.
Foreign Key:
A
foreign key
is a column (
or a set of columns
) in one table that links to the primary key of another table.
It is used to create a relationship between two tables and ensure referential integrity.
the value in the foreign key column must exist in the referenced primary key column.
A foreign key can contain
NULL
values if the relationship is optional.
Example:
In a table of
Orders
, the
EmployeeID
can be a foreign key that references the
EmployeeID
in the
Employees
table.
In this example,
Orders.EmployeeID
is a foreign key that links to
Employees.EmployeeID
, enforcing that any
EmployeeID
used in the
Orders
table must exist in the
Employees
table.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
It‚Äôs hard to understate the place the primary and foreign keys have played in the classical OLAP and OLTP data models, and still play, especially in the RDMBS world
(Relational Database Management System
).
Without a knowledge of what makes a particular record different from another one in the same dataset, you really don‚Äôt understand the data at all.
And if you don‚Äôt understand what we‚Äôve described above, than everything else falls apart. Metrics don‚Äôt work, dashboards won‚Äôt be right, nothing will be correct.
A core component of any data model is the ability to understand what makes anyone record UNIQUE.
The ability to concretely tie one record to another via foreign keys is also indispensable.
The assurance and logical clarity of a primary key in one table resulting in a single record that can relate precisely to a record in another table is a foundational data modeling prinicpal.
So why don‚Äôt they exist in the Lake House?
Share
The death of primary and foreign keys ‚Ä¶
I  feel the death of the primary and foreign keys might be a little premature, although there is an argument to be made against them. Some might say ‚Äú
Hey, Databricks and Delta Lake provide primary and foreign keys!
‚Äù
Yeah, well ‚Ä¶ not really.
‚ÄúPrimary key and foreign key constraints aren't enforced in the Databricks engine, but they may be useful for indicating a data integrity relationship that is intended to hold true.‚Äù
Databricks docs
Is a primary and foreign key real if it is not enforced?
Not really.
In name only, not in practice. Is something better than nothing? I suppose so.
Yet, there is a more excellent way my friend.
Using hashes in the Lake House
While we could use the fake-ish primary and foreign keys in Databricks, if you want less of a vendor locked-in type of primary key, may I suggest a
simple hash
?
The way to create a primary key in the Lake House architecture, at least one you can trust logically, which is important, is to‚Ä¶
identify which combinations of columns make a row unique
create a hash of those columns
move on with life
There you have it, a unique string that represents a logical primary key for the data. Of course, this requires you to do the groundwork to understand beyond a doubt what the ‚Äú
business defines as the primary key
.‚Äù
Many times this key, similar to primary and foreign keys, can be cascaded throughout datasets to ease the logic around ‚Ä¶
INSERT
UPDATE
MERGE
JOIN
There is nothing worse than opening up some nasty old SQL file with MERGE or UPDATE statement joining 9 different columns.
What a pain!
But what about enforcement??
Subscribe now
Enforcement of logical primary keys
This might bring up the question of enforcement again, what do we do about that? In a traditional RDBMS like Postgres, we can depend on the database to puke when such a thing is violated (
duplicate primary key
).
Never fear, we are engineers, and we can code our way around this problem.
It‚Äôs easy enough to ‚Ä¶
dedupe data on INSERT to avoid duplicate keys
run periodic checks to find duplicates
JOIN in such a way to avoid possible duplicates
ensure keys exist somewhere else
Sure, this sort of thing can add a little bit of code, but it isn‚Äôt rocket science and we can still gain all the benefits of logical primary keys.
They ease the use of Lake Houses and add that one last piece of functionality we were missing all along.
What say you friend?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
150115276.what-makes-smart-engineers-so-stupid,"What makes ""smart"" engineers so stupid.","... yeah, you read that right",2024-10-21 08:39:54.396-05,"Don‚Äôt be such a baby ‚Ä¶ if you don‚Äôt like the title ‚Ä¶ I really don‚Äôt care. Go write your blog. It‚Äôs true. Over the years I‚Äôve had the unfortunate opportunity to meet a lot of
incredibly smart engineers who are very stupid.
Compound the fact that for years I‚Äôve been kicking the tires on newish tools and packages that get released, trust me, I have plenty of run-ins and communication with a wide range of ‚Äúgeniuses‚Äù who want to set the record straight.
Whether it‚Äôs some stranger on the interwebs who is totally incensed about the review of their new tools I wrote, or literally working in the same room with ‚Äú
God‚Äôs gift to all Software Engineers
‚Äù savants, smart though they are, they are incredibly predictable.
Today we are going to take those wicked creatures to task, set them straight, and hopefully, we can all learn important career and personal lessons about how NOT to be.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Where smart engineers go wrong.
I‚Äôve no doubt if you've been writing code for more than 3 years you‚Äôve probably run into that walking
Dementor of a Software Engineer
who you learned to hate for their ability to be impossibly smart, yet dense.
It‚Äôs not all that complicated either ‚Ä¶ what makes a smart engineer so dumb that is.
We need ‚Äúsmart‚Äù engineers who build wonderful things.
We need ‚Äúsmart‚Äù engineers who can teach us and upskill us
There is nothing inherently wrong with smart engineers.
Truth be told, we couldn‚Äôt operate without them. We wouldn‚Äôt have the tools we use every day without their existence. But that doesn‚Äôt mean they aren‚Äôt a bunch of rascals who need a whopping.
The problem with a subset of the ‚Äúsmart‚Äù ones is that they are too smart for their own good. I‚Äôm sure you‚Äôve run across people like this in normal and everyday life.
terrible people skills
unable to read the room
horrible teammates and mentors
can‚Äôt relate to others
inability to impart or teach their wisdom to others
can‚Äôt see past the technology itself, to the humans on the other side
This is a frustrating position to find another engineer in. In the Age of ChatGPT, we don‚Äôt need more code writers, we need code writers who have people skills, critical thinking skills,
the ability to mesh the business needs and code together.
These are skills that are harder to teach and obtain than simply being a god at writing code.
HAVING the skills mentioned above takes a ‚Äúsmart‚Äù engineer and makes them into a unicorn.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
The inability of the ‚Äúsmart‚Äù engineer to relate to the ‚Äú
average
‚Äù engineer is at the core of their downfall. For some reason, they think everyone should be at their level, and can‚Äôt understand how ‚Ä¶
someone doesn‚Äôt see the ‚Äúobvious‚Äù thing
would do something a certain way
why people skills matter more
why such a thing as marketing or documentation exist
It‚Äôs a tragedy truly. I feel bad for them and the beautiful things they build, things that are forever buried in obscurity and hidden simply because the ‚Äúsmart‚Äù engineer has built a house of cards built on their intellect and failed to relate to anything or anyone in the real world.
I‚Äôve seen amazing open-source projects that are buried and hidden because ‚Ä¶
the README is trash and zero documentation exists
they forgot the ‚Äúaverage‚Äù engineer must be able to set up and reason about the thing
unable to take critical feedback from someone trying to use the thing who runs into trouble
The truth of the matter.
In the end, we live in a world of humans, even us software-driven humans, we are mostly still human, and most of us aren‚Äôt able to be classed into that category of ‚Äúnext-level‚Äù smart.
We use the tools, we don‚Äôt build them.
What makes some ‚Äúsmart‚Äù engineers a unicorn is that they can not only build the thing ‚Ä¶ they can
market the thing
write documentation on the thing
understand the average users and their needs
listen the community at large and take feedback
be pragmatic in their approach and designs
Sure, we see examples of Engineers moving into lots of positions of power in big tech companies and running things.
But, there is a reason that Software Engineers have the classic reputation for being basement dwellers unable to communicate or work well with others ‚Ä¶ it‚Äôs because it‚Äôs true most of the time!
I think the average engineer CAN and SHOULD learn from this in their career, especially in the Age of AI!!!!
Now and in the future, it‚Äôs more important than ever, with AI being able to write code (
and it will continue to get better
), that you work on skills that set you apart from your peers and the legions of other Software writers.
Your human nature is your salvation.
learn to listen
learn to mentor and teach others
learn to communicate written and verbally
learn to be empathetic to others
learn to be a team player
Don‚Äôt get caught in the trap that being the smartest and best coder is going to solve all your problems and make others worship at your feet.
But, don‚Äôt get me wrong. You should be smart and be the best at what you do. When you‚Äôre going on that journey, don‚Äôt leave your human soul behind ‚Ä¶ you will need it later.
Leave a comment"
150071132.weekend-forecast-02b,Weekend Forecast, ... life updates and more,2024-10-12 08:08:38.585-05,"It‚Äôs been a while since I sent out a weekend and life update article, better late than never. I‚Äôve been working on a few things in life and professionally, and I highly encourage you to do the same.
Being uncomfortable and pushing the bounds for yourself mentally and physically comes with reward, nothing worse than being stagnant. I‚Äôm getting old and crumbly and this is becoming more and more attractive (
doing nothing
).
We should all be trying to do more ‚Ä¶
exercise
being outside
learning new things
reading more
It‚Äôs almost Christmas season, throw an old Data Engineer a bone. Your support helps keep this going!
Get 30% off for 1 year
Here is what I‚Äôve been up to‚Ä¶
spending more time outdoors and traveling with the family.
recently did a
Boundary Waters canoe trip with friends
.
spent some time in
the Smokies with the family
trying to bring my value to Data Engineering Central
transition towards more of a 50/50 split on paid vs free posts
trying
out a Podcast
to go along with the Newsletters.
trying to be more consistent
on my YouTube channel
.
reading non-fiction books (at least 1-2 a month)
lifting weights 5 days a week
Some things are going better than others, it takes a lot more effort than you think to keep a weekly pace (at minimum) for the Substack Newsletter. Lots of writing, researching, responding to people, looking for sponsors that come and go like the wind
.
I‚Äôve been thinking a lot lately about how to continue to provide more value for the readers of Data Engineering Central. I‚Äôm thinking about ‚Ä¶
increasing and expanding the Podcast to do interviews and the like
offering limited office hours and AMA sessions
writing another book
more coding and how-to type content
more GitHub-style coding problems and exercises
other things?
Please let me know in the comments what you are interested in, or if you have other ideas.
Leave a comment
Anyways, thanks for reading, I appreciate you all following along with my hot takes."
149819668.data-engineering-central-podcast-f59,Data Engineering Central Podcast - 02,Episode 2,2024-10-04 14:04:48.933-05,"Welcome to the Data Engineering Central Podcast ‚Äî‚Äî
a no-holds-barred discussion on the Data Landscape.
Welcome to Episode 02
In today‚Äôs episode, we will talk about the following topics from the Data Engineering perspective ‚Ä¶
Using OpenAI‚Äôs o1 Model to do Data Engineering work
Lord Save us from more ETL tools
Rust for the small things
Hosted (SaaS) vs Build"
149817513.apache-airflow-vs-databricks-workflows,Apache Airflow vs Databricks Workflows,... have at em',2024-11-04 06:50:41.474-06,"As a long-time Databricks user, I‚Äôve sat on the sidelines lazily watching Databricks Workflows for some time now with a yawn and half an eye open. Being an avid Apache Airflow user for longer than most of you have been programming, I‚Äôve felt no compunction to switch or even poke at it.
It‚Äôs been nagging at the corner of my mind, like calling my mom, for years now that I should probably give Databricks Workflows the ol‚Äô poke in the eye with a stick. That day has come.
I mostly want to accomplish ‚Ä¶
Learn more about Databricks Workflows and what features it offers
How it compares to Apache Airflow
All this is in the light of running workloads on Databricks.
I‚Äôm going to spend very little to no time giving an overview of Apache Airflow because if you are ignorant of that, you‚Äôve got other things to be reading besides this.
But, on the other hand, I will spend more time trying to get and give, a good overview of Databricks Workflows as I would imagine that you, like me, are probably less familiar.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Databricks Workflows.
‚ÄúDatabricks Workflows has tools that allow you to schedule and orchestrate data processing tasks on Databricks. You use Databricks Workflows to configure Databricks Jobs.‚Äù -
Databricks
So what is a
Databricks Workflow
? As per my wonderful art above ‚Ä¶
Workflows are a combination of Job(s)
Job(s) can be made up of one or more Task(s)
Task dependencies allow you to establish control flows within a job
Notifications send updates when a task starts, succeeds, fails, or breaches defined duration thresholds.
With Retries, you can specify how many times a task should be retried
Duration Thresholds, you can define the execution time limits
Anyone familiar with Airflow or the classic DAG idea will see that as a familiar concept. It‚Äôs just Databrick‚Äôs way of letting us orchestrate and create complex data pipelines made up of many tasks and jobs, with complex interdependencies.
In the Databricks documentation (
which of course uses Notebooks for Jobs, those dirty hobbits
), they give this below image as an example Workflow.
If you‚Äôre anything like me and have some PTSD from something like AWS Pipeline and all those nasty JSON-defined pipelines ‚Ä¶ aaaak.
If you go scrounge around the Databricks documentation, you can find the Jobs API, which can give you an example of what a complex workflow might look like. Hold your breath.
Note: the format is called out as ‚ÄúMULTI_TASK‚Äù ‚Ä¶
The summary of what you see above and what you could expect in a Databricks Worfkow ‚Ä¶
list of Tasks
lots of dependencies call-outs
run_if
depends_on
trigger
schedule
etc.
I mean we could go on, but you get the idea. Just defining a bunch of tasks a million different callouts to do different things.
Check it all out here.
Of course you could probably do this in the Databricks UI, but that would not be very smart or scalable would it.
Technically you could use the
databricks-sdk with Python
to do the same thing and create Jobs like this, here is an example from their docs.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Thinking about Databricks Workflows in light of Apache Airflow.
We have to ask ourselves, generally, how this all compares to the giant in the room, Apache Airflow, and when you would pick one of these solutions over the other.
I personally think that‚Äôs a pretty obvious answer but to each their own.
Choose Airflow when you have Databricks + Non-Databricks workloads and tasks in your pipelines.
If you have LARGE amounts of pipelines with complex tasks, Airflow is going to feel more natural and better for organization, etc ‚Ä¶
If you have a bunch of Databricks ONLY pipelines, it can make sense to just use Worfkows.
Databricks Workflows will most likely never reach the complexity and feature parity with Apache Airflow.
Databricks provides BETTER and a WIDER range of options to create and manage pipelines ‚Ä¶
CLI
SDK
UI
API
It just all feels a little disjointed. Which one should we use? With options come confusion and it needs more cohesion.
On the surface, much like anything Databricks does, Workflows to create and orchestrate complex Data Pipelines appear to be a reasonable choice, probably underrated considering the options and features you get.
It‚Äôs hard to take on the 500lb elephant in the room know as Airflow. People love their DAGs and Python ‚Ä¶ so it‚Äôs going to be hard to get them to leave their first love.
If Databricks wanted to make inroads into getting those Airflow users to switch they would have to change their approach.
They would need to shift their focus from the API with JSON (actually not really) and pour some more effort into the Python SDK for the Databricks Jobs/Workflows.
Honestly the docs, examples, and overview for the Python SDK for Databricks, as seen above, just look dirty, and confusing, and clearly isn‚Äôt a top priority for them.
So why use it then?
The truth is most Data Engineers use and love Python every day, hence their love for Airflow and DAGs. If Databricks tried to put some more effort into making Workflows accessible and buildable in a manner more similar to an Apache Airflow DAG, that usage would no doubt explode.
Honestly, the little thing matters. There is a reason why Notebooks are so popular and made Databricks the GOAT of the Data Engineering space.
They should take note of that, and do the same thing for Workflows.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
149805026.daft-vs-spark-databricks-for-delta,Daft vs Spark (Databricks) for Delta Tables (Unity Catalog),... in real life,2024-10-13 19:17:07.014-05,"Daft
is one of those tools I just can‚Äôt get enough of. I‚Äôm not sure what it is. Well, it might have something to do with stuff built in Rust, being incredibly fast, and wonderfully easy integrations with things like Delta Lake.
It never ceases to amaze me what gets popular and what doesn‚Äôt. Goes to show marketing matters. Today I want to see if we can swap out PySpark for Daft, on a Databricks workload, read Unity Catalog Delta Tables ‚Ä¶ and save money ‚Ä¶ aka make things run faster.
I have written about Daft before, how it is much faster than Polars, an impressive feat.
But I am curious if just pip installing Daft on a Databricks Cluster and running it instead of PySpark can be faster and save money in certain instances (w
here things could be done on a single node
).
The truth is, and we all know it, we use Databricks across the board for all our workloads because it makes things simple and easy, one tool, one platform, it just reduces complexity. But, there is always a subset of workloads that don‚Äôt NEED to Spark to run.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Installing Daft on Databricks and reading Unity Catalog Delta Lake tables.
All this code was run on a single node Databricks Cluster, shown below.
Hopefully, this is boringly easy, good for real life, bad for Substack, but whatever. First things first, simply install Daft. You could do this in a Notebook, or you could do it with an `init.sh` script for a Databricks Job.
The init script might look like this ‚Ä¶
#!/bin/bash

/databricks/python/bin/pip install getdaft[unity]==0.3.8 deltalake
or in a Notebook (
if you‚Äôre evil like that
) ‚Ä¶
%pip install getdaft[unity] deltalake
dbutils.library.restartPython()
After that, the code to connect Daft to Unity Catalog is very straightforward and can be found in their docs, you just need your URL for Unity Catalog (
your Databricks Account in this case
), and a Token (
you can generate a personal token via the Databricks UI
).
Next, you do the obvious.
At this point, we can see how utterly easy and painless this is,
something that should not be lost on you
, we should all get up and give a resounding round of applause to Daft, but let‚Äôs move on to crunch some data, and compare runtimes to Spark.
Did we speak too soon, first error!
It's not that big of a surprise, always something they don‚Äôt tell you hiding in the corner. That‚Äôs real life.
This error will lead you to some
Databricks documentation
about ‚Äúcredential vending,‚Äù more or less it‚Äôs the ability to allow ‚Äúexternal engines‚Äù from getting at Unity Catalog data.
As a side note, be careful with this, and Databricks is pretty clear about this in the documentation, allowing external data access has a lot of security risks, and they make all the defaults to block this sort of access default for obvious reasons. Proceed with caution.
This process of ‚Äúcredential vending‚Äù looks something like this ‚Ä¶
Once you have enabled ‚Äú
external data access
‚Äù (GRANT) to a user(s) this should allow Daft to do the ‚Äú
credential vending‚Äù
itself.
Daft will do this part for you, very nice, rather than having to make the API calls yourself to get the creds.
Don‚Äôt be a hobbit, get access to paid posts, the Podcast, and full archives. If you‚Äôre spending $20 on your avocado toast, you can throw some coin my way.
Get 30% off for 1 year
Another Error.
So now that we have solved the credentials/external engine data access ‚Ä¶ it‚Äôs time to look at the next error that popped trying to simply read a Databricks Unity Catalog table with Daft and doing a simple
show()
on it.
We get an error that is little help and makes no sense at all. ‚Äú
AttributeError: 'pyarrow.lib.StructType' object has no attribute 'field'
‚Äú I mean it doesn‚Äôt seem to have anything to do with permissions, so we solved that issue, but this seems obscure and related to either Daft or Delta Lake packages ‚Ä¶ or both.
Typically what I do when this type of error is rewind the versions I‚Äôm installing. But, after thinking about it, I had an epiphany, when I pip installed `
getdaft[unitycatalog]
` and pip installed `
deltalake
` separately, I probably made the wrong decision, I need the `
daft-deltalake
` integration.
So to solve this problem I need to do ‚Ä¶
Sorta obvious, but sorta not. The error was not helpful that‚Äôs for sure.
Now `
count()`‚Äôing
the Daft dataframe actually works.
Success, now we have access to our Delta Lake table in Unity Catalog via Daft with 21 million records. This should be enough to run a simple aggregate with GROUPBY with Daft vs PySpark and see what‚Äôs shaking.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Spark on Databricks vs Daft (with Delta Lake) runtimes.
Ok, so we‚Äôve done the work to get Daft working with a Databricks Cluster to read a Unity Catalog Delta Table, we got it hooked up to something with 21+ million records.
Let‚Äôs do a simple aggregation query and write results to s3 to see who‚Äôs faster, Daft of PySpark on Databricks.
You won‚Äôt believe the results. Insane.
(
Note I had to hardcode my AWS Keys to get Daft to be able to write the results to s3.
)
Remember this aggregation was/is happening on 21+ million records.
>> 7s
That little bugger ran in 7 seconds. I had to double-check the results in s3 to make sure it actually worked, which it did.
Same thing in PySpark.
Now the moment of truth, let‚Äôs run that same code in PySpark on this Databricks cluster and see how long.
And the runtime, you guessed it, a lot longer.
>> 20s
Daft is almost 3 times faster. Not bad!
Now I‚Äôm sure all the angry Reddit geniuses and Data Engineering savants are warming up their keyboards to write me emails about this and that,
you can all send the email to your mom instead
. Go spread your genius elsewhere.
I‚Äôm not about the science here, I‚Äôm about trying things and kicking the tires in real life. It‚Äôs not about the queries I wrote, how I wrote them, this or that, it‚Äôs about trying out new things, roughly, to see what we can find out.
I have no interest in doing real benchmarks. Leave that for the birds.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What we proved.
Ever since I saw/heard the Daft added support for Unity Catalog on Databricks I‚Äôve been sorta skeptical. More than half the time it‚Äôs usually marketing fluff which means nothing is always toy tutorials for milk toast goobers who aren‚Äôt doing anything real.
I wanted to see if Daft + Unity Catalog (with Delta Lake) on Databricks was REAL.
I wanted to see what could really be done, if there is a real use case here for real Databricks and Unity Catalogs users.
Can we, with a reasonable amount of effort, save real money and compute resources.
Daft (with minimal configuration) can easily read Unity Catalog Delta Lake tables.
Daft is way faster than Spark
Daft can read large-ish tables no problem. (it crunched 22 million in 7 seconds including file IO) (so in theory it could scale up way way past that)
Daft could be deployed as part of your Databricks environment
Daft could save you real money by reducing workload runtimes
What a GOAT.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
149713582.small-engineering-changes-pr-reviews,Small Engineering Changes (PR reviews),... for a big win,2024-10-09 10:22:52.844-05,"Engineering Changes ‚Ä¶ PRs if you will ‚Ä¶ are the elephant in the room, no one wants to look them directly in the eye, they are there, take up a lot of space, and cause all sorts of problems, yet little time is devoted to them.
Also, there are probably few things more frustrating for anyone writing code than the dreaded PR process. I‚Äôve worked in places where I would break in a sweat while my finger hovered above the Submit button. I‚Äôve worked in places where I get PR crazy, doing a handful a day.
What gives?
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
The  infamous PR (Pull Request)
Nothing will embed fear, anger, mistrust, bitterness, and general mayhem into an Engineering or Data Team like a good ole‚Äô PR battle. Comments flying back and forth, hurt feelings, accusations, comments a paragraph long.
PRs (Pull Requests) are a central part of a
good
Data Engineering and Software lifecycle, yet they one of the spaces most fraught with problems.
Is there any way to overcome these issues, can we agree on anything? How can we make the PR process less painful and more constructive? How can we turn this devil‚Äôs red hot iron into the soothing and healing hand an angel sent to calm our worries and fix our problems?
I have no idea. Well ‚Ä¶ maybe I have a few.
Today we shall dive into what we can do to calm the raging seas of angry engineers inside the PR process.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The basics of the Pull Request (PR).
Now I‚Äôm sure there are a few savants out there that think they have the words of Life when it comes to the PR review. But I‚Äôm a simple man, born in a small town, and I have small and simple ideas.
But, I just think that might be the key to solving many of our PR problems. I‚Äôm going to talk about generally what I‚Äôve seen and experienced and throw my two cents into the ring.
There is code you can write, code you want to write, and code you should write.
One thing you need to get over in the beginning is thinking that big PRs make you look ‚Äúsmart‚Äù or whatever. I assure you, on behalf of your code reviewers,
that bigger is not better in this case
.
I find that in life and code, there is a constant battle we fight, or should fight, to keep things simple, because in our heart of hearts, we know what is good for us.
I know it‚Äôs hard, but rule number one is to keep the PR simple and small at all costs.
This will honestly solve a major portion of the PR pains and agonies that most people fall into.
small code changes are easier to review
small code changes are easier to reason about (as submitter and reviewer)
you‚Äôre more likely to get quicker and better feedback on small changes
There are fewer chances for differences of opinion on small PRs
Short, sweet, and to the point will be your saving grace?
But what if I‚Äôm working on a big change?
Yeah, yeah, yeah ‚Ä¶ everyone says that but if you insist, here are some ideas ‚Ä¶
Break it up into smaller logical changes
Open multiple small PRs that are adjacent (I didn‚Äôt say 20, maybe 3)
If you need to keep it big, set up a meeting before or immediately after you open the PR and review and do a code walk-through with the reviewers live.
Other ideas around PR size.
don‚Äôt throw in extra formatting or nonessential changes into your PR.
Open a separate one, don‚Äôt muddy the waters.
don‚Äôt address multiple bugs in a single PR.
Let‚Äôs move on to other PR topics.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Other ways to ease the pain of the PR (Pull Request)
Other than keeping the PR small and to the point, there are a few different tips and tricks I‚Äôve used and run across over the years that help to keep everyone happy and to keep them from pulling their hair out.
I‚Äôm going to lay these out in no particular order and maybe comment on them here and there.
Use CI/CD to enforce Linting and Formatting requirements.
don‚Äôt waste finger typing about this kinda stuff on a PR, make a computer do it for you.
Be kind to a fault when reviewing or responding to PR comments.
learn to be more open minded and less critical
Focus your review of a PR on the big picture, not on incidentals and coding preferences.
Give some comments as suggestions for next time, or just general ideas ‚Ä¶ not as ‚Äúdo this or I won‚Äôt approve it.‚Äù
Ensure your PR includes a complete and in-depth description of the change and why it‚Äôs needed, and your thought processes.
Also, I think it‚Äôs important to throw in another cavet on PR‚Äôs into the mix.
Engineering culture has a major impact on the PR process and how much software writers either love or hate the process.
You should drive towards a Engineering culture that has ‚Ä¶
clear expectations on PRs.
uses CI/CD to enforce the obvious stuff (linting, formatting, whatever)
encourages explicitly small engineering changes
is an open culture that fosters learning and growth over being ‚Äúright‚Äù or ‚Äúsmart‚Äù
I‚Äôm curious to hear your thoughts and experiences on Pull Requests. Do you have nightmare stories? Do you love the way your team does it? Why?
Share below!
Leave a comment"
149026436.data-engineering-central-podcast,Data Engineering Central Podcast,Episode 01,2024-09-17 15:55:02.253-05,"Welcome to the Data Engineering Central Podcast ‚Äî‚Äî
a no-holds-barred discussion on the Data Landscape.
Welcome to Episode 01
In today‚Äôs episode we will talk about the following topics from the Data Engineering perspective ‚Ä¶
Snowflake vs Databricks.
Is Apache Spark being replaced??
Notebooks in Production. Bad."
148974105.i-used-chatgpt-o1-to-do-postgresql,I used ChatGPT o1 to do PostgreSQL basics,replacing Data Engineers with AI,2024-09-23 07:38:38.451-05,"Weap and moan all you keyboard jockeys, you simple peddlers of code. Your doom and what you feared greatly is coming to pass. You‚Äôve been replaced.
At least that‚Äôs what the AI pundits are saying about
the new o1 OpenAI model
that apparently blows the socks off the previous 4o model when it comes to programming and other benchmarks.
I‚Äôve been trying to ride down the middle of the tracks when it comes to AI and programming. I‚Äôve been writing code and doing Data Engineering long enough to see the obvious flaws in the current AI models. I‚Äôve also embraced AI into my development lifecycle for probably 2 years now.
This has left me pessimistically optimistic if there is such a thing.
AI has been frustratingly stupid when it comes to anything beyond simple code, BUT, it has also increased my efficiency as a programmer by at least %30, plus or minus, depending on the project.
I‚Äôm very curious to put this new AI model to the
simple
Data Engineering test.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Putting OpenAI‚Äôs o1 to the Data Engineering test.
Now that this new and improved o1 model is out and supposedly a next-level programmer compared to previous models I think it‚Äôs time we kick the tires together.
But, I want to do something outside the box ‚Ä¶ yet simple enough to be a basic Data Engineering task. It‚Äôs a fine line to walk.
I‚Äôm going to treat OpenAI o1 like I would a Junior Data Engineer. We are going to pretend we are having a problem with performance in PostgreSQL ‚Ä¶ and we need the Junion Engineer to set up an end-to-end test to prove what the fix should be.
Here‚Äôs the challenge.
So here is the sample problem we are going to have o1 work on. This is based on an actual real-life problem I fixed recently. This problem will serve two problems ‚Ä¶
can OpenAI‚Äôs o1 solve query performance information
without telling it what the solution is.
once we get the model on the right track ‚Ä¶ can the model HELP us to create an end-to-end test?
aka, generate DDL, table data, queries to test etc.
This is what I would consider a very basic Data Engineering problem, something I would expect a Junior Data Engineer to be able to tackle, solve, and design a test that can be run outside Production to prove the proposed solution works.
Postgres table with
200 million
100 million + records
UPDATE statement has been running slow
Inspection of indexes on the table shows only a single primary key
Proposed solution to add an extra index and test end-to-end to prove our idea
Ok, so next let‚Äôs just dive into the problem at hand and see how o1 does. I will show the questions I ask it and the responses etc, including where, if anywhere, it flubes.
Diving into OpenAI o1.
So let‚Äôs get our first prompt written up.
Ok, I‚Äôm not even sure where to start with the response. It‚Äôs intense and large. I will include the whole response in the companion YouTube video.
But, the o1 model does at least provide a summary at the end.
Let‚Äôs continue down the road that we are dealing with a Junion Data Engineer and go down the track that o1 gave us to use indexing to improve the performance.
Here is what OpenAI‚Äôs o1 tells us to do ‚Ä¶
1. Add Indexes on Join and Filter Columns
a. Index on
customer_info.customer_id
Since you're joining
customer_info
and
temp_table
on
customer_id
, adding an index on this column in the
customer_info
table can significantly speed up the lookup:
CREATE INDEX idx_customer_info_customer_id ON customer_info(customer_id);
b. Index on
temp_table.customer_id
If
temp_table
is large, creating an index on
customer_id
in the temporary table can also help:
CREATE INDEX idx_temp_table_customer_id ON temp_table(customer_id);
Note
: For temporary tables, indexing might not always be beneficial due to the overhead of index creation versus the time saved during the update. Evaluate the size of
temp_table
before indexing.
c. Index on
temp_table.last_activity_date
If
temp_table
contains a large number of records with varying
last_activity_date
values, indexing this column can help filter the records more efficiently:
CREATE INDEX idx_temp_table_customer_id ON temp_table(last_activity_date);
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Let‚Äôs go for it ‚Ä¶
I mean it is going down the right road I think. I did like that the answers o1 gave were wide-ranging‚Ä¶ that is part of the best use of AI in programming, listen to its ideas ‚Ä¶
make sure you are thinking outside the box and considering all options.
Remember, we want the AI to prove what it‚Äôs doing by setting up test, so let‚Äôs ask it to generate the 200 million + sample data.
Not bad, seems pretty legit.
Before we put this OpenAI o1 solution through the wringer, I‚Äôm going to set up an AWS RDS instance. I know a lot of milk toast programmers try to use local stuff like Docker, which is sometimes ok, but I also want to test this in a
REAL
environment.
I highly suggest you do the same, set up a personal AWS account and use it to learn and test things. Honestly, if you don‚Äôt set up much stuff and delete it when you're done ‚Ä¶ your bill be less than what you spend on coffee.
Well, now that I‚Äôm done typing my RDS instance is available.
Time to run that SQL to create the database tables, and once that is done start to populate the main table.
It appears that the Python script to generate the 200 million records is working like a charm ‚Ä¶ albeit a slow charm. I checked the database using a count on the table and could see it slowly ticking up.
postgres=> SELECT COUNT(*) FROM customer_info;
 count  
--------
 260000
(1 row)

postgres=> SELECT COUNT(*) FROM customer_info;
 count  
--------
 270000
(1 row)

postgres=>
The next thing we will need to make this test work is to create another table, same as the above, and call it
customer_info_tmp
, and we will copy ‚Ä¶ say ‚Ä¶
6 million
records from our
customer_info
table over to it?
This will allow us to run the UPDATE statement ‚Ä¶
one as is to see the performance (
aka no indexes created beside the default primary key
)
again after the indexes are created
Moving on.
Ok, so first things first to report, the code that o1 gave us to populate 200 million records into the Postgres database was probably the slowest thing ever written on the face of the earth.
It took over 24 hours before I finally killed it ‚Ä¶
postgres=> SELECT COUNT(*) FROM customer_info;
   count   
-----------
 116170000
(1 row)
We got over 100 million records and that is just going to have to do. So, we‚Äôve learned just in the beginning phase here at OpenAI‚Äôs supposed programming savant cannot do well when trying to populate records from Python into Postgres SQL in a reasonable manner.
(
aka it must struggle to understand scale out of the box
)
This is an interesting finding, I wonder if it‚Äôs a harbinger of things to come or not.
Ok, so now I‚Äôm going to setup the temp (second) table and put 6 million records into it.
INSERT INTO customer_info_tmp
(customer_id,
    customer_name,
    customer_address,
    customer_city,
    customer_postal,
    customer_state,
    customer_country,
    last_activity_date,
    total_number_of_orders)
SELECT 
    customer_id,
    customer_name,
    customer_address,
    customer_city,
    customer_postal,
    customer_state,
    customer_country,
    last_activity_date,
    total_number_of_orders
FROM customer_info
LIMIT 6000000;

postgres=> SELECT COUNT(*) FROM customer_info_tmp;
  count  
---------
 6000000
(1 row)
So we got our two tables, one with 100 million+ records, the other with 6 million. Now, let‚Äôs run the UPDATE statement
WITHOUT
indexes just to get a baseline on how slow that might be.
Here it is ‚Ä¶ and we will put it inside some Python to get a time.
Well ‚Ä¶ that did take some time, but not terrible ~2 minutes.
Time taken to update:  0:02:00.890669
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
And remember that OpenAI o1 suggested two different indexes as well, so including the indexes we‚Äôve seen above, it wanted ‚Ä¶
CREATE INDEX idx_temp_table_last_activity_date ON customer_info_tmp(last_activity_date);

CREATE INDEX idx_temp_table_customer_id ON customer_info_tmp(customer_id);
‚Ä¶ on both tables just to be safe.
(
There are only 8k records getting updated.
)
postgres=> SELECT COUNT(*) FROM customer_info_tmp WHERE last_activity_date = '2022-10-18';
 count 
-------
  8228
(1 row)
So now that we have the two separate indexes on each table (customer_id and last_activity_date) ‚Ä¶ let‚Äôs re-run the example same query.
Time taken to update:  0:00:08.580337
That‚Äôs a BIG difference for sure.
Looks like OpenAI‚Äôs o1 model came through for us, cutting the query down from 2 minutes to 8 seconds with its suggestion for adding the indexes.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Thoughts on AI for Data Engineering.
Don‚Äôt worry, I hail from the group who things it‚Äôs not the end of the world for Software Engineers when it comes to AI, but, that being said, I‚Äôm also not going to ignore what‚Äôs happening in the AI world as related to code and Data Engineering.
I think we proved, maybe or maybe not, that OpenAI‚Äôs o1 model is at least a Junior Level Data Engineer don‚Äôt you think? Sure, probably some mistakes here and there (
not being able to write a scalable way to ingest a few hundred million fake records into Postgres
), but at the same time hitting the nail on the head with the index suggestion to increase runtime of a UPDATE query.
What I think the takeaways from this test should be ‚Ä¶
AI will not take a good Data Engineers job anytime soon
newer AI models can probably solve most problems as well as any Junior Data Engineer
they will probably speed up your troubleshooting time (
by summarizing ideas you would have found on Google
)
you should try to include AI in your developer workflow to increase your efficiency"
148868967.should-you-use-duckdb-or-polars,Should you use DuckDB or Polars?, or both ... ?,2024-10-07 08:03:55.649-05,"This is an interesting question indeed, is it not? What to use, what to use? Both DuckDB and Polars seem to be flying high at the moment, the new cool kids on the block. Everyone talking about them, and 1% of the people actually using them. Typical.
But, the question remains. What should be learning and using DuckDB or Polars?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
A person would think that on the surface these tools do the same thing, and are for the same target audience. Just data tools right? Hmmm ‚Ä¶ I think not.
I always like to take what the flavor of the day marketing is telling me with a grain of salt, instead, I prefer to use the tools, watch others who are using the tools, and have general common sense applied.
So no, I don‚Äôt think these tools are the same thing, yet they do have ALOT of overlap.
What is DuckDB?
We need to ask the question in case you're a Neanderthal living in a cave.
‚Äú
DuckDB
is a fast in-process analytical database. DuckDB supports a feature-rich SQL dialect complemented with deep integrations into client APIs.‚Äù
What is Polars?
And for Polars?
‚Äú
DataFrames for the new era.
Polars
is an open-source library for data manipulation, known for being one of the fastest data processing solutions on a single machine. It features a well-structured, typed API that is both expressive and easy to use
.‚Äù
So what next?
Ok, so right off the bat we can see a slight divergence from each other in the way they even market themselves to the general public. This should tell us that although they might be similar tools in our minds, they are sorta pointing themselves in different directions.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
What you should use.
Even though they are similar tools with similar use cases, they are built for a different target market, even if their makers wouldn‚Äôt say so.
DuckDB is SQL-centric and built for those people who work with SQL on a daily basis.
Polars is the new Pandas. It‚Äôs for the Data Science/ML and DataFrame crazy folk who program on a daily basis.
Yeah, you can write SQL with the Polars SQLContext, but if you‚Äôre building a SQL thingy, you should probably reach for DuckDB ‚Ä¶ you will be happier in the end.
If you‚Äôre a programmer who lives in Python, works in Databricks a lot, works around Data Science and ML, Polars is going to feel more familiar and feel better in your fingers.
For example, check out this simple GitHub repo I wrote a while ago, just to show a basic data pipeline with Polars and DuckDB.
One of these pipelines is just going to feel more familiar and better to you. Kinda like vanilla or chocolate ice cream, which one do your reach for?
import polars as pl
import pyarrow.dataset as ds
from datetime import datetime
import s3fs

def main():
    t1 = datetime.now()
    bucket = ""confessions-of-a-data-guy""
    key = """"
    secret = """"

    fs = s3fs.S3FileSystem(key=key,
                           secret=secret,
                           config_kwargs={'region_name':'us-east-1'}
                                          )

    s3_endpoint = f""s3://{bucket}/""

    myds = ds.dataset([y for y in fs.ls(s3_endpoint) if "".csv"" in y], 
                      filesystem=fs, 
                      format=""csv"")
    lazy_df = pl.scan_pyarrow_dataset(myds)

    lazy_df = lazy_df.groupby(""started_at"").agg(pl.count(""ride_id"").alias(""ride_id_count""))

    with fs.open(""s3://confessions-of-a-data-guy/harddrives/metrics-polars"", ""wb"") as f:
        lazy_df.collect().write_parquet(f)
    
    t2 = datetime.now()
    print(f""Time taken: {t2 - t1}"")

if __name__ == ""__main__"":
    main()
or ‚Ä¶
import duckdb
import pandas as pd
from datetime import datetime

t1 = datetime.now()
cursor = duckdb.connect()

df = cursor.execute(""""""
                        INSTALL httpfs;
                        LOAD httpfs;
                        SET s3_region='us-east-1';
                        SET s3_access_key_id='';
                        SET s3_secret_access_key='';
                        
                        CREATE TABLE data AS SELECT CAST(started_at as DATE) as started_at_date, count(ride_id) as ride_id_count
                        FROM read_csv_auto('s3://confessions-of-a-data-guy/*.csv')
                        GROUP BY started_at_date;
                    
                        COPY data TO 's3://confessions-of-a-data-guy/ducky-results.parquet';
                        """""").df()
t2 = datetime.now()
print(f""Time taken: {t2 - t1}"")
I mean they get you to the same place. One of them probably just fits in your architecture and background a little more than the other, and that‚Äôs ok.
You should use whichever one you want. I wouldn‚Äôt say no to someone using or the other, or both in production, in a data pipeline.
My guess would be folk could find that both DuckDB and Polars would probably fit in perfectly in different parts of their pipelines. Most likely save some money too. Tools like these are the future.
Spark is here to stay for many years to come, but there does seem to be a new trend in town.
The single node and fast data processing tools are popping like hotcakes. I myself have replaced Spark on Databricks with Polars on Airflow to save money. It works great.
I imagine there is a fair amount of Snowflake users who could migrate a portion of their workloads to DuckDB on a single node and same some money.
DuckDB in the wild.
I mean this is
an interesting question
, who is using Pandas in the wild, and for what?
According to Reddit users, those zombies and infernal rabble, DuckDB is used for what you think it would be. A replacement for Pandas and other data wrangling.
It is hard to understate the importance of DuckDB in the future of Data Engineering. People are using it locally for development and data processing, people are finding places in Production pipelines to swap in DuckDB. People are also building tools with it.
DuckDB is one of those quiet tools that sneaks up on you, and before you know it, it‚Äôs going to be everywhere. Its ease of installation and simple usage are the keys to its wide adoption. You can‚Äôt argue with something that simply works.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Polars in the wild.
The funny thing is, unsurprisingly, that it‚Äôs hard to find the same sort of hype in the wild about Polars. This, for some of you, might seem strange, but it‚Äôs really not.
DuckDB is SQL-centric and will take center stage with the average Data Engineer for that reason. Polars will have a harder journey.
People go to Polars because it‚Äôs nicer than Pandas, is cleaner, and can even replace Spark in some instances. There just isn‚Äôt a large base of people doing this ‚Ä¶ yet.
Also, the truth is, Polars is the perfect tool for larger datasets, (which most people actually use), but again, many data users simply don‚Äôt work with large datasets.
I personally pick Polars over DuckDB, but that‚Äôs because I like programming over SQL, I also work with very large datasets, but that is not the major use case for most.
DuckDB and Polars are here to stay.
Both these tools have made inroads into the Data Engineering community and will not be leaving anytime soon. If you are a SQL person, pick up DuckDB, if you enjoy programming more and work with Spark, you should learn Polars.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
148503248.rust-for-the-small-things,Rust for the small things?,... but what about Python?,2024-09-09 09:56:32.209-05,"I‚Äôve come to realize now that the demise of Python has been greatly exaggerated, in fact, it‚Äôs fake news ‚Ä¶ not going to happen. The rise of LLMs ‚Ä¶ with its attending scaffolding of Python has ensured that.
Things like Polars have cemented the idea of Rust-based data tools wrapped with Python is probably our future.
But, that doesn‚Äôt mean I don‚Äôt want to write in Rust for the everyday and mundane Data Engineering tasks. It does beg the question though ‚Ä¶ what would happen if we did?
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Of course, there is a tradeoff with everything though ‚Ä¶
Python requires less code
Does speed and safety matter in every application (probably not)
Developer efficiency matters
I suppose one could argue that every few milliseconds, or seconds, saved here or there ‚Ä¶ across an entire Data Platform ‚Ä¶ would add up to something real. Also, the idea that rusts strict compiler rules around the Borrow Checker, etc, would also add up to a more reliable and less breakable Data Platform.
All that being said, humans are humans and it ain‚Äôt going to happen.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Testing Rust in the small things.
So, today I want to test Rust in the small and mundane parts of Data Engineering in general. I was thinking we could start with doing some s3 operations ‚Ä¶ from the perspective of ‚Ä¶
the lines of code written
the performance
So, without further ado, let‚Äôs get cooking.
First, let‚Äôs simply count the number of files in a bucket and prefix location with Rust.
So here is my Rust code, I‚Äôm no expert so take it for what it is.
And the results
Found 250624 objects in my-top-secret-bucket
Time taken: 42.79s
There are roughly 43 lines of code. I find it‚Äôs not overly verbose or hard for even one of those lowly Python coders to follow what‚Äôs happening. In fact, when I had ChatGPT convert the Rust code to Python for me, it only went down to 31 lines of code.
I know people say Rust is too hard to write, or too much overhead, but it probably has more to do with the problem space itself.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Get 50% off for 1 year
More small things with Rust.
I tried to think of other smallerish but uniquely Data Engineering type tasks I‚Äôve had to do in the past 10 years ‚Ä¶ and one popped to mind.
Sometimes we get strange flat files that have to be pre-processed before they get ingested into downstream systems, like Spark. I‚Äôve run across this one before ‚Ä¶
‚Ä¶ a flat file that has a header and footer that need to be stripped out.
This one has come up more than once. You get CSV or TSV-type files, but they come with a maybe timestamp in the header and footer that needs to be stripped before you can actually load them. Very strange, but surprisingly common for files being pumped out of legacy systems.
I just took one of the D
ivvy Bike trip free datafiles
(CSV) and modified it to have this junk header and footer row(s).
So we need some Rust code that would maybe be running inside a Lambda or triggered elsewhere that would take these incoming files, strip out the first and last record of every file (the junk) ‚Ä¶ and pump out a clean CSV.
Pretty fast.
File processed successfully.
Time taken: 410.71ms
Seems to have worked well.
I do have to admit, if you‚Äôre a Python person used to working with files then this Rust might start to get a little weird, mostly because Rust‚Äôs verboseness starts to show its ugly head.
We had to create a File ‚Ä¶
let temp_file = OpenOptions::new()
        .write(true)
        .create(true)
        .truncate(true)
        .open(&temp_file_path)?;
as well as a Writer
let mut writer = BufWriter::new(temp_file);
Not only that but the logic I used is a little strange.
let mut lines = reader.lines().skip(1);
let mut prev_line: Option<String> = None;

while let Some(Ok(line)) = lines.next() {
    if let Some(prev) = prev_line.take() {
            writeln!(writer, ""{}"", prev)?;
    }
    prev_line = Some(line);
}
We skipped the header line out of the box with skip(1) ‚Ä¶ easy enough. But we needed a way to skip the last line. (
I guess we could have just counted the number of lines upfront
)
In my above approach, everything seems a little awkward, it makes you stare for a minute or two, to catch on.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Python check-in.
We should probably check in with Python at this point, so no one gets their feelings hurt. What would this last little tidbit in Rust that started to get complicated and verbose look like in Python?
Now don‚Äôt yell at me about the Python either, we are here for fun, not for work. What I want to notice now is how few lines it takes in Python.
File processed successfully.
Time taken: 0.74 seconds (740 ms)
Whereas our first easy Rust script didn‚Äôt differ much from the Python version (we asked ChatGPT to convert it), this second little guy ‚Ä¶ when the requirements start to get complex, the Rust is roughly double the code size.
Now I‚Äôm not saying one is better than the other, that‚Äôs up to the engineer in my opinion.
if you need bulletproof and fast, Rust is always the answer.
if you want easy and simple, Python is always the answer.
The Reality
The truth of the matter is that Rust probably will never be used for the small stuff in Data Engineering. Even though I wish it would. I think Rust is fun and it‚Äôs a hard teacher that makes us all better programmers, especially those of use that use Python on a daily basis."
148349934.there-are-3-types-of-data-engineers,There are 3 Types of Data Engineers.,and you better be the right one.,2024-09-13 09:36:03.374-05,"Then there were three. The final three. Only three.
It‚Äôs the truth,
there are 3 types of Data Engineers only
, and I promise you, you fall into one of the three groups. The truth can be hard, the truth can hurt ‚Ä¶ but the truth can also heal and lead to growth.
Aren‚Äôt you just bubbling over with excitement to know what kind of Data Engineer you are???
Group 1, Group 2, Group 3?
Maybe you‚Äôre not happy with where you are at. Maybe you want to move from group 1 to 2, or 2 to three, trust me,
you can‚Äôt move from 1 to 3 just in a snap
‚Ä¶ doesn‚Äôt work like that.
Group one is always jealous of two, two looks with longing at three. Group three looks down from their high towers onto the peons below them in one and two.
It‚Äôs an age-old story.
I encourage you to take your medicine like a champ, no moaning and groaning ‚Ä¶ no babies here. It‚Äôs best to accept who you are, you will be happier in your own skin.
Sure, you can change your group with hard work. I did. I started in group one and went to group two. I thought I had it in me to get to group three, but eventually, I just accepted my fate as group 2 and embraced it.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Get 50% off for 1 year
The 3 Types of Data Engineers.
Remember, I don‚Äôt make the rules, don‚Äôt be mad at me. It is what it is, and you are what you are.
Without further ado, here are the three types of Data Engineers.
It‚Äôs clear to me after working in data for well over a decade that almost all Data Engineers can be put into one of these three categories. Of course, you always have the oddballs that don‚Äôt fit the mold, but someone not falling into one of these three slots is rare.
I don‚Äôt think any one group of Data Engineer is better than the other, they all have their place,
and they are all needed.
We need people who love SQL, are wizards, and are adept at complex analytics.
We need people who are good programmers, can build anything, lead projects, and architect Data Platforms.
We need the Data Engineering savants who are next-level programmers, building the next gen of data tools with Rust etc.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Defining the 3 Types of Data Engineers.
You saw those definitions in the image above, they are what they are. Which one are you? Do you want to be in a different group?
Leave a comment
SQL and Analytics (Group 1)
These Data Engineers spend most of their time in SQL, producing analytics and Dashboards. They work extremely closely with the business. They probably don‚Äôt write code every day, although some of them are good with Python.
The easiest way to transition into Data Engineering.
SQL experts
Builds data marts, and dashboards, close to the business.
Senior Level Programmers and Architects (Group 2)
This second set of Engineers are the writers of code ‚Ä¶ complex code. They use a lot of Python, but also Golang, Rust, Scala, etc. They can debug Spark pipelines. They build CI/CD, DevOps, and are comfortable on a Linux server. They build Data Platforms and plan projects.
More comfortable on the command line than the UI
Broad expert across many tech stacks
Understand distributed systems
Can design a good Data Platform from the ground up
Tooling Builder (Group 3)
This is the least common of the Data Engineering types, the Yodas of the data space. They build the tooling, private or open source that others use. They are the best of the best, Software Engingeers who specialize in data.
Building open-source tooling
Expert level programmers
Lead/Staff level engineers
I always thought I wanted to be a Group 3, but I had to accept after years of programming that I just wasn‚Äôt able to make the switch ‚Ä¶ I suppose anyone can get there with enough work, and I wasn‚Äôt willing to do the work.
I think there be some confusion about Group 2 and Group 3 Data Engineers. One could be a Staff Data Engineer and not really a Group 3.
You might just be really good at building Data Platforms, leading teams, vision, and get to the Staff+ Level. But that doesn‚Äôt mean you can build the next Polars.
There is a difference.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Inter-group challenges.
Me thinks, and have seen, some sparks flying between these groups of Data Engineers if they get mixed in together ‚Ä¶ or have to work together.
Engineers and buildings of software (especially in Data) have a high sense of self-worth, they get paid a lot across the board, and generally are proud ‚Ä¶ which can be a problem.
I don‚Äôt think it needs to be this way. There is room for all, and what‚Äôs more,
there is a need for all
. Typically your SQL crunchers using analytics and building Dashboards don‚Äôt want to be building the next Pandas tool with Rust.
Many of your Group 2 programmers don‚Äôt want to be writing SQL all day to build Dashboards. That‚Äôs ok. People are different and want different things.
I think the challenge is when people want to move groups, especially 1 to 2. This can be a slow and tough process with few shortcuts.
My movement between groups.
I started out many moons ago as a Group 1 Data Engineer. I wrote SQL all day long on SQL Servers and became a next-level expert in the most complex SQL queries possible. After a while, I became tired of all the SQL, Dashboards, and analytics.
I decided I wanted to be a Group or Type 2 Data Engineer. I wanted to code more. So I simply taught myself.
I spent a lot of extra time writing code. Learning Spark. Deploying services on Kubernetes. Learning Linux, Bash, CI/CD, DevOps, etc.
I started to plan and implement bigger projects etc. Eventually, I made the switch to full-time Type 2. It was just what I wanted. Less SQL and more code.
I‚Äôve been a 2 for years and years. I thought I wanted to be Type 3, but came to realize I just wasn‚Äôt a good enough programmer. Sure, I can write Rust, but that‚Äôs not the same thing as being able to build Distributed Systems.
I‚Äôm super curious my readers ‚Ä¶ what kind of Data Engineers are you??
I‚Äôm also curious about a few questions ‚Ä¶
Are you happy where you are at?
What group do you want to be in?
Have you tried to move to a different group?
Have you had good or bad experiences with other groups?
Leave a comment"
148348723.lord-save-us-not-another-etl-tool,"Lord Save Us, Not Another ETL Tool Please!",just what we need ... another one ...,2024-09-16 08:45:16.256-05,"Just when you think you can‚Äôt take it anymore, not even a single one, not even a small tiny little bitty one. Pop. Another one comes out of the proverbial tech oven, nice and hot.
That‚Äôs what it feels like these days with ETL tools. Oh? You‚Äôre one of those people who‚Äôve seen the light and you don‚Äôt do ETL anymore? Reverse ETL? Streaming. Low Code/No Code for you?
Doesn‚Äôt really matter what your persuasion is, they pump them Modern Data Stack ‚Äúdata transfer and transformation‚Äù tools out like hotcakes. Even your grandmother would be impressed.
Get 50% off for 1 year
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Building tools.
I suppose it‚Äôs just a classic engineering problem, isn‚Äôt it? We would rather just build the wheel over for the 50th time than use one that already exists. We love creating things, writing code, and going through that whole process.
We are doomed to repeat the failures of our fore-fathers. History repeats itself, isn‚Äôt that what the wise one‚Äôs say?
I mean there are two sides to the story. We clearly have a plethora of tooling available, with new ones coming out weekly, but it also begs the question of why ETL tools in the first place.
This is a good question. Why?
Every tool has its place.
Even inside Engineering a lot of the reason why we build things comes down to automation, trying to make our lives easier. We layer tools on tools on tools.
There are tools for conceivable problem under the sun. There are tools for every use case under the sun.
No Code / Low Code Tooling.
Streaming Tooling
Batch Tooling
DevOps and CI/CD Tooling
Tooling for Cloud Providers
Tooling to Monitor Your Other Tooling
Cost Tooling
Tooling to Manage your Code
Analytics Tooling
Faster Query Time Tooling
and the list goes on
Probably one of the reasons we have so much tooling, with more being built everyday ‚Ä¶ is that we have a wide variety of Data Teams who deal with different kinds of data and are comfortable with different types of solutions.
Some people aren‚Äôt comfortable with lots of custom code, and are less technical, so they want Low Code or No Code connectors to pull their data into Snowflake via a UI.
Some teams work with very specific Machine Learning use cases and have little tooling besides some Monitoring and Cloud Provider basics.
But still, I say there are too many tools.
A new explosion of tools.
I‚Äôve noticed in recent years a new glut of tooling coming into the Data Engineering space ‚Ä¶ and it‚Äôs driven by one thing ‚Ä¶
Rust.
The new cool kid on the street, faster than anyone else, Rust has broadened the horizons and minds of those working in Data and Data infrastructure.
New Rust-based Streaming tools, dataframe-based tools, and people building distributed systems of these new Rust-based tools. Along with the rise of AI ‚Ä¶
Innovation breeds a glut of tooling.
When something new like Rust comes along ‚Ä¶ smart people say ‚ÄúSpark has some shortcomings ‚Ä¶ I bet I can build something better.‚Äù Then you have it a short time later ‚Ä¶ a new tool.
Picking new tools.
It can be hard to know what to take seriously. I tend to be on the pessimistic side, just because something new was created doesn‚Äôt mean it was good, and that I should use it.
I need a real reason. It needs to provide real VALUE.
What is that value? Well, that is different for everyone, it depends on the context, problem, and team in question. One person‚Äôs salvation could be another person‚Äôs nightmare.
Choose new tooling wisely.
Does it add unwanted complexity?
Does it reduce or increase the size of the codebase?
Does it cost a lot when scaled?
What does its integrations look like?
Does it work in the cloud, who hosts it?
Does its concepts ‚Äúfit‚Äù into your current paradigm?
You should probably have a problem that already exists, and then you find a tool that fits that problem-sized hole. You shouldn‚Äôt find a tool and then force it into a spot(s) ‚Ä¶ probably not going to work well.
Refer a friend
I still think there is too much tooling.
Even after all that said, I still there is a plethora of tooling. Too much tooling. Do you know how many Airflow alternatives have popped up?
Dagster
Mage
Prefect
MWAA
GCP Composer
Astronomer
‚Ä¶ etc.
What‚Äôs a bloke to chose?
Sometimes there might be no right answer, the answer could be any tool that works. Typically a POC would answer your question."
148060797.databricks-delta-lake-table-versions,Databricks. Delta Lake. Table Versions. Polars. Insidious Features.,and a tale of lessons learned.,2024-09-02 08:04:47.846-05,"Everyone once in a great while, well maybe more often than that but don‚Äôt tell anyone, I managed to foot-gun myself in amazing ways. Ways that not even I, poor little I, could have imagined.
It reminds me that no matter how long you‚Äôve been writing code, how much you‚Äôve used certain tools and platforms, none of us are immune to bugs and good ole‚Äô ‚Äúgotchas.‚Äù
Code and Data Platforms, even managed ones like Databricks can be inherently complex, and the minute you wander away from the ‚Äúnormal‚Äù use case, which is needed, you are walking on uncharted ground with hidden proverbial quicksand waiting to swallow you up.
Today I am going to share one such story. I do not that the particular ‚Äúbug‚Äù or ‚Äúfeature‚Äù in itself is that surprising or novel, but I think the lessons learned from the larger picture are interesting to talk about.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Databricks. Delta Lake. Table Versions. Polars.
Some time ago I wrote a nice little article about how to
Replace Databricks Spark Jobs (using Delta) with Polars
. The post was very popular and of course, I patted myself on the back for being so witty and smart, and a general credit to my profession.
Then recently I broke it.
That is the story I will share with you today, how I managed to break said wonderful production pipeline running Polars on Databricks reading a Unity Catalog Delta Lake table.
Let me give you the background of this simple pipeline.
Polars installed on an Airflow worker.
Said Airflow worker reading a Unity Catalog Delta Lake table.
Said Airflow worker + Polars iterating those records and acting upon them.
(
the idea is to use Polars where possible instead of Databricks Compute to reduce cost and bring in more flexibility to the pipelines
)
That is a sample of the function. Very basic and simple indeed. The results return a Polars Dataframe with which you can do what you please.
So how could such a simple thing break?
The ‚Äúfeature‚Äù I foot-gunned myself with.
One day I was happily plodding along, writing code, and taking names, the sun was shining and the birds were singing. I was working on a new pipeline that would use this wonder Polars Delta Lake table reader ‚Ä¶ and then this happened.
The problem seemed pretty obvious to me right away, but the ‚Äúwhy‚Äù or ‚Äúhow‚Äù took some time to come to me. Knowing that many errors can be misleading and that sometimes things are never as they seem, I still figured immediately that this had something to do with ‚Äú
versions
.‚Äù
_internal.DeltaProtocolError: The table's minimum reader version is 3 but deltalake only supports up to version 1.
The surprise itself wasn‚Äôt so much that I received a version error, annoying as that is, anyone who‚Äôs been doing Data Engineering with Python tools knows this is the cross we will bear to the end of our ragged and sad days, it was ‚Äúhow‚Äù did this happen out of nowhere.
Nothing had been changed recently that I knew of with Polars that I knew of. It just happened out of the blue. Sorta.
A quick gander through the interwebs turned up something interesting.
This is an Open issue with the `delta-rs` package which underlies, albeit many layers back, probably what we are using under the hood with Polars, or it must be a similar code at least I would imagine.
Python Polars on top of Rust Polars with the deltalake Python package as well. Who knows ‚Ä¶ I didn‚Äôt feel like digging in.
That got me thinking, yes Deletion Vectors are a newerish thing, I‚Äôve heard talk of them, yes, it‚Äôs probable and likely that Polars doesn‚Äôt support this yet, or at least the version of Polars I am using.
But the question still remained, how did it break out of the blue. What changed ‚Ä¶ Polars or the Delta Table? Had to be one of the two.
Since Polars version in our CI/CD and environment didn‚Äôt change, it must have been the Delta Lake table itself.
And then it came to me. I did it. I done broke it. Me.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Dangers of innocuous small and manual changes.
Unwhiteningly a week or two before, this Unity Catalog Delta Table had been dropped and recreated. Of course when that happened two things happened ‚Ä¶
the Delta Table features/versions were upgraded by default
the deletionVectors were turned to True
the min reader version went from 1 to 3
This is apparently a common foot-gun for a lot of people, (
unknowing upgrade table versions
) if you do some Googling. It just isn‚Äôt obvious this is going to happen.
It seems to mostly depend on what DBR version you are running as to what Delta Lake table version and features you are going to get. It‚Äôs like silent feature upgrades.
This has been so much of a problem Databricks released into
Public Preview recently the ability to DROP FEATURES and DOWNGRADE protocol versions.
(you can find other people online doing the same thing and pulling their hair out trying to figure out what went wrong)
I tried this approach for this table, and it did not work.
Instead, I opted to DROP and recreate the table with the following table property defined.
TBLPROPERTIES ('delta.enableDeletionVectors' = false);
That did the trick, wala, Polars was able to read the Delta Lake table again.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Lessons Learned.
I find it amusing after low these many years, that I still manage to find wonderful ways to break things for myself. I find it‚Äôs very good for the soul, to be put in your place, by yourself. It‚Äôs just a needed reminder that we are all humans ‚Ä¶ and that as programmers we can‚Äôt trust or assume anything at all."
148020549.mlflow-with-databricks-thoughts-and,MLflow ... with Databricks. Thoughts and more.,the answer to your machine learning woes?,2024-08-26 07:45:22.091-05,"I‚Äôm not sure how many of you
ne'er-do-wells
who gave your poor mothers heartburn in your youth, now work in the ML (Machine Learning) space, but this one is for those few weary and battered souls who have been given that sad task.
Recently I spent some time evaluating
mflow
,
managed Databricks mlflow
that is, and I have some reports to deliver to you my dear readers.
Check out this fully virtual upcoming Data Engineering for AI/ML conference!
The perfect place to learn the intersection of Data and AI. There is a stellar lineup with 40+ incredible¬†speakers.
I have been a fan of the MLOps Community Virtual Conferences because of the quality talks and funny break activities.
Managed MLflow with Databricks.
So, back to the topic at hand. mflow.
MLflow
is an open-source platform designed to manage the complete machine learning lifecycle. It provides tools to track experiments, package code into reproducible runs, and share and deploy models
Think of MLflow as a tool that tries to tame the Wild West of Data Science and manage the Data Engineering aspects of what it takes to do the Machine Learning Ops (MLOps) portion of that job.
I‚Äôve been working with Machine Learning for about ‚Ä¶ hmm ‚Ä¶ 6 years maybe? And I‚Äôve learned a few things in that time.
Namely, that Data Science is an R&D-heavy job, and when you have a lot of experimentation going on, and custom code being written to ‚Ä¶
gather and transform data
create features
run models
save results
validate results
etc.
Those steps can look vastly different depending on the
TYPE
of Machine Learning model(s) being worked on, and what exactly the
DATA
looks like.
This is the essence of why MLOps is still a relatively new field that has yet to find it‚Äôs feet on solid ground yet.
I‚Äôve worked with image data, tabular data, and everything in between when preparing data for ingestion into Machine Learning models.
The data is as varied as the people doing the work.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The Ops part of MLOps.
One of the surprising parts of Machine Learning to those new to the discipline is when they crack Pandora‚Äôs Box open and see what it actually takes to put models into Production ‚Ä¶
it isn‚Äôt the actual training of the model that is difficult
it isn‚Äôt the exercise of prediction part of that model that is difficult
For example ‚Ä¶ it usually goes something like this.
Of course, things get a little more complicated, but you get the point. No black magic or rocket science going on here.
Nay, this isn‚Äôt the difficult part. Just like the writing of any code for Data Engineering rarely becomes a problem.
It‚Äôs the processes and tracking of what is going on that becomes the problem, it‚Äôs getting some consistency and reproducibility in the process that becomes the problem.
Ok, so enough beating around the bush, let‚Äôs get back to the issue at hand, managed MLflow on Databricks, what it does, and why it is useful.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Managed MLflow with Databricks.
So after my long-winded discussion above on what actually makes Machine Learning and MLOps difficult and important, let‚Äôs do the 10,000-foot view of MLflow on Databricks and I think it will jump out at you why it‚Äôs an important tool.
Here are the key concepts to understand about MFflow on Databricks.
Tracking
Models
Projects
This picture sums it up well.
So what can you do with MLflow on Databricks?
You can have a
Project
, which helps encapsulate all the code and configurations needed to run a particular Machine Learning experiment. Super helpful for consistency, fast iteration, and having multiple people working on the same or similar projects.
YAML files are used for these Projects.
Of course, if you are familiar with Databricks, you know you can clone Git repos up there, as well as those dreaded Notebooks ‚Ä¶ think about a Project as all that stuff, your code, and configs that will actually run ‚Ä¶ all in one spot.
Experiments and Tracking
Of course, once you have whatever code you want to run, the next obvious that is typically hard from bespoke MLOps operations is the Tracking and Monitoring of an ‚ÄúExperiment‚Äù ‚Ä¶ aka the Data Scientist or Machine Learning Engineer will probably ‚Ä¶
run the code many times over tweaking parameters and datasets
save the results from those many runs
want to review everything they tried and share those results
be able to find different ‚Äúartifacts‚Äù from the experiments (
datasets, graphs, charts, configs, params, etc
).
This is also where Managed MLflow on Databricks shines. Below is some sample Databricks code, you can note the Tracking and Logging of the Experiments.
It‚Äôs hard to overstate how beautiful of a thing this is, speaking from personal experience. Imagine with me, a team of 12 Data Scientists, all working on the same or a few different models.
How do you ensure a similar workflow? How do track what everyone is doing? How to reproduce it? Where do you store it? How do you display it and make sense of it??
With MLflow on Databricks these simple API calls above can do all that like it‚Äôs nothing. This is a game changer and will absolutely make the ML lifecycle much quicker and less error-prone.
No more Wild West.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Models.
The last important piece we have not talked about, that can be a pain for MLOps ‚Ä¶ is managing all those nasty little Model Objects themselves.
It‚Äôs a pain to, in a bespoke way, manage ‚Ä¶
storage of model objects
model object versions
serving of those model objects
pulling models in and out of production
monitoring those models
etc
You get the drift. Once you have the model, that‚Äôs only half the battle ‚Ä¶ the real war starts when
you have to manage those models
.
This can all be done with Managed MLflow on Databricks via the UI or of course many different API calls from code.
Conclusion.
I know we glossed over pretty much everything very quickly, no detail is found here, but what I really wanted to do was communicate at a high level the difficulties of MLOps and how Managed MLflow by Databricks provides solutions for those problems.
This is the solid ground that all those shakey MLFlow feet have been looking for all these years.
Hopefully, in coming articles, we can dive deeper into each one of these MLflow components and start to learn their inner workings.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
147883498.realtime-streaming-data-from-postgresql,Realtime Streaming data from PostgreSQL to Delta Lake (Unity Catalog),... with Estuary,2024-08-20 09:45:23.883-05,"It‚Äôs been a while since I kicked the tires on a new tool, you know I love to pick and poke at things. Also, in my never-ending attempt to keep on top of the changing landscape and keep the wolves (
newer, smarter, younger engineers
) from snatching my throne, it‚Äôs imperative I at least attempt to keep up with the times.
So, in the spirit of trying new and fun things, today we are going to do something interesting ‚Ä¶ namely, trying to stream Postgres data to Delta Lake.
Of course, we older and wiser Data Engineers who‚Äôve battled mountains of old and unruly code have come to realize that it‚Äôs best to test and try every solution to a problem with open-mindedness and a view to
keeping complexity and code to a dull roar.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
So, I set up a free account with
Estuary
.
‚Äú
The only platform built from the ground up for truly real-time ETL and ELT data integration, set up in minutes.
‚Äù -
Estuary website
.
We are going to prove out this tool and see if it can deliver what it claims, I will give you the raw unfettered truth. I‚Äôm going to walk through creating an Estuary account all the way to attempting to stream data from an AWS RDS Postgres instance to a Databricks Delta Lake table.
Doing the thing.
In a nice change of pace,
the free account only took me three clicks
and required no credit card or any other shenanigans.
Without further ado I‚Äôm going to jump straight in, document the process, and comment along the way,
I will leave the success or failure up to you to decide.
Trying to sync data from Postgres to Delta Lake with Estuary.
They make it super easy, I just clicked
New Capture
,
A bunch of Connectors popped up, and PostgreSQL of course shows up as one of the main ones, easy enough so far. It appears the two main concepts are ‚Ä¶
Connector
(
source
)
Materialization
(
destination
)
No problem, I used my personal AWS account to set up a nice new,
albeit tiny
, RDS Postgres instance.
Next, back to Estuary where I enter those connection details.
First error,
no go on the connection
.
Luckily I found some documentation with step-by-step for what needs to be set up on the AWS RDS side to make this all work. That‚Äôs no surprise really, it always takes a little tweaking to get things to talk.
(side note, the error logs were extremely easy to read and right in your face)
https://docs.estuary.dev/reference/Connectors/capture-connectors/PostgreSQL/amazon-rds-postgres/
I had to do the following ‚Ä¶
mess with the VPC to allow incoming traffic from 34.121.207.128
create a new parameter group
and turn on logical replication, assign it to the RDS.
restart database.
I also need a simple sample table to create in my Postgres RDS to use for this test.
CREATE TABLE pickle_bob (id bigint NOT NULL, name text);
That should do.
Next, the
Estuary
documentation tells me I have to connect to RDS (I did via psql) and run the following script.
Notice I had to add my table
pickle_bob
into that last command. Basically letting
Estuary
do its thing.
This time the Connection creation in Estuary gets past the first error, but pops this following new one.
The first error about logical replication is interesting,
you can see below in my parameter group this is already set to 1
, and the instance was restarted.
Interestingly, after running this query the RDS shows it as off.
A second restart of the RDS instance seemed to do the trick on that one, thanks Stackoverflow for the tip.
That seemed to be the final trick, Connection setup in
Estuary
. What I need now is a
Materialization
or a target for my Postgres table source.
This is the part I‚Äôm nervous about, can I get Estuary to write to a Delta Lake table stored with a s3 backend???
Time will tell.
Well, it‚Äôs good and bad news. Estuary provides a Databricks connector, but it requires an account with Unity Catalog ‚Ä¶ aka you have to have a fullblow Databricks Premium account and can‚Äôt write to a custom or standalone Delta Lake table you‚Äôve created and managed yourself on somewhere like s3.
At least not out of the box.
And, on top of that, you need an SQL Warehouse apparently, the SQL Warehouse part is strange, not sure why you would need that.
SQL Warehouse can be kinda expensive.
The good news for you, my dear readers, is I do have a personal Premium Databricks account that I try to keep to a
dull roar
, as it will easily eat your bank account for lunch if you‚Äôre not careful.
(
I also set up this Delta Table table in my Databricks account
)
CREATE TABLE pickle_bob (id bigint NOT NULL, name varchar(25));
Creating the Connection / Materialization to my Databricks account is pretty straightforward. Just fill out some account info along with a
PAT (personal access token)
.
(
this required me to have a few simple things like a Catalog setup in my Databricks Workspace that I could use for this work etc.
)
But, easy enough! Done! Not painful at all. You can see my new Databricks ‚Äú
Materialization
‚Äù in the Estuary UI.
Honestly, it‚Äôs been pretty smooth so far ‚Ä¶ not a lot of code that I‚Äôve to write (none at all) to get connection setup from a Postgres table to a Databricks Delta Lake (Unity Catalog) table.
As you get older in your career of fighting code and data you will start to realize, like me, that easy solutions that require less code = less trouble in the future. (code = bugs).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What next?
Well, I guess all there is to do now is to simply write some records (
INSERT
) into our Postgres table and then see if we can get them to show up in our Databricks Unity Catalog Delta Table!
I will be impressed if this works the first time.
Ok, we got our records into the Postgres table.
Well, look at that, in the
Estuary UI
we can see that it picked up some data obviously, (
I inserted the records and then went straight to the UI and refreshed, so it‚Äôs on the ball
).
Unfortunately, when I go to my Databricks Notebook and run a query on that table, I don‚Äôt see anything showing up.
After rechecking my Materialization setup, it appears I never set a
refresh interval
, aka I set it to 1 hour, figuring it would auto sync right away, it was nothing before.
Well, I‚Äôll be. There those little records are!
Impressive if I do say so myself, and I‚Äôm usually picky handing out those.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Notes on real-time data capture with
Estuary
between Postgres and Delta Lake tables (Unity Catalog).
I‚Äôve worked a lot with Postgres and Databricks over the years, and I have to accept a slick product when I see one. I can imagine writing a lot of code to solve this same problem, who knows, Spark Streaming, Delta Live Table, and the list goes on.
At this point in my life, I know that ‚Äú
more code
‚Äù equals ‚Äú
more problems
.‚Äù
Sure, there are times when hand-rolling your own solution makes sense for certain use cases. Streaming data from Postgres to Delta Lake tables in Databricks (Unity Catalog) is not one of them.
I mean think about the amount of code and complexity you would have to create to simply GET the data from point A to point B. Let alone do what you want with the data.
This one from
Estuary
is a no-brainer.
They abstract all the code and complexity away, all you have to do is play with the data like you want to anyway.
It‚Äôs exciting to new tools like this come into the marketplace, with such slick and easy integrations with tooling like Databricks.
For me, it‚Äôs an easy sell ‚Ä¶ reduce code, simple integration, few button clicks, and whamo, you got data streaming from Postgres into Delta Lake.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
147846549.no-excuses-answer-the-dang-questions,NO EXCUSES! Answer the dang questions!,Guest Post by Boromir4thewin,2024-08-23 07:37:17.304-05,"Hello!
This is a guest post by Boromir4thewin ‚Ä¶ This is someone with one of the hardest jobs in the world. They think they have something to say to all of us Milk Toast programmers.
Three questions to rock your world:
When was the last time you had a physical?
Are you physically able to save your life and the life of another? (
Or are you physically able to be an important person in the lives of your kids/family/workplace
)
Do you have skills that would assist in saving your life and the life of another?
I‚Äôm not just slinging shame to modify your behavior and sell a soft drink, this is real life coming at you.
How are you doing at being you?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What is life? Is life more than writing code?
Life is more than a sum of fat bank accounts, success at work, or whether no one is mad at you today.
What is the full measure of a person?
Are you healthy of mind and body? Do you contribute to the physical and mental needs of those who rely on you within your home, community, workplace? Can you enjoy the good things in life or spend your days slaving away for ‚Äú
the next thing
.‚Äù These questions could be a catalyst for a life you could only dream of or at the very least help you and someone else live a little longer‚Ä¶
When Was Your Last Physical?
If the Answer to this question was ‚Äúwhen I wanted to play sports in 7th grade‚Äù you need to schedule a physical today. Maybe you‚Äôve never had a physical because you didn‚Äôt play sports or your parents were against modern medicine.
Fine. But you‚Äôre an adult now, people rely on you, and getting a physical is a simple way to determine your baseline of physical health.
Just because you feel fine doesn‚Äôt mean you‚Äôre doing great. You could be halfway to Heart Disease City or teetering on the edge of
Diabetesville
.
Wouldn‚Äôt you want to know? If your reasoning is ‚Äú
I‚Äôm too busy
‚Äù then I‚Äôd like to remind you‚Ä¶ your work can‚Äôt love you back.
Are you tirelessly laboring away drinking energy drinks with reckless abandon trying to meet deadlines and accomplish great feats of whatever the heck you do
?
‚ÄúHe let his body waste away because he loved work and felt too overwhelmed to make a simple appointment‚Äù doesn‚Äôt look good on the headstone.
GET A PHYSICAL! (
And make sure u ask for bloodwork.
)
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Are you physically able to save your life and the life of another? OR are you physically able to be an important person in the lives of your kids/family/tribe/whatever)
How is the ol‚Äô ‚Äúphysical fitness‚Äù category for you? There is no perfect one-size-fits-all for physical fitness as the variations between bodies, daily physical demands, and physical abilities are incredibly varied. That being said it‚Äôs ok to expect a little more out of yourself in this area.
I‚Äôm thinking really basic stuff such as, ‚Äú
are you easily winded ascending small flights of stairs and can‚Äôt open the pickle jar?
‚Äù
Or the more complicated: If you were in a rollover accident could you climb out of a car? Could you pull someone else out of the wreckage before the vehicle is engulfed in flames?
Bad times to wish you were stronger‚Ä¶
If you make it to your 70‚Äôs/80‚Äôs and fall, will you have the strength to get back up? Will your bones be conditioned from years of minimal strength training and resist injury?
Strength and fitness aren‚Äôt just helpful on your worse day. Being in some form of conditioned ‚Äúshape‚Äù is great for making the overall quality of your life better in about every way. Tired of feeling tired? Get after it.
Find something tangible and start small.
walk
run
yoga
bike
canoe
hike
swim
lift
Find something that qualifies as movement and DO IT. If you don‚Äôt know what to do look online, reach out to someone with knowledge or really great biceps, and ask what they do.
Try finding an area of your life that you want to be easier and start training to do that well or to make it easy.
Example
:
I love hiking. If I know I‚Äôm going to the mountains I spend months preparing for a week-long trip doing hundreds of squats, lunges, and stability exercises along with a fair amount of running. My main goal is to train so the things I love to do are easy and enjoyable.
I also train for the greatest physical challenge of life: being a dad.
Example: My daughter (age 9) asks for a piggyback ride up the stairs almost every night before bed. Can I rise to the challenge? She won‚Äôt ask this forever, I‚Äôm going to be ready while she still cares. What if my 4 and 6-year-old sons want a ride too‚Ä¶ at the same time?
Well ‚Ä¶ Bring it on, I‚Äôm ready.
Do you have the strength and energy to be a person your children and loved ones remember with joy long after you‚Äôre gone? Will they remember the piggyback rides or your frequent ‚ÄúI‚Äôm tireds‚Äù and ‚Äúl can‚Äôts?‚Äù
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Do you have the skills to save your life and the life of another?
To be blunt, everyone should know how to perform CPR and the Heimlich maneuver, apply a tourniquet, have knowledge of basic wound care, and swim a little bit.
These are no-brainers when you think about humans needing oxygen and having soft skin in a world full of water and sharp things.
But thinking ahead and being prepared is not often where people spend a lot of mental energy.
To put things in perspective: I have had two children choke on three separate occasions (one grape, a handful of Cheerios, and one vanilla wafer), Was talking to someone who had a massive heart attack (100% blockage) mid sentence and collapsed (CPR worked), assisted in two water rescues while on vacation in Florida (riptides are a real thing people!), and cinched a tourniquet on a guy who was stabbed with scissors (severed his brachial artery‚Ä¶ he lived).
Without the skills I‚Äôve laid out I would have buried two children, watched several people drown, and watched someone bleed out. That‚Äôs all I have to say about that.
In Summary:
Life is precious, learn how to preserve yours and others. Condition and push yourself to live the life you actually want and never settle for getting by. You only get one shot at this ‚Äúlife‚Äù thing.
Leave a comment"
147495034.using-sql-with-python-the-ultimate,Using SQL with Python. The Ultimate Chad Stack.,like it or not ...,2024-08-19 06:50:16.085-05,"You know, you can hate ‚Äòem all you want. I have even done that on a few occasions (
SQL and
Python) when¬†overtaken by the heat and passion of the moment. But, maybe I will soften in my old age.
Me‚Äôthinks it might just be pure SQL only ‚Ä¶ or pure Python only which is the problem.
When someone is married to a single solution and not willing to budge or move.
It‚Äôs typically what I see on different Data Platforms, an unfettered love for SQL ‚Ä¶ or Python ‚Ä¶ and never shall the twain meet. That love for one or the other is usually baked like some hot and smoldering roadkill into the road and culture of a place. It‚Äôs hard to dislodge.
I say what better way to solve problems than to combine them both? SQL + Python. Probably the ultimate Chadstack.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Sure, all those Rust and Golang zealots foam and froth at the mouth with those nasty uncompiled and not statically typed Python mongrels running the streets. But, you can‚Äôt argue with the results.
Python runs that data world, and always will.
SQL + Python
I give Data Engineers a hard time when all they can write is SQL. Eventually, they stumble, bumble, and fall down. You simply cannot be a one-trick pony and be effective in the long run.
Same goes for Python only engineers ‚Ä¶ I suppose the same goes for any language. I‚Äôve meant my fair share of (Senior) Software Engineers that could code LeetCode around your head until you passout.
But, throw a database into the mix, a little more than a simple SELECT required ‚Ä¶ and they cast themselves onto the ground, writhing and wriggling in pain, blaming the DBA and AWS RDS for being complete idots.
The moral of the story is simple ‚Ä¶ SQL + Python ‚Ä¶ or Python + SQL is the Ultimate Chadstack.
Both dead simple to learn, and master ‚Ä¶ nay to be expert in ‚Ä¶ such a person can churn and burn some Data Engineering tasks.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Taking a gander through time.
I think to understand the SQL + Python or Python + SQL space we should take a trip back through time
when I was just a young gun, wet behind the ears, hacking code and taking names.
Back in those early days when people were still hitting rocks together and rubbing sticks to make fire, we had to use something called
psycopg2
(
if working with Postgres
), or maybe mysql-connector-python if you were a sucker on a poor MySQL.
This was how you combined Python and SQL, it was pretty much the only (popular way).
This was long before the days of Polars, the popularity of PySpark, Databricks, Snowflake, and the all-consuming SparkSQL that blurred the line between code and SQL ‚Ä¶ (
am I saying SQL is not code???
).
Back when when had to code both ways up hill to get to school, you had to know these terms very well if you were a ‚ÄúData Engineer.‚Äù
connection
error catching and handling
cursors
execute
I mean, something like the following was a daily occurrence believe it or not.
Like I said, both ways uphill, in the rain and snow.
I mean, did you ever have to push multitudes of CSV file records in Postgres back in the day without COPY INTO??
That‚Äôs what I thought you milk toast key scratcher.
Enough of that junk, let‚Äôs talk about today.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Python + SQL in these here fancy days.
What I find amazing, and most others take for granted because it‚Äôs all they know ‚Ä¶
is the way the lines have been blurred between using Python and SQL ‚Ä¶ at the same time.
Back in the good old days, it was a pain to pull records out of a database with Python ‚Ä¶ using some embedded SQL ‚Ä¶ and then get them records into a usable state because you need the power of Python to send something to an API or write to a file, or whatever.
I would say in general the
interoperability
between SQL and Python was just not good many years ago. You could sometimes use tools like Pandas to do a little this and a little that, b
ut of course, it couldn‚Äôt handle anything at scale.
Today, things are different.
Tools like PySpark,
Polars
,
Datafusion
, and
Apache Arrow
have blurred those once-distinct lines.
The ability to switch between Python-centric Dataframes and pure SQL with a SQL-Context has never been easier. Honestly, 10 years ago me would have died in a swoon of happiness if you showed me this code from today.
I mean, if this is all you are used to, then I guess you just take it for granted, but this sorta stuff is making Data Engineering easier ‚Ä¶
it‚Äôs the obvious things that these new tools like Polars are doing that are game changers
‚Ä¶ they change how we solve problems and write code ‚Ä¶ I call that a big deal.
Having the option to switch over to SQL to accomplish a task that just
feels
better in SQL is a wonderful thing.
I think that more data engineers need to take this approach of mixing and matching. Who says you have to use Python for the entire data pipeline? Who says you have to use all SQL for every transformation??
This type of single-mindedness is what causes bad things to happen, bad pipelines to be built, and problems to be perpetuated down the line ad infinitum.
Being willing to use the best tool for a particular job or data transformation is powerful, be it Python, SQL,
or most likely BOTH
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
147353708.maestro-netflix-open-sources-workflow,Maestro - Netflix Open Sources Workflow Tool,just what we need ... another one ...,2024-09-30 08:25:15.329-05,"I ain‚Äôt going to lie to you. I‚Äôm a little bitter about this one. Talk about thinking you‚Äôre about to open the lid and find a pot of gold, but all you get is a couple of rusty old pennies.
That‚Äôs my summary of the exciting news of Netflix open-sourcing their workflow tool called Maestro. Anti-climatic and all-around disappointing.
You know, once I started watching
The Primeagen on YouTube
, my once rosy picture of all the ‚Äúthings‚Äù Netflix does (from a tech perspective) slowly started to dissolve.
After looking at Masetro, the supposedly wonderful new Orchestration tool they open-sourced with much fanfare, my fears were confirmed.
Netflix can make crappy software just like anyone else.
Imagine my utter surprise. They are human as well, I didn‚Äôt see that coming.
I thought Netflix paid every a million dollars a year and you had to sacrifice a small baby to
Dagon
to even get on the list to work there?
Believe it or not, I once had an offer to work as a contractor there ‚Ä¶ they were being generous they said, because I was obviously too stupid at that point to get a chance with a full-time role. (I‚Äôm def. not bitter or anything like that can‚Äôt you tell)
I should probably get on with the show.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Diving into Maestro
Ok, so let‚Äôs just learn what we can about Maestro ‚Ä¶ who knows what will happen, maybe we will get to run it, maybe not, but let‚Äôs just learn about it ‚Ä¶ (
in the light of things like Airflow
).
Let‚Äôs see what they have to say about themselves.
Maestro
is a general-purpose workflow orchestrator that provides a fully managed workflow-as-a-service (WAAS) to the data platform users at Netflix.
It serves thousands of users, including data scientists, data engineers, machine learning engineers, software engineers, content producers, and business analysts, for various use cases. It schedules hundreds of thousands of workflows, millions of jobs every day and operate with a strict SLO even when there are spikes in the traffic. Maestro is highly scalable and extensible to support existing and new use cases and offers enhanced usability to end users.
Of course on the surface it sounds impressive, serving and running all those workloads, but you and me know better,
the proof is in the puddin‚Äô
like your grandma says.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
Pile o‚Äô Junk.
I mean, it‚Äôs hard to know where to start with this pile o‚Äô junk. First, off, the first thing they should have done when releasing
Maestro as open-source on GitHub
should have been to have someone work on a reasonable README and documentation.
I mean it literally looks like a README I would have made after being up for 24 hours straight on a weekend to make a new project. I mean ‚Ä¶ I have done that ‚Ä¶ and written better READMEs.
It‚Äôs a joke.
nothing about basic core concepts
nothing about links to documentation
nothing about links to examples
no diagrams to give the overview
It‚Äôs literally like they had their grandma write the README. It‚Äôs so bad you barely know where to start.
I mean I literally had to do a bunch of Googling to finally find some
half-baked article
that actually explained the tool.
Apparently it has a ‚Ä¶
UI?
CLI?
API?
You wouldn‚Äôt know that from besides if you search the darkest depths of the internet, but whatever. I will summarize the tool concepts for you, since Maestro itself refuses to do that.
In Maestro, a workflow is a
DAG (Directed acyclic graph)
of individual units of job definition called Steps. Steps can have dependencies, triggers, workflow parameters, metadata, step parameters, configurations, and branches (conditional or unconditional).
The system consists of 3 main microservices‚Ä¶
Maestro launches a unit of work (a.k.a. Steps) in a container and ensures the container is launched with the users/applications identity.
Workflow Engine -
Workflow engine is the core component, which manages workflow definitions, the lifecycle of workflow instances, and step instances.
Time-Based Scheduling Service -
Time-based scheduling service starts new workflow instances at the scheduled time specified in workflow definitions.
Signal Service -
Maestro supports event-driven triggering over signals, which are pieces of messages carrying information such as parameter values.
There are a few other important notes ‚Ä¶
CockroachDB
is used for persisting workflow definitions and instance state.
We made some assumptions about user proficiency in programming languages and they can bring their business logic in multiple ways, including but not limited to, a bash script, a
Jupyter notebook
, a Java jar, a docker image, a SQL statement, or a few clicks in the UI using
parameterized workflow templates
.
Maestro provides multiple domain-specific languages (DSLs) including YAML, Python, and Java, for end users to define their workflows, which are decoupled from their business logic. Users can also directly talk to Maestro API to create workflows using the JSON data model.
Source: https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c#360e
I did find it interesting all the examples used Notebooks ‚Ä¶ of course they did.
Also, I almost forgot to mention the biggest fail of them all.
Very very naughty indeed.
Trying to run Maestro
I‚Äôm not going to enjoy this I think, but I must try to run this unwieldy beast for myself. Considering they give you literally like two lines in the README.
./gradlew build
./gradlew bootRun
Running the build command after cloning the repo gave me this.
FAILURE: Build failed with an exception.

* What went wrong:
A problem occurred configuring root project 'maestro'.
> Could not resolve all artifacts for configuration ':classpath'.
   > Could not resolve com.diffplug.spotless:spotless-plugin-gradle:6.+.
     Required by:
         project :
      > Dependency requires at least JVM runtime version 11. This build uses a Java 8 JVM.
The End.
Honestly, at this point I simply don‚Äôt feel like going any farther. What I should do, and maybe will do in the future is get a good Docker container setup and working so we can test this out more.
Call me lazy, call me what you want, but at a certain point I just am not that excited about a tool, and my will to keep going starts to die off.
I‚Äôm not excited about Maestro at all, especially the JSON-driven config etc. The lack of good documentation hopefully continues to improve quickly, Helm charts for Kubernetes, Docker containers etc.
Leave a comment"
147108619.the-rise-of-the-notebook-engineer,The Rise of The Notebook Engineer,off with their heads!,2024-08-06 07:49:16.663-05,"It takes me a lot of sweat, blood, and tears to bear the brunt and ire of the internet overloads to bring you raw and unfettered Data Engineering content. It would mean a lot if you would consider becoming a paid subscriber and help me keep doing the Lord‚Äôs work.
Get 50% off for 1 year
I feel like I‚Äôm sitting on the edge of the ocean, not a nice wonderful sandy beach where the gentle waves are lapping at my feet and the breeze blowing through my non-existent hair, but rather a boiling and roiling ocean pounding the shoreline and destroying my beautiful castle of sand (
ideal Data Engineering world
).
This is my parable. Heed my words all you heathens.
The ocean is the Data World. The pounding waves are the insolent peddlers of destruction embracing a Notebook-driven world, they are zealots bent on the destruction of all I hold dear ‚Ä¶ fundamental truths held since the beginning of Data Time.
These rapscallions are intent on casting down laws of the universe we‚Äôve held in common for low, these many years.
proper development lifecycles and practices.
doing whatever you want, whenever you want, with impunity.
teaching others to do the same
We must band together, the faithful few, and fight like
Spartacus
against the pagan hoard of
Notebook Engineers
arrayed against us.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The atrophy of Data Engineering ‚Ä¶ lead by Notebook Engineers.
Ok, let‚Äôs cut right to the heart of the matter, if you don‚Äôt agree with me (
meaning you abuse Notebooks and encourage others to follow you in your wickedness
), hear me out, and let me set you straight.
Repent in dust and ashes
.
One thing is clear.
99% of Engineers and Data Folk who regularly use Notebooks as part of their development and production lifecycles ‚Ä¶ abuse, overuse, and do so at their own peril and the peril of their Data Platforms at large ‚Ä¶ and suffer the grave consequences as such.
The only Data Engineers who laud the destructive use of Notebooks are those that are either ‚Ä¶
skill issue - it‚Äôs all they can technically do
rabid snake oil salespersons of giant Corporations
In fact, to do an experiment of sorts ‚Ä¶ BEFORE this article was published, I posted a preview of the title on my social media ‚Ä¶ knowing full well I could bait the pundits to prove my point.
And low, to my great
not surprise
‚Ä¶ The Great Dragnet caught something ‚Ä¶ a well-known
Databricks-sponsored
pundit saying they love notebooks.
The rise of the
Notebook Engineer
has been orchestrated and sponsored by not
The Average Engineer
‚Ä¶ nay ‚Ä¶ but it‚Äôs been peddled by the modern Data Stack creators hell-bent on hooking and addicting their unsuspecting victims
into practices that waste time and money, fostering bad engineering, and care little for widely accepted industry best practices.
What, you say? You think I wax strong and poetic?
How dare you.
This has been happening for years
. Case and point.
Go read the comments in the above Reddit conversation, then decide for yourself. This is only scratching the surface of the woes visited upon us by those sinners.
People use Notebooks
because it‚Äôs easy
,
low barrier to entry
, and is
the shortcut
to getting things done. They can do what they want, when they want, getting the dopamine hit right off the bat without putting in the proper work.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Notebooks are like drugs ‚Ä¶ use sparingly at best.
Heck, I have no illusions that I, piddly and small me, can fight against these behemoths and Titans of Industry. I don‚Äôt have Acme Corporation and their marketing machine at my back ‚Ä¶
I‚Äôm just a lone prophet standing in the breach calling the faithful to fight back.
Anyone who has a reasonable amount of experience in Data knows some things ‚Ä¶
Data Engineering and Platforms have classically lacked and lagged behind acceptable SWE best practices.
Data Teams are largely made up of non-SWE background folks (I count myself in this number).
Vendors push tools and processes that at one point were meant to help, but over time they, and their followers, become addicted to their own hurt.
This is what I‚Äôm against. I‚Äôm not against Notebooks in themselves as a critical piece of the overall Development lifecycle. They are critical for Data Analysts and Scientists.
They have a purpose and they serve it.
But, they are absolutely NOT critical for a good Data Engineering lifecycle.
The problem is when, say a vendor (cough cough), publishes every single tutorial they have using Notebooks, markets them, and literally TELLs people to use them in
Production
.
They will spout nonsense like the ability to put Notebooks in Git repos etc ‚Ä¶ no crap ‚Ä¶ I can put a picture of Mom in a Git repo if I feel like it ‚Ä¶ SO WHAT??
I want you to re-read that comment from Reddit above at least twice. Just let it sink into your hard heart.
The truth is that someone who uses Notebooks as their main Development tool, in the vast majority of cases (
not in every case you ding dong
),
are doing so for very specific reasons
.
Unfamiliar with local and IDE development (classic SWE) lifecycles.
Do not test code at all, and have no plans to test code.
Don‚Äôt have a reliable way to deploy code to production (aka poor CI/CD, they have no choice but to use Notebooks).
Simply skill issues
A workplace culture of get-it-done attitude only.
Simply rely 100% on their vendor for all things like a little birdy with their mouth wide open.
Well, what do we do? I guess folk will have to learn their lessons the hard way. Humans are designed to err, don‚Äôt like change, and are liable to follow the crowd like a lemming.
I adjure you to not follow them down their vain path of tears and heartache.
That notebook may taste as sweet as the honeycomb in your mouth but will turn bitter in your belly
. It will poison you over time unless you are already in the 5 AM workout club and have immense self-control.
For the few angry Notebook zealots out there, I assure you I know there are probably a few unicorns running around out there able to use them wisely (
I am one
), but sadly,
this world is mostly made up of horses and donkeys.
If I, a lone ranger of sorts, can convince one single Data Engineer to embrace a classic SWE development lifecycle, and SLOWLY start to reduce their dependence on Notebooks ‚Ä¶ then they are a diamond in the rough, destined for greatness, an Engineer worth hiring who will grow in their knowledge and understanding.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
146962212.deploying-spark-streaming-with-delta,Deploying Spark Streaming with Delta on Kubernetes using Terraform,Spark. Kubernetes. Terraform.,2024-08-01 08:44:55.255-05,"This is a guest post by
Junaid Effendi
.
You can read and check out
his Substack here
.
Looking to deploy a Spark Streaming App on Kubernetes? then this article is perfect for you. The article will focus on deploying the components using Terraform, with scripts provided. I will provide a very easy-to-use template, if you have the right foundation then it would be a copy-paste approach, just update the required fields and you should be able to see your app running instantly.
Deploying Spark Streaming with Delta on Kubernetes using Terraform
Spark Structured Streaming is a unified engine, meaning you can use the exact same code for batch and stream jobs, just need to update the reader and writer functionality which I will share later.
Pre Requisite:
You have a Kubernetes Cluster up and running with all the required setup and have the basic knowledge of K8.
You have a Spark application (even batch) and working knowledge of Spark.
Architecture
Spark Streaming Architecture on K8
Kubernetes Infra
Namespace
A dedicated namespace would help in separation and ownership becomes easier:
resource ""kubernetes_namespace"" ""<my_namespace>"" {
  metadata {
    name = <my_namespace>
  }
}
Service Account
Service account is attached to the IAM role to handle all permissions, e.g. writing to the Delta Lake (Object Store), reading from source Apache Kafka in our case.
resource ""kubernetes_service_account"" ""<my_service_account>"" {
  metadata {
    name      = <my_service_account>
    namespace = <my_namespace>
    annotations = {
      // mapping of cloud role. e.g. if you are using aws:
      ""eks.amazonaws.com/role-arn"" : ""<aws_iam_arn>""
    }
  }
}
Cluster Role
To allow Spark Operator to handle the life cycle of Spark Job, it needs access to Kubernetes resources:
resource ""kubernetes_cluster_role"" ""<my_cluster_role>"" {
  metadata {
    name = ""<my_cluster_role>""
  }
  rule {
    api_groups = ["""", ""extensions"", ""apps""]
    resources  = [""secrets"", ""configmaps"", ""pods"", ""pods/log"", ""services"", ""persistentvolumeclaims""]
    verbs      = [""create"", ""get"", ""list"", ""update"", ""watch"", ""patch"", ""delete"", ""deletecollection""]
  }
  rule {
    api_groups = [""sparkoperator.k8s.io""]
    resources  = [""sparkapplications""]
    verbs      = [""create"", ""get"", ""list"", ""update"", ""watch"", ""patch"", ""delete""]
  }
}

resource ""kubernetes_cluster_role_binding"" ""<my_cluster_role_binding>"" {
  metadata {
    name = ""<my_cluster_role_binding>""
  }
  role_ref {
    api_group = ""rbac.authorization.k8s.io""
    kind      = ""ClusterRole""
    name      = kubernetes_cluster_role.<my_cluster_role>.metadata[0].name
  }
  subject {
    kind     = ""ServiceAccount""
    name     = kubernetes_service_account.<my_service_account>.metadata[0].name
    
    namespace = <my_namespace>
  }
}
Spark Operator
Spark Operator has changed the game, managing the Spark lifecycle was very tedious, you would have to set up lot of things just to make it work on EKS, with Spark Operator just follow the template below to deploy Spark application, this works for both Batch and Streaming Jobs.
resource ""helm_release"" ""spark"" {
  repository = ""https://kubeflow.github.io/spark-operator""
  chart      = ""spark-operator""
  version    = ""1.2.5""

  name             = ""spark""
  namespace        = <my_namespace>
  create_namespace = false
  wait             = false

  set {
    name  = ""sparkJobNamespace""
    value = ""<my_namespace>""
  }
  set {
    name  = ""enableBatchScheduler""
    value = true
  }
  set {
    name  = ""enableWebhook""
    value = true
  }
  set {
    name  = ""webhook.enable""
    value = true
  }
  set {
    name  = ""webhook.port""
    value = 443
  }
  set {
    name  = ""podMonitor.enable""
    value = false
  }
  set {
    name  = ""metrics.enable""
    value = false
  }
  set {
    name  = ""image.tag""
    value = ""v1beta2-1.4.2-3.5.0""
  }
  set {
    name  = ""image.repository""
    value = ""ghcr.io/kubeflow/spark-operator""
  }
  set {
    name  = ""uiService.enable""
    value = true
  }
}
Data Pipeline
The pipeline consists of just one streaming job, but you can easily add multiple layers of Streaming jobs if needed like Raw and Processed.
üí°Example will be based on Scala, its similar for Python.
Data Source
Source can be any Messaging System like Apache Kafka. You can use the Kafka Data Source in the Spark App to read the messages. Make sure to install the connector library, e.g.
spark-sql-kafka
.
If you are reusing the same source code that is used for Batch, then your source just needs to be changed, e.g. you can read and write using this:
// Subscribe to 1 Kafka topic
val events = spark
  .readStream
  .format(""kafka"")
  .option(""kafka.bootstrap.servers"", ""host1:port1,host2:port2"")
  .option(""subscribe"", ""<my_topic>"")
  .load()

// real time transformation and logic goes here

// write data to delta path
events.writeStream
  .format(""delta"")
  .outputMode(""append"")
  .option(""checkpointLocation"", ""<checkpoint_path>"")
  .start(""<output_path>"")
üìñ More examples can be found
here
Streaming App
Deploying the actual Spark Application, assuming you already have a working artifact for Spark App which applies the business logic, transformation, filtering, PII handling etc.
Leveraging the above k8 infra:
resource ""kubernetes_manifest"" ""sparkapplication_<my_namespace>_<my_job>"" {
  manifest = {
    ""apiVersion"" = ""sparkoperator.k8s.io/v1beta2""
    ""kind""       = ""SparkApplication""
    ""metadata"" = {
      ""name""      = <job_name>
      ""namespace"" = kubernetes_namespace.<my_job>.id
    }
    ""spec"" = {
      ""arguments"" = [] // Optional
      ""restartPolicy"" = {
        ""type""                             = ""Always""
        ""onFailureRetryInterval""           = 10
        ""onSubmissionFailureRetryInterval"" = 20
      }
      ""deps"" = {
        ""jars"" = [] // If you want to set dependencies during runtime
      }
      ""driver"" = {
        ""cores""  = 2
        ""memory"" = ""2g""
        ""serviceAccount"" = kubernetes_service_account.<my_service_account>.metadata[0].name
      }
      ""executor"" = {
        ""cores""  = 2
        ""memory"" = ""2g""
      }
      ""image""               = ""<path_to_docker_image>""
      ""imagePullPolicy""     = ""Always""
      ""mainApplicationFile"" = ""<path_to_jar_artifact>""
      ""mainClass""           = ""<path_to_class>""
      ""mode""                = ""cluster""
      ""sparkConf"" = {
        // this section contains spark config: e.g.
        ""spark.kubernetes.executor.podNamePrefix"" = ""<job_name>""
        // delta configs
        ""spark.databricks.delta....""
      }
      ""hadoopConf"" = { 
        // this section contains all hadoop config, e.g.:
        ""mapreduce.fileoutputcommitter.algorithm.version"" = ""2""
      }
      ""sparkVersion"" = ""3.5.0""
      ""type""         = ""Scala""
    }
  }
  depends_on = [helm_release.spark]
}
üìñ Read more:
Spark Operator Api Docs
‚≠ê Consider using
Kafka to Delta Connector
if no transformation/logic is needed in real time.
Delta
Delta is pretty easy to setup, setting the spark configs in the config section above and installing the library
delta-spark
.
üí°Make sure to use compatible
versions of Spark and Delta
.
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
üìñ For more checkout:
Quick Start Guide
üí°It is worth considering an orchestrator like Airflow or ArgoCD to deploy Spark Streaming Jobs.
Post Deploy
Some handy commands for debugging and monitoring:
Checking if Spark App is running:
kubectl get SparkApplication -n <my_namespace>

kubectl get pods -n <my_namespace>
Port forward the Spark UI:
kubectl port-forward <my_job>-driver -n <my_namespace> 4040:4040
Ideally, the Spark UI should be up and running independently, this can be achieved via Spark Operator as well which is not covered in this article, see the reference section for more information.
References
Additional resources and references can be found below:
Kubernetes: https://developer.hashicorp.com/terraform/tutorials/kubernetes/kubernetes-provider
Spark Operator Docs: https://github.com/kubeflow/spark-operator/blob/master/docs/api-docs.md
Spark with Kafka: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
Everything about Delta: https://docs.delta.io/latest/delta-intro.html
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
146940158.apache-datafusion-comet,Apache Datafusion Comet,making Apache Spark scream,2024-08-28 07:48:48.85-05,"Y‚Äôall knew this was coming, didn‚Äôt you? I mean think about it. We live in a TikTok and Instagram culture, we want stuff quick, now, and above all
fast
. Of course, it‚Äôs never fast enough, we need it faster.
We are Data Engineers, after all, we spend our days cracking the proverbial whip on those bits and bits, we want to make them scream for mercy.
It‚Äôs every engineer‚Äôs obsession is it not? To make a thing faster?
Get 50% off for 1 year
Today, we are going to poke at something that‚Äôs trying to make Apache Spark faster. No surprise there. With the rise of Databricks ‚Ä¶ Apache Spark has become the de facto tool to process data ‚Ä¶ Big Data or NOT.
Meaning there is a real need to have Spark work fast on datasets that aren‚Äôt 300TBs. Everyone wants Spark to be faster.
‚Äú
Apache DataFusion Comet
is an Apache Spark plugin that uses Apache DataFusion as a native runtime to achieve improvement in terms of query efficiency and query runtime.‚Äù
A shift in Data Engineering tooling.
I want to take a moment to talk about something interesting, a trend that I‚Äôve perceived in Data Engineering over the last 10 years+.
Basically, it can be summed up as ‚Ä¶
‚Äú
Rust and Rust based tooling is replacing Java/Scala tooling and infastructure in many places
.‚Äù
Things like this don‚Äôt happen overnight, they take time, and they happen by degrees. With the advent of things like Apache Arrow, tools like Polars and Datafusion, and the general popularity of Rust for building ‚Ä¶ there seems to be an ever-increasing flow of Rust making inroads and supplanting the JVM as the defacto tool.
Heck, even Databricks has started to develop ways around the JVM to increase things (think Photon etc).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Back to Apache Datafusion Comet.
Let‚Äôs get back on track before we run this train off the track. Apache Datafusion Comet. If you are
unfamiliar with Datafusion
you should probably check yourself first, second just do some reading and get with it.
Datafusion is Rust-based, built for Dataframes and SQL, and is quickly becoming the defacto tool ‚Ä¶ for building other tools.
It‚Äôs the GOAT, and now apparently it‚Äôs being used to ‚Äúspeed up‚Äù Apache Spark by bypassing the JVM.
‚Äú
Apache DataFusion Comet is a high-performance accelerator for Apache Spark, built on top of the powerful
Apache DataFusion
query engine. Comet is designed to significantly enhance the performance of Apache Spark workloads while leveraging commodity hardware and seamlessly integrating with the Spark ecosystem without requiring any code changes.
‚Äù
Below is the classic TPC benchmark with this tool, which we of course no one ever messed with or lied about.
Today we are going to see if the Apache Datafusion Comet is a bunch of dirty old liars or if they are the real deal. We shall
install it
run it
performance test it against vanilla Spark
We shall see if our dreams will be fulfilled or dashed upon the rocky crags of reality.
First, let‚Äôs just see what the instructions are like for installing and running Apache Datafusion Comet on a Spark cluster ‚Ä¶ because that is the point after all isn‚Äôt it?
Here is the official install guide.
It appears you need ‚Ä¶
>= Apache Spark 3.3
JDK8+
Linux or Mac
Next, the instructions say you can build the binary from scratch using the Git repo, or use the
official pre-build binaries located here.
It is not clear at all, which `
tar
` I‚Äôm supposed to download. I decided to go with the `
datafusion-40.0.0`
one.
After unpacking and inspecting the `tar` file, there is a TON of files but nothing obvious to do next ‚Ä¶ the README is no help.
How do I get from here to the apparent JAR file I need to submit a Spark job and include Apache Datafusion Comet? Your guess is as good as mine.
The instructions say ‚Ä¶
Building from a source release is mostly the same as building directly from the GitHub repository but requires the use of the command make release-nogit ...
But that results in ‚Ä¶
danielbeach@Daniels-MacBook-Pro Downloads % cd apache-datafusion-40.0.0
danielbeach@Daniels-MacBook-Pro apache-datafusion-40.0.0 % make release-nogit
make: *** No rule to make target `release-nogit'.  Stop.
Ok, we can‚Äôt give up that easily, let‚Äôs just switch over to cloning and building from Git.
git clone https://github.com/apache/datafusion-comet.git
make release PROFILES=""-Pspark-3.5""
Might as well go to lunch, that build takes FOREVER ‚Ä¶ in classic Rust fashion.
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Comet Project Parent POM 0.2.0-SNAPSHOT:
[INFO] 
[INFO] Comet Project Parent POM ........................... SUCCESS [ 52.958 s]
[INFO] comet-common ....................................... SUCCESS [02:52 min]
[INFO] comet-spark ........................................ SUCCESS [03:32 min]
[INFO] comet-spark-integration ............................ SUCCESS [  4.623 s]
[INFO] comet-fuzz ......................................... SUCCESS [ 20.085 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  07:42 min
[INFO] Finished at: 2024-08-06T18:23:25-05:00
[INFO] -----------------------------------------------------------------
Now the instructions don‚Äôt tell you what to do next, I‚Äôm assuming, reading the following instructions we need to go find a JAR that can be used along with the Spark Submit command.
But of course, they don‚Äôt tell you that, or tell you where to find it. We are on our own.
I can assume from the build messages, that this might be The One True JAR we are looking for.
[INFO] Building jar: /Users/danielbeach/Downloads/datafusion-comet/fuzz-testing/target/comet-fuzz-spark3.5_2.12-0.2.0-SNAPSHOT-jar-with-dependencies.jar
Well, we have a JAR we think we need, now what?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Trying out Apache Datafusion Comet.
The first thing you will need to do is have access to Spark, in my case 3.5. You can do this in many ways, local install, Docker, Databricks ‚Ä¶ whatever you have access to.
I just used a Docker image with Spark 3.5 and included my above JAR file in the image as well. I used their (Comet‚Äôs) instructions to see if Comet would work properly or not.
I ran this ‚Ä¶
/usr/local/spark/bin/spark-shell \
    --jars $COMET_JAR \
    --conf spark.driver.extraClassPath=$COMET_JAR \
    --conf spark.executor.extraClassPath=$COMET_JAR \
    --conf spark.sql.extensions=org.apache.comet.CometSparkSessionExtensions \
    --conf spark.comet.enabled=true \
    --conf spark.comet.exec.enabled=true \
    --conf spark.comet.exec.all.enabled=true \
    --conf spark.comet.explainFallback.enabled=true \
    --driver-class-path $COMET_JAR
Well, I saw a Warning about a failure with Comet immediately on Spark startup. This is good and bad, good in the fact Comet appears to be alive and getting picked up (attempted) by Spark, bad that it‚Äôs a failure.
24/08/08 10:01:04 WARN SparkSession: Cannot use org.apache.comet.CometSparkSessionExtensions to configure session extensions.
java.lang.ClassNotFoundException: org.apache.comet.CometSparkSessionExtensions
        at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:72)
It‚Äôs hard to know what is wrong at this point, I could have done something bad all the way in the beginning trying to build the JAR file.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Going the Docker route.
Instead of trying to figure out what went wrong, for the sake of time, I figured I would try out the Docker route, better than nothing.
Turns out that the
GitHub had a Dockerfile we could try building
. After a few tries of copying the Dockerfile and cloning the GitHub repo and running the build, command I finally got it done.
If you want to get the Dockerfile to build, here are some notes that they won‚Äôt tell you.
You can‚Äôt just copy the Dockerfile and try to build it, you have to clone the entire repo.
You MUST run the Docker build command from the main repo directory, you can‚Äôt change directories into the directory where the Dockerfile lives, it will fail.
Finally, at this point I‚Äôm just happy to apparently have a working version of Spark and Comet running.
docker run -it comet /bin/bash
When I look inside the JAR directory for `/opt/spark/jars` I can see ‚Ä¶
comet-spark-spark3.4_2.12-0.2.0-SNAPSHOT.jar
So this is a good sign. At this point let‚Äôs just try to running something and see what happens.
Above is what I ran to see if things would just work. (I had to play with the instructions given in the Datafusion Documentation, there were some ‚Äî-conf they had that seemed to blow up Spark. The above worked for me.
As far as I can tell my sample script worked fine. It appears the Comet JAR loaded with Spark without any errors. Below is the simple script I am running.
Upgrade a free subscription to paid and get 50% off.
Get 50% off for 1 year
Seeing if Apache Comet Datafusion is as FAST as they say.
Ok, so now that I‚Äôve burned a few weekends working on this for all you unappreciative milk toast programmers, the time has finally come to see if this tool can speed up some smaller data processing scripts by a significant amount.
This is going to be about as unscientific as it gets, but that‚Äôs alright, we are just learning a new tool. It‚Äôs the journey that matters most usually.
I went ahead and downloaded some of the
Divvy Bike Trip opensource dataset CSV files
inside this Docker image.
root@9b1556e100fa:/# ls /data
202401-divvy-tripdata.csv  202402-divvy-tripdata.csv  202403-divvy-tripdata.csv  202404-divvy-tripdata.csv  202405-divvy-tripdata.csv
I then converted those CSV files to Parquets ‚Ä¶ apparently, Comet doesn‚Äôt support CSV files right now.
We are going to run this very simple script ‚Ä¶ twice ‚Ä¶
Things to note ‚Ä¶ I‚Äôm reading and writing Paruqets since that‚Äôs what Comet supports, I‚Äôm also printing the EXPLAIN on the final dataframe to see if COMET shows up ‚Ä¶ to ensure it‚Äôs working.
once without Comet ‚Üí
Time to run pipeline: 0:00:09.511450
once with Comet ‚Üí
Time to run pipeline: 0:00:09.243452
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[date#26, _groupingexpression#80, _groupingexpression#81, _groupingexpression#82, rideable_type#1], functions=[sum(cast(ride_id#0 as double))])
   +- Exchange hashpartitioning(date#26, _groupingexpression#80, _groupingexpression#81, _groupingexpression#82, rideable_type#1, 200), ENSURE_REQUIREMENTS, [plan_id=171]
      +- HashAggregate(keys=[date#26, _groupingexpression#80, _groupingexpression#81, _groupingexpression#82, rideable_type#1], functions=[partial_sum(cast(ride_id#0 as double))])
         +- Project [ride_id#0, rideable_type#1, date#26, year(date#26) AS _groupingexpression#80, month(date#26) AS _groupingexpression#81, dayofmonth(date#26) AS _groupingexpression#82]
            +- Project [ride_id#0, rideable_type#1, cast(started_at#2 as date) AS date#26]
               +- CometScan parquet [ride_id#0,rideable_type#1,started_at#2] Batched: true, DataFilters: [], Format: CometParquet, Location: InMemoryFileIndex(1 paths)[file:/data/parquets], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ride_id:string,rideable_type:string,started_at:string>
We can see if the EXPLAIN that CometScan and CometParquet shows up, so I‚Äôm going to take this as it‚Äôs doing its thing.
Interestingly, Apache Datafusion Comet doesn‚Äôt seem to make much of a difference at this level.
It probably has something to do with the data size maybe?
I mean we are only doing a GROUP BY and AGG on about 1,694,242 rows. Even Pandas can do that.
I‚Äôm curious if increasing the data size wides the runtime gap between Comet and No Comet. I added another 100 files, see what happens. This now is about 16,182,111 records.
once without Comet ‚Üí
Time to run pipeline: 0:00:24.621078
once with Comet ‚Üí
Time to run pipeline: 0:00:25.065360
Interesting! Comet (confirmed EXPLAIN still showed Comet) appears to be a little slower this time around with more data!
(
this probably has to do with Comet only being used for CometScan and CometParquet ‚Ä¶ not for the aggregations
)
That‚Äôs enough for me today. I think Comet is going to be an upcoming and powerful tool as it continues to get developed and adds features!
Very promising project, hopefully they continue to work on it and market it well so Data Engineers know about this ‚Ä¶ super interested to find out if others are using this in production yet.
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share"
146797573.youre-doing-data-engineering-wrong,You're Doing Data Engineering Wrong.,Majoring in the Minors,2024-07-25 09:07:59.834-05,"That‚Äôs right you pasty little hacker. You. Yeah, you. Over there on a Friday afternoon when you should be skipping out of work early. Just putting in a few more lines of code. I mean it‚Äôs just a few more.
We just need more code. We can solve this problem, just give me some time to write the code. Did you see that code they wrote? That‚Äôs not how you should write that code. Aren‚Äôt you writing Rust yet? Golang? Hey, you heard about Zig? I heard it‚Äôs lit.
Is it really all about the code? Leetcode junkies snorting one more line of algo goodness into their already exploding brain. Why write one line when you can write five?
I mean gotta keep that GitHub profile screaming that green. Besides, what will your teammates think of you if you aren‚Äôt writing a ton of code? Lots of code = doing your job.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
What mid Data Engineers are doing wrong.
Maybe this should be about what we are all doing wrong. I would say in all my years of Data Engineering probably the single most important lesson I‚Äôve learned is ‚Ä¶
‚Ä¶ when to NOT write code.
No empty platitudes here, truth. This is what all the mid-engineers get wrong. Yeah, you have to be a good programmer to Senior+ levels, and that requires writing lots of code over time and getting experience.
Nothing worse than a programmer who simply can‚Äôt program.
That doesn‚Äôt negate the fact that when you reach those Senior+ levels that the proverbial rug is pulled from under your programming feet. The world you live in gets flip-flopped.
You reached your job by being a good programmer, better than average, probably better than those around you. So why the sudden change? Senior+ engineers spend most of their time ‚Ä¶
Planning projects
Helping others solve problems
Upskilling and mentoring others
Architecture
Working with the business units (non-engineering)
Translating requirements and needs back to Engineering
You will note that nothing in the list involves programming (
although it probably involves the assumption you‚Äôre a good programmer
).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
You‚Äôre not special.
I hate to break it to you. I don‚Äôt care what your mom told you. She lied. You‚Äôre not that special. If we post your job onto the interwebs by tomorrow morning that would be 1000 applications with people who can all write SQL and Python.
Many of them might be below average ‚Ä¶
but guess what ‚Ä¶ you can teach someone how to become a better programmer!
So many people mistakenly think that being a better Data Engineer or coder is all about getting a higher rank of LeetCode. That simply isn‚Äôt the case. Anyone who programs consistently over time will become better. It‚Äôs a relatively easy skill to teach.
You know what isn‚Äôt that easy?
Being a team player
Being someone who can solve problems with hand-holding
Excellent written and verbal communication skills
Constant learner
These skills are way harder to learn, hone, and teach than simply how to write SQL and Python. I mean we have ChatGPT now ‚Ä¶ it‚Äôs as good as most of the mid-Engineers.
If you‚Äôre on a team of Data Engineers, or trying to get hired onto a team, do you really want to spend all your time becoming the best programmer? Sure, might not be a bad idea. But, if you want to stand out, you need to build the skills that separate Senior+ Engineers from the rest.
The engineers that provide ‚Ä¶
Business value
Cross-functional relationships
Upskill the team
We need more Engineers who think before they code.
This is how you‚Äôre doing Data Engineering wrong.
Instead of responding to every single problem with more code, immediately, we need more Data Engineers who are willing to stop for a moment ‚Ä¶ and think.
Good engineers understand that every single line of code written is a liability, something that can and will break. Something that is tech debt.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Majoring in Minors
There are lots of ways to major in minors, to walk down the path of ill-fated Data Platforms that are a spaghetti of tools, half-baked code bases, no tests, and general crap.
I mean, this is where most Data Engineers spend their time, majoring in minors, while the important stuff molders away in the corner.
Arguing about whether to use Python vs Scala vs Golang vs Rust
Not agreeing on whether to use SubQuerys of CTEs
Python or DataFrames vs SQL
Snowflake vs Databricks
AWS vs GCP
OOP vs imperative vs procedural vs functional
Kimball vs Inmon vs other data modeling
Managed service vs self-hosted
Data Warehouse vs Lake House vs Data Lake
In the age of marketing drivel pumped out in a never-ending stream of regurgitated old ideas, the SaaS companies you worship are releasing new ‚Äúmust-have‚Äù features every week. You don‚Äôt need them all to have a good Data Platform or to be a good Data Engineer.
In fact, there is nothing more telling than when folk bluster on about the brand new this or that, acting like it‚Äôs come to deliver us from our purgatory of data. How foolish.
Did we not survive up until this point?
Don‚Äôt major in the minors like everyone else.
Worry about building Data Platforms and Data Products that are ‚Ä¶
reliable
simple
monitoring and logging setup
good development lifecycles
delivered on time
happy customers (end users)
provide real value
cost-effective
Heck, I want you to be a good coder. I think there‚Äôs nothing wrong with spending time on Leetcode and honing your craft, we should all spend time becoming better writers of code.
But, being an excellent programmer requires more than good code, a lot more.
Getting these values swapped around is probably what keeps most mid-level and junior engineers from getting to the next level. Simply refusing to accept the fact that writing code is all that matters.
What good is your beautiful code if it‚Äôs not what the business wants or needs? What good is it if it costs too much? What good is it if it‚Äôs over-engineered? Instead of providing value, many engineers simply write as much code as possible and end up destroying the very thing they are supposed to curate, their Data Platforms.
So don‚Äôt do it.
You don‚Äôt need just one more line of code. It isn‚Äôt always the answer.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
146564249.date-and-time-manipulation-with-duckdb,Date and Time Manipulation with DuckDB,quack quack,2024-07-24 07:49:07.653-05,"In my never-ending quest to
pound DuckDB into the dirt
(
don‚Äôt take it personally, I do it for the good of all Data Engineers
) I realized I‚Äôd forgotten something oh-so-important. I mean there are few more tedious things that we have to do day in and day out than
date
and
datetime
manipulation.
So, today we will do the boring.
My hope is that it will be boring, I dare say that would be a good thing. When it comes to this sort of thing, the piddling and meddling with timestamps and the like, you don‚Äôt want it to get exciting.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The importance of the small stuff.
One thing you will note, if you are an astute observer of my general modus operandi, is that when it comes to newerish tools, I typically do the basic stuff first.
I simply build a normal pipeline. I read data from s3, munged the data around, and then write the data to s3. I mean what is more boring and ordinary than that?
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
The point is, you would be surprised how many times you find holes in the bottom of the bucket when doing this stuff. I find it a breath of fresh air amidst the muck and mire we call data content that has been infiltrated by those marketing pirates who seem to have swallowed us all whole.
That is why we are going to do something boring, like play with
date
and
datetime
manipulation in DuckDB.
We gonna squeeze the ducky till it makes a noise.
DuckDB date and datetime buggering
It‚Äôs hard to know where to start, but I want to make it as realistic as possible. There are a few general
date
and
datetime
manipulations that cover 80% of what actually needs to be done most of the time.
Probably one of the most common manipulations that Data Engineers do for
datetime
manipulation is turning STRING dates into real dates. Think about getting some
date/datetime
column in a CSV file and needing to cast it to something useful.
Luckily and surprisingly DuckDB has something that all Python people will recognize ‚Ä¶
strftime
and
strptime
.
We will be using
Divvy Bike Trips open source datase
t to do our playing around with.
strftime and strptime
This should be straightforward, let‚Äôs convert the
started_at
column which is coming from a CSV file to an actual
timestamp
, for example, `
2024-06-11 17:20:06.289
`.
It‚Äôs also important to note that DuckDB has a CSV autoloader that can do a great job sensing formats, including complex
datetime
formats, like in this example.
Above you can see auto-reading the CSV file and the timestamps for
started_at
were interpreted correctly.
But, we can do the same thing with similar text using the method
strptime
.
If you‚Äôre like me and are used to using ANSI SQL-type statements, I‚Äôm curious if this works.
That‚Äôs a good sign, having a CAST that works with date and
datetimes
can really make life easy.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Adding and subtracting days
Another one I‚Äôm curious about, because it can be a pain if it‚Äôs not supported, but comes up often, is the simple addition and subtraction of days.
Many times we find ourselves with a date and we want to go back 30 days, or forward 30 for that matter. Let‚Äôs see what DuckDB has to offer us in this respect.
Well, that is kinda funny and refreshing. Nothing easier than adding a few days apparently. Simply using the addition + operator to add some days makes it no no-brainer.
Never fear, DuckDB has a normal method for this also, if you want to be explicit, adding days, months, or whatever ‚Ä¶ using the
date_add()
method.
Nothing worthy of note there, which is a good thing.
Date parts
Again, in an effort to do the mundane, another piece of date magic we all do on a regular basis is pulling date parts out, say a year, month, or day of the month etc.
Easy stuff, good stuff, DuckDB making it easy.
Random date stuff
Wonderfully, there is much more
date
and
datetime
stuff in DuckDB, almost too much to go through at one time. But, here are some useful methods that will probably come in handy for normal Data Engineering work with DuckDB.
We can get a
date_diff()
for two dates, either by day, month, etc, whatever.
Get the day of the week.
The very important ‚Ä¶
give me the last day of the month from said date.
DuckDB and dates
Well, that was excitedly unexciting after all, which is good. I was hoping for no surprises and found none. I‚Äôm glad DuckDB has wide and familiar support for general
date
and
datetime
manipulation that are fairly common across Data Engineering.
I think it‚Äôs a great show of maturity that DuckDB can do all these simple things like adding dates, pulling date parts, converting text to date and timestamps etc.
It‚Äôs a clear sign of immaturity when they cannot support base date manipulations like we saw today.
It‚Äôs the small things that matter when it comes to Data Engineering tools that might be used in a Production setting.
Sure, it‚Äôs great if you can read a CSV file from s3 ‚Ä¶ but the real question is ‚Ä¶ what can do with it once you have it??
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
146506102.aint-no-room-for-ai-in-my-workflow,Ain't no room for AI (in my workflow),guest post by anonymous Rust Dev,2024-07-15 07:19:42.522-05,"Today we have a guest post from anonymous Rust Dev. He is taking on the much-debated topic of AI in the workflow of a programmer ‚Ä¶ or lack thereof.
For a while there, it seemed like AI (
or, as I like to call it, ""AIIEEE~EEE""
) was going to take all our (programmers) jobs. I, for one, am a holdout, and neither fear my job's long-term viability nor do I have any desire to change what I'm doing today. Am I just being an old git who yells at clouds, or am I onto something?
I'll let you decide, but first, I'll explain myself.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
""Kids these days""
That's a phrase you've likely heard, probably from stodgy old coots that just can't adapt to the times. You know, when some ignorant person, set in their ways, doesn't take the time to understand what they're looking at and defaults to a reductive judgment.
It's not always the wrong approach, either; not every ""
shiny
"" is truly worthy of our collective attention. If someone really wants to wear hammer pants, the right thing to do is to not spend any time thinking about it - it's their business, and without grilling them on it you don't know enough about the situation to make any definitive statements.
That said, I feel like I'm channeling a bit of my inner Ned Ludd as I rail against the system.
Skepticism
Okay, so I'm going to make the claim that I'm skeptical not out of blind ignorance, but rather from some observations and reflection. I watch, for instance,
as Microsoft (via Github) makes liberal use of the code that people host with them
.
Did any of the authors of that code know, when they chose to host with the benevolent overlord that is Github, that their code would inevitably be used in AI training? And, while I don't often beat on the ""
You can't trust M$
"" drum that older open-source advocates are too familiar with, their position as the gatekeeper of many companies' private code leaves some
room for skepticism
.
And, because we're talking about Microsoft, has anyone been paying attention to the release of the
Snapdragon X product line
? As a Mac (M1) owner, I've long been excited for that kind of performance but without the whole Apple ecosystem attached to it.
As it starts to look like we have a contender for that throne from Qualcomm, though, I'm starting to notice a disturbing trend: the conversation has shifted from ""
powerful and efficient ARM chips
"" to ""
Copilot+ PC
"".
Like, who even asked for this? I think most people saw how badly Microsoft fumbled the ball on this one; instead of touting energy efficiency and a familiar experience taken to the next level, they chose instead to focus on their
""Recall""
product and make the whole thing about how great AI is and how they're doing AI.
Trust
Of course, nobody trusts Microsoft with their most private data, and the backlash has been extreme. However, undergirding this buzz is a more fundamental question:
did they really think anybody wanted AI edge computing capabilities?
Macs have been shipping chips with their NPU hardware since the introduction of the M1, but for most of their customers, this component has largely been ignored because, let's be honest, it doesn't really help most of their customers.
And, as if to urinate on Microsoft's tombstone, they are also rolling out their own answer to the Copilot+ PC by giving their own
Apple Intelligence
functionality to users of their recent hardware. But again, was anyone asking for this? Will this make anyone's life better?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
I've played this video game before
No doubt many people are familiar with the
Metal Gear Solid series of video games
by Hideo Kojima. It's full of absurdity, shameless
name-dropping
, and of course plenty of stealth action. The games also have a tendency to explore some very interesting topics throughout their storytelling, and of particular significance to this conversation is its exploration of information control using AI. The second game in the MGS series was a
prescient exploration
of how AI would eventually come to govern the flow of information.
Think about it briefly; you may have come across this article with the help of The Algorithm. It's no secret that social media platforms like Facebook
drive users into echo chambers
.
Where the MGS2 game narrative diverges from reality is in the driving force - rather than being the product of authoritarian rule √† la 1984, the reins of power are instead in the hands of free-market forces, fine-tuned to produce outsized response with minimal effort (
a.k.a. ""clickbait""
).
Worse, as companies feed the internet into their model training, a feedback loop of misinformation and
algorithmically curated media
directs the next generation of AI-driven content, trending toward the day when we can no longer be certain that our information isn't purely the product of an
AI-fueled fever dream
. In essence, we can expect an
AI inbreeding problem
, as well as an arms race between AI content and the ability to detect it.
As a human, I am more and more questioning my own ability to distinguish generated content from the organic variety. In fact, with a human at the helm managing just enough, the content today can be
very convincing
in terms of quality.
Also, let's be clear, that quality wasn't created in a vacuum, but from models that sample actual art (or, in a developer's case, code); how certain can we be that the next generation of code isn't rife with unintentional plagiarism?
Productivity
As I started this morning with a coffee and a quick trip into
WebStorm
to write this article in markdown, I was greeted with a notification from my IDE overlord about the awesome
AI Assistant
.
Its pitch, a familiar refrain in the programming world at this point, is that you can provide a high-level query to either unravel the universe's mysteries or generate the code you need. It gives off some serious
""Tea, earl grey, hot""
vibes.
As a junior developer, you might struggle to write a logarithmic backoff routine for a web client with a semaphore lock. In fact, having just asked ChatGPT for that very thing, I got an excellent Python script that fits the bill almost perfectly, accompanied by a detailed explanation of what is going on.
The thing is, these aren't the problems that occupy my time, nor are they the kinds of problems that require much effort - in fact, I'd personally take the lazy way out and just look to see if it's an already solved problem in a shared library.
This takes me to where I am as a developer:
far more of my time is spent dreaming up what needs to happen than is spent on actually writing the code to make it so.
Yes, I write code, and it can be pretty dense at times, but I have no effective way to get an AI prompt to understand my problem domains to such a degree that they can give me meaningful output.
In this, there is a long road for AI to travel before it can replace what I do.
What I see when I look at the AI tools is a snippet generator, a novelty that can save me a few keystrokes but will require me to be diligent and second-guess the quality of what I was given at every turn. I've heard enough horror stories of ""almost but not quite"" solutions from AI prompts; maybe someday, ""programmer"" will be an anachronistic stand-in for ""AI babysitter."" Who knows, maybe I'm just ignorant and don't know that we already have a feature that can turn a Jira ticket into a hotfix without human intervention.
Actually, I can see a future where the Jira ticket idea becomes a reality. By analyzing an existing codebase, it's not a stretch to think that an AI-assisted software tool could one day understand the nature of an application and offer contextually relevant feedback - even to the point of rolling out new features based on customer prompts or product reviews.
We could very well be heading to a WALL-E or Idiocracy future where humanity exists solely to consume, leaving the means of production to robot slaves.
In the meantime, I'll enjoy slapping my keyboard like some kind of demented chimpanzee in the hopes that it might result in a Shakespearean-esque work of Rust prose.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Some trailing thoughts
Our society in recent generations has seen some pivotal paradigm shifts, and its ability to adapt is both awe-inspiring and fodder for despair at the same time. Near-instantaneous communication, even as far back as the telegraph, has allowed for the dissemination of information in ways that couldn't have easily been predicted by those who came before.
Arguably, the success of the Allies in WW2 can be attributed in part to information management and control - Navajo Code Talkers over radio,
the OWI
propaganda machine, and cracking the
Enigma
ciphers are just a few examples of game-changing uses of this dissemination.
Except, what came after was frequently predicted, often by lofty-minded dreamers or sci-fi writers. Arthur C. Clarke
foresaw telecommuting
, among so many other things we currently take for granted. Vannevar Bush predicted Wikipedia
as far back as 1945
, and upon his shoulders stood Doug Engelbart in
the Mother of All Demos
. Isaac Asimov was one of many who predicted the ease of which we'd be able to tap a global information network from the comfort of our home.
Raising a child in the touchscreen era, I can't help but think that our technical capabilities outstrip our ability to keep up. Some of it is probably just the tendency of old people to reject novelty, but the rapid changes in my lifetime give me more than a bit of worry. Rather than the optimism of the aforementioned prognosticators, I instead think of the WALL-E and Idiocracy predictions in which our collective hubris becomes our undoing.
I don't fear the robot uprisings of the Terminator or Matrix franchises. Instead, given how quickly tech has changed (consider
Will Smith's newfound abilities to eat pasta
), I see multiple ways it could backfire on us. Given the potential ways our advancements could lead to harm,
I predict that many will default to an authoritarian effort to take the reins akin to 1984 or
Equilibrium
. State actors today conduct information warfare, and players like Russia or China actively influence politics or infrastructure in other nations (well, the
US is also guilty of this
). AI is going to accelerate things in ways that are predictable, but the rate at which it will do so will blindside us collectively.
We're little children, playing with the fires of Prometheus. I fully expect AI to burn us, and I don't want it anywhere near my code."
146468849.suck-less-next-time,Suck Less Next Time,Guest Post by Boromir4thewin,2024-07-29 08:40:46.093-05,"Hello!
This is a guest post by Boromir4thewin ‚Ä¶ This is someone with one of the hardest jobs in the world. They think they have something to say to all of us Milk Toast programmers.
Hate to break it to you, but you will suck at something. Don‚Äôt get all hurt about it and start making excuses, as if every moment in your life is supposed to be some ‚Äú
star-aligned perfect endeavor.
‚Äù
Being productive or having success in any endeavor typically takes a lot of time and effort and no matter how much you try, you will occasionally suck at stuff you do.
People who are innately good at things, without practice or previous experience are rare and you would find they suck at stuff too if you got to know them better.
It‚Äôs nice to think whatever we endeavor to accomplish will work out if we try hard enough and the reason (
insert task or function
) was so easy for us is because we‚Äôre awesome.
Or maybe a project went well and there were not a lot of issues because I‚Äôm just incredibly gifted, but more than likely you‚Äôve just practiced and have experience. You might do all the following perfectly ‚Ä¶
practice and experience
your diligent attention to detail
getting more training
making lists
writing stuff down
using whatever new whatever is out there to keep track of something
‚Ä¶ you‚Äôre gonna suck it up sometimes.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What next?
Then you will want to get sad or mad, or both, and you‚Äôre gonna be tempted to let that suckiness define your entire being and your professional ability.
We‚Äôll‚Ä¶ don‚Äôt to that. This article is here to help you on the day that is for sure coming when you demonstrate the human tendency to suck at something. When it happens don‚Äôt lose your competitive edge, don‚Äôt get all ‚Äú
I‚Äôm terrible at everything
‚Äù and don‚Äôt start gaslighting yourself, don‚Äôt cower in your basement and eat all the chips.
Just try to suck less next time. It‚Äôs that simple.
The History of People sucking at things
Highly skilled people give good effort, and average to low-skilled people trying hard and sucking at something is what the human race has dealt with since it was created.
Any human involvement in a task creates a margin of error where a whole project could fail and resources get wasted. I don‚Äôt feel obligated to spend a lot on this point because all of you have taken high school history or just been through high school or some formal education, and probably observed someone talking about rampant humans, sucking in the moment.
Where do you think ‚Äú
measure twice cut once
‚Äù came from? (
I bet the guy who came up with that still read his tape wrong after double-measuring
‚Ä¶)
You WILL suck at all kinds of things and in different areas of life.
Sucking happens in every area of life, at tasks within a professional situation, and in relationships.
For example ‚Ä¶ forgetting to pick up your wife from her 10-hour shift in the ER ‚Ä¶ when it‚Äôs raining ‚Ä¶ and she‚Äôs miles from your house.. and she‚Äôs 33 weeks pregnant .. then she decides to walk home, crying the whole way (
not that I would know anything about that
)
There‚Äôs no real way to recover from that situation when you pull the car up to her a few blocks from your house after you remember and she‚Äôs soaked to the bone, her mascara is running down her cheeks‚Ä¶ (
you know that could really happen to somebody
).
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Fixing the sucking.
What to do if you just sucked at something.
Since the odds are you will mess something up i.e. ‚Äúsuck‚Äù it‚Äôs important to put the following three recommendations into regular mental use:
Number One:
don‚Äôt overthink.
Don‚Äôt extend a failure past the actual jurisdiction it resides in. Do try to figure out what happened because if you don‚Äôt know why something messed up, that‚Äôs probably why it messed up.
It could just take a little more attention to detail or maybe you need to learn a skill and practice. Once again, if it makes you do the practice you need to gain the skills you need to do better, which is still a positive.
Number Two:
don‚Äôt be paralyzed, preventing further effort.
This is the natural byproduct of overthinking (of part number one) but don‚Äôt become the overthinking paralysis by analysis person who dwells so long on a failure that he misses good opportunities and can‚Äôt find anything good to build off of.
If you find out things didn‚Äôt go well because you actually need training or need to brush up on another skill make sure you find people who are good at those things and absorb all their knowledge
Number Three:
think ‚Äútry to suck less next time‚Äù.
This should be a positive way to think. I mean it‚Äôs not the end of the world.
Responding to someone who just sucked at something
I‚Äôm not opposed to existing in a culture where if I fail someone I trust is tasked with trying to get the best out of me ‚Ä¶ looking me in the eye and saying‚Ä¶ ‚Äú
try to suck less next time.
‚Äù
Having gone through this experience, I found it quite profound. When I wasn't constantly obsessing over my perceived failures and instead discussed what I thought I was bad at, I realized that most people had forgotten about it.
But, then when I brought it up they were like yeah you really suck at that and we had a great life laugh.
Now if you work at a place where verbally telling people ‚Äú
to suck less next time
‚Äù is considered inappropriate and helping someone work through points one and two listed above could be really helpful.
But, that will really start with you being a person who understands human sucking at stuff happens so that you‚Äôre not surprised by even skilled people making a mistake.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
146408238.5-data-engineering-mistakes,5 Data Engineering Mistakes,and what do about them,2024-07-22 07:51:56.827-05,"Some lessons are learned hard, some not so much, but I generally think we humans are predictable and mossy old stones stuck in our own little divots of comfort and ease, not easily budged.
I dare say when the older us looks back on the younger us we can be envious of all that spitfire and vitriol that propelled us forward to where we are today. We can also shake our heads in wonder that we actually managed to land where we are today.
The lessons seem obvious now, not such back then.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Today we will delve into 5 of the most common Data Engineering mistakes I‚Äôve run into over these long and languid years. It seems to be a never-ending humdrum of lackluster Data Platforms, one after the other, like some indomitable Model-T assembly line from the turn of the century.
I end up wringing my hands in wonder at the empty sky, asking why is it so difficult to find that diamond in the mud and swap we call Lake Houses, once called Data Lakes, once called Data Warehouses. Where are those who follow the way of old?
Before I wax any more poetic, I should turn myself towards the matter at hand.
The most common Data Engineering mistakes ‚Ä¶ and what to do about them.
Instead of tempting you with bits and pieces, luring you to the end of the article, instead, open your mind wide, and I will give you your portion in full.
5 Common Data Engineering Mistakes
Not embracing simple architecture and design.
Not having a good local development environment.
Not having a good orchestration and dependency management tool.
Not testing code and pipelines before release.
Not doing something hard.
Let‚Äôs review these little evils one by one and see how they are the bane and true enemy of every Data Engineer looking to build reliable Data Platforms.
Here we go.
Not embracing simple architecture and design.
There is nothing more classic than a Data Platform that apparently has some FOMO. At least that‚Äôs what you would think when you survey the excessively large number of tools in use.
This can apply at both the macro and micro scale
inside a Data Platform. Do you really need both Postgres, Snowflake, Redshift, and Databricks? No, you do not my friend. This simply indicates a fractured architecture with no direction or driving force.
Do you have more than one testing tool?
Do you have multiple data stores?
It‚Äôs unclear where to run compute or store data.
Simplicity is underrated when it comes to Data Engineering. Having a simple architecture made up of a few good tools will reduce errors, and bugs, and increase the speed of development and features.
One thing Data Engineers often forget is that every time you add yet ‚Äúanother‚Äù tool to the stack, you are adding another ‚Ä¶
breakpoint
integration point for other tools
tech debt
more complexity
more decisions
The more you work in and on Data Platforms, the more you will realize that complicated is not good and is often unneeded except in exceptional cases.
I‚Äôve worked at startups that have a good 50/50 split between Postgres and MySQL! I mean really! Just pick one already!
Are you a Data Engineer who wants to grow in your team, build the next new best thing, or do something awesome?? Then do the opposite of what everyone else is doing ‚Ä¶ adding new things and new features and this and that.
Simplify. Combine. Reduce.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Not having a good local Development environment.
The next most common mistake has probably been experienced by everyone lucky enough to be reading this article. You show up to a new job, excited and nervous, ready to do some small work, you‚Äôre a good Engineer.
But, surprise surprise, when you clone the repo(s), that may or may not contain all the code running in production ‚Ä¶
you crack your fingers, get ready to write some code, and run the tests ‚Ä¶ just kidding ‚Ä¶ you CAN‚ÄôT!
Come to find out they ‚Ä¶
they want to write tests but haven‚Äôt got there yet.
no one has had anytime time to set up Docker and containerize the tooling and code
everyone just installs stuff on their machine and does whatever
some people log onto a ‚ÄúDev box‚Äù instance and test things there
I can think of a few things more depressing and that are more of a harbinger of bad times to come.
This is a common downfall of many Data Teams, the unwillingness or inability to spend the time that is needed ‚Ä¶ upfront ‚Ä¶ to ensure Development Environments are set in a way that
makes it EASY to write, develop, and debug new or old code.
You should ask yourself these questions.
Can anyone clone our repo and run our tests (if they exist)
Does someone have to spend a week installing a plethora of tools on their laptop to even get to the point of writing code?
Do we have a Docker image or some containerization of the tooling and stack?
Do we have a standard local development process for our Engineers?
If you don‚Äôt have a reasonable way for Engineers to write, develop, and test codes on their local machine ‚Ä¶ why don‚Äôt we all take a wild guess where most of the code gets tested and run?
Production
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Not having good orchestration and dependency management tool
These days there is not much excuse for this problem, considering all the good options out there on the market. Some people might not think a good Orchestration and Dependency management tool for all their data pipelines matters, but I assure you it does.
For example, do you have a single place that anyone on the Data Platform can navigate to, visually, and see ALL of the running Data Pipelines along with the status of those processes??
This is key to onboarding team members, getting other teams and people up to speed, being able to find things, troubleshooting things, and generally being productive and having VISIBILITY into the entire Data Platform and how things are running.
The time is past for CRON jobs, sorry.
Again, simplicity matters. You want discoverability and visibility into the Data Platform operations that are running with a few clicks of a button, this should not be a difficult or confusing task ‚Ä¶ scattered around or requiring CLI commands.
Not testing code and pipelines before releasing.
We sort of talked about testing, locally that is, a little bit ago, but this topic deserves more commentary, and it really comes down to ‚Äú
Testing in Production
,‚Äù which happens all too often in Data Engineering communities.
There are some many factors that play into why this happens, some very technical, some more cultural.
Lack of ability to test locally
No Development Environment that mirrors the Production
Hustle culture of ‚Äúgetting it done‚Äù
Lots of pipeline breakage and fires (self-fulfilling prophecy)
Based on my experience the whole ‚Äú
we don‚Äôt test, but if we do it‚Äôs in production,
‚Äù comes down to culture, and it‚Äôs all driven by the leaders of the Data Platform.
If said leaders only care about delivery, and live in a constant state of breakage, with zero ability to call for a slowdown and focus on root cause and the improvement of the development lifecycle, then a culture of getting it done will reign supreme.
It‚Äôs hard work.
Creating end-to-end integration testing is hard
, unit testing sorta but not really. It takes some serious willpower to go from zero to a fully capable Development environment that mirrors Production and where new code and changes can be staged and tested.
Sure, unit tests can catch SOME bugs, but they will not catch all. Data Platform needs to have the ability to run end-to-end data pipelines just like it will in Production.
Again, this is another area where Engineers can have a big impact.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Not doing something hard.
This last piece of advice is for all Data Engineers, old and young alike, new or seasoned professionals. I saved this little bit till last, but for individual contributors,
this is probably the single most important thing you can do.
And no mistake, although it‚Äôs good for you as an individual, it feeds back into the bigger picture and team you work on.
You have to do hard things ‚Ä¶ on a semi-regular basis.
Learn a new piece of technology.
Write code in a new language.
Work on a non-coding skill (leadership type).
Read a new-to-you book at least once every few months.
Build something interesting (or contribute to open-source).
It‚Äôs hard to go to the gym. Hard to eat healthy. Hard to get enough sleep. But, we all know the benefits of doing these things, they pay back in spades.
The same goes for you as a Data Engineer. To grow and become better, you need to do hard things. The real truth is you are either coasting (
which is fine for rest or other reasons
) but for your own good and long-term happiness ‚Ä¶ you need to do hard things."
145997286.terraform-sucks-long-live-terraform,Terraform Sucks. Long Live Terraform.,blah blah blah,2025-01-13 07:14:43.609-06,"I‚Äôm not sure what precisely about
Terraform
makes me want to cry one minute and laugh another. It has to be something, or maybe it‚Äôs just me, but I'm not sure.
Terraform
made IAC (infrastructure as code) famous or infamous, depending on your view.
Just a few little lines of Terraform are all you need to spend buttloads of money, destroy architecture and bring a Data Platform to its knees. Or just a few lines of
Terraform
, and you look like a genius with everyone falling at your feet in worship.
What‚Äôs it going to be?
Today, I want to explain Terraform, how to use it, and the benefits and drawbacks of drinking the IAC Kool-Aid and going down a road from which you will never return.
What is Terraform?
How do you use it?
Benefits
Drawbacks
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Introduction to Terraform
Terraform is an
open-source Infrastructure as Code (IaC) tool
developed by
HashiCorp
. It allows poor souls in their mothers basement to define, provision, and manage infrastructure resources in a declarative configuration language (
HCL - HashiCorp Configuration Language
).
Terraform is a response to a few common problems.
adhoc creation of resources (
reproducibility
)
simplifying the management of large architectures and infrastructure (
at scale it can bring sanity to madness
)
The marketing drivel and acolytes might try to convince you of more, but at the end of the day, those two bullet points encompass the main problems solved by Terraform.
Key Features:
Declarative Syntax
: You describe the desired state of your infrastructure, and Terraform determines how to achieve that state.
Multi-Cloud Support
: Works with major cloud providers like AWS, Azure, GCP, as well as on-premises solutions and third-party services.
Infrastructure Management
: Allows you to create, update, and destroy resources like VMs, networks, databases, and more.
State Management
: Maintains a state file to track the current state of your infrastructure, enabling efficient updates and change tracking.
Modules
: Reusable and shareable configuration blocks for managing similar setups across different environments.
It‚Äôs funny, you would think that with all this glowing and wonderful praise of Terraform and what it provides that everyone would be of one mind and of one accord, with every single Data Team you run across built with Terraform. But, this simply isn‚Äôt the case.
Why?
Thanks for reading Data Engineering Central! This post is public so feel free to share it.
Share
You typically only find Terraform‚Äôd infrastructure at either very large companies, or super technical one‚Äôs. The truth is, just like anything else that‚Äôs hard, it takes some dedication upfront and over time for an entire team to learn and become comfortable with Terraform.
Some people/companies simply don‚Äôt have the bandwidth, time, or the benefits simply aren‚Äôt high enough priority to warrant the work.
Common complaints
you will hear ‚Ä¶
Steep learning curve
complex syntax
modules and reusability
Complex state management
Difficult debugging
monolithic nature
Dependency management between stacks
My personal opinion here is that like any other technology, adoption and effective use really comes down to spending to LEARN the thing.
Non of use can argue that the ability to have say, Apache Airflow, S3 buckets, or a Databricks Workspace, Users, Groups, and Permissions captured in Terraform for easy management and deployment is better than NOT having it codified somewhere.
What happens, and it alway does, when someone bombs a S3 bucket away, or Airflow decided to go kaput? I mean your options are to rebuild tools AND configurations from scratch ‚Ä¶ or with a Terraform command to have it all back in an instant.
What could you do BESIDES Terraform?
There are a few options depending on the cloud environment you are working in, to achieve the same sort of codified architecture and reproducibility of infrastructure.
Let‚Äôs say you are working on AWS and Databricks, would could be your options if you don‚Äôt want someone simply clicking UI buttons to create everything?
CLI‚Äôs
APIs
For example,
Databricks provides a CLI and API
to do every single thing you can imagine on their platform.
You can easily
create Databricks Jobs
etc via API calls.
The point being ‚Ä¶ if you don‚Äôt want to learn Terraform and work on a small team ‚Ä¶ it‚Äôs easy enough to write some Python and API calls to give you a similar experience AND outcome of using something like Terraform.
Sure, it‚Äôs not exactly the same thing, but for many teams it solves the problem of being able to recreate infrastructure in an automated manner with a little planning.
Share
Back to Terraform
Dang, you‚Äôre probably thinking I‚Äôm trying to talk you out of Terraform ‚Ä¶ not at all, I promise. I think this is more about a Data Team evaluating their ‚Ä¶
risk tolerance
desire to harden infrastructure
what they have time for
priorities
Some every small teams with simply platforms like Airflow, S3, and Databricks simply might not be worried about it. One a three person team maybe they can spin everything back up manually in a worse case in half a day.
Would I tell that Data Team they should consider at least some sort of simple IAC solution? Probably. Would it be a hill to die on? Probably not.
‚Äú
I find that IAC is much like TDD, the zealots will claim do or die, the teaming masses will continue about their business as usual.
‚Äù - Me
Is Terraform that bad, what does it look like?
Let‚Äôs take a moment to look at some simple Terraform code, some things that probably any Data Platform might have. We don‚Äôt have time to go through all the complicated bits, but we can at least scratch the surface for you, give you an idea of what to expect.
Note
:
We are going to gloss over a lot of details here, this isn‚Äôt a how-to on Terraform.
What about creating a S3 Bucket?
What about doing something more complicated like
Databricks Workspace setup
with Terraform?
Well that‚Äôs going to look more complicated, and that‚Äôs just the Workspace. That‚Äôs my point. The more complicated the tools and architecture, the more complicated you will find the Terraform setup to manage.
There simply isn‚Äôt anyway around it. You can only hide the pea of complexity under so many cups, it‚Äôs going to show up somewhere.
The reality is that if you have a fairly complicated (at least not simplistic) Data Platform than you are going to have to invest some serious time into learning, teaching, using, or migrating to Terraform.
But, it‚Äôs just like any other technology or programming language, you just have to use it to become comfortable with it. The question again comes back too ‚Ä¶
Do you have the time, or are you willing to setup aside the time to learn Terraform?
If you DONT use Terraform can you use APIs or CLIs to accomplish similar things?
Are you willing to accept the risk of NOT having ANY IAC (
Infrastructure as Code
)?
this is different for every team
I say Long Live Terraform ‚Ä¶ Terraform also sucks. There is a reason many large teams have dedicated DevOps or Platform Engineers who can spend their days worrying and learning things like Terraform.
Many Data Teams simply struggle to complete the projects and tasks assigned to them on a normal Sprint Cycle. Adding something like Terraform into the mix requires time, lot‚Äôs of time. People must learn it, start to use it, migrate existing or new infrastructure to it, there isn‚Äôt a free lunch."
145948780.kubernetes-sucks-long-live-k8s,Kubernetes Sucks. Long Live K8s.,Data Engineering with Kubernetes?,2024-07-18 08:24:42.077-05,"Kubernetes. K8s. Man, I remember those days when it was hot hot hot, and you were an idiot if you weren‚Äôt deploying something onto Kubernetes. If you didn‚Äôt know how to use
kubectl
then you were a prehistoric human, almost as bad as those SSIS users.
Well, I‚Äôve had my fun in the sun. I used Kuberentes for a few years before leaving that cult for greener pastures.
You should also check out
Statsig
, the sponsor of the newsletter this week.
Statsig
automates experiment analysis either on top of your warehouse data or in their cloud, and they‚Äôre announcing a new feature every day this week.
Companies like Notion, OpenAI, Brex, and Anthropic use
Statsig
to power their experimentation and feature management.
Statsig
is celebrating experimentation week by shipping a new feature every day ‚Äì today is Interaction Effect Detection, which provides peace of mind that two experiments aren‚Äôt interacting in an unexpected way. Click
here
to see the release post, and follow along with all of the launches this week.
It seems to me, as far as I can glean from putting ye‚Äô old ear to the ground, that I hear and see far less ‚Äú
Kubernetes for Data
‚Äù marketing drivel being poured down the throats of unsuspecting victims.
I think this is generally a good thing, although Kubernetes for custom data platforms is probably the only real option.
Let me explain.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Kubernetes for Data Platforms.
Let us wind the back the clock of time a few years ‚Ä¶ say circa 2019.
This is just a few years after the release of Kubernetes in late 2014 early 2015.
It‚Äôs basically at the height of the hype train.
I mean, to be fair, anyone who has worked with pools of EC2 instances and half-baked tools like AWS Fargate will sacrifice their firstborn to the Kubernetes gods for good reason.
I myself, in my young glory days when I was full of bitterness and vitriol, have used GKE (
Google‚Äôs hosted K8s
) to build a large Data Platform capable of processing hundreds of TBs of spatial data with many thousands of PODs running in parallel. Sometimes K8s are the only real option (
at least back in the day
).
What does maddened Reddit rabble have to say about K8s in a Data context
(
not that we should trust them for anything, since they are a fickle and bloodthirsty crowd
)?
More or less, Kubernetes can play a few key roles.
Used to host third-party tools that are K8s-ready.
Ex, many tools provide HELM charts to deploy their tool.
Custom-built applications that need scale.
To host service-oriented data platforms.
This can, and will, usually end up on one end or the other of a spectrum of suckage depending mostly on the company and environment (
and the dreaded Platform Engineering team
).
What makes Kubernetes so attractive as a Data Platform, and for deploying ‚Äúthings‚Äù in the first place?
Because it‚Äôs so broad, it doesn‚Äôt care what is being deployed ‚Ä¶ as long as you can containerize ‚Ä¶ aka Dockerize your ‚Äúthing‚Äù ‚Ä¶ there is a good change you can run it on Kubernetes with low effort.
So what then you say? If you can simply package up your Data Platform tools into Docker and throw that bag over the wall onto a Kubernetes cluster‚Ä¶ why doesn‚Äôt everyone do it???
When does Kubernetes suck?
While all the Platform teams sharpen their butter knives to put me out of my misery,
I will call their bluff and tell you how some Platform Engineering teams drank the kool-aid and foot-gunned themselves into ding-dong land, all the while, destroying the productivity of hundreds of Software Engineers.
I will try to be succinct.
Travel with me (
a real place I‚Äôve been
) to a 1,000-person growing Startup. Said startup is in full growth mode and thought they needed
Google-scale
with a few hundred users, and drank the kool-aid.
Kubernetes to the rescue.
The gate-keeping Platform Engineering team only allowed things to be deployed on Kubernetes, that‚Äôs it, no exceptions. K8s' baby, ride or die.
No documentation, all tribal knowledge on how to get something deployed in their ‚Äúspecial way.‚Äù
Any new ‚Äúthing‚Äù that needed to run was 80% yaml files, 20% code.
You had better be a Service savant and not get to greedy with your resources, only 1/2 CPU for you bad data engineer!
Even if you needed a Postgres database ‚Ä¶ no-no RDS ‚Ä¶ must deploy your own POD.
Large organization, slow to respond to tickets or requests for help, you were at their mercy.
Eventually, they came to see the light, costs, and the sun beaming down from on high and their finding tools like AWS Lambdas actually existed (
I joke not
), eventually loosened things up.
It was almost as if there was no other world but k8‚Äôs, its dirty little fingers found their way into every single decision and design. If something needs to be done, it HAD to be in the context of K8s.
This is not the way. Tools we use should enhance our design and software, not put it in a box.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
The world has moved on.
Technology changes, not everything needs to be run on a single tool. Should you run Apache Spark on Kubernetes? What‚Äôs the matter with you?! It‚Äôs called Databricks.
Need Airflow?
It‚Äôs called AWS MWAA or GCP Composer or Astronomer for crying out loud.
You don‚Äôt need to deploy your own Airflow setup on Kubernetes
(
yes I‚Äôve actually done this
).
Need a database? It‚Äôs called RDS.
It‚Äôs so amazing that some people get caught up on a single piece of tooling that they literally don‚Äôt think technology has moved on and the market has provided better options.
This has nothing to do with ‚Äú
Kubernetes is bad
,‚Äù this is a simple fact that the universe we live in tends to chaos and atrophy. New things get designed, they inevitable do things better than the old things. That‚Äôs just life.
It isn‚Äôt trivial to run your own Kubernetes cluster, it isn‚Äôt trivial to deploy complex services onto Kubernetes. Managed services have won the day, get used to it.
The problem with saying ‚Äú
Just deploy it on K8s
,‚Äù is that it‚Äôs a naive way to approach solving problems. THERE IS NO FREE LUNCH, and K8s comes with its own set of problems.
Who‚Äôs managing the Kubernetes cluster(s)
It‚Äôs ‚Äúharder‚Äù for devs to work on something deployed on K8s than a simple managed service.
YAML and DevOps in a K8s-centric org become half the job of every single Software Engineer.
I wax poetic and long.
Long Live Kubernetes.
But, here we are at the end. Long Live Kubernetes! Long Live K8s! What a wonderful tool it really is.
It‚Äôs hard to put into worlds the impact K8s has had on the Software and Data Engineering world. The fact that so many tools provide HELM charts and you can deploy a complicated data orchestration tool onto Kubernetes with a few CLI commands is amazing and speaks volumes to the power of K8s.
When you run into custom Data Platforms that need to be built like I did
oh those many years ago
‚Ä¶ if it wasn‚Äôt for Kubernetes ‚Ä¶ I probably wouldn‚Äôt be here today. I would still be blithering over my keyboard in some dark and musty basement of a mental institution.
I doubt Kubernetes will be going anywhere FAST for the next decade. It will be here, back in the corner quietly and methodically hosting all those PODs like Atlas holding up the world.
But I dare say there will be a slow decline. It‚Äôs the way. The amount of SaaS products available today and what they can do has taken a chunk out of the ole‚Äô lunch of K8s.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
145926910.replace-databricks-spark-jobs-using,Replace Databricks Spark Jobs (using Delta) with Polars,Saving Money and Doing Things,2024-07-12 09:41:23.785-05,"I‚Äôm usually not an advocate of being a dreamer, talking big talk but having it all be a pipe dream. I‚Äôm a dream killer most of the time. Maybe I‚Äôve been around too long ‚Ä¶ probably.
Every tool in its place and every place needs a tool. It‚Äôs not that I‚Äôm against any certain tool,
although you may reasonably
think I hate DuckDB
(untrue)
, I‚Äôm typically just against folk overstating a tool and what it can do.
But, I am a fan of
tools that can actually do a thing ‚Ä¶ like for real
‚Ä¶ in Production. It‚Äôs fine to play around with something on your laptop, talk about how it can do this or that. We should all have a special place in our hearts for a tool like Polars that is capable of actually running Production workloads.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Polars and low-hanging fruit.
Today we are going talk about eating on the juicy low-hanging fruit that‚Äôs right there in front of us. Just go ahead, reach for it, take a big ole‚Äô bite.
Instead of saying something ridiculous like ‚Ä¶ ‚Äú
I‚Äôm going to replace ALL my Databricks Spark jobs with Polars or DuckDB
..‚Äù which is just silly unless you work somewhere you should be using Pandas all along ‚Ä¶ we do something obvious. We pick the low-hanging fruit.
When new tools come out‚Ä¶ we all have a few questions floating around, I mean there is a difference between hype and reality.
Is it reliable enough for Production?
Is anyone else using it for Production?
What is it
really
capable of?
How do I ease my way in without breaking everything?
Can I save money or reduce complexity?
I tend to try and avoid the hype most of the time, I enjoy trying new stuff out, but when it comes to Production level Data Engineering ‚Ä¶
I take a slower approach and prioritize
‚Ä¶
stability and reliability
a large community of users
simple and approachable tech
So can we actually introduce Polars into a Databricks + Airflow platform ‚Ä¶ in real life ‚Ä¶ and why would we do such a thing?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Replacing Spark Jobs with Polars (using Delta Lake).
Ok, so let‚Äôs talk Delta Lake and Databricks (PySpark) for a minute.
There‚Äôs no doubt that Databricks + Delta Lake is the new 500lb gorilla in the room, everyone is either using this stack or trying to.
One thing about going all in on any Data Stack is that you always have ‚Äúthings‚Äù on the peripheral, the edges, that you force into your tool simply for simplicity's sake. It makes sense.
Making the tradeoff to have ALL your processing done in homogeneous manner provides a lot of benefits.
Ease of debugging and troubleshooting
Ease of development
Ease of onboarding
Less error-prone.
For example, it‚Äôs not uncommon, like in my case, to have tables with hundreds of TBs of data that fit like a glove into the Delta Lake + PySpark pipeline ideology. But, it‚Äôs also common to have a few pipelines and data needs that don't fit into the needs of requiring PySpark ‚Ä¶ or Delta Lake for that matter ‚Ä¶ but we do because ‚Ä¶
Historically PySpark on Databricks is the path of least resistance ‚Ä¶ aka the only real option.
But that has changed with tools like Polars.
Really, the change was simple. We had some ‚Äú
small-ish
‚Äù Delta Lake tables inside
Unity Catalog
. A few parts of our different dataflows required pulling data from those Delta Tables and then doing some action on those individual records, like deploying other downstream Pyspark Pipelines, etc.
Running this sort of logic with PySpark via Databricks was actually antithetical to the workflow, but it was really the only option historically.
This is where Polars, and since we were already using Airflow as an orchestrator and dependency tool for our entire Data Platform, it made perfect sense to save some money and move some of these PySpark pipelines to Polars running on our Airflow workers.
Ease of setup.
Unlike Spark, setting up and using Polars is a Data Engineer‚Äôs dream come true.
Simply a `pip install` and you‚Äôre on the way to glory.
Also, if you‚Äôre already using PySpark and SparkSQL on Databricks ‚Ä¶ it‚Äôs a simple transition to Polars ‚Ä¶ which provides a SQLContext option as well if you‚Äôre one of those buggers who can‚Äôt work with Dataframe APIs.
How simple? Here is how we read out Delta Lake tables in Unity Catalog with Polars on an Airflow worker.
I mean ‚Ä¶ that‚Äôs it, my friend. What? You wanted some black magic, some super cool and complicated code that could replace PySpark Databricks Jobs?
Sorry to disappoint you.
Literally with that single method you now have a Dataframe of results at hand from which you could execute SQL or any other transformation you‚Äôre doing today with PySpark or SparkSQL.
It has some nice benefits.
It‚Äôs cheaper to run Polars on pre-existing compute like Airflow.
It‚Äôs faster than running PySpark in 99% of the cases.
It requires no major infrastructure changes.
It‚Äôs a flexible option that is so similar to PySpark that you aren‚Äôt adding tech debt or complexity.
I‚Äôm assuming most Data Teams have something, somewhere, that is able to run straight Python code that isn‚Äôt inside Databricks. At least I would hope so.
With Polars, you gain flexibility, and the ability to do data processing on Delta Lake (
yes even Unity Catalog Delta Lake
) tables outside of Spark/Databricks. Believe it or not, this will save you money ‚Ä¶ obviously, and actually introduce flexibility into your Data Stack, expanding how you design your Data Pipelines.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The Rest Of The Story
For those of you skeptical you can use Polars to replace some Databricks + Delta Lake pipelines that easily, I assure you it is that simple. I‚Äôm sorry, not sorry, there isn‚Äôt more to it.
Of course, there is a little weirdness here and there. You have to know or be able to retrieve the physical location of Unity Catalog Delta Tables.
For example, here is an actual location of a UC table in s3 ‚Ä¶
In my case, I‚Äôm actually iterating on those Delta Lake ‚Ä¶
now Polars dataframe records
‚Ä¶ to kick off numerous downstream Databricks Jobs via Airflow DAGs.
More or less like follows ‚Ä¶
I mean the sky is really the limit. I would suspect some serious money could be saved in certain use cases that don‚Äôt require massive amounts of data (RAM) ‚Ä¶ in my experience, only a handful of the folks using something like Databricks actually need it for ‚ÄúBig Data.""
But don‚Äôt get me wrong, Databricks is THEE platform to beat all Platforms and I would pick it hands down every time, no matter the data size. The user, development experience, and features they offer are simply too good to pass up.
If you are not using Polars in production, I ask you, why not? It‚Äôs a production-ready tool capable of replacing Spark jobs on Databricks, as I know from first-hand experience ‚Ä¶ what else are you waiting for??
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
145573286.duckdb-100-lets-kick-the-tires,DuckDB 1.0.0 - Let's Kick The Tires,poke poke poke,2024-06-24 07:54:37.39-05,"Sometimes you have to take your own medicine, eat your own words, and swallow that bitter pill. I keep wondering if I‚Äôm going to have to do that with my DuckDB takes. I mean if you watch talking heads at Databricks Data and AI Summit ‚Ä¶ it looks like DuckDB is really going mainstream.
Probably not that big of a surprise with an SQL tool. Data Engineers are like drug addicts when it comes to SQL, they don‚Äôt even ask questions, they just use.
Either way, I‚Äôm going to kick at the ole‚Äô tires of DuckDB once again. See what‚Äôs what. There is a feature or two that I‚Äôm very interested in.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What I‚Äôve written about DuckDB in the past.
Lest you think I had behind the pillars of the interwebs flinging stones at unsuspecting software (ok, I do sometimes do that), let me list all the different DuckDB articles I‚Äôve written low these many years.
DUCKDB VS POLARS FOR DATA ENGINEERING.
DUCKDB + DELTA LAKE (THE NEW LAKE HOUSE?)
DuckDB vs Polars - Thunderdome.
DuckDB for Data Engineering
What's all the hype with DuckDB?
Why DuckDB is losing to Polars
DUCKDB HAS MAJOR PROBLEMS! OOM ERRORS.
DUCKDB OUT OF MEMORY ‚Äì HAS IT BEEN FIXED?
By George, I do seem like ye old Gandalf Storm Crow don‚Äôt I?? Cry me a river, I will mark it down in my book.
What I‚Äôm Watching with DuckDB Right Now
So, I‚Äôm not an expert in DuckDB since I mostly spend my time trying to break it every time I use it ‚Ä¶ I‚Äôm going to give you my take on
the announcement of DuckDB 1.0.0
‚Ä¶ and what I‚Äôm paying attention to.
DuckDB has its own custom-built data storage format
(
that is now stable
)
DuckDB now has first-class Delta Lake support. (
link
)
Have they fixed the OOM errors??
Mother Duck
(
serverless compute in the cloud
)
I‚Äôm probably most interested in talking about the DuckDB storage format, testing Delta Lake reads, and writes to remote tables in s3, and checking out OOM memory problems again.
It appears DuckDB has a free 30-day trial for Mother Duck which we will have to poke at again in a separate article. I‚Äôm interested in seeing if I can get a rough calculation of cost savings using Mother Duck vs Databricks for some pipelines.
Diving In.
Let‚Äôs start by doing the easy one. We‚Äôve tested DuckDB‚Äôs ability to handle larger-than-memory datasets out on the cloud, but it always blew up no matter what we tried.
Here‚Äôs another go at it. I‚Äôve got a Linode with 4GB of RAM.
Next that data.
9.1GB of data for our 4GB machine, should be enough to see if our new DuckDB 1.0.0 has solved some of the prior OOM errors.
(duck) root@localhost:~# pip3 install duckdb
Collecting duckdb
  Downloading duckdb-1.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)
Downloading duckdb-1.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18.5/18.5 MB 105.7 MB/s eta 0:00:00
Installing collected packages: duckdb
Successfully installed duckdb-1.0.0
The Python we will be running is this ‚Ä¶
Welp. I would say I‚Äôm totally surprised. But then I guess this wouldn‚Äôt be the first time some marketing fluff disappointed me.
From what I can tell I‚Äôm still not the only one with these problems, as recently
as 2 days ago on Hacker News there were plenty of other people complaining about OOM errors
.
The simple fact is that I‚Äôm not going to fight anymore with DuckDB and its OOM issues, I will simply use something like Polars that can do larger-than-memory execution without a hiccup. Why fight it?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Testing DuckDB‚Äôs Delta Lake Support.
Now I need something to cheer me up after the OOM failures, and another better to do that with than a little remote Delta Lake table reading and writing with DuckDB.
Just to get the full picture I think I will do the following ‚Ä¶
read raw CSV‚Äôs in s3 with DuckDB
take the above results and write them back to a partitioned Delta Lake table on s3
do an analytical query against that s3 Delta Lake table.
This seems very straight forward and a common task any tool working with Delta Lake should be able to do without much problem. We will probably time each step as well, to get a baseline, and then have Polars do the same thing and compare results.
First, I wanted to ensure DuckDB could read my creds and talk to the raw data. Easy enough, it can connect to the raw CSV files on s3 and print them out no problem.
Next, let‚Äôs write a script to take that data and push it to an s3 Delta Table.
Again, I would say I‚Äôm surprised this failed, but at this point, I‚Äôm not. At least for me, nothing is as simple as it seems with DuckDB. This error is coming from the
deltalake
package, ‚Ä¶.
you would think
if DuckDB is lauding the native integration of duckdb and deltalake on their own blog
they would make sure the integration is tight and not janky.
Since we have nothing better to do, and apparently Delta Lake and DuckDB don‚Äôt work THAT WELL together, let‚Äôs give Delta Lake the creds to s3 we already gave to DuckDB.
Well by George we got a little bit closer.
Apparently whatever DuckDB thingy I‚Äôm passing isn‚Äôt working. I know little about DuckDB but this seems easy enough to overcome.
Turns out in the examples they did a .df() when passing to Delta Lake ‚Ä¶ so ‚Ä¶
write_deltalake(f""s3://confessions-of-a-data-guy/deltaduck/mytable"", df.df(),  partition_by=[""date""], storage_options=storage_options, mode=""overwrite"")
Try ‚Äòer again.
This time it ran for about 15 minutes ‚Ä¶ AND THEN ‚Ä¶ died.
For context, we talking about 8,466,958 records in the dataset. 8 million, that‚Äôs it.
Ok, I promised myself I would not be such a Gandalf Storm Crow on this one, I said I would be nice and kind. But arggggg!
I get it that DuckDB is nice for some people who could get away with using Pandas for their datasets. Guess what? A lot of us don‚Äôt have that option.
Seriously? 8 million records and it chokes?!?!
(running on my Macbook with 16GB of memory)
How is this supposed to be a scalable and reliable toolset for Data Engineers to build products on top of? As for me and my house, it simply isn‚Äôt going to happen.
Writing a few GBs of data to an s3 location just blows up? There are simply too many other tool options out there like Spark, Polars, Datafusion, Daft ‚Ä¶ that DON‚ÄôT have these types of problems out of the box.
I know at this point I had promised to look into the new DuckDB internal file storage format. I was planning to do some benchmarking vs say Parquet files for some datasets, but I‚Äôm just too depressed at this point to keep going.
I draw the line at the inability to work with small datasets on s3. If it can‚Äôt do that well I‚Äôve got no time for it, and neither should you.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
145548295.aws-rds-disk-space-alerts,AWS RDS Disk Space Alerts,Building a Python Open Source Package,2024-08-05 08:16:33.766-05,"Unlike the other saints on the internet, I‚Äôve never had much time for open-source software, even though I‚Äôm an avid user of it. I just consume and forget to give back. But, that‚Äôs changed in the last two years or so. Begrudgingly.
I‚Äôve just started to dabble here and there, making some minor contributions to random things, and publishing a PYPI package here and there. Mostly not serious stuff, just piddling.
I think it‚Äôs good generally to do things on a regular basis that you aren‚Äôt familiar with or used to doing. Just like going to the gym, do the uncomfortable, don‚Äôt avoid it.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
RDS Disk Space and Alerting on Such Things.
Today we are going to make a PYPI Python package, a small one, a little dude. Nothing fancy, and it's not going to break the internet with this one.
The other day at work I had to make a little Python mumbo-jumbo to alert me when an RDS instance is running or getting low on disk space. Much to my chagrin pretty much the only ‚Äúeasy‚Äù option was to set up a CloudWatch alert etc.
I was sort of mad at all the open-source gurus who‚Äôve been apparently doing nothing all these years. Seriously? With the ubiquitousness of AWS RDS instances and Python, no one has made me a few-liner Python package where I can just simply find out how much disk space is left.
Inconceivable.
In a fit of rage, I promised myself to fix this problem for the next person 50 years from now who has this problem. Maybe they will hunt me down in the old folks home and put me out of my misery.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Here goes nothing.
So the task is as simple as possible.
Python code that can return the amount of Free Disk Space.
I have a theory that you should leave the details to the raving and crazed mobs of developers, try not to do too much for em, because they probably won‚Äôt like what you‚Äôve done anyways.
Time is short, like your attention span, so let‚Äôs dive in.
Step 1. Use Poetry to set up a new project.
First things first, we need a good descriptive name for this project and a command to run.
poetry new sparklepop
Well, we know that
sparklepop
will need at least boto3 as a dependency on the backend.
poetry add boto3
um‚Ä¶ ok ‚Ä¶ now have a project to work with, but what next?
Step 2. Figure out the interface.
This is probably what I struggle with the most, giving some interface to the end-user that actually makes sense and isn‚Äôt pure nonsense. But, on the other hand, since 5 people might use this in the future I‚Äôm not sure if I care or not.
I do want to leave the interaction open in the future for additions and not be so narrow that I foot-gun myself. I also added a Class,
but even I must swallow my bitter pills on occasion
.
That‚Äôs a start I suppose.
Next, we can do the core API call to find out the
FreeStorageSpace
on the RDS.
Next, we can convert the answer to GB‚Äôs and add some useful logic to check on the free space left vs the minimum.
The last two methods are the ones that we will put in the docs and README for uses to use.
just return the free storage space left in GBs (
get_free_disk_space
)
check on the free disk space and Raise an error if it doesn‚Äôt meet a threshold (
check_on_free_disk_space
)
Ok, so that‚Äôs pretty much it for coming out of the gate with this package. Let‚Äôs write up the README (
I asked ChatGPT to do it for me
) and then deploy it to PYPI.
Easy as ‚Ä¶
poetry build
poetry publish
>> Publishing sparklepop (0.1.0) to PyPI
 - Uploading sparklepop-0.1.0-py3-none-any.whl 100%
 - Uploading sparklepop-0.1.0.tar.gz 100%
Bing, bang, boom ‚Ä¶
it‚Äôs on PYPI now
.
Let‚Äôs test it in the real world, and see what‚Äôs happening ‚Ä¶ if the little bugger works.
Looks like the installation works.
(env) danielbeach@Daniels-MacBook-Pro sparklepop % pip install sparklepop
Collecting sparklepop
  Using cached sparklepop-0.1.2-py3-none-any.whl (2.6 kB)
Requirement already satisfied: boto3<2.0.0,>=1.34.124 in ./env/lib/python3.11/site-packages (from sparklepop) (1.34.124)
Requirement already satisfied: botocore<1.35.0,>=1.34.124 in ./env/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.124->sparklepop) (1.34.124)
Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./env/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.124->sparklepop) (1.0.1)
Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in ./env/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.124->sparklepop) (0.10.1)
Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./env/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.124->boto3<2.0.0,>=1.34.124->sparklepop) (2.9.0.post0)
Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./env/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.124->boto3<2.0.0,>=1.34.124->sparklepop) (2.2.1)
Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.124->boto3<2.0.0,>=1.34.124->sparklepop) (1.16.0)
Installing collected packages: sparklepop
Successfully installed sparklepop-0.1.2
Ok, not bad. Does it work?
>>> from sparklepop import SparklePop
>>> rds_instance = ""my-rds-instance""
>>> region = ""us-east-1""
>>> sparklepop = SparklePop(rds_instance, region)
>>> free_space = sparklepop.get_free_disk_space()
>>> print(f""Free disk space: {free_space} GB"")
Free disk space: 339.3316192626953 GB
>>>
Way to go SparklePop! Must be something to magic unicorn dust. Long live SparklePop.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
145545821.snowflake-is-dying-on-the-vine,Snowflake is Dying on the Vine?,"Like Sparks Fly Upwards, Snowflake Is Born To Trouble",2024-07-08 08:03:50.722-05,"An old man once said that man is born to trouble as sparks fly upwards. I think maybe he meant to say Snowflake. I know well that probably many of my readers have drunk the forbidden cup of fandom-koolaid meant to blind all those who taste that drink.
But, fan or no, it‚Äôs good to be critical of the software we use every day, besides, it can be pretty entertaining.
It‚Äôs no secret that I‚Äôm a big fan of Databricks, but even so, I have no compunction to pull my punches, I see what I see and will call out the silly things when I see them.
I understand the draw of Snowflake and we all have to acquiesce to the reasons that Snowflake became as big and popular as it is today. They did something right to get there. Bringing SQL at scale, seamlessly to the masses.
But, it doesn‚Äôt take much figuring to do the calculations and figure out that Snowflake has been getting a bumpy ride of late. Get out the popcorn, let‚Äôs dig in.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Digging into Snowflake.
I‚Äôm not sure if it‚Äôs just a string of bad luck or what, but Snowflake has been put through the wringer the last few weeks. I mean can you imagine working in Marketing or Sales at Snowflake right now?
Yikes.
Snowflake is embroiled in a large and much-publicized data breach.
Databricks steals Tabular out from under Snowflake
(
swallowing iceberg
) and slaps them in the face with open-source Unity Catalog.
I mean dang it. The stock price of Snowflake over the last few months pretty much tells the picture of what has been going on for them recently.
Now the real question is this ‚Ä¶
Is this simply some bad timing, a little blip on the overall trajectory of Snowflake‚Äôs long-term outlook that they will weather, or is it indicative of larger issues?
This is something to think about, I‚Äôm sure companies have survived much worse. But, it does make one sit up and pause a moment and ask yet another very important question that does have ramifications.
Is Snowflake still a good place to park your data and invest time, money, resources, and future development ‚Ä¶ or should you jump ship?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Data Platforms and Choosing Your Poison.
The reality is that every day a new Data Platform comes online, a team decides to migrate tools, and a company looks to expand. What does that mean?
That means that Data Folk go on the hunt for new tools and starts the comparison game of ‚Äú
what tool should we adopt at the core of our data processing
.‚Äù
I mean think about what someone is going to find if they start researching now. They are going to find all the bad stuff, true or not, they will see all the negative news and Snowflake will get a black mark out of the box before things have even started.
It‚Äôs clear Snowflake is being beaten badly in the AI and ML space.
They got whipped in the Data Catalog and open Data Lake format war.
Anyone who walks in the Data Land day in and day out also probably would have noticed that Snowflake has been greatly lacking in the sort of ‚Äú
look we are innovating and releasing new things space.
‚Äù
All you have to do is pursue the
Snowflake blog
and look for big announcements, not much going on.
I mean there were a few token AI stuff that are basically glorified Chat Bots. But nothing earth-shaking.
It‚Äôs easy to get caught up in the bad news, but what do the numbers of the ground look like?
What do you notice about these summarized reports on Snowflake? I noticed the customer numbers for revenue greater than 1 million and the number of Forbes Global customers.
They are increasing.
If it was really all doom and gloom things would be shrinking not increasing.
But is Snowflake really going anywhere?
I think the demise of Snowflake is greatly exaggerated. If you go around and look on
r/dataengineering
you can see that there are lots of Data Engineers who use Snowflake and
LIKE
Snowflake.
When Engineers like a tool, it‚Äôs probably going to stay around.
People forget the mass Legacy Systems still running.
This is what most people forget about, those who have been using Databricks and Snowflake for years ‚Ä¶ that there is a whole set, a VERY LARGE SET of companies and Data Teams who still are running old systems and are looking to upgrade and migrate.
SQL Server
Oracle
Redshift
MySQL and Postgres
There are many companies still using the above technologies to run Data Warehouses. Every one of those places is having FOMO and WILL at some point adopt the new technologies ‚Ä¶ and one of those options is Snowflake.
When you have been fighting and tuning SQL Server Data Warehouses for years, moving to Snowflake will be like a blast of fresh air, and Engineers and Data Platform teams will LOVE IT.
Snowflake vs Databricks vs ‚Ä¶?
That still leaves a valid question for Data Platform teams ‚Ä¶ should I pick Snowflake or something else? Sure, I would argue that Databricks has spanked Snowflake in AI and ML space ‚Ä¶ heck, even in the Open Catalog and Lake House format space.
BUT, that doesn‚Äôt mean there are a ton of SQL-heavy teams who will always be more comfortable moving from SQL Server to Snowflake rather than Databricks.
Snowflake is SQL, SQL always wins the hearts of Data Engineers. Snowflake might be wilting on the vine a little, but I doubt it will die anytime soon.
The biggest fear is that Snowflake will be the next IBM if they don‚Äôt start to get cracking.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
145496706.cicd-for-data-engineers,CI/CD for Data Engineers,Automate Automate Automate,2024-06-21 13:34:32.704-05,"The older and more crusty I get, the more my fingers hurt from pounding those keys in data frustration, one thing has become clear as I look through glassy eyes at the last few decades of writing code and squeezing data till it screams.
Automation is the name of the game, an unsung hero ignored by the teaming masses of new developers who spew onto the software landscape year after year.
Part of me thinks that it‚Äôs just not something that comes naturally to someone in the beginning. When we start our coding and data journies, we skip along the trail of life happy and flamboyant, full of future dreams and expectations.
We pretty much only think about ourselves and the wonderful (in our eyes) code our flexible and supple little fingers pound out.
Any freeloaders out there want 50% off for a year to a paid Subscription. Click the link below.
https://dataengineeringcentral.substack.com/justbecause
Get 50% off for 1 year
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
What is CI/CD in a Data Engineering Context?
In case you live under a rock or just slid off the software engineering assembly line ‚Ä¶ what does ChatGPT say CI/CD is?
‚ÄúCI/CD stands for Continuous Integration and Continuous Deployment (or Continuous Delivery), and it's a set of practices and tools in software engineering that aim to deliver code changes more frequently and reliably.‚Äù
You know, me thinks this is a high and lofty goal. Honestly, most companies fall into one of two categories, in my experience.
Already the masters of CI/CD and need no help.
Little to nothing in place.
Sometimes you find people wandering around in the middle of these two, but not that often.
I would like to propose a few basic ideas for Data Engineers looking to learn about and integrate CI/CD into their companies and workflows.
In essence, can we boil the stewing cauldron of bubbling ideas down to a few actionable and straightforward points that everyone can embrace?
While I‚Äôm sure there are a number of Platform and DevOps Engineers who will spit and moan while reading my simple steps for CI/CD for Data Teams, pooh on them.
While there are many ways to make this complicated, the fact is that most Data Teams struggle to even do the simple things.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Diving into CI/CD for Data Engineering - Testing
Let‚Äôs start from the beginning, the very beginning, where God said ‚Äú
Let there be Testing ‚Ä¶ and there was testing.
‚Äù I think when it comes to CI/CD and needing to start from somewhere ‚Ä¶ that somewhere is testing.
Why?
Because those who don‚Äôt test at all are going to have a certain culture, codebase, and architecture that probably doesn‚Äôt lend itself to CI/CD.
If you are unwilling to have tests ‚Ä¶ but think you will embrace CI/CD? Don‚Äôt kid yourself.
I can‚Äôt tell you how many people I‚Äôve talked to and data teams I‚Äôve come across that when pushed and prodded, and they can‚Äôt answer the following questions is a ‚Äú
yes
.‚Äù
‚ÄúIf cloned your data repo today, could I with one or two commands, run all the tests locally? Also, if I made a change to said repo and pushed a commit, would tests run somewhere?‚Äù
Just take a gander around
r/dataengineernig
and you will discover testing is a question often asked ‚Ä¶ meaning lots of people don‚Äôt do it today.
I‚Äôm not really going to spend any time trying to convince you that testing your data pipelines and transformations is something you should do. It‚Äôs kinda obvious and unbelievers are anathema to me.
Also, I can‚Äôt go into depth to teach you everything about testing, so I will link some articles below if you need more info.
3 TIPS FOR UNIT TESTING PYSPARK PIPELINES
INTRODUCTION TO UNIT TESTING WITH PYSPARK.
END-TO-END PIPELINE INTEGRATION TESTING FOR DATABRICKS + DELTA LAKE.
Unit Testing for Data Engineers
Unit Testing for Data Engineers #2
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Why automated testing?
What it really boils down to though is automated testing. What do I mean?
Anyone can run tests locally on their machine after touching and developing a codebase.
Anyone pushing changes to a repository should trigger automated tests to run.
This is critical to protecting codebases and can act as a first line of defense since we are all human, we all make mistakes from time to time.
And lest you think this is hard or some black magic, here is a piece of a production YAML file for CircleCI to run automated tests on a commit.
And yet again, here are some GitHub actions running on
tinytimmy
, an open-source package of mine.
It can seem simple, but that is kinda the point, it should be seamless and just part of the everyday process. Testing is important for well-rounded data teams, and automated testing is a huge piece of that pie.
Diving into CI/CD for Data Engineering - Automated Deployments
This is another small but important piece of CI/CD for Data Teams and any team developing software. The automated deployment of whatever exists in your repository is a baseline for Data Platforms that aren‚Äôt held together with shoe-string and glue, and ‚Äú
oh crap they are on vacation ‚Ä¶ I don‚Äôt know how to do that.
‚Äù
This is really what most CI/CD comes down to in the end.
How do take humans out of the boring process that can break?
Every single Data Platform usually has different tools and architecture, they all run their code slightly differently, and it‚Äôs untypical for two companies to run the exact same stack.
If you have certain pieces of code and infrastructure that need to be deployed to certain spots in a certain way ‚Ä¶ you can‚Äôt just rely on new engineers to do this, or even experienced ones to remember everything all the time.
So, some simple scripting can deploy your code the way it needs to every single time. You know what to expect.
Instead of Engineers spending time piddling with manually pushing code or infrastructure changes here and there via random scripts or the UI, automated deployments ensure they can focus on what they need to‚Ä¶
producing quality software
.
And, honestly, it would only take a few days to set up the automated deployment process in the first place, and save serious time and trouble downstream.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Diving into CI/CD for Data Engineering - Clean Code
One thing that most Data Teams can fall victim to as well is the atrophy of code. Everything usually starts with good intentions, but life and timelines take over and things start to fall off the wagon.
‚ÄúCode atrophy in software engineering refers to the gradual degradation or obsolescence of code over time.‚Äù - ChatGPT
It can be hard to keep on top of code quality and expectations with just the PR and code review process. Probably depends on what kind of day someone is having, are they going to pay attention or even care from one day to another?
Some folk might find it easy to pooh-pooh this sort of clean code and expectations of quality, but in large codebases with multiple people and teams committing changes, things can get messy and out of hand easily.
It‚Äôs hard to understate the real cost of an unclean codebase when it comes to feature addition and debugging.
I mean the first and simplest step would be to run a `ruff` check as part of the automated checks when someone commits and code and fail the commit if it doesn‚Äôt come back clean.
Of course, there are other formatting tools like `black` for Python etc.
Wrapping it up.
As you and I journey through the never-ending labyrinth of data engineering, it's clear that automation, especially CI/CD practices, is indispensable. The path from manual to automated processes may seem daunting, but the benefits far outweigh the initial effort.
It‚Äôs what separates good Data Teams from the Milk Toast cadre of developers.
Embracing testing and automated deployments not only enhances code reliability but also frees up precious time for innovation. Remember, the foundation of a robust CI/CD pipeline starts with testing.
Ensuring your code is clean and deployable without human intervention creates a resilient and efficient workflow. So, take the plunge into CI/CD, and watch your data engineering processes transform.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
145496694.data-validation-of-data-engineers,Data Validation for Data Engineers,Beyond Data Quality,2024-07-02 09:19:57.062-05,"As the years pass by in a dizzying blur of data tools, companies, and new ideas, I‚Äôm still taken aback by the lack of Data Quality in the Data World. It just has not become mainstream yet.
Sure, a precious few DQ tools are bouncing around like
Soda
or
Great Expectations
, those being still fairly new and unadopted by the madden masses of Data Engineers on the whole. It does make a guy wonder, how long can you kick against the goad?
If folks wanted some Data Quality they would put it high on the priority list, allocate resources, and get it done.
But they don‚Äôt.
At least not until they have nothing else to do.
Classic
.
We are not going to dive into the entire DQ implementation discussion, but just a subset of that theory, namely Data Validation as an option for Data Engineers looking to dip their parches and weary feet in the cool and cold stream of Data Quality.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Recently, Prefect announced the open-source technical preview of
Prefect 3.0
.
Data teams face a relentless challenge. Modern businesses rely on a complex web of workflows and automations, making it critical to build resilient systems that are easy to trust.
Prefect 3.0 is our answer to that challenge, providing a framework for building workflows that are resilient by design
.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Data Validation
So if you cannot go full bore with something like
Databricks DLT Validations
,
Great Expectations
, or
Soda
‚Ä¶ there are still plenty of ways to implement some sort of Data Validation with a reasonable amount of effort.
We can call this a poor man‚Äôs Data Quality.
If you‚Äôre using the fancy tools we mentioned before, the sky is the limit for Data Validation and Quality, you can even get alerts when data points drift out of the norm.
But, if you are like me and have been around Data for a long time, we all know that 99% of the Data problems that arise could be solved with very simple Data Validation checks. No rocket science needed.
Schema Validation
WAP Pattern
Uniqueness and Deduplication
NULLS
Schema Validation
With the rise of the Lake House and tools like
Delta Lake
and
Iceberg
, schema validation has become the simplest form of Data Validation available to every single Data Engineer on pretty much every platform.
For some reason, schemas get ignored, along with most Data Modeling, yet they can provide the most upfront and reliable baseline to control data possible.
If you think about Data Engineering in general, much of what we do is play with
Data Types
. They are important and have an impact on every aspect of data pipelines and business.
Add and enforcing schemas against your data can solve many common problems.
Protect against malformed incoming data.
You can reason more firmly about data and needed transformations.
Ensure buggy data writes cannot happen.
One thing is for sure, Data Teams who do not enforce schema validation checks via something like Delta Lake ‚Ä¶ will end up with dirty and malformed data without a doubt.
You can control many things with schemas beyond the simple ‚Äú
is this is a string or a number.
‚Äù
NULL is acceptable or not
Precision of decimals or floats
Correct date or timestamp formats
Constraints like VALUES IN ‚Ä¶
A well-thought-out and written Schema can avoid many common data quality issues ‚Ä¶ and the best part is that it happens immediately upon data ingest.
Uniqueness and Deduplication
The next most common problem that exists in 80% of Data Platforms is uniqueness and duplication problems. These are probably the most widespread and insidious issues that can arise and cause serious problems.
When it comes to business logic and analytics you can bet your bottom dollar that most of the issues that arise and drive people crazy are because of duplicates and data that is not unique.
Every single data pipeline should include data deduplication checks on the front and the backend of the processing. Many times before important analytical (groupBY etc.) functions are called, deduplication should be done.
Most duplication problems occur because of bad JOINS.
You can solve them with Window functions many times, also most tooling comes with built-in duplicate checks.
Uniqueness is a tricky topic and can be separated from duplicates because it has more business context wrapped up in it.
Typically a Data Engineer working on any dataset should first spend time understanding what makes a record unique and then dedup on that definition.
If you spend time to dedup and understand the uniqueness of a dataset, most of your problems will disappear.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
NULLs
Why is it all the most common data problems seem to be simplest to deal with and the most often passed over? NULLs. Some people like them, some people hate them.
Even Reddit talks about NULLs.
I feel like NULLs, after decades of Data Warehousing, still leave a divided field behind them, people just can‚Äôt agree on how to handle them.
Do you allow NULLs to occur in the first place?
Do you think about NULLs when aggregating data?
If you are processing data for some unique business logic, it‚Äôs imperative to ensure that you think through the fact that NULLs can or do exist, and what effect they will have on your transformation(s).
Conditional Logic
: Apply conditional logic in data transformation processes to handle NULLs (e.g., using SQL COALESCE function or equivalent in other languages).
Aggregation
: Handle NULLs appropriately during aggregation to ensure accurate results (e.g., using COUNT instead of COUNT(*) in SQL).
Understanding and running tests on how NULLs affect certain common transformations is probably worthy of its own blog post.
WAP Pattern.
This last one, WAP (Write-Audit-Publish) is the amalgamation of all the Data Validations you can think of pilled into one ‚Äúthing.‚Äù Surprisingly there seem to be very few Data Teams that adopt WAP as a way to do Data Validation and Quality checks.
What is WAP?
Write
: Ingest data from various sources and prepare it for its final state.
Audit
: Validate and ensure the integrity and quality of the data through deduplication, anomaly detection, business logic checks, etc.
Publish
: Approve and move the data to production environments like Data Lakes or Data Warehouses.
The benefits of a good WAP pattern ensure high data quality and integrity, as well as reducing data bugs and problems in production. It can also centralize data quality checks and validations into a single spot.
Challenges with WAP can be seen with high costs associated with writing, storing, and reading large amounts of data. Increased compute and storage expenses, particularly in cloud environments.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Thoughts
In the end you don‚Äôt always have to have some fancy Data Quality tool. Most of the time if poor data quality exists, it‚Äôs because many of the above basic steps were not put in place from the beginning, causing data quality issues to arise.
Starting your Data Engineering projects with a Data Validation mindset from the start, say the schema, for example, using WAP patterns, dealing with NULLs, and stopping duplications from happening ‚Ä¶ those things will reduce 99% of DQ issues."
145482590.real-life-example-of-the-quicksort,Real Life Example of the QuickSort Algo (Rust),For Data Engineering ,2024-06-11 07:44:20.528-05,"I, probably much like you, have been tired of hearing the DSA Prophets preaching their good news that Data Structures and Algorithms are here to save us all and enshrine us in everlasting brilliance and glory.
I always think ‚Ä¶ ‚ÄúMe you say? This old dude who‚Äôs been writing SQL and Data Pipelines with eyes crusted over from all the tears of boredom and monotony that wash over me every day? Me? DSA is going to bring me to the Promised Land?‚Äù
Beg your pardon while I roll my eyes. Well, that‚Äôs typically what I think about that. But, even I succumb to the DSA devils every once in a while. I‚Äôve done a series on DSA before my dear readers, that can be found here.
Introduction to Linked Lists.
Introduction to Binary Search.
Introduction to QuickSort.
Introduction to Graphs and Breadth-First Search
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
This series called ‚ÄúDSA for the Rest of Us‚Äù was wildly unpopular and I can‚Äôt imagine why. Yawn.
One thing I‚Äôve learned about all the internet hobbits like you over the years is that y‚Äôall like the Milk Toast stuff, nobody wants the complicated stuff.
But, I‚Äôm a glutton for punishment and I don‚Äôt take no for an answer if I don‚Äôt feel like it. I‚Äôm going to pour some more DSA down your throat whether you like it or not.
Real Life Example of the QuickSort Algorithm.
I was hacking around this weekend on a project called
Puddleglum
. It‚Äôs an idea I‚Äôve had for a while that is ‚Ä¶
Puddleglum is a
Rust
-based
Python
package that allows for complex Data Quality checks on AWS
s3
buckets.
It‚Äôs still under development, so don‚Äôt hold your breath. But basically I‚Äôve gotten tired of handrolling my own S3 bucket gyrations all the time ‚Ä¶
When was the last time I got a file
How big is it
How many files did I get this week
How many files did I get last month
blah, blah, blah
This basically involves something like boto3, gathering a bunch of files into a list and than doing this that and the other thing to answer some questions.
Puddleglum is all writing in Rust right now, with plans to use pyo3 to port it to a Python package eventually.
What does this have to do with QuickSort?
Well, I was hacking away at the project and I realized something, I thought to myself ‚Ä¶ ‚Äú
By Gandalf‚Äôs beard! This is a perfect use case for a QuickSort application!
‚Äù
I was coding the answer to the following questions about a s3 bucket and prefix.
Get the most recent file.
Get N most recent file(s)
Get N day(s) ago file(s)
Get N week(s) ago file(s)
Count files and group by day for n days
etc.
These are just some of the questions I want to be able to answer, among others. What is the obvious thing about answering all of these questions?
If I had a sorted list of files from an s3 bucket and prefix, it would make answering many of these questions trivial, quick, or both.
What better way to sort a list than with the QuickSort algo? In my case I‚Äôm using Rust and we would want to sort the list by DateTime from newest to oldest.
To set the stage we first need to be able to get everything out of a s3 bucket matching a prefix. Something like that.
Now with that out of the way we can do something like ‚Ä¶
Once we have done get_em, we have a Vector of unordered results from s3. In our case remember we want to answer a number of different questions based on the
last_modified
property of the objects.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
So, it would make sense if we are going to ask an arbitrary number of questions based on the DateTime of said objects, we should do the work to Sort them upfront, to make the downstream questions simple and fast.
Enter our QuickSort algo.
If you need an in-depth and slow explanation of QuickSort, go back above to my links I shared and you can read through QuickSort in Rust.
So what‚Äôs the big deal?
Ok, so we now have our Vector (array) of objects sorted by DateTime, does it really make answering questions easier?
Give me the most recent file?
Bing, bang, boom. If our date_sorted Vector is None, sort it, if it is, we pop the first item. Very easy.
What if I want the N most recent files?
No problemo, I got you. Little for loop through N and we got what we need.
What about making quicker to solve ‚Ä¶ DateTime problems that are a little harder?
Give me files from N days ago?
Again, we know this is going to run quickly why? Because our object set we are iterating is sorted, so we know that all the records we need are going to be right next to each other, not scattered around through the entire list.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Never say never.
I guess you shouldn‚Äôt say ‚Äú
I will never do that, I will never need that.
‚Äù Maybe there is a little DSA sprinkeled all around our life waiting for us to notice them. Crying in the corner like a lonely middleschooler that no one wants to dance with. Poor little bugger.
So, let‚Äôs raise our glasses to the road less traveled, to the smart people in their mom‚Äôs basements who love the DSA and spend all their life hacking in anonymity.
Heck, even a Data Engineer gets a chance to do something smart every once in awhile, better be careful or it might cause a habit to appear.
I guess the idea is that there are surprising things lurking around in the corners of our code if we stop and give em‚Äô a chance. Yeah, most of use don‚Äôt need to know anything about Binary Trees, most of that is for the birds, but a little salt, pepper, and DSA on your taters probably wouldn‚Äôt kill ya.
Leave a comment"
145267613.lazy-is-ambitious,Lazy is Ambitious,synchronous vs asynchronous,2024-07-01 07:42:03.805-05,"Laziness is rarely regarded as a positive trait, yet when employed in code it has the potential to make our applications more industrious than ever.
I am, of course, not talking about the kind of laziness that parents often accuse their children of. Instead, we're here to explore laziness in the context of software, along with some related concepts; and how these ideas can translate into greater performance.
The naive way - synchronous code
If your parents were like mine, you were expected to do what you were told when you were told to do it. You couldn't put the work off, nor could you be doing it with divided attention (
e.g. talking on the phone while you're working
).
Were my folks really getting the most out of me by being so strict?
As a programmer, you likely cut your teeth on coding with a principle known as
eager evaluation
, likely without even being aware at the time that there was any other way. In a nutshell, your code is evaluated in the order encountered, analogous to how when reading a book (as an English speaker) you typically evaluate your words from left-to-right, then top-to-bottom.
With code, it's a bit more complex when accounting for the order of operations with operators, parentheses, or scope; but otherwise, your code occurs in a predictable sequence.
What am I talking about? Well, to frame this discussion, I'll attack a common scenario: fetching data, via web requests:
Our server code, using Node.js, will give us a timed delay and trivial text response depending on the trailing URL path component:
const http = require('http');

const PATH_PATTERN = /.*\/(.*?)$/;

console.log('Starting server on 8001');
http.createServer((req, resp) => {
    const lastPathPart = req.url.match(PATH_PATTERN)[1];

    let delay = 0;

    switch (lastPathPart) {
        case 'socks':
            delay = 50;
            break;
        case 'shoes':
            delay = 75;
            break;
        case 'hats':
            delay = 25;
            break;
    }

    setTimeout(() => {
        resp.end(`Here are your ${lastPathPart}`);
    }, delay);
}).listen(8001);
As can be seen, depending on whether we ask for socks, shoes, or hats, we'll end up with
50ms
,
75ms
, and
25ms
response delays respectively.
To see what this is like in practice, let's make some requests in Python:
import time
from urllib.request import urlopen

started_at = time.time()

socks = urlopen(""http://localhost:8001/socks"").read()
shoes = urlopen(""http://localhost:8001/shoes"").read()
hats = urlopen(""http://localhost:8001/hats"").read()

elapsed = 1000.0 * (time.time() - started_at)

print(""Requests took %dms"" % elapsed)
In the above code, we obtain our
socks, shoes
, and
hats
in that sequence, moving from one to the next statement as results come in and not a moment sooner. This procedural way of evaluating statements means that the behavior is intuitive and fairly predictable.
For instance, suppose it typically takes about 50ms to pull down socks, 75ms for shoes, and 25ms for hats - notwithstanding real-world conditions or processing overhead, that script would take roughly 150ms to execute.
In reality, when testing this out locally, the time was consistently closer to 167ms, close enough to my predictions that it shouldn't make much difference.
This sequential, or ""synchronous"" approach to those web requests, means that we potentially need to wait long stretches of time for external systems to do their thing - time that could be spent doing stuff on our end.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Asynchronous execution
If all you needed was a quick-and-dirty script to test some requests, our above effort is probably good enough. However, in the real world, we probably end up making some non-trivial requests, and quite likely more than just one time. We want to explore how to get that execution time down as much as possible.
Assuming our environment supports it, we could conceivably fire off all three requests simultaneously, and instead of stacking all of the responses' times together we would instead only have to wait for the longest-running (75ms) query to have our results.
Some things are out of your control - maybe your API is throttled or limits concurrent connections, or bandwidth constraints prevent you from hitting your service as hard as you'd like. For now, assume these real-world considerations aren't in play, but it helps to be aware of them if you're looking to hit a service hard.
I'll be leaning into
grequests
to perform the asynchronous requests, as it does the heavy lifting for me; rather than executing the requests in sequence, this library will handle all of the requests simultaneously, in theory only taking as long as the worst of the three to produce results.
import time
import grequests

urls = [
    'http://localhost:8001/socks',
    'http://localhost:8001/shoes',
    'http://localhost:8001/hats'
]

started_at = time.time()
requests = (grequests.get(url) for url in urls)
responses = grequests.map(requests)
elapsed = 1000.0 * (time.time() - started_at)

print(""Requests took %dms"" % elapsed)
This time, I'm expecting the whole process to take much less time (in theory, without the earlier mentioned overhead, approx. 75ms).
In my own testing, I saw 86-87ms.
In the weeds
Under the hood, in order to accomplish asynchrony*, grequests uses
gevent
; that library is an abstraction around
""greenlets""
, or coroutines, to spool up multiple concurrent processes and to collate their results. It's not exactly multi-threading, though in practice you could think of it in those terms.
*: In case that word hurts your head, I had to do some digging - ""synchronicity"" is apparently reserved for psychological phenomena, and is apparently
quite distinct from the term ""synchrony""
.
So, I keep acting in my examples above as though the only thing holding us back is our third-party service when it comes to performance. In reality, there are some real-world considerations that go into the move from a synchronous to an asynchronous paradigm - namely,
asynchronous isn't a free lunch
.
Coroutines are a good fit for Python and Javascript because they are both constrained by a single application thread. What a coroutine does is divide (time-slice) processes up so that small bits of work can be done while letting others get a chance at the CPU.
By way of analogy, consider a factory with a few machines but one employee running them; while one of the machines is doing something, the operator can switch to another machine and do some work there, and can cycle through all of the machines eventually.
At some point, you might have more work than the single employee can perform, but if all you can hire is one employee, you know you're getting the most out of them.
Native threads, on the other hand, can be working simultaneously and independently (sort of; the operating system does need to do some plate-spinning work of its own). If your processes are compute-intensive and don't have lulls where they're waiting on something, they'd be better suited to a thread than a coroutine. However, each thread is a potentially expensive venture, requiring the operating system to allocate process resources and administer each process.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Throwing threads at the wrong problems can lead to spending more effort preparing or managing a workload than it is actually worth. Going back to the factory employee illustration, a thread is basically a new employee; but HR has to do a bunch more work to accommodate them, and you'll need a manager who can properly coordinate the multiple employees' workloads and ensure each is doing their job correctly.
Finally, going back to the original naive approach, sometimes it's simply for the best to just do the work synchronously. Not every workload benefits from being broken up and done in chunks. Multitasking and multithreading should not be something you reach for automatically without at least having a workload that benefits from doing so (making this a problem of premature optimization), and you might even consider doing some benchmarking to determine which approach is the best when you eventually do head down this path.
Lazy evaluation (or, doing the least amount of work I can)
Up to this point, I've been exploring the problem of dealing with long-running concurrent processes. Those aren't the only problems that can benefit from deferred work, though. Consider the following script, and see if you can find fault with it:
for i in range(100):
    if i == 3:
        print(""Found my three!"")
Spoiler alert
: there's no way past this block of code without touching every number between 0 and 99 in that
for..in
loop. This can be mitigated by adding the venerable
break
statement immediately after our
print
call, since there's no reason to keep looping once we've done that.
Okay, how about a different illustration?
people = [""Alice"", ""Bob"", ""Carol"", ""David"", ""Edward""]

for person in people:
    if len(person) > 5:
        print(f""Wow, {person} has a long name!"")

print(f""We have {len(people)} people on our hands."")

person_with_longest_name = """"
for person in people:
    if len(person_with_longest_name) < len(person):
        person_with_longest_name = person

print(""The person with the longest name is "" + person_with_longest_name)
A new problem arises: we traverse
people
multiple times, and in this particular case there isn't any good reason why we can't do it with just a single pass and a bit of restructuring.
The thing that unites both of these contrived problems is that they're doing unnecessary work.
In the last example, we could refactor our code so that all the workaround
people
is consolidated in one place:
people = [""Alice"", ""Bob"", ""Carol"", ""David"", ""Edward""]

person_with_longest_name = """"
for person in people:
    if len(person) > 5:
        print(f""Wow, {person} has a long name!"")
    if len(person_with_longest_name) < len(person):
        person_with_longest_name = person

print(f""We have {len(people)} people on our hands."")
print(""The person with the longest name is "" + person_with_longest_name)
However, so far we've only talked about reducing how much work the computer does. We haven't quite cracked the code at this point...
Iterators
Under the hood, your
for
loop is actually doing something special:
iteration
. Specifically, we could rewrite the last bit of code ever so slightly to expose some explicit iteration:
people = iter([""Alice"", ""Bob"", ""Carol"", ""David"", ""Edward""])

people_count = 0
person_with_longest_name = """"

while True:
    try:
        person = next(people)
        people_count += 1
        if len(person) > 5:
            print(f""Wow, {person} has a long name!"")
        if len(person_with_longest_name) < len(person):
            person_with_longest_name = person
    except StopIteration:
        break

print(f""We have {people_count} people on our hands."")
print(""The person with the longest name is "" + person_with_longest_name)
Well, that's not exactly any more readable than our original example, but seeing it this way helps to explain what really happens under the hood when you do your standard
for..in
loop over a list. Namely, we have the following components:
Loop
Next item in iterator
Exit condition for end of iterator
Well, what do we get if we remove the loop but keep everything else?
people = iter([""Alice"", ""Bob"", ""Carol"", ""David"", ""Edward""])

people_count = 0
person_with_longest_name = """"

try:
    person = next(people)
    people_count += 1
    if len(person) > 5:
        print(f""Wow, {person} has a long name!"")
    if len(person_with_longest_name) < len(person):
        person_with_longest_name = person
except StopIteration:
    pass

print(f""We have {people_count} people on our hands."")
print(""The person with the longest name is "" + person_with_longest_name)
What do you think our output is here?
We have 1 people on our hands.
The person with the longest name is Alice
Basically, we've done a single pass of our original loop. Importantly, we never encounter Bob, Carol, David, or Edward in our list, because our iterator doesn't ever progress that far. The only way we move forward in that iterator is by calling the
next()
function, and if we don't do that then we run out of work to do.
This might not seem like it has immediate value, but there is a point to this: until we actually call
next()
,
we've done no work in that iterator
. We can do that at our convenience, and in this fashion, we have a ""lazy"" iterable.
In Python, multiple types (
list, string, dictionary
) support iteration in this way. Anything that you can ""loop over"" or ""get the next thing from"" can be described as iterable, but so far everything we've looked at is bounded with well-defined start and end points. What if the thing we were iterating over was less well-defined? Consider the following ""clock"":
import time

class WeirdClock:
def __iter__(self):
return self

    def __next__(self):
        time.sleep(1)
        sec = int(time.time() % 60)

        if sec % 60 == 0:
            raise StopIteration
        else:
            return sec

myclock = WeirdClock()
secs = iter(myclock)

for sec in secs:
print(sec)
The starting point depends on when you start execution, but it consistently prints the current time's seconds component up through 59. Basically, iteration stops not because we ran out of numbers or items in a list, but because we explicitly chose to stop with that
StopIteration
exception. If we remove the exit condition and never raise that exception, our clock would continue to emit values for as long as the consumer continues to ask for them.
Another thing to consider: if we wait several seconds between attempts to iterate over that clock, we're not likely going to get consecutive numbers.
I came here for Rust, not Python
Well, aren't you just a snooty little thing? Okay; you've got it, but trying to explain async/await in Rust (which is where that discussion inevitably takes us) is its own can of worms. For now, I'm going to trust that you either already know how it works or can follow along.
HTTP requests with reqwest
So, that
grequests
example earlier has a pretty clear parallel in Rust - taking us to the
reqwest
crate.
I personally like this one because you can type the name with only one hand (assuming US keyboard layout). Try it!
Reqwest's readme leads with an example showing off a trivial example:
use std::collections::HashMap;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let resp = reqwest::get(""https://httpbin.org/ip"")
        .await?
        .json::<HashMap<String, String>>()
        .await?;
    println!(""{resp:#?}"");
    Ok(())
}
The astute among you might notice more than one
.await
in there - what's the deal? Well, there are two potentially long-running tasks to account for here:
The request itself
Converting the text response to a hashmap via JSON-parsing
This illustration doesn't really offer a good reason to use the async/await syntax, though; and it's actually worth mentioning that they also offer a
""blocking API""
for those times where you're not leaning into an async/await runtime. I won't spend time on that, except to say that if you're already in an async/await environment you'll want to avoid it (they even warn you in the docs to steer clear or jump through hoops).
A better example, though, to stay consistent with our earlier illustrations, would be something more like:
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let started_at = std::time::Instant::now();

    // Fire up three simultaneous requests:
    let socks_req = reqwest::get(""http://localhost:8001/socks"");
    let shoes_req = reqwest::get(""http://localhost:8001/shoes"");
    let hats_req = reqwest::get(""http://localhost:8001/hats"");

    // Collect their results:
    let socks_to_text = socks_req.await?.text();
    let shoes_to_text = shoes_req.await?.text();
    let hats_to_text = hats_req.await?.text();

    // ...And process the conversions from responses to text:
    println!(""Socks: {}"", socks_to_text.await?);
    println!(""Shoes: {}"", shoes_to_text.await?);
    println!(""Hats: {}"", hats_to_text.await?);

    let duration = started_at.elapsed().as_millis();

    println!(""Total time to execute: {}ms"", duration);

    Ok(())
}
‚ûú  reqwest-example git:(main) ‚úó cargo run --release
   Compiling reqwest-example v0.1.0 (reqwest-example)
    Finished `release` profile [optimized] target(s)in 0.75s
     Running `target/release/reqwest-example`
Socks: Here are your socks
Shoes: Here are your shoes
Hats: Here are your hats
Total time to execute: 162ms
Okay, cool, so...
hold on there...
that's not right. Even running that with the
--release
flag I still got the ""bad"" time we got from our earlier synchronous code. What's the deal?
Well, this is one of those async gotchas in Rust - just because you're ""doing async"" doesn't necessarily mean that the executor is going to put processes in the background; for that, we need
tokio::spawn
; let's just say finding that out took a bit of work, but
this SO question
goes into a bit more detail.
For the uninitiated, you don't get to put something to the background for free (this might bite someone coming from C#'s async/await implementation, for instance) - see also: the
async book
.
With the new understanding:
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let started_at = std::time::Instant::now();

    // Fire up three simultaneous requests:
    let socks_req = tokio::spawn(async move { reqwest::get(""http://localhost:8001/socks"").await });
    let shoes_req = tokio::spawn(async move { reqwest::get(""http://localhost:8001/shoes"").await });
    let hats_req = tokio::spawn(async move { reqwest::get(""http://localhost:8001/hats"").await });

    // Collect their results:
    let socks_to_text = socks_req.await??.text();
    let shoes_to_text = shoes_req.await??.text();
    let hats_to_text = hats_req.await??.text();

    // ...And process the conversions from responses to text:
    println!(""Socks: {}"", socks_to_text.await?);
    println!(""Shoes: {}"", shoes_to_text.await?);
    println!(""Hats: {}"", hats_to_text.await?);

    let duration = started_at.elapsed().as_millis();

    println!(""Total time to execute: {}ms"", duration);

    Ok(())
}
This time we get: ""Total time to execute: 86ms"". Much nicer! Also a bit weirder, since we need to handle the results of awaiting the
JoinHandle
before getting to the response, as can be seen in the
.await??
bits in the results collection section.
Polars
If you're a Polars user, you might be thrilled to know that they explicitly offer the choice between
lazy and eager APIs
. That's the link to the Python API, but we also get the
same concepts in Rust
, so what you learn in one space translates over to the other.
Actually, if you look at that first link, you'll find the following blurb in the Polars docs:
In many cases the eager API is actually calling the lazy API under the hood and immediately collecting the result. This has the benefit that within the query itself optimization(s) made by the query planner can still take place.
Basically, if you're too ""lazy"" to use the lazy API, there's a real possibility that it gets used regardless. By taking ownership of this process, though, you can ensure that you're avoiding premature materialization of your data and allowing the logical plan to handle your data pipelines efficiently.
Also, we won't be seeing a bunch of async/await here; the API itself handles the ugly bits. Anyway, the Rust API offers multiple ways to get a lazy frame. Hitting their docs, we can see this from a couple of examples:
use polars::prelude::*;
use polars::df;
let df = df![
    ""range"" => [1, 2, 3, 4, 5, 6, 8, 9, 10],
    ""left"" => (0..10).map(|_| Some(""foo"")).collect::<Vec<_>>(),
    ""right"" => (0..10).map(|_| Some(""bar"")).collect::<Vec<_>>()
]?;

let new = df.lazy()
    .with_column(when(col(""range"").gt_eq(lit(5)))
        .then(col(""left""))
        .otherwise(col(""right"")).alias(""foo_or_bar"")
    ).collect()?;
Above, we can see the
.lazy()
call on the DataFrame instance, which starts us off in a LazyFrame.
use polars::prelude::*;

 let df = LazyCsvReader::new(""reddit.csv"")
    .has_header(true)
    .with_separator(b',')
    .finish()?
    .group_by([col(""comment_karma"")])
    .agg([col(""name"").n_unique().alias(""unique_names""), col(""link_karma"").max()])
    // take only 100 rows.
    .fetch(100)?;
This time, the reader itself (
everything between instantiation and
.finish()
) produces the LazyFrame instance.
Regardless of how you get your LazyFrame, up until the point where you actually
.collect()
your results, you're not actually executing any queries, but rather describing your workflow; consider what the
docs
says about LazyFrame:
It really is an abstraction over a logical plan. The methods of this struct will incrementally modify a logical plan until output is requested (via collect).
In other words, you're describing your workflow in declarative terms - shaping how your data flows, rather than actually doing the transformation yourself. It's purely at the discretion of the logical plan how the actual transformation will take place under the hood.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
145174345.exploring-nulls,Exploring NULL(s),The little data devils,2024-08-12 07:52:52.69-05,"Is there anything more insidious roaming about the world of Data Engineering than those little buggers ‚Ä¶ the
NULLs
? I mean who even thinks about
NULLs
? We have much bigger problems to solve, more important things you know? We have pipelines to build, SQL to write, Python to wrangle, and Sprints to finish.
NULLs. Pissh. Who cares about NULLs?
I have this feeling down deep in my tummy that NULLs are probably one of the first things you stumble over when starting your Data journey, and also one of the last things you stub your toe on before you
blackout at the end of your Data career and are carried off into the great blue yonder
.
So simple, yet not so simple.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
I think it‚Äôs high time we take these little NULL devils behind the
woodshed and give ‚Äòem the what for
.
Exploring NULLs with Delta Lake and PySpark
It‚Äôs hard to know where to start when thinking about a topic like NULLs, since they have become the thing of everyday commonplace in the data landscape, I mean you hardly even notice them anymore.
Yet NULLs are important, if ignored they can cause things to go bump in the night. How we think about them, how to treat them, and coding with the acknowledgment of their existence.
I think it‚Äôs also true that learning what we can about dealing with NULLs is a plate best served piping hot. Piping hot PySpark that is.
So today that is what we shall do, simply serve a platter of NULLs to the hungry and veracious appetite of our PySpark monsters, let them chomp and crunch, see what comes out the other side.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
NULLs in data.
Sometimes I wander around aimlessly looking for something new and exciting, if I don‚Äôt find something, I just pick up something old and blow the dust off it. See what I‚Äôve forgotten and what sparks my interest.
Let‚Äôs just write some Spark ‚Ä¶ with Delta Lake involved and see what happens, shall we?
Maybe we are working on a hobbits project.
In the high volume of data age we live in, it‚Äôs super common to end up with random NULLs from incoming data. In our case, we have a
name
that is NULL, as well as a
home
.
So, the question is, are NULLs a problem? I guess the answer to that depends on what we try to do with the data, and how the NULLs will affect those operations.
What if we want to count the number of hobbits by
home
?
Interesting, that might be nice depending on your perspective, but it also shows the sneakiness of NULLS and the WHERE and WHAT you are acting on.
Because we GROUPED BY
home
, it‚Äôs nice we didn‚Äôt lose that NULL record.
But, you will notice we DID LOSE (
depending on how you deal with NULL
) one record from the Shire, because the
name
was NULL.
This isn‚Äôt unexpected if you think about it, you can‚Äôt count NULL.
It does make you wonder though, how many people actually think that hard about their operations if what would happen with NULLs? Very few.
NULL with aggregations.
To be more clear, and depending on how familiar you are with NULLs in different types of aggregations, take a look at this code.
Here are the results of the two Dataframes.
SUM
: The
sum
function ignores
NULL
values and only sums the non-NULL values.
AVG
: The
avg
function calculates the average only based on the non-NULL values.
COUNT
: The
count
function with a column argument only counts non-NULL values, while
count(""*"")
counts all rows, including those with
NULL
values.
This last piece of information with how COUNT() reacts to nulls depending on your input, and what you are trying to do, can surprise many unsuspecting Data Engineers.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Nulls in methods and functions.
Of course, aggregations aren‚Äôt the only things that can be affected by NULLs when doing Data Engineering, your nice little methods you write and unit test, which you sprinkle throughout your ETL code are not impervious either.
In your precious PySpark for example, when functions are applied to
NULL
values, they
CAN
return
NULL
. We can see this in the following example using the
concat
function.
It might be obvious what happened while we are talking about NULLs, but when you are in the middle of the Sprint, pumping out some code, and you aren‚Äôt even thinking about NULLs ‚Ä¶ when they probably exist, you should code defensively.
And it gets worse.
NULLs with filters.
Just when you thought you were safe, pumping out some dataset real quicklike for someone because you said in standup that would be no problem and super easy ‚Ä¶ then you end up looking like a hobbit because you didn‚Äôt think about NULLs.
Let's see how standard equality (
==
) in PySpark reacts when
NULL
values are involved in comparisons.
Specifically, we'll see that comparing
NULL
with any value using
==
does not return
TRUE
or
FALSE
, but rather
NULL
.
Interesting if you‚Äôre not looking for it. Notice how the is_Alice comparison results in some NULL values,
NOT
false ‚Ä¶ like some might expect.
Conclusion.
I don‚Äôt know about you, but I rarely deal with NULLs like I should ‚Ä¶ upfront ‚Ä¶ knowing they are littered through all the datasets we touch, with rare exceptions.
It can feel tedious and boring to pretend and DEFEND against NULLs in all the data we deal with ‚Ä¶ it almost feels overkill. Yet, we have to acknowledge the reality of not dealing with them ‚Ä¶ not thinking about the consequences.
Maybe we should deal with them when someone yells at us about some result not being correct, realizing our sins, and then fixing it. Oh well.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
144788580.introduction-to-mlflow,Introduction to MLflow,Machine Learning Workflows,2024-05-24 08:54:03.035-05,"For those of us who‚Äôve been unlucky enough to find ourselves working with Machine Learning as Data Engineers low these many years, one thing is certain,
it‚Äôs no surprise to us that most ML projects fail.
Data Science has always been the Wild West of data no matter where I‚Äôve worked and whether the team was a dozen or one person, didn‚Äôt seem to make much difference. It‚Äôs a difficult space to operate in with little to no stability or standards.
No two Machine Learning projects are the same, without even counting the different organizations, ML between different organizations is as varied and colorful as a middle school dance.
MLflow is here to change all that.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Machine Learning Problems.
Before we dive into MLflow, let‚Äôs talk quickly about ML in general and some of the most common tasks and workflows that exist across most all Data Science and Engineering teams working in the Machine Learning space.
Here are some of the major processes that you will find across most ML pipeline work, most of which are, or should be, built by Data Engineers so they are reproducible and automated.
Building, storing, and managing features.
Training models
Inference (prediction) on models
Managing models
Running and managing experiments
Ensuring reproducibility and automation.
That is high level, obviously a lot of detail hidden away in there. Truth be told, most of the problems that you will find on Data Science and ML projects, reasons why things don‚Äôt go as planned or work out, are because of a failure in one of the above steps.
Problems that are solvable if a little rigor is applied. Data Science and Machine Learning, overall, severely lack structure processing for the ML lifecycle, a problem that increases with the increase in ‚Ä¶
models
people doing ML
Hence the need for a tool like MFflow. So, after all that, knowing the MLflow is the answer to all our problems and can lead us into the Promised Land of glory, let‚Äôs step forth.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
MLflow at a high level.
What is MLflow?
ML and GenAI made simple.
Build better models and generative AI apps on a unified, end-to-end,
Open-source MLOps platform
More or less MLflow is supposed to be an ‚Äú
all-in
‚Äù tool. If you drink the Kool-Aid you need to chug it. It‚Äôs meant to make all of the above steps we talked about and wrap it, plus more, into a single tool.
It‚Äôs not hard to see, when reading the list of things that have to be done to have ‚Äúgood ML pipelines and workflows, that building those features yourself while not impossible, is improbable you will be able to get everything done yourself with any amount of coverage that comes close to being enough.
It‚Äôs also important to note that
managed MLflow is provided by Databricks
.
MLflow and code.
I want to just reiterate something that may or may not be familiar to you depending on how much you interact with Machine Learning. Every Engineering and Data Science is going to write different code when it comes to training, iterating, and predicting on model(s).
The more people doing the above, the more variations you will have.
Take for example my crappy SparkML Pipeline from many years ago.
This is where MLflow steps into the breach. When you are writing ‚Äúcustom‚Äù code to iterate, test, and predict on Models, how do you track what you have done and what has happened, or what IS happening?
It‚Äôs almost impossible.
How and where do you store model objects?
How do you track and trace between model iterations?
Where does all the metadata go?
How do you collaborate and share workflows etc?
How do you predict on, and serve your models?
This is the single biggest argument for why MFflow. Standardize. By unifying the framework by which we do this work we eliminate a whole series of problems and issues.
For example, here is an MFflow code snippet from their docs.
Cleaner, obvious, standardized, tracking, etc.
Of course, MLflow comes with a UI, which is a big draw in today‚Äôs world. You don‚Äôt always have to mess around on the command line. Imagine, all that tracking and work done with a few commands and available at your fingertips. Powerful.
source - https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html
This takes the guesswork out of building, deploying, and managing ML models and lifecycles.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Other MFflow concepts.
There are some other MLflow concepts that you should at least know about in the beginning, it‚Äôs more than just writing code. It‚Äôs about the whole end-to-end lifecycle.
Projects
‚ÄúAt the core, MLflow Projects are just a convention for organizing and describing your code to let other data scientists (or automated tools) run it. Each project is simply a directory of files, or a Git repository, containing your code.‚Äù
MLflow Tracking
‚ÄúThe MLflow Tracking is an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results‚Äù
MFlow Models
‚ÄúAn MLflow Model is a standard format for packaging machine learning models that can be used in a variety of downstream tools‚Äîfor example, real-time serving through a REST API or batch inference on Apache Spark‚Äù
MFlow Model Registry
‚ÄúThe MLflow Model Registry component is a centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model.‚Äù
MLflow Recipes
‚ÄúMLflow Recipes (previously known as MLflow Pipelines) is a framework that enables data scientists to quickly develop high-quality models and deploy them to production.‚Äù
I think what becomes clear after diving into all that MLflow has to offer is that it‚Äôs more than just another ML toolset. To get the full benefits of MLflow, which are many, you need to standardize on using MLflow for as much of your ML work as possible.
The more you use it for each piece, the more centralized all your ML workflows, development, and processing becomes.
MLflow reduces the chance of project failure just be automating and standardizing what goes wrong most of the time ‚Ä¶ the small details.
Typically folk focus on training models and then predicting them. They are obsessed with metrics and getting the most accurate model possible. In the process they forget the massive MLOps effort (80% of the work) required to do things correctly and keep things running well in production while iterating.
Do you really want 3 different Data Scientists doing whatever feels right? Saving models here and there, who knows about the metrics, trying to figure out how that model was built 3 months ago, and who is using it now?
MLflow for the win."
144486261.cost-savings-for-databricks-users,Cost Savings for Databricks Users,Finding those pennies,2024-05-15 07:33:02.812-05,"It comes to this point eventually, no matter who we are are what platform we are working on. It‚Äôs a vicious cycle. We drink the Kool-aid like maddened and crazed cult followers, frothing at the mouth we plow our mind and energies into our Tool Savior until there is nothing left to give.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Then one day our bosses show up and slap us backside the head and ask ‚Äú
What‚Äôs the matter with you? Have you seen this bill lately? Do something about it!
‚Äù
Then it‚Äôs a mad rush of Googling and piddling. Where can we cut costs?
Today we will list some basic cost-saving measures for Databricks workloads and environments. Nothing fancy, just plain old common sense, no snake oil being sold here.
It's not uncommon for people to think that there is some special sauce or unicorn glitter that you can sprinkle on your Databricks setup resulting in %80 cost savings.
Dream on. Real cost savings are typically boring and revolve around good Engineering principles.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Let's get started. These are in no particular order.
First, we have two maintenance commands that are specific to Delta Lake, both should probably be run on a daily basis, depending on the volume of incoming data and updates in said Delta Tables.
OPTIMIZE
Compact small files into the perfect size. One of the most common performance problems encountered in Spark, when reading files (parquet ‚Ä¶ even Delta Lake) is the ‚Äú
small file problem.
‚Äù
For extra cost savings, on large tables (many TBs+) you don‚Äôt need to OPTIMIZE the entire table every time/day. Use Partition Pruning to OPTIMIZE a subset (say recent) part(s) of your Delta Lake tables.
VACUUM
Remove unused and delete ‚Ä¶ aka unreferenced files.
For extra juice, look into the Delta Lakes transaction log and default retention (7 days).
Another small but easy win is to make sure you are not building up technical debt in the sense of not upgrading your systems. One simple change a lot of folks forget about is the DBR (Databricks Runtime version) version.
Upgrade DBR versions
Many speed improvements (say to MERGE statements) happen in newer DBR releases, keep your clusters (both Job and All Purpose) on some sort of upgrade schedule to take advantage of new features and performance improvements.
Try out Photon
Photon is a quiet hero of Databricks, it‚Äôs not much talked about but can lead to wonderful speedups in Job performance. Of course, test everything carefully yourself.
‚Äú
Photon is a high-performance Databricks-native vectorized query engine that runs your SQL workloads and DataFrame API calls faster to reduce your total cost per workload.
‚Äù
It‚Äôs important on Databricks to at least attempt to keep up with the new features being released like hornets from a nest you just threw water on. They pump ‚Äòem out fast and it can be hard to keep up. Liquid Clustering is one of those new features you might have missed.
Things like Liquid Clustering are low-hanging fruit and will save you query performance ‚Ä¶ aka Compute Dollars at the bottom line. Do it.
Liquid Clustering
‚Äú
Delta Lake liquid clustering replaces table partitioning and
ZORDER
to simplify data layout decisions and optimize query performance.
‚Äù
Read more here where I did a test and got massive query performance improvements on Delta Lake tables with LQ
.
Tune Cluster Sizes
Another obvious way to save a serious amount of pennies is to simply review all your Jobs and code running on every cluster you have. Many times when new pipelines are released it‚Äôs the ‚Äòol spit in the air and test the wind for choosing Cluster node sizes.
Look at your Logs Monitor your Jobs, and tune cluster sizes to ensure you aren‚Äôt wasting resources, there‚Äôs a 90% chance you probably are right now.
It might be small changes, but across a lot of Jobs you can affect the bottom line very easily.
Reduce All-Purpose Cluster Usage
This one is so obvious most people don‚Äôt think about it or never consider it. It‚Äôs well known (or should be) that All-Purpose compute on Databricks is a good +50%
more expensive than Job Compute. Do something about it.
Not everyone needs their own personal All-Purpose Cluster to play around in Notebooks all day. Share one. Also, teach people to Develop locally before moving to a Notebook.
The next two tips are just general Big Data and Spark optimization ideas, probably the hardest ones to deal with ‚Ä¶ aka it‚Äôs going to make you learn and grow. Not a bad thing.
Tune Spark Jobs
Start with the basics, and use the 80/20 rule. Find your long-running jobs and figure out what‚Äôs going on with them. Use best practices like cache(), broadcast joins, filtering, reducing skew, etc.
Use correct Data Types in Delta Lake
Another not though of much problem in Databricks with Big Data workloads (multiple TBs+) are the data types selected for Delta Lake schemas. Things like joining on Strings instead of Integers, for example, can have a big impact on performance. Incorrect Data Types can also lead to storage bloat.
Some things are so obvious they are just hard to see.
Lower Cluster Inactivity Shutdown timeout
I would be willing to put forth that there is many a dollar (millions) being spent on Databricks every day by Clusters not being used that just sit because someone leaves them on and doesn‚Äôt even think about turning them off after playing in a Notebook. Dare I tell you what the default timeout is on an All-Purpose cluster? Enough to curl your toes.
The next step of cost savings have to do with understanding how Notebooks development, although the easy path, might not be the best for the wallet.
Migrate Workloads to Job Clusters
This is again, yet another obvious one. Don‚Äôt run Production Workloads on Notebooks tied to All-Purpose compute. Just not a good idea unless you really enjoy literally burning money. Move to a better Development cycle, use Jobs.
Reduce data size for Notebook R&D
Closely related to the prior discussion on Notebooks, while great for R&D, if you have a Data Science team or Analysts who seem only to be able to use a Notebook to do EVERYTHING ‚Ä¶ then coach them on proper development ‚Ä¶ aka start small and work up.
If someone is testing an idea or doing analysis code. Tell them to start with a few thousand rows. You don‚Äôt need 3TBs of data to test an idea and some code.
Replace Notebook Development with Local Development
Again, since we just talked about this, a quick way to save real money is to ensure all Engineers (and Non-Engineers where possible) are using a correct Development lifecycle. That means building Data Pipelines on Databricks can and should start in a local IDE with Unit Tests etc. Only once the first pass is done should Development move into live environments.
If you have a whole team of Engineers, Scientists, and Analysts ‚Ä¶ this will add up to real money if they are all using Notebooks + All Purpose compute to do ALL development.
Move to Fleet Clusters
Another great new feature on Databricks and AWS that no one talks about is the use of Fleet Clusters. ‚ÄúDatabricks fleet clusters
enable customers to launch clusters using EC2 Spot instances while adopting EC2 Spot best practices by using diversified node types
.‚Äù This is a no-brainer to save compute.
Reduce Cluster Disk Sizes
Anytime you are setting up Spark Clusters you can get caught in the same old hum-drum and not put your thinking cap on. Do you really need 1TB of storage attached to all your Spark Nodes? Probably not. Think about it.
SPOT with Fallback for Clusters
Here is one that sneaks by most people. SPOT vs On-Demand can make a big impact. Set your Driver node to be Spot with Fallback and Spot for the rest. Or at least play with it. Use as many SPOT instances as you can get away with.
Autoscaling Clusters
If you aren‚Äôt sure what kinda of resources you will be using on some new Job(s), make sure to use
Auto-Scaling clusters
and set your minimum and max workers accordingly ‚Ä¶ aka set your min number of workers pretty low at the start.
I could probably keep going but that should keep anyone busy for a little bit at least. Note, there is no rocket science buried in here anywhere, it‚Äôs mostly about paying attention to detail and ensuring best practices are followed.
It‚Äôs easy on a beautiful platform like Databricks to get enamored and start pumping out pipelines left and right, which is all find and dandy but take the time to think about things first, you can probably make a real dent in your bill simply by following good practices.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Rarely is there a silver bullet that will cut your costs in half, unless you really doing some terrible things in the beginning ‚Ä¶ cost savings don‚Äôt have to be some big complex problem, something new rocket juice thingymabob to save you.
Before you throw your hands up in the air and say ‚ÄúI don‚Äôt know,‚Äù go through that above list. Focus on the basics, that‚Äôs where most Data Platforms go off the rails ‚Ä¶ right from the start.
Two-hour Cluster Timeout/Shutdowns? Easy fix. Everyone running their own giant All-Purpose cluster? Easy, tell everyone to share like in kindergarten."
144369786.premature-optimization-is-not-the,Premature Optimization is NOT the root of all evil?,The Worm Has Turned.,2024-06-10 07:45:56.784-05,"If you pay attention to the rumblings and mumblings bubbling and moving under the surface of the Software Engineering community on Twitter, Reddit, and YouTube you might have noticed, like me, the worm is starting to turn.
What do I mean by the worm turning?
It appears, at least on the surface, and unsurprisingly, certain axioms that have been passed down from our forefathers through time are starting to be questioned.
One of these 10 Commandments given to us to follow blindly simply because it gets parroted ad infinitum is the adage‚Ä¶
‚ÄúPremature Optimization is the root of all evil‚Äù
Let the fighting begin.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Premature Optimization ‚Ä¶ is now a good thing?
I‚Äôve always been a fan of questioning pretty much everything anyone says, regardless of the source. I think one of the most important skills a Senior Engineer can have when it comes to making technical decisions is to ‚Äú
Steel Man
‚Äù all options.
‚ÄúSteelmanning is a rhetorical technique that involves
creating a version of an opponent's argument that is stronger than their own
.
It's the opposite of a straw man argument, which is when someone presents an opponent's argument in a way that makes it easy to refute. Steelmanning involves understanding the opponent's position and then presenting it in a way that makes it difficult to refute.‚Äù - AI
A good way to find something that needs to be Steel Maned is to see what kinda things get repeated over and over ‚Ä¶ without any explanation.
For example, do a search on ‚Äúpremature optimization‚Äù on Linkedin ‚Ä¶ it‚Äôs a never ending list of people simply posting this quote over and over again.
I find it slightly amusing that both sides, those for, and those against this idea of ‚Äú
premature optimization
‚Äù seem to talk past each other and fail to examine both sides of the argument, and in the process lose out on some good insights.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
The backlash against ‚Äúbad‚Äù code.
This isn‚Äôt the first time the tides have started to turn against widely held and popular beliefs or axioms in the Software Engineering space. Recently there has been a glut of negativity surrounding the ‚ÄúClean Code‚Äù prophets.
People are simply getting tired of seemingly meaningless chuff that has no real roots to grab hold of.
I can sympathize with that I suppose, it‚Äôs one thing to hold a sign and wave it around telling people not to prematurely optimize code, but the question is, especially for newcomers, does that simple statement get ingested correctly?
Probably not.
Newbies to Software Engineering can easily misunderstand and apply vague sayings.
With the rise of AI-assisted coding and the never-ending glut of BootCamp grads ‚Ä¶ people get grumpy about
bad
code more.
It‚Äôs easy to place the blame on popular beliefs that get twisted by others.
Good parts of avoiding Premature Optimization.
Anyone who‚Äôs written code for any amount of time, especially around the presence of Junior engineers knows exactly the type of trap some Software Engineers fall into.
I‚Äôve seen it many a time myself.
Example‚Ä¶
‚ÄúA less experience Engineer is given the task of writing some Python that will can, given a s3 bucket and prefix, download the most recent file that matches said bucket and prefix, (said file will be a CSV, and covert that file into a parquet file partitioned by some date column, and upload the result back into s3.‚Äù
Again to reiterate ‚Ä¶
take a s3 bucket and prefix
find the most recent file matching the above
download that CSV file
convert to Parquet with partition by date
upload back to s3
Let‚Äôs ask our new Junior Engineer to write this code for us ‚Ä¶ (ChatGTP ‚Ä¶ don‚Äôt tell anyone).
In the question of to be someone who prematurely optimizes or not, the real problem is where should a person spend most of their time on this little project???
Now I can see someone spending most of their time working out how they would like to find the ‚Äú
most recent file
‚Äù given the bucket and prefix.
Like probably an inordinate amount of time finding the best way to get the most recent file ‚Ä¶ trying to find the most clever way possible. When in the real world that is the smallest part of this problem when it comes to ‚Äúwork‚Äù that this program is going to have to do.
I would say the tools used to process the CSV file and the Parquet work are going to matter more. Also, possibly keeping things in memory with IO buffers (
file-like
) rather than using Disk for example.
But, at the same time, I can see how folk who are tired of dealing with bad and poorly written code make the argument that it should not be ‚Äúhard‚Äù to write the entire program efficiently and effectively from end to end.
Of course, this is contrived but you can start to see both sides of the argument playing out ‚Ä¶
where do you spend your development time
what is good enough vs what needs extra thought
do some things simply not matter as much as others?
where and how do you make those tradeoffs?
Should you just write something good enough, focus on the I/O stuff and ship it? Come back later to piddle with the stuff that is piddly?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
How do you choose?
I get it, I get both sides. I‚Äôve been annoyed by what I think is sloppy code and things that just seemed to have been forgotten or glossed over that came back to be a pain.
I‚Äôve seen code that should have been thought about more and been ‚Äú
better
‚Äù ignored in lieu of something meaningless to the overall codebase and central problem at hand.
I think the real world, creating Software is filled with nuance and tradeoffs
. Unless you work in a dungeon of cubicle boringness where it‚Äôs ok to take 2 months to write something that takes 3 days ‚Ä¶ there is always an incessant pull between shipping the good enough for the business and doing it correctly to reduce tech debt and bugs.
Hindsight is usually 20-20, it‚Äôs hard at the moment to know what is correct.
Should you prematurely optimize your code?
Depends on the situation. If you have the time and bandwidth ‚Ä¶ do it!
Labor over your code like it‚Äôs your baby because it is
. Do it right, make it perfect, why not?
If you don‚Äôt have that time then knock it off, get the project done, ship it, and focus on what matters, not what doesn‚Äôt."
144369735.how-tech-debt-databricks-and-spark,"How Tech Debt, Databricks, and Spark UDFs ruined my weekend.",Lessons in tech debt and vendor funny stuff,2024-05-20 07:36:15.62-05,"It‚Äôs been a while since I‚Äôve spent the weekend working ‚Ä¶ (
forced to work that is ‚Ä¶ because of failures ‚Ä¶ Engineering failures ‚Ä¶
my failures
). It‚Äôs good to be kept humble I suppose. Getting complacent is human nature because when all is well we lower our guards, kick our feet up, start laughing, and congratulate ourselves on how smart and wonderful we are.
Then it all breaks. On a Saturday.
I figured my tale of woe would serve as a good lesson for all of us Data Engineers on how important it is we keep on the offensive, deal with tech debt, and how we can stay humble.
Technical Debt
My story of woe starts with the most classic of all Engineering problems ‚Ä¶
tech debt
. I‚Äôm not sure why technical debt doesn‚Äôt get much focus from the talking heads on the internet, but that isn‚Äôt that surprising, it‚Äôs sorta a boring topic.
It‚Äôs something we all live with, knowingly, we pet it, poopoo it, cuddle it, we simply keep it around.
But what is technical debt? Let‚Äôs ask the old AI.
Technical debt in software engineering refers to the concept of incurring future costs due to taking shortcuts or making suboptimal decisions during the development process to achieve short-term gains. It's analogous to financial debt, where borrowing money provides immediate resources but requires repayment with interest over time. In software development, this ""debt"" can manifest in various ways ‚Ä¶
Pretty much agree with that. Technical debt can exist in code and outside of code, it‚Äôs a thing with a life, an amorphous blog.
When most of us think of technical debt we think of ‚Äú
bad code
,‚Äù and this is true to an extent, but not all-encompassing. On the average Data Platform, there are lots of technical debts of various kinds.
Bad or Complex Code
Old Architecture
Vendor Features and Platforms
Data Quality
Data Monitoring and Alerts
Poor CI/CD and DevOps
These things combined can build up over time and create ‚Äúcomplex‚Äù technical debt that is more than just ‚Ä¶ ‚Äú
Hey, this method or function is leaky and dirty.
‚Äù
We are talking REAL technical debt that involves vendor platforms, tooling, etc.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
But, technical debt isn‚Äôt that simple, there are so many other factors that come into play that cause the existence of technical debt to take place.
Size of the company
Culture of the company
How fast technical team is expected to move.
The truth is we can‚Äôt have our cake and eat it too. There are tradeoffs we make every single day, and that is part of being Engineers. We consider what lies ahead and behind, given our tasks, we make decisions.
We accept technical debt in some areas and reject it in others.
Ok, let me spin you a tale of reality, the reality of working with limited resources, where you have to most fast and make numerous tradeoffs on what can and cannot be worked on. Where you have no DevOps Team, Platform Team, or Backend Team ‚Ä¶ you ‚Ä¶ you are everything.
Let me paint a picture for you.
It all started at 4:45 AM on a Saturday morning. Of course. Slack Alert. Pipeline failures.
This alert with sent from Airflow from a Databricks Pipeline. The following Spark error appeared in the logs.
Of course, very cryptic, and this particular pipeline had been running for years, it had been recently, say within the last few months migrated from a Standard Databricks account to Unity Catalog.
Anyone who‚Äôs worked with Spark and sees this kind of error thinks OOM and Memory issues right away. The Driver Node obviously going poof for some reason.
Suffice it to say I‚Äôve been around Spark a long time, and tricky pipeline errors, with it being a Saturday and having better things to do, I pretty much threw the kitchen sink at the problem, just trying to get the pipeline to run.
For context, we are taking a few hundred billion records.
So far ...
increasing cluster size and drive size doesn't make a difference
tried changing the DBR runtime, but no good
tried persisting the DF immediately prior to CSV write.
it doesn't seem to be OOM on the surface, plenty of 758GB RAM on the driver
looked at data volumes, seem to be stable no big spikes of data recently
Clearly, something funny was going on as the pipeline had been running fine and out of the blue decided to stop for no reason, also the fact that doubling the size of the resources made no difference was very troubling.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Adding some context
Let's take a break from this story of suffering and woe to give you some much-needed context, and take a minute to talk about tech debt.
This story starts about 3 years ago. At that time the Data Platform in question was on its literal last legs.
AWS EMR for Spark + some EC2
AWS Data Pipeline for orchestration
AWS S3 with Parquets for Data Lake.
It's a complex machine-learning pipeline. The code base was unusable, no tests, poorly written code is an understatement, excessive use of UDFs and Loops, a mix of Python and PySpark, and to top it off, no Development Environment.
And that is putting it nicely. Daily errors were the norm and it was taking 18+ hours to run the pipelines end to end.
Classic Technical Debt
It was a clear case of technical debt not being dealt with over many years, at many levels.
Code-level tech debt
Design (problem/business) level tech debt
Tooling tech debt
The combination of these three can lead to serious problems that are hard to overcome without herculean effort.
So that's what happened, a year+ long project to migrate to the new stack.
Databricks to replace EMR
Delta Lake to replace S3 Data Lake
AWS MWAA to replace Data Pipeline
Refactor all code
Focus on CI/CD and DevOps, along with testing and Development.
How do you deal with such large amounts of technical debt? At a certain point after waiting too long, the combination of code and tooling makes the only viable option typically total migrations.
Back to the story.
Anyways, back to the problem at hand. It‚Äôs a Saturday and the pipeline has been broken since 4:45 AM and it‚Äôs about 11 AM with no end in sight.
But, the story behind the story is that this ‚Äúsection‚Äù of the pipeline had never been fully rid of all technical debt. During the 1.5-year-long migration process, most all the code had been rewritten and brought into this century, as well as obviously made to run on the new infrastructure.
All, expect this bit.
For reasons now buried in the past, this bit of code now failing hadn‚Äôt been deemed important enough to take ALL the way over the finish line.
It was a known problem, but because of other priorities had simply been seen as an acceptable risk, but at some point that risk continues to rise over time and eventually the pot boils over.
After playing around with DBR versions and downgrading, the same error was appearing but now referring to UDFs. This code was full of them (well over 25), unnecessary ones, but full of them. The code has never been refactored.
It was clear that something had happened that was out of our control, it‚Äôs a well-known problem that classic UDFs don‚Äôt play well with Unity Catalog. Something had magically changed on the Databricks side, probably overnight, and was now adding the final straw to the technical debt we hadn‚Äôt dealt with.
In the heat of the moment, I almost went down the path of refactoring all the UDFs into normal PySpark functions but although that might be the long-term fix it wasn‚Äôt the solution for the moment because ‚Ä¶
such changes need rigorous testing
doing things in the heat of the moment is a bad idea
going down rabbit holes when you need a fix now is a bad idea
Getting lucky, being careful with vendors.
This is especially true when you have parts of a codebase that are overwhelmed with technical debt, but relying 100% on a particular vendor, especially anything fairly new, can be a foot gun or a ticking time bomb.
No matter who or what your cloud provider or SaaS product is, even if we are in love with them, you still need to hold them at arm‚Äôs length and design your codebase in such a way it doesn‚Äôt have a death embrace with any particular ‚Äú
thing
.‚Äù
In the case of my broken pipeline, I was getting desperate to move along with my weekend plans and stop the bleeding, not to mention the broken pipeline has customer impact.
I got lucky.
We had recently gone through a small migration from Databricks Standard account to Unity Catalog, and with some stroke of luck and a little distrust ‚Ä¶ we had not torn down the Standard Databricks account yet.
Any Data Engineer who‚Äôs been around the block a little knows not to wave the victory flag too soon. Better safe than sorry. It was my weekend‚Äôs saving grace.
I moved the pipeline back to the Standard Databricks Account, ran it, and it ran without a hitch.
Lessons Learned
There are a number of lessons to learn during times like this, here are some of them in no particular order.
No pipeline is immune from failure no matter how long it has ‚Äú
been running fine.
‚Äù
You can‚Äôt entirely trust any one vendor never to ‚Äú
get you.
‚Äù
You need to have some sort of Disaster Recovery plan in case of not only complete vendor failure, but simply things changing and needing a solution NOW, not 48 hours later.
You and your tech debt are your own worst enemy
Tech debt will catch up with you eventually.
There are more types of tech debt than just ‚Äúbad code.‚Äù
The amount of tech debt should stay level or always be declining, not going up.
There are problems with moving fast, eventually that pied piper will come calling for you.
When solving a stressful problem, resist the urge to get complicated, KISS will end up fixing it most times.
What it really comes down to is that getting caught by tech debt is going to happen, and it got me, and it will always happen when you don‚Äôt want it too.
We take risks, we decide what is important to work on and what isn‚Äôt, we listen to the business, we solve problems, and keep things running.
But, we have a responsibility as Engineers to understand the dark corners and edges that we have decided to let slide.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
I got lucky with a leftover account from a migration that turned out to be a turnkey answer to my problem. I did spend hours trying to debug the un-debuggable, my own fault.
It‚Äôs hard in the moment of pressure to step back and think about all the options, even if you know you could probably ‚Äúcode your way out of it,‚Äù and most likely cause another set of problems while doing that."
144303724.weekend-forecast,Weekend Forecast,Updates and Ramblings,2024-05-04 08:44:23.582-05,"It‚Äôs been a while since I‚Äôve sent a weekend update, you know how life gets. First, let‚Äôs get business out of the way. Here‚Äôs what I‚Äôve written elsewhere lately (
aka not on this Substack
).
Why Did Golang Lose To Rust for Data Engineering?
If:Else Logic and Complexity ‚Äì Hiding The Pea.
How to Join Datasets in Polars ‚Ä¶ Compared to Pandas.
Also, if you haven‚Äôt noticed (shame on you) I‚Äôve been trying to do more
YouTube videos
of late, just trying new things. For example, I did a reaction to Google Firing Python.
Also, I want to give all of you cheapskates a
50% off coupon for the paid version of this Substack Newsletter
. Click below.
Finally, the last few pieces of business at hand. Can you vote below and let me know what content you want to see more of?
And last but not least, if I were to work on a new project ‚Ä¶ do you want a book or a course?
And what do you think about these topics?
Anyways, thanks for your input. I hope you get out this weekend and do something with your family and friends, or in the great outdoors.
Taking time away from the computer is key to building long-term success. I put my money where my mouth is. What have I been doing? Of course, fishing, hunting, and other adventures."
144216487.json-with-rust,JSON with Rust,Thoughts on being Explicit vs Vague,2024-05-02 08:01:33.649-05,"While recently working on a side project to make myself rich off the stock market using Rust, I ran into a problem of sorts. Well, not a problem per se, but a conundrum.
I had a choice to make, not unlike choices we have to make as Data Engineers writing code all day long. To be explicit or vague. I mean if you are in a hurry just take the easy route?
It seemed wrong.
In this particular instance, I was using Rust to process some JSON. I think this will make a great example for us to talk about Explicitness vs Vaugeness, regardless if you‚Äôve even written Rust or not.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Moving fast ‚Ä¶ or correctly. Vague vs Explicit.
As someone who‚Äôs been forced to use Python to write the majority of my production code over the years, say it ain‚Äôt so, decades ‚Ä¶ there are certain downsides. Namely, being vague with code because you have no other choice really.
This isn‚Äôt something I endorse and still don‚Äôt like. I enjoy code to be explicit. Say what you mean. Be verbose.
Instead of starting with Rust let‚Äôs start with our odd cousin still living in Aunts‚Äô basement ‚Ä¶ Python.
I was recently working with a JSON file that is a list of tickers for publicly traded stocks. It looks like this.
How would most people read this JSON with Python? Probably something like as follows ‚Ä¶
This probably looks very normal to you. But, something is missing if we were working in a real codebase where some of this logic is encapsulated ‚Ä¶
read and file I/O in one spot
processing the JSON in another
A few things I wouldn‚Äôt know looking at the code if I was making changes, additions, or whatever to this code.
I don‚Äôt know what the actual structure of JSON is or should be.
I don‚Äôt know data types.
Sure, in Python I could add some extra libraries like ‚Ä¶
pip install jsonschema
and then go about the business of writing some validation code etc. Most people never do this, it‚Äôs Python.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Reading JSON with Rust.
I mean I did say this was a Rust thing in the beginning, and it is, I just wanted to set the stage. Now that we have set the stage with our dirty little Python code, I‚Äôm sure you‚Äôve heard people, including me, laud the safety of Rust and its wonderful Type system.
This is true, the static Types at compile time with Rust is one of the reasons many people love it, and it‚Äôs probably why it‚Äôs quickly becoming the de facto standard for backend Data Engineering systems.
Anyway, I digress. Back to the problem at hand.
Rust + JSON.
Let‚Äôs say I wanted to read this same JSON file with Rust. How might I go about it? One would expect, considering Rust‚Äôs ideals that this would be verbose code.
Let‚Äôs try.
First, let‚Äôs set a
struct
and
type
to hold our and
prove
upfront what our JSON will be once we load it.
In my case, I was loading the JSON and then pushing the results of the above JSON structure into a SQLite database.
Basically, what is happening ‚Ä¶
Read file
Use
serde_json
to get results into the TickerMap (
which includes our Ticker struct of course
)
Iterate those tickers and access the struct attributes with dot notation.
I mean it does feel very natural to Rust to do it this way. It‚Äôs very
Explicit
and not
Vague
at all, just what would we expect.
Unlike Python, we aren‚Äôt sort of hoping that things work out unless we add the extra package and force-check the schema. It‚Äôs sorta built-in with Rust, isn‚Äôt it?
We are kind of pushing possible problems back as far as we can in the code, getting things to pop earlier if they are not what we expect. Also, the code is easy to reason about out of the box.
Writing lazy Rust.
But wait, there‚Äôs more. Can we be lazy in Rust and be more like Python? Just load up who knows what JSON and hope for the best?
Sure we can. If we use ‚Ä¶
use serde_json::Value;
we can write the below function.
Going this route we can delete the Struct and Type we used before.
I mean it‚Äôs code reduction right??? Less code, less problems???
I guess when we look at this sneaky Rust code it does seem a little more obtuse to me, compared to the verbose first example. At least for me.
I just don‚Äôt like how the second option
feels
. It feels like cheating, it feels like I might as well use Python if I‚Äôm going to write Rust like that.
The Programmer vs the Program.
I guess after all this hullabaloo it brings me to the point of the whole thing.
Sometimes I forget it‚Äôs just as much about how the Programmer decides to solve the problem as much as it is about the semantics of a particular language, in our case Python or Rust.
We all like to worship at the altar of something, after years of Python I like to spend my time Rusting my way through data. It‚Äôs a joy to write and when it compiles I get a warm feeling in my tummy much like that first swig of a
Pale IPA
.
Rust does a better job than pretty much any other language to date in protecting us from us. That‚Äôs why it‚Äôs popular. But at the end of the day, the developer is the developer and they are going to do what they are going to do.
I still find myself in the camp of being verbose. Use Types, be explicit when you can,
and be verbose.
I‚Äôm probably just getting old and grumpy but me thinks not enough emphasis is put on readability and debug ability in codebases.
In the age of ChatGTP and Copilot, the draw to be lazy is ever near and calling your name.
Leave a comment"
144126447.redshift-vs-snowflake-vs-bigquery,Redshift vs Snowflake vs BigQuery vs Databricks vs ...,Why is the Data Warehouse Battle still going?,2024-05-09 11:31:27.395-05,"The other day I read a post on that most sinister of websites,
r/dataengineering
, known for its unapologetically ruthless hoard of so-called Data Engineers.
That wicked rabble of ne'er-do-wells was pontificating on the current state of the Data Warehouse, in the context of the big cadre of tooling at our disposal.
Redshift
Snowflake
BigQuery
Databricks
etc.
One thing I find slightly amusing after these years of data life is the
vendor-led confusion
as to what exactly is or isn‚Äôt a Data Warehouse.
It‚Äôs unfortunate we find ourselves where we do, simply because marketing and produce teams have conspired against us all, but here we are nonetheless.
What do we know ‚Ä¶
Data Warehouses came first, they ran on SQL Server, Oracle, etc in the beginning. Many people still associate the word Data Warehouse with these RDBMS, which is not necessarily the case.
Data Lakes came next. That‚Äôs what people call dumping files of various sorts (CSV, Parquet, JSON, etc.) into cloud storage buckets like s3.
Tools like Delta Lake came (a combination of the above two ‚Ä¶ file-based systems that provided DW-like features, aka ACID, CRUD, etc.) Vendors coined this Lake House.
But, you have to remember, Kimbal‚Äôs Data Warehouse toolkit is still the Bible of Data Modeling ‚Ä¶ at least for now.
What happens if you build a Data Warehouse with facts and dimensions ontop of Delta Lake? What is it? Kick the old Bronze, Sliver, Gold nonsense to the curb.
Databricks would probably tell you it‚Äôs an incorrect Lake House, and Data Warehouse folk on SQL Server would probably tell you it‚Äôs a Data Warehouse gone wrong. Maybe we are all wrong. Or right. Maybe it‚Äôs
the Twilight Zone
.
Your data store is what you make it be. You‚Äôre the data people, you‚Äôre the Engineer. You can build what you want.
What it comes down to is that there is a battle for your data, companies, and tools grinding their axes and eyeing each other, all vying for a chance to engulf your data with promises of neverending sunshine and problem-free Data Engineering forever.
The Tooling Battle.
Redshift vs Snowflake vs BigQuery vs Databricks. Do you ever feel like a crazy person?? Why does it have to be so hard? Why does everything have to be so complicated, why do people defend their tool of choice to the death?
I think we should cut through the layers of marketing and product detritus that has built up and just say out loud which tools are good for what.
Sure, you can make any tool work for you if you want, it‚Äôs amazing what Engineers will do to simply use what they want. They will absolutely get that square peg through a round hole one way or another.
But, say no more, let‚Äôs just tell people who to use which tool and leave the rest for the rabble to squabble over.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Which Tool to Use For What?
Few things in life are as wonderful as making people mad by poking a stick in the eye of their golden calf. So let‚Äôs have a go.
Redshift
You should never use Redshift.
It‚Äôs expensive and you can equate it to an oversized and overpriced SQLServer.
If you are an AWS Shop and you‚Äôve been using SQL Server, Oracle, or whatever, and you want to make your bosses happy by doing something cool ‚Ä¶ dump your data in Redshift and move on.
Snowflake
Snowflake is for DBT and SQL junkies who are like tweakers when they haven‚Äôt written a SQL query in the last 15 minutes.
For people who are bad programmers.
Don‚Äôt like doing ML stuff.
Like burning money.
BigQuery
Only goody-two-shoes and bamboozled ninnies who think GCP is the greatest use of this tool.
It's old and never changes much.
Simply lacks 3/4 of the features of the other tools.
Will never have the market share of the other tools.
Warehousing tool for those who don‚Äôt know how to build real Warehouses.
Databricks
The best option to pick.
Unparalleled in features and options.
Machine Learning GOAT.
For good programmers.
SQL suckers can still use it.
Best in class features.
Well, did I make you mad? I‚Äôm probably not far off from the truth, at least my truth.
What can we boil it down to?
People can build whatever they want with whatever they want. Often times it‚Äôs less about the tools and more about the Engineers, processes, and data.
Sure, at some point the data outgrows the old RDBMS and you have no choice. But, beyond that, do whatever you want.
Just do it well and understand the tradeoffs you are making.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
144107994.i-see-window-functions-everywhere,I See Window Functions Everywhere,Guest Post from Anonymous Rust Dev,2024-06-03 07:26:11.251-05,"Hello readers. Today I‚Äôm bringing you a guest post from an anonymous Rust developer.
He‚Äôs no stranger to Data Engineering Central
.
This is someone I know personally, have been on car rides with, camped in the woods with, sat around a fire with. They are one of the smartest and most wonderful programmers I have ever met.
Enjoy.
If you're new to window functions, you might be tempted to Google the term to see what pops up. When I do so, and see the relevant
Wikipedia article
, I get the following definition:
In signal processing and statistics, a window function (also known as an apodization function or tapering function) is a mathematical function that is zero-valued outside of some chosen interval.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Whoops, wrong page. They have a different one for the SQL variant (
which is why we're here
)... and yet, maybe we can take something away from this definition. Let's look again at that statement:
... function that is zero-valued outside of some chosen interval
And therein lies the gist of it: a window function is aggregating; and, in SQL statements, this can be done on the row level, so maybe by adding one and one you can guess that this is a tool for row-level aggregation.
Is that the whole story, though? Stick around to learn more about one of my favorite tools for building meaningful queries.
The naive approach
First, to know what a window function is, we should look at the kinds of problems they solve. For this, I'll be turning to the trusty Northwind database, and in particular, I'm interested in the
orders
table.
To start, a lay of the land:
SELECT
        min(order_date), max(order_date), count(*), count(distinct customer_id) as distinct_customers
    FROM orders;
Over a roughly two-year period, we can see 830 orders placed by 89 customers - surely, at least one of those customers has placed repeat orders. We might want to see on a per-customer basis how they rank in terms of order frequency, and this could be done in a number of different ways; for now, we'll use temp tables to collate our information.
First, we'll need to know the count by
customer_id
:
select customer_id, count(*)
    from orders
    group by customer_id
    order by count(*)
... and so on.
As can be seen above, the same count could be shared between multiple customers. Also, scanning all of the results, you might even see gaps in the counts:
...So, clearly, we can't just use the count as a rank since we jump around a bit. So, the first thing we need is to know how the counts rank; and, since our ranks are probably assigned in terms of descending quantity, we'll also need to ensure our largest count gets the lowest rank figure.
CREATE TABLE count_rankings (
    ranking serial primary key,
    quantity int
);

INSERT INTO count_rankings (quantity)
    SELECT distinct count(*)
        FROM orders
        GROUP BY customer_id
        ORDER BY count(*) DESC;
If you were to peek at the contents of our
count_rankings
table, you'd notice something like:
With this as a lookup table, we can then cross-reference our row-level counts as we consider each customer:
Surely, there's an easier way (?)
...and you wouldn't be wrong. But first, let's think about why we had to break it up into multiple steps.
Basically, we needed state, but a SQL query is by its nature stateless. And, by ""state"" I'm referring to some kind of running tally or accumulator to keep track of how one count compares against the next. The lookup table
count_rankings
in the above example served as this state, but it's pretty clear to see that this is a tedious way of figuring out the ranking.
Alternatively, thinking of this problem in terms of code (e.g. Python), you might approach this as follows:
Create a dictionary of counts (value) by customer ID (key), by looping over all of the orders
Create a set of counts (or create a list and deduplicate it)
Sort the counts list in descending order
Update the dictionary of counts so that the value is a tuple of a) count, and b) the index of that count in the sorted list
Any way you look at the problem, it seems like it shouldn't be possible to do it with a one-liner... right?
Enter:
DENSE_RANK
:
The DENSE_RANK() assigns a rank to every row in each partition of a result set. Different from the RANK() function, the DENSE_RANK() function always returns consecutive rank values.
Wow, that's a mouthful. Note from the description that the
RANK()
function doesn't give us consecutive values, though if you had approached this problem naively you may have been led to believe it would.
Also, there are a few keywords I would like to pluck out from that description:
rank
partition
consecutive
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
The first of those, ""
rank
"", is what we're attempting to accomplish.
The second, ""
partition
"", describes how we break apart our initial results to determine how things are ranked. For now, since our rankings apply over the entirety of the order history, we won't further partition the results.
Finally, we wanted to describe a sequential (""consecutive"") ranking of our counts. Two customers with the same order count are ranked identically using the
DENSE_RANK()
function; had we wanted to give unique ranks, we might instead have gone with the
ROW_NUMBER()
function.
So, with all that in mind, let's first identify our base query:
SELECT customer_id, count(*) AS count_of
    FROM orders
    GROUP BY customer_id;
Adding a ranking value is as simple as adding a new projection column with a call to the appropriate function:
SELECT
        customer_id,
        count(*) AS count_of,
        DENSE_RANK() OVER (ORDER BY count(*) DESC) AS ranking
    FROM orders
    GROUP BY customer_id;
This gives us identical output to what we got with our naive approach, and with far less effort. Also, it's probably a lot easier to follow along with than the multi-statement approach from earlier.
Partitioning
Because this example skipped over partitioning, I'd like to revisit that by modifying our query's requirements a bit: now that your Pointy-haired Boss (PHB) knows that you can rank your orders, he might next suggest we figure out how to rank salespeople within a team (defined by a shared manager).
As before, we'll need to start with a base query that defines these grouping characteristics:
SELECT
        CASE
            WHEN employees.reports_to IS NULL THEN '(None)'
            ELSE TRIM(CONCAT(managers.first_name, ' ', managers.last_name))
            END AS manager,
        CONCAT(employees.first_name, ' ', employees.last_name) AS salesperson,
        count(*) AS count_of
    FROM orders
        INNER JOIN employees
            ON orders.employee_id = employees.employee_id
        LEFT JOIN employees managers
            ON managers.employee_id = employees.reports_to
    GROUP BY
        employees.first_name,
        employees.reports_to,
        employees.last_name,
        managers.first_name,
        managers.last_name
    ORDER BY
        managers.first_name,
        managers.last_name,
        employees.first_name,
        employees.last_name;
This one was a bit more work; I've added in employee names for both the manager and the salesperson, and because some salespeople don't actually have anyone they report to it was necessary to account for that.
So, to illustrate why we need to partition our data, let's first just slap a
DENSE_RANK
call onto this and see what happens:
SELECT
        CASE
            WHEN employees.reports_to IS NULL THEN '(None)'
            ELSE TRIM(CONCAT(managers.first_name, ' ', managers.last_name))
            END AS manager,
        CONCAT(employees.first_name, ' ', employees.last_name) AS salesperson,
        count(*) AS count_of,
        DENSE_RANK() OVER (ORDER BY count(*) DESC)
    FROM orders
        INNER JOIN employees
            ON orders.employee_id = employees.employee_id
        LEFT JOIN employees managers
            ON managers.employee_id = employees.reports_to
    GROUP BY
        employees.first_name,
        employees.reports_to,
        employees.last_name,
        managers.first_name,
        managers.last_name
    ORDER BY
        managers.first_name,
        managers.last_name,
        employees.first_name,
        employees.last_name;
In case it isn't immediately apparent, we're pitting each salesperson in the company against each other.
And, let's be clear - Andrew Fuller is too important to be compared to the likes of his subordinates, so of course your PHB's going to push back.
This is where partitioning comes in; we want to divide these results up so that only those within the same team are ranked against each other. I'll start by modifying the DENSE_RANK's
OVER
clause to include the partitioning scheme:
DENSE_RANK() OVER (PARTITION BY managers.employee_id ORDER BY count(*) DESC)
If you made this change and ran it right away, you'd be yelled at for referencing a field that isn't accounted for in our statements
GROUP BY
clause. I'll make that change now, and the resulting query should transform into:
SELECT
        CASE
            WHEN employees.reports_to IS NULL THEN
                 '(None)'
            ELSE TRIM(CONCAT(managers.first_name, ' ', managers.last_name))
            END AS manager,
        CONCAT(employees.first_name, ' ', employees.last_name) AS salesperson,
        count(*) AS count_of,
        DENSE_RANK() OVER (PARTITION BY managers.employee_id ORDER BY count(*) DESC)
    FROM orders
        INNER JOIN employees
            ON orders.employee_id = employees.employee_id
        LEFT JOIN employees managers
            ON managers.employee_id = employees.reports_to
    GROUP BY
        employees.first_name,
        employees.reports_to,
        employees.last_name,
        managers.employee_id,
        managers.first_name,
        managers.last_name
    ORDER BY
        managers.first_name,
        managers.last_name,
        employees.first_name,
        employees.last_name;
Not everything's a contest in life
...and not every window function is a
DENSE_RANK
. As mentioned earlier, there's also the commonly used
ROW_NUMBER
, which works similarly to
DENSE_RANK
except that it doesn't reuse the same number - if two of those salespeople had shared the same count of sales, they would still have been ranked differently were we to swap out one function for the other.
So far, everything you've seen is based on what PostgreSQL can do. Window function support may differ from one platform to the next, and some may even be vendor-specific. You can see the list of Postgres-supported window functions in their
docs
.
BONUS
: Also in the same vein, but not exactly a traditional window function, is the aggregate function
ARRAY_AGG
, which will effectively turn a column of data into a list value:
SELECT
        CASE
            WHEN employees.reports_to IS NULL THEN
                 '(None)'
            ELSE TRIM(CONCAT(managers.first_name, ' ', managers.last_name))
            END AS manager,
        CONCAT(employees.first_name, ' ', employees.last_name) AS salesperson,
        count(*) AS count_of,
        ARRAY_AGG(DISTINCT orders.customer_id) AS customers
    FROM orders
        INNER JOIN employees
            ON orders.employee_id = employees.employee_id
        LEFT JOIN employees managers
            ON managers.employee_id = employees.reports_to
    GROUP BY
        employees.first_name,
        employees.reports_to,
        employees.last_name,
        managers.employee_id,
        managers.first_name,
        managers.last_name
    ORDER BY
        managers.first_name,
        managers.last_name,
        employees.first_name,
        employees.last_name;
Last thoughts
Hopefully, it's clear at this point that you can save a LOT of work by making good use of window functions where appropriate. It gives power to SQL developers that have in the past only been available either through great effort or would have been handled in post-processing by developers of downstream applications.
Because they're stateful, there is a bit more work for the query planner to handle, but if that's important to you then you should benchmark for yourself the various ways you could solve such problems.
All things considered, this is a powerful tool and one that should hopefully bring both brevity and clarity to your SQL.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
143951840.on-call-hell,On Call Hell,"Hello, Hello ... is anyone there?",2024-05-27 09:09:37.279-05,"It‚Äôs something I will probably never forget, dragging that stupid phone around with me everywhere I went. Being on edge, dang thing usually went off about 11:30 PM. Horrible. Not a way to live.
I still remember saying ‚ÄúForget this,‚Äù and going to a concert. Got in trouble the next day for not answering. I thumbed my nose at them and got away scot-free.
On call. The dreaded bane and destroyer of Engineers and Developers since God said let there be light. Let there be ‚ÄúOn Call,‚Äù and it was so. And may Engineers by the sweat of their brows plow the dusty earth of code at midnight. Our penance.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
On-Call in real life.
This is an interesting topic in the context of tech folk. Everyone has had their own experiences with on-call, good, bad, and some in between. There are various types of on-call, cultures, and times, so no two on-call rotations are alike in most companies. Startups to corporate giants, all trying to suck the life out of us.
Yet, even with that, it is universally despised.
Why? Why do we hate it so much? I mean we techies are lampooned as being overpaid buggers, so what‚Äôs the big deal about being on call every once in a while?
We will get to that later, but for now, let‚Äôs talk about what types of on-call lie around in the shadows waiting to jump on you.
I‚Äôve experienced every type of on-call duty over the years, as I suppose many of you have as well. But, I think that is largely dependent on the type of company you work for, probably the size and the products you support.
Some companies take it seriously, and some don‚Äôt, for various reasons. Like it or not, the style and type of on-call Engineers deal with has a MAJOR impact on work-life balance, attitudes, career, and team dynamics.
First, let‚Äôs talk about on-call types.
Always or never on-call
Semi-structured (ex. on during week, off weekends)
Super-structured (defined days and times, rotation)
Each of these types has its ups and downs, the good and the bad, mostly dependent on the culture and size of the company.
Always On
Every Engineer is different, some enjoy corporate life, big companies that are more structured and slow-moving. Some people like startup life, fast-paced and always moving.
The ‚Äú
Always On
‚Äù is probably the most dangerous type of on-call that leads to one of two outcomes.
Extreme burnout and dissatisfaction.
The easy life, no problems.
How can you have two extremes for this type of always-on or off with zero definition or expectations? It‚Äôs completely dependent on the architecture and codebase, how stable and well it‚Äôs run.
This is generally true for all types of on-call, but especially in the case where nothing is defined. If the systems run well and rarely break, then it doesn‚Äôt matter ‚Ä¶ because no one ever needs to be called.
I‚Äôve been a victim myself of both of these in this category. Working at startups where everything breaks all the time leads to bad attitudes and burnout, it‚Äôs unavoidable, you can‚Äôt have a life if you work every weekend and weeknight.
Semi-structured
Semi-structured is probably the most common type of on-call for many teams. This is common in medium-sized companies with Dev and IT teams with a handful of people.
Unlike in the zero-structure environments, semi-structured usually have various rules or expectations that are at least somewhat defined depending on the business ‚Ä¶
Weekends are off-limits (zero support unless something is burning down)
Weekdays are on within reason, say until 10 p.m.
There is no ‚Äúrotation‚Äù simply because the team sizes don‚Äôt support it.
Again, this can be good or bad depending on the system. But, many times this can be the best situation to be in if the systems are good. Not having to worry about being on-call during the weekends and lead to a better work-life balance. As long as the nights during the weekend only happen sporadically, life is good.
Very Structured
The last and another common type of on-call rotation is super structured. This is most common on very large teams and large companies for the most part. They can have thousands or more of employees and operations can be considered critical.
There is an extreme level of expectations and escalations. You can expect
‚Ä¶
semi-frequent on-call rotations (every few weeks if you‚Äôre lucky, or one day a week, including weekends)
On-call can mean all night long.
It‚Äôs expected and can be soul-sucking.
Based on conversations with many developers, you get opinions on one of two extreme ends. Either people love it, because it‚Äôs defined and they know what to expect, or it‚Äôs horrible and sucks their soul because it‚Äôs grueling.
Of course, this is really more about company culture than it is about the on-call itself. If you work at a place that does not value developers and treats them as commodities ‚Ä¶ the on-call rotations can be horrific. Long hours, all night long, no break, lots of calls.
Take this post from Reddit for example
. This is not uncommon.
Or there can be clearly defined on-calls, that have great outcomes that people love,
like this post from hackernews.
What can we learn from these two opposite stories? Both have defined on call, one person is being eaten alive, the other is happy and people are jumping at the chance.
Culture. Being on-call is more about the culture you already exist in, rather than the nuances of the on-call itself, within reason.
Me? I value independence and choice. I want to be in control, it‚Äôs just my personality. When I had to be on call with a phone that could ring anytime and I couldn‚Äôt go out with my friends to a concert on a Friday night ‚Ä¶
that made me angry even if I didn‚Äôt get a call!!
I personally like the approach that I deal with today. Startup where I‚Äôm always on call.
Do you know why?
Because I‚Äôm at the mercy of the systems I build. I have skin in the game. If I build good and reliable systems ‚Ä¶ I might have to work on a weekend once or twice a year.
I like this approach because I‚Äôve built reliable data systems that don‚Äôt break often ‚Ä¶ and I get to reap the rewards of that hard work. Good work-life balance and no late nights or weekends.
Maybe others would not like this though. Again, as we read above, some teams have good systems and they get paid to be on call, so you have people jumping at the chance because they
feel
like they are in control.
What has been your experience? What do you prefer and why? Leave a comment and let‚Äôs talk about it!
Leave a comment"
143855364.data-engineering-survey,Data Engineering Survey,Whatcha Been Up To???,2024-04-23 09:34:44.848-05,"I think this could mostly be interesting for all Data Engineers. No marketing fluff. No vendor funny stuff. Just plain old Data Engineering questions so we can all get the lay of the land.
Do it. Do it now.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
143530190.saas-vendor-lock-in,SaaS Vendor Lock In,A Tale of Caution,2024-04-18 07:51:20.213-05,"Something happened the other day that gave me pause. It shook me from the Data Engineering stupor I was languishing in, a kick in the pants to wake me up. It was about time for some excitement.
Today we are going to talk about the recent decision by Databricks to rid themselves of the Standard Tier and force the move into Unity Catalog.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
First, let it be known I‚Äôm a huge Databricks and Delta Lake fan. I‚Äôve written many wonderful things on those topics over the years. That doesn‚Äôt mean I‚Äôm not going to bring you The Truth as I observe it.
Above you can see the email that Databricks sent announcing the removal of Standard Tier.
Vendor Lock-in and a tale of caution.
Let‚Äôs talk about Vendor Lock-in and use this current example of a company doing what they want, how it affects users, and how we, as Data Engineers, should view and prepare for such things.
This tale of Databricks getting rid of Standard Tier, (and thereby doubling costs of some workloads)
blew up on Reddit
when I posted about it. I didn‚Äôt foresee the firestorm coming (all the comments, clearly this is a topic people care about),
AND Databricks hunting me down at work to chastise me
.
I do hope you poor souls appreciate the beating I take to bring raw and unedited content, not moderated by the Modern Data Stack Gods that overwatch us.
It doesn‚Äôt really matter what side you fall on in this particular debate.
What matters is that we recognize what underlies changes like this and how they affect real data teams in real ways!
Nothing is free in this life.
When it comes to vendor changes like this, whatever the case may be, there are multiple considerations to take into account, including, but not limited to ‚Ä¶ the straight ‚Äú
cost of the product.
‚Äù
The cost for Engineering to migrate to the new tool, features, whatever.
The cost to learn and upskill a team to the new tool, features, whatever.
Making other technology upgrades to integrate with the new tool, features, whatever.
The actual product cost of X thing.
When someone, like Databricks, decides to drop say Standard Tier and force a migration to Unity Catalog (what is underlying this change.), you can‚Äôt simply think about ‚Ä¶ say ‚Ä¶ the doubling of Job Compute costs.
You have to think about ALL the other work that must take place for someone to swap, in this case from Standard Tier to Unity Catalog.
As Data Engineers who build Data Platforms, pipelines, and tooling, we would be remiss if we didn‚Äôt at least
think
about vendor lock-in, what it could mean, and the potential impact it could have.
The Unity Catalog Example.
Let‚Äôs use this specific example of being forced to migrate to Unity Catalog. (I speak as someone who has literally planned and executed a Databricks migration from Standard to Unity Catalog.)
Here is what, at a minimum, I had to do ‚Ä¶
Stand up new Databricks Account (so as to not affect current production)
Read copious amounts of Documentation for weeks.
Stand up new clones of Dev and Production attached to Unity Catalog for testing.
Migrate mass amounts of data.
Migrate mass amounts of data pipelines.
Test every pipeline and data set works in the new system.
Reconfigure orchestration tools and configuration.
Re-do new CI/CD processes to deal with the new Unity Catalog etc.
Train and upskill other team members to use the Unity Catalog.
Set up AWS resources to support Unity Catalog.
Setup and Design new concepts like Catalogs, Groups, Workspaces, Principals, and Object Permissions that are specific to Unity Catalog.
Test all of the above.
Does this sound like nothing to you? This is the reality of being locked into a Vendor, that Vendor deciding to do something as simple as ‚Äúyou must upgrade to this thing.‚Äù
We haven‚Äôt even talked about the actual underlying billing and cost increases!!!
Things are never what they seem.
It‚Äôs funny. In the before-mentioned
Reddit
blowup, you will find no mention of the actual underlying challenges related to a Data Team having to go through the upgrade process and address all the above issues I mentioned.
Everyone focuses on ‚Ä¶
The obvious cost implications
The fact that Standard Tier sucks
Any number of other pointless things.
Any Senior Level+ Engineer knows that these things are just the tip of the iceberg. The real challenge comes with doing a seamless upgrade and migration to the ‚Äúnew thing‚Äù without any downtime while upskilling, training, and learning an entire ‚Äú
new thing.
‚Äù
This is a tale of caution for all you Data Engineers who are in charge of designing Data Platforms, choosing tools, and the long-term health of the data systems.
Anything can happen at any time. Are you prepared for that? Do you design your systems in such a way that you can easily do such an upgrade when necessary with minimal problems?
It is at times like these when the proverbial rocks come out of the water ‚Ä¶ when the tide goes out.
If you want to hear me rant more on this topic, watch the below video
on my YouTube channel
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
143358799.spark-connect-what-is-this-madness,Spark Connect - What is this madness?,Using Rust to write Spark? Lord save us.,2024-04-09 07:52:33.255-05,"I wasn‚Äôt sure if my poor old eyes were deceiving me or not, but yet, there it was. After years and decades of doing data work, my heart has become cold and my fingers feel wizened and dry as I leaf through the endless supply of digital new snake oils being hawked on the street corners.
But I could not deny this miracle elixir, Spark Connect, like a fly to the light, even if it was my death. There it was, in my face, staring back at me like some mad old crazed person ‚Ä¶ saying ‚Äú
Come hither my son, feast upon my delectable delights.
‚Äù
Spark Connect ‚Ä¶ behold it has come to save the world.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
I‚Äôm still unsure why there hasn‚Äôt been more fanfare, general horn-blowing, and shouting from the rooftops about Spark Connect. It seems as of
Apache Spark 3.4, Spark Connect
was quietly released to the mindless masses without much to do.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I happen to think it‚Äôs one of the greatest moves Apache Spark could have made to keep itself relevant and on top of the hill as Lord of us all.
With the advent of tools like
Polars
and
Datafusion
, and the ever-present threat they could be distributed in the near future, it‚Äôs as important as ever for Spark as the defacto tool to keep invocation on the move.
What, pray tell is Apache Spark Connect?
‚Äú‚Ä¶
a decoupled client-server architecture that allows remote connectivity to Spark clusters using the DataFrame API ‚Ä¶
‚Äù
and
‚ÄúIt is a thin API that can be embedded everywhere: in application servers, IDEs, notebooks, and programming languages ‚Ä¶‚Äù
What does this mean to the average Data Engineer? It means one could take Golang, Rust, Python, whatever, from wherever, and connect to a Spark Cluster, do a thing, and get results back.
It‚Äôs hard to express how big of a deal this is in the realm of opening the floodgates of possible other innovations of toolsets and products based on Spark.
Lest you still not understand what‚Äôs going on with Spark Connect ‚Ä¶
How Spark results are typically dealt with.
If you work in the land of Spark for any period of time, building pipelines and datasets for use by downstream applications and business units, one thing is taken for granted that must be done.
We are constantly writing results sets up to intermediate storage solutions and ingesting said results into other systems like Postgres, and MySQL, or simply more munging and massaging with tools like Python, etc.
We write results to ‚Ä¶
Parquet files
CSV files
Delta Lake tables
etc.
Of course, there are some tools that allow us to dump Dataframes directly to Postgres, etc, but they‚Äôve always been a little rickety.
The main ‚Äústicking point‚Äù has always been the large wall of JVM that has been built up around Spark, blocking off easy access from the rest of the world.
This is Spark Connect, my friend. It busts down that wall. Let‚Äôs prove the point.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Writing an Apache Spark pipeline with Rust.
So because I can, I will, that‚Äôs my way in life. Let‚Äôs remove all doubt about this 7th Wonder of the World, Spark Connect, and give it a try ourselves.
We will do this by installing a Spark Cluster of our own, on a remote server in the cloud, then connect to it with Rust and get some results.
Step 1:
Create a remote server. (
I use
Linode
and Ubuntu shared instances
).
Install crap like Java, UFW, etc, etc.
Step 2:
Harden access to the server via things like UFW and fail2ban etc. Only allow access from whitelisted IP address(s) of your choice etc.
root@localhost:~# sudo ufw status
Status: active

To                         Action      From
--                         ------      ----
22/tcp                     ALLOW       Anywhere                  
Anywhere                   ALLOW       217.180.228.XXX           
22/tcp (v6)                ALLOW       Anywhere (v6)
Step 3:
Get and install Apache Spark 3.4.0 onto the server and start the Spark cluster.
>> wget https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
>> tar xvf spark-3.4.0-bin-hadoop3.tgz
>> sudo mv spark-3.4.0-bin-hadoop3 spark
Get the cluster up and running ‚Ä¶
root@localhost:~# ls
spark  spark-3.4.0-bin-hadoop3.tgz
root@localhost:~# cd spark
root@localhost:~/spark# ./sbin/start-master.sh
starting org.apache.spark.deploy.master.Master, logging to /root/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-localhost.out
I can see the Cluster is up and running from my local machine by hitting the IP address of my remote machine + port 8080.
Also, we need to start the Spark Connect server.
./sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.4.0
Step 4:
Dude, let‚Äôs write some Rust Spark! Boy, never thought I would say those words together. Amazing.
Luckily, there is
spark_connect_rs
waiting for us. So let‚Äôs set up a new Rust project and give this a try.
cargo new rust-test-spark
cargo add spark_connect_rs
cargo add tokio
Let‚Äôs also create a small `CSV` file on our Spark server, and see if we can read it from Rust.
mkdir /data
vim /data/test.csv
Here is my Rust code.
use spark_connect_rs::{SparkSession, SparkSessionBuilder};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {

    let spark: SparkSession = SparkSessionBuilder::remote(""sc://172.233.217.239:15002/"")
        .build()
        .await?;

let df = spark
    .sql(""SELECT * FROM csv.`/data/test.csv`"")
    .await?;

    df.show(Some(5), None, None).await?;
    Ok(())
}
Trying to build and run this problem fails.
error: failed to run custom build command for `aws-lc-sys v0.14.1`

Caused by:
  process didn't exit successfully: `/Users/danielbeach/code/rust-test-spark/target/release/build/aws-lc-sys-be37136d54f41db2/build-script-main` (exit status: 101)
  --- stdout
  cargo:rerun-if-env-changed=AWS_LC_SYS_INTERNAL_NO_PREFIX
  cargo:rerun-if-env-changed=AWS_LC_RUST_INTERNAL_BINDGEN
  cargo:rustc-cfg=aarch64_apple_darwin
  cargo:rerun-if-env-changed=AWS_LC_SYS_STATIC

  --- stderr
  Missing dependency: cmake
After doing a `
brew install cmake
` on my local Mac (where trying to build the Rust), I got past that issue.
Good Lord, it worked!
`
cargo run ‚Äîrelease
`
danielbeach@Daniels-MacBook-Pro rust-test-spark % cargo run --release
   Compiling rust-test-spark v0.1.0 (/Users/danielbeach/code/rust-test-spark)
    Finished release [optimized] target(s) in 12.66s
     Running `target/release/rust-test-spark`
+--------------------+
| show_string        |
+--------------------+
| +---+-------+----+ |
| |_c0|_c1    |_c2 | |
| +---+-------+----+ |
| |id |name   |null| |
| |1  |billbo |null| |
| |2  |gandalf|null| |
| |3  |samwise|null| |
| +---+-------+----+ |
|                    |
+--------------------+
That is quite amazing if I do say so myself. I used Rust on my local machine to connect to and run Spark commands on a remote Spark Cluster ‚Ä¶ and return the results.
What‚Äôs next, flying pigs?
Is a new era of ETL and Data Engineering upon us?
This is another one of those big turning points that makes me wonder if people have the wherewithal to take advantage of what‚Äôs laid before them.
Think about it. Could it be the dawn of a new era of writing data pipelines and ETL ‚Ä¶ against massive Spark Clusters in the cloud ‚Ä¶ all from the safety of your Python, Golang, or Rust code?
You have to admit, it does open up a new horizon of vast possibilities for the expansion of Apache Spark into other verticals and new, powerful ways of creating Data Apps powered by Spark Connect.
What does the future hold? What new tools and apps will the open-source and other companies build on top of Spark Connect? Will it really change the way we do Data Engineering with Apache Spark?
One can only hope.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
143244357.delta-lake-map-and-array-data-types,Delta Lake - Map and Array data types,Hidden Gems?,2024-04-22 08:49:48.865-05,"If you‚Äôre anything like me after years of the same old same old, you can get lulled into a dull stupor of SQL and Data Types that never seem to end.
¬ª String
¬ª Int
¬ª String
¬ª ohhh ‚Ä¶ look at that a FLOAT!
We wizened old Data Engineers who‚Äôve been encrusting ourselves with the same old Data Types since the days when SQL Server and Oracle ruled the world get a little bleary-eyed when it comes to something new.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
We think a FLOAT is exciting. Wait until MAP and ARRAY data types hit the scene. It‚Äôs like a high school keg party behind the old oak tree at midnight. Things are about to get crazy.
Delta Lake
is one of those amazing and interesting tools that people don‚Äôt appreciate enough. 15 years ago when were all using Postgres or storing Parquet files on s3, who would have thought that something like Delta Lake could exist?
ACID, CRUD, and SQL, all through an API ontop of Parquet files in the cloud. Amazing. Combining Delta Lake with Dataframe libraries like PySpark or Polars has become the new standard of Lake House and even Data Warehouse modeling.
In an effort to expand the horizons for you Data Engineers who are stuck in the doldrums, today we will take a look at two Data Types available in Delta Lake, the lesser-worn road that only the brave dare walk.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Array and Map Data Types
Let‚Äôs talk about these two underused Data Types and see what we think.
ARRAY
-
Represents values comprising a sequence of elements with the type of
elementType
.
MAP
-
Represents values comprising a set of key-value pairs.
Most people are probably more familiar with the concept of Arrays and Maps from programming, not so much
storage data types
. I think that creativity is one of those rare traits that make for really great Engineers. The ability to think outside the box, being able to see the solutions that are not that obvious.
This is probably why you don‚Äôt run into Map and Array much in the wild. They are simply forgotten. There is no creativity when examining problems and finding solutions.
Let‚Äôs look at a quick example of what these data types look like, just to get an idea.
>> SELECT ARRAY(""Old"", ""Grandma"", ""Huckleberry"") as WhosYourGrandma;
[""Old"", ""Grandma"", ""Huckleberry""]
Ok, so that probably looks familiar to most programmers, the ‚Äúlist‚Äù type syntax of `
[]
`.
I mean, how else does it act like an Array?
>> SELECT ARRAY(""Old"", ""Grandma"", ""Huckleberry"")[0] as WhosYourGrandma;
Old
Interesting, I guess with can slice and dice with indexes as we would expect. What else?
>> SELECT EXPLODE(ARRAY(""Old"", ""Grandma"", ""Huckleberry"")) as WhosYourGrandma;

WhosYourGrandma
1 Old
2 Grandma
3 Huckleberry
Not bad. Pretty creative for messing around in SQL uh?
What about Maps?
>> SELECT map('Hobbits', 'Shire', 'Orc', 'Mordor');
{""Hobbits"": ""Shire"", ""Orc"": ""Mordor""}
All right, all right, the classic dictionary.
>> SELECT lotr['Hobbits']  FROM VALUES(map('Hobbits', 'Shire', 'Orc', 'Mordor')) AS T(lotr);
Shire
Not bad. Seems like it could get complicated quickly. Sometimes you need complicated though.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Real World Stuff
All this begs the question of how we could use Array and Map in the real world of Data Lakes for Data Engineering. Of course, this wouldn‚Äôt be the ‚Äútypical‚Äù everyday use case that is for sure, but where it is usually, it‚Äôs probably a great innovation.
When we talk about Data Types we should always be talking about Data Modeling.
Probably, most of the use cases for
Array
and
Map
data types come from two sources ‚Ä¶
The handling of semi-structured data
Being creative at the
TB+
level with access and storing data
If you‚Äôre just used to playing with
CSV
and tabular data, most likely you don‚Äôt have much use case for these data types. But, if you use or ingest semi-structured data like
JSON
, or you have many TBs+ of data,
you need to be ‚Äúcreative‚Äù with Data Modeling because it matters at scale
.
For example, say we are ingesting JSON-type values into a Data Lake ‚Ä¶
customer_id | customer_name | customer_address_1_2
   325234       Billdad        {'address_1': '123 Boring Street', 'address_2: '456 Ding Dong Road'}
This can be for simplicity's sake, keeping data close together. It‚Äôs just another way to view and interact with the data.
customer_id | customer_name | last_6_monthly_sales
   325234      Billdad          [500, 800, 700, 200, 100, 350]
For example, in the above picture, you will note that the last 6 months of sales are stored ALONG WITH the customer record, this can be extremely helpful at scale.
It may not make sense if you‚Äôre storing data in Postgres, but when you have 300TBs of data in a data lake, expressive data modeling like this can have a major impact on analysis and analytics, down the bottom line, saving tons of compute and money over the long term.
For example, read this excerpt
from Zach Wilson
, the Staff Data Engineer and very popular creator.
‚ÄúARRAY and STRUCT data types can be a slam dunk for representing very high cardinality data in a compact way. Imagine you needed to represent the next 90 days of predicted prices.
You could have a schema like:
id BIGINT
predicted_price DECIMAL
probability_of_purchase DECIMAL
predict_date STRING
ds STRING
The problem with this is you‚Äôll have a lot of rows as the data grows (90 X number of distinct ids). If that number is in the tens of millions to billions, this schema breaks down since you‚Äôd have 90 billion records if you have a billion ids.
Imagine an efficient schema like this
id BIGINT
predicted_prices_next_90d ARRAY(STRUCT(probability_of_purchase DECIMAL, price DECIMAL))
ds STRING
The array index of predicted_prices_next_90d is the number of days after ds the prediction is for. If you use this schema instead you have 1 billion records and the data volume is dramatically less.
Definitely check stuff like this out. I think it‚Äôs the future of data modeling.‚Äù
Zach Wilson
This example shows perfectly the power of out-of-the-box thinking and data modeling with Array and Map type objects.
Learning important lessons.
It‚Äôs important to recognize we can all get caught up in the day-to-day cycle of building and shipping pipelines and data projects. We use the same old data types year in and year out.
We forget that these powerful new tools like Delta Lake have a lot of hidden gems hiding right under the surface. We should also recognize, as Data Engineers, we have the responsibility to slow down and think critically about the things are are building.
It‚Äôs important to at the very least be well-versed and knowledgeable with the tools we use. There are a lot of people using Databricks and Delta Lake to build data products at scale.
The number of those people using Array or Map data types is probably so small it‚Äôs like
Horton Hears a Who
.
Just remember, be creative, think about the best way to build out solutions, and consider Map and Array next time.
Have you used these days types before? In what way? Please share your experience below in a comment and help others learn and grow.
Leave a comment"
143244348.sql-vs-python-data-pipelines,SQL vs Python Data Pipelines,Data Engineering Decisions,2024-04-15 08:29:30.876-05,"Ever since the days of yore, SQL has been the bread and butter of everyone from the old school DBAs to the new school Data Engineers. Of course, Python is probably the bread bag if SQL is the sandwich, or is it the other way around?
There have been many attempts to unseat both SQL and Python over the years ‚Ä¶ Scala, Golang, and Rust have all been calling for the death of Python. GraphQL and REST APIs eliminate the need for SQL. Yeah right, dream on.
So, if we are doomed to use SQL and Python forever, then that begs a different question ‚Ä¶
When should you use Python and when should you use SQL in your data pipelines?
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
To SQL or not to SQL, or should I use Python?
I imagine the decision of when and where to use Python and SQL probably starts out very straightforward in the beginning. However as time starts to pass the the codebase grows, and things probably start to become less clear.
At some point, every Data Team is probably faced with a crisis of identity. Are we a SQL shop? Are we a code shop? WHAT ARE WE??!!
We probably start with good intentions, trying to drive that car straight down the center of the road, but over time as we get busy writing code and adding new features things start to warp. Maybe it even depends on the person doing the work that day.
What happens is you wake up a few laters and realize that ‚Ä¶
My code base is %80+ SQL
My code base is %80+ Python.
I mean we could simply ask the question ‚Ä¶ is there a right and wrong answer here?? Probably there is.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Striking the correct balance.
I‚Äôm a fan of all things in moderation. I think this applies to the question of Python or SQL for data pipelines. The answer should be both, use the right tool at the right time, and stay away from the slippery slope where you become a one-show pony.
This idea of getting sucked in the ditch of using too much of a single tool is bad for the business, the developers, and the culture.
You want to be flexible, able to have good data pipelines that are easy to use, and put feature additions on without incurring mountains of technical debt. That requires making good decisions upfront.
The problem is, if you hitch your wagon to either SQL or Python wholesale you will eventually find yourself an inflexible mess on your hands.
The difference between a matter of taste and good technical decisions.
Instead of pontificating more, let‚Äôs actually look at real-life code and examples. How can we make accurate decisions that are good technical choices from an engineering perspective, and which choices are merely a matter of taste?
It can be hard to summarize into specific rules, so it probably goes on a case-by-case basis.
PySpark vs SparkSQL example.
A great example and very popular today based on the Databricks platform is the use of
SparkSQL vs PySpark Dataframe API
. The two code bases would look completely different, wouldn‚Äôt they?
And what does the same code look like in SQL?
I mean what can we say about these two pipelines concretely? They both have functions that are fairly small and straightforward to unit test, which is key to a good codebase.
Is one more readable than the other? That‚Äôs probably just a matter of taste.
I suppose it‚Äôs worth noting that the Python code in the first example is ‚Äúbroken‚Äù up slightly more than the SQL code. As in there are two functions getting the entire job one, where with the SQL code this is all rolled into a single SQL statement.
There is good and bad to that.
Most
good
engineers agree that having less code overall reduces bugs etc.
Most good engineers agree rolling too much logic into a single place is probably not a good idea.
So again, we come to an impasse. Technically the problems above could exist in either codebase, it really has to do with the team of developers working on the code.
But, is the coagulation of large pieces of code into giant monstroties more common in SQL or Python? I will leave that to you smart folk to answer.
Thinking about the work being done ‚Ä¶
I think more or less, the decision to use more Python or SQL really comes down most of the time to what type of data you are working on, and the requirements of the business that probably drive the inherent complexity of the tasks.
If you‚Äôre working with tabular data either in an RDBMS or that comes from an RDBMS, with fairly normal business requirements and analytics ‚Ä¶ probably makes sense to stick with SQL.
If you‚Äôre working with semi-structured data or even tabular data that require complex transformations, anything in the ML space, APIs, or extreme reliance on testing for mission-critical data ‚Ä¶ you might be better off with Python.
At this point, some people might be asking ‚ÄúWhy don‚Äôt you just look to the strengths of the data team, what they are comfortable with?‚Äù
Well, that is a slippery slope. We should let the technical requirements and good sound engineering decisions drive the tools we pick for our pipelines ‚Ä¶ not ‚Äúconvenience‚Äù or ‚Äúbecause I‚Äôm used to this more.‚Äù That‚Äôs how bad things start to happen.
Just because you
COULD
do something in SQL or Python doesn‚Äôt mean you
SHOULD
.
Think about it ‚Ä¶
If you have two large datasets in CSV format in s3 ‚Ä¶ one is `sales` information, the other is `customer` information and you get asked to build out a dataset that is the combination of these two datasets with some rolled-up analysis done ‚Ä¶
Should you pick Python?
Of course not!
This problem is best modeled and solved with SQL. It‚Äôs what SQL was made for ‚Ä¶ joins and rollups. Therefore this solution in SQL will be the least amount of code, easiest to understand, and the best solution long term.
Let‚Äôs say we are tasked with doing some feature engineering for a new ML model. Why in the world would you choose SQL? Feature engineering is an intricate problem with different transformations for different features, each requiring multiple unit tests to verify accuracy, etc.
Go with Python of course.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
What about the stuff that isn‚Äôt so obvious?
Of course, every decision isn‚Äôt this easy. Sometimes is the thin grey lines that give us the most trouble. My suggestion is just to be a good engineer.
Examine the problem closely, maybe do a POC, what
feels
better, SQL or Python?
Look at your tech stack closely, does one solution fit in better than the other?
Examine your codebase, is it becoming too lopsided? Have you crossed the line of too much of a good thing?
I think it‚Äôs extremely important as Data Engineers we overcome our biases and tendencies to be comfortable. We should challenge ourselves to make good engineering decisions while thinking critically about the problem and proposed solution.
Let‚Äôs take a moment to speak out loud about the obvious pitfalls of each approach. Honesty is good practice.
What to be careful about with SQL pipelines.
Since SQL is probably the most common tool used for most Data Engineering pipelines, we should give a warning about the pitfalls that should be avoided when you find yourself slipping into the deep side of the pool.
Allowing spaghetti SQL queries to get out of control.
Not having any of the SQL queries tested (unit testing).
Allowing the same business logic to get ‚Äúspread around.‚Äù
Not having good standards (dbt, etc.) on HOW you expect your SQL to be written.
Not writing idempotent data pipelines and queries.
When we become comfortable with anything, it is easy to let stuff go bad. Anyone who‚Äôs been around the block a time or two knows that out-of-control SQL queries are very commonplace.
Too much logic in one place, spaghetti SQL queries, and the same logic spread around and not being reused. These are very common issues that lead to serious tech debt and will set the stage for the degradation of the data platform as a whole.
What to be careful about with Python pipelines.
SQL isn‚Äôt the only one with the problems, Python data pipelines have their own set of problems and gotchas that seak up on the best of engineers.
Not having unit tests to cover logic.
Not following a coding standard (code looks like anything and everything)
Not writing clean and functional code (methods with 50+ lines of code).
Not managing dependencies well.
Not having a good development environment.
One of the hardest parts about coming into a Python-centric data platform is that you never know what you are going to find. When it comes to SQL, it might be dirty, but you will figure it out. But, when it comes to Python, it can get insanely dirty and impossible to debug quickly if ‚Äú
good clean code practices
‚Äù are not followed.
Closing Thoughts
When it comes to SQL or Python for data pipelines, I vote for both. Both in the right place at the right time. Also, I think it‚Äôs easier than most people think to know when to use what.
Many times the problem being solved often dictates what tool to use. It‚Äôs also important for teams to understand which way their biases lie and to make sure they don‚Äôt fall into the ditch on one side.
I‚Äôve seen my fair share of horrible SQL and Python repositories. I‚Äôm convinced it isn‚Äôt about which tool you use, but HOW you decide to approach using that tool. It‚Äôs about the Engineers who make choices every day as to what their solutions will look like.
You can write clean SQL and Python data pipelines, you can also write very bad ones."
143168269.my-saas-is-faster-than-yours,My SaaS is faster than yours.,Modern Data Stack Overload.,2024-04-29 08:38:20.366-05,"I‚Äôm not sure how else to say it, without sounding a little jaded. Well, maybe I‚Äôm just old and grumpy. I‚Äôm ok with that. As most of you know, I don‚Äôt tend to pull my punches, I try to give you the lay of the land as I see it.
An unbiased opinion if there is one, although I think I‚Äôm probably as biased as they come when it comes to technology and data. Today will be no different. Today we take on the peddlers of the Modern Data Stack SaaS world.
I saw we put ‚Äòem in the ring, make ‚Äòem fight for it. Too long did they ride the coattails of easy money, low interest, and general software boom times.
The world was bound to change. Layoffs, cost cutting, people actually caring about the bottom dollar and ROI. We knew it was coming.
What‚Äôs the best part?
All the MDS SaaS prophets are going to have to get in the Thunder Dome and fight it out. Only the strong will win. So be it.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Enough with the faster thing.
This past Friday as I was doom-scrolling Linkedin looking for something besides the same old influencers talking about the same old things, I ran across yet another MDS prophet spreading the good Word sent to save us from our misery.
My heart sang, my spirit lept within me, and I danced and shouted with joy. Hooray! We are saved, another ‚Äú
fast SQL query engine
‚Äù comes to save us!
Yeah, not. Seriously?
Do People Still Fall For This?
I can‚Äôt imagine the old guard of Data Engineering actually falling for this, I can‚Äôt imagine what would possess anyone to think that the way they will capture the hearts and minds of Data Engineers scattered over this broken land would be to stand on a hill and shout ‚Äú
We have the fastest tool!
‚Äù
This isn‚Äôt what makes a tool good or bad anymore. We are passed that.
You don‚Äôt have tool that is fast? Then it won‚Äôt survive. We simply expect reasonable speed from our tools. Nothing annoys a Dev like an old slow tool. That in itself will cause the sound of crying and wailing to reach the stars.
When Senior, Staff, or whatever level Engineers are working on a new project, thinking about implementing a new solution to solve a problem ‚Ä¶ here is what they don‚Äôt do.
‚ÄúGoogle, tell me what is the fastest tool to do X job. Oh wow! Look, this new tool runs this benchmark (that was most likely messed with) about 50ms faster than tool Y. Let‚Äôs do it.
I can‚Äôt think of a worse way to make a decision.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
What Engineers SHOULD care about.
Sure, give us a fast tool. But don‚Äôt make that the crux of your sales pitch or marketing plan.
We expect more.
Tools with the best developer experience.
Excellence documentation.
Solves REAL problems in a novel way.
Has a thriving community of users.
Is fairly painless to implement.
Lots of integrations and extensions with popular ‚Äúthings‚Äù (think Snowflake, Databricks, Postgres, AWS, blah, blah).
Has a clear cost structure.
Is reliable.
Just to name a few. Of course, we don‚Äôt want it to be slow. Why would we? 99.9% of Engineers aren‚Äôt looking for a solution that is 30ms faster than something else.
We have to be practical. We use the tools every day and others are relying on us to make informed decisions that will benefit the future us.
Enough of the my tool is the fastest. What is this?
The middle school playground or neighborhood?
Give me a break."
143129658.easter-sale-50-off,Easter Sale - %50 Off!,Everybody needs a little something for Easter,2024-03-31 11:02:10.768-05,"Happy Easter to you all. I hope your day is filled with food and family.
%50 Off for a year
for all of you needing a little pick-me-up today.
https://dataengineeringcentral.substack.com/4af837fd"
142940575.how-to-build-an-open-source-python,How to Build an Open Source Python Package,Step by step,2024-04-08 07:57:48.055-05,"Have you ever wondered what it‚Äôs like to work on open-source software? Wanted to try it out but can‚Äôt find a good place to start. It can be intimidating when looking for your first open-source contribution. Almost impossible to find the perfect package that will give you something more than an ‚Äúupdate this doc‚Äù experience.
Why not kill a few birds with one stone?
What about making your very own open-source baby? Something that is useful and through which you learn a bunch of new skills that will take away the fears about contributing to open source?
All it takes is a little creativity and elbow grease.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Making your own open-source Python package.
Today I‚Äôm going to show you how utterly easy it is to create and work with open-source packages with Python. It‚Äôs so easy that anyone can do it.
And, it comes with a bunch of side benefits.
You learn about Python packaging.
You get to be creative.
You get to practice thinking about users.
You learn how to build and deploy Python packages to PyPi.
You pick up skills around build and dependency tools like Poetry.
You get to add something REAL to your resume.
If I could wind the gear back on the clock of time and reverse myself to when I was a young developer, green under the ears, running around writing code and taking names ‚Ä¶ I wish I had started making my own open-source packages and contributing to open-source much sooner.
It‚Äôs truly amazing what a person can learn by building your own open-source Python package. It‚Äôs an end-to-end project in itself. You have to think about a problem to solve. How users would want to interact with your solution. How to write good documentation, make examples, write code, package, and deploy the code.
That‚Äôs just the obvious learning, the fluff on the top.
Example open-source Python package.
We will walk through an open-source
Python package available on pypi
that I built in a matter of a few hours on a weekend.
lakescum
is the name of the package.
Step 1 - Find a problem that needs a solution.
The first part of making an open-source Python package is simply finding a problem that needs a solution. The best place to start is with your own work and life as a developer.
Think about something you struggled with recently, or thought ‚Äú
This sucks, why hasn‚Äôt something made this better or dealt with this.
‚Äù Chances are you are not the only one.
In my case, I was recently working inside Databricks Unity Catalog and ran into the problem of wanting to read Unity Catalog Delta Lake tables with Polars ‚Ä¶ easily.
The problem is with managed Delta Lake tables most users don‚Äôt know the physical backend location of the Delta Lake table ‚Ä¶ (usually in cloud storage somewhere).
Built-in methods for most tools assume you know this ‚Ä¶ and even if you do, it‚Äôs problematic and repetitive to get this table into tools like Polars, DuckDB, or PyArrow.
I decided I would make a Python package to make that easier.
Check - step one done.
Step 2 - Use Python build and dependency packages like Poetry
When building a Python package from scratch you want to be open-source, so you will be planning on deploying it to PYPI eventually, it‚Äôs a good idea to make the future you happy with the current you.
Use a package build and dependency manager
like Poetry
.
All you need to know is a few simple commands.
You can create a new Python project template with all the files you need. Add other Python dependencies to your project. Build the wheel files for your finished project ‚Ä¶ and finally, deploy your package out to PYPI.
Step 3 - Register for a PYPI account (for free).
You will need to get a PYPI account
, for free, and get your creds and a secret so you can publish your package later.
Step 4 - Write the code.
Share
This is the fun part. You can start to solve your problem in a way that you think other people will find useful. This probably involves putting a little more thought into your code than you normally might.
Since this will be an open-source package, and assuming you want others to actually use it. You should probably think about how to make using the package as easy and seamless as possible.
In my case, I wanted three functions to allow anyone to pass in a Unity Catalog Databricks Delta Table and get back data that could either be
Polars, DuckDB, or PyArrow
.
Simple and straightforward.
Step 5 - Write the README and examples.
Next, we would want to write some sort of great README and documentation, including examples that will help people understand quickly what the tool is, what problem it solves, and how to use it.
You can see I clearly explain what the package does, then then give examples of how to use it for the different use cases. You can r
eview more details in the GitHub Repo
.
Step 6 - Close your eyes and publish.
Sometimes you just gotta take the leap. Nothing and no one is perfect, perfection is the enemy of moving forward and learning sometimes. Especially if it‚Äôs the first Python package you‚Äôve built and put out on PYPI.
You need to just take the plunge.
That‚Äôs what open source is anyway. It‚Äôs people coming together who have the same interests and problems and working together, to make something great.
You‚Äôre simply a `
poetry publish
` away from it happening!
Once you‚Äôve done it a few times, and got the feel for it, you will feel much more confident and comfortable diving into the great expanse of open-source software and start contributing in meaningful ways to other projects that interest you!
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
142888883.why-analytics-is-a-lose-lose-game,Why Analytics is a Lose Lose Game,It's for the birds,2024-05-13 07:34:15.167-05,"Spent any time in and around data and data teams have you? Moved around yourself from job to job have you? Then you will know without a shadow of a doubt that Analysts, Analytics Engineers, Report Developers, and Business intelligence folk by far are the ones to be pitted most.
They have a hard life.
Data Engineers have it bad, but folks working with
analytics
every day have it far worse.
It‚Äôs funny, you would think after decades of reports being written in everything from SSRS and SAP to Tableau and Looker, that things would have improved and changed for those developing analytics. But it hasn‚Äôt ‚Ä¶ like at all. Not even a bit.
Analytics is a bitter job, full of travail, hatred, stress, anger, miscommunication, broken dreams, and hopes, basically all that will grind a person to dust and pulp and spit them out the other end.
We should explore why. Why is that? What‚Äôs going on? Why are analytics the meat grinder leaving tears and sorrow in their waste?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Smoke and Mirrors - Business Analytics.
You would think we humans would learn a little bit more over the eons as they grind on and on. Analytics. Why are they so hard?
Why is that day-to-day job filled with such ‚Ä¶
overwork
stress
bugs
demands
never-ending tweaking
never-ending requests
and more
As someone who spent some time in the analytics space working on reports and dashboards, who left that life a long time ago, I still watch from afar the same struggles take place with no seeming end in sight.
The problems that cause analytics to be more ‚Äúwork‚Äù and ‚Äúharder‚Äù than just your average Software Engineering job (like it or not, it‚Äôs true), is that fact that those who work in Analytics, Data, Reporting, whatever ‚Ä¶ tend to be much more closely tied with ‚ÄúThe Business‚Äù than your average developer.
This comes with inherent risks. Namely, there is no layer of protection to filter all the fiery darts that get short their way.
When you‚Äôre ‚Äú
close
‚Äù the business you will bear the brunt of all inquiries
‚Äú
The Business
‚Äù wants answers now.
‚Äú
The Business
‚Äù always has questions, about everything.
‚ÄúThe Business‚Äù always expects perfection by default.
‚Äú
The Business
‚Äù stuff has to be translated into technical speak.
From the Technical Analytics perspective ‚Ä¶
Rarely has the Engineering support needed.
Spends a lot of time translating requests and questions into technical answer
Deals with a lot of dirty data and data quality issues.
What‚Äôs the problem? Is there a solution?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
I spend a lot of my time in my day job-solving big problems, figuring out who to get things done, planning big things, getting others to execute projects, and simply watching the wheels and cogs turn between groups and people trying to accomplish a thing.
If the problems are the same, we know what they are, why are things not changing? Why is Analytics a meat grinder of people?
Solutions.
I‚Äôm just going to crack the nut open and pour out the contents, mind dump my thoughts on how to fix the Analytics Problem. Some are practical steps that can be taken, others are more of a dream, things that will never happen because the world is as it is.
Let‚Äôs just list them.
‚Äú
The Business
‚Äù is typically bad at planning and details.
Analytics folk can also be bad at planning and big-picture stuff.
Analytics need more Engineering support and training.
Analytics needs more ‚Äúprotection‚Äù from the daily influx from ‚ÄúThe Business.‚Äù
‚ÄúThe Business‚Äù needs training on basic data thought patterns and DQ challenges.
Analytics needs the ability to address Technical Debt regularly.
Analytics and ‚ÄúThe Business‚Äù need to have better (and correct) documentation on decisions, definitions, and data sets.
Analytics needs to apply more ‚Äúmethodology‚Äù to the madness of ‚Äúanalytics work.‚Äù
Analytics needs to double the expected time required to complete a task.
Analytics needs to take ownership of Data Quality.
There are probably a few themes we can glean from this mess and ideas we can boil the list down to. Actionable steps.
People in Analytics need to apply a more methodical and engineering centric approach to taking on requests and solving for those problems.
Also,
‚ÄúThe Business‚Äù needs to be held accountable for being crazy about requests, time restraints, planning and the like. They need to be made to have skin in the game.
The problem with most Analytics jobs ‚Ä¶ whatever they may be, Report Developer, Analytics Engineer, whatever ‚Ä¶ is that they are ruled by chaos.
The chaos comes from ‚Äú
The Business
‚Äù that runs itself on the high-speed adrenaline of the next customer problem or next import request from the ‚ÄúC-suite.‚Äù
Sure there are things that have to be solved now.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Of course, we all understand that in any business there are high-priority things that arise that we must step up to the plate with and solve for.
The problem is the culture of chaos that ‚ÄúThe Business‚Äù operates in bleeds into the culture of Analytics and over time turns them into an extension ‚ÄúThe Business‚Äù that has no Engineering Excellence and operates on the daily whim of the next request.
The solution is to slow down to speed up over time. This is no different from what Software Engineers have understood for a long time (Data and Analytics are just a specialized subset of SE).
Being methodical and maniacal about testing, CI/CD, automation, validation, planning, etc pays off dividends in the long run that bring about better results.
Simply put, we are all a product of the daily grind we exist in. Analytics has that classic reputation of always being ‚Äúunder the gun‚Äù, constantly changing and tweaking reports, and being high-stress for a reason.
It‚Äôs the culture we‚Äôve let them live in.
Instead, we should push for change, change for the good. Give Analytics more power and autonomy. Give them space to think, design, and work on tech debt.
Give them a buffer between ‚Äúurgent requests,‚Äù pushing back on ‚ÄúThe Business‚Äù as to why there have been 10 urgent requests this week so far and what is the UNDERLYING PROBLEM that is leading to this being the case.
Maybe it sounds like I‚Äôm saying Analytics should be ingested into Engineering and be at the mercy of the same rules and processes. Maybe that is the case, would things get better?
Let me know what you think in the comments. Do you work in Analytics? Do have these problems? Have you solved them? How?
Leave a comment"
142813294.when-to-rust-for-data-engineering,When to Rust for Data Engineering ... and when NOT to.,To Rust or not to Rust ... that is a question,2024-06-20 07:35:10.144-05,"It seems like the topic of Rust just won‚Äôt go away these days. It‚Äôs on YouTube, and Linkedin, don‚Äôt even mention X (Twitter), it‚Äôs a firestorm between Rustacians and C/C++ zealots, even the few odd Golang people throwing some rocks. Constant bashing, blaming, arguing.
I mean even The White House had something to say about it. Things are getting out of hand I believe.
Why Rust Won‚Äôt Go Away.
I think one thing has become obvious, Rust isn‚Äôt going away. What was once labeled a fad for the weirdos living in their mom‚Äôs basement banging out code, has now become mainstream and dare I say ‚Äú
cool
.‚Äù
That‚Äôs the inflection point I think. Culture is culture, and Developer Culture is no different. Of course, there are culture wars, I mean the static-type people have been wringing their wiry little hands about Python for 20 years now.
Yet Python rules the land now. It‚Äôs clear a lot of C++ and Golang folk as some angst against Rust, nothing will stop them from grinding their axes.
Yet, with the rise of tools like
Polars
, ruff,
Datafusion
, etc, Rust has been accepted as a top-tier tool. But, that raises a question ‚Ä¶ for what? For what kind of work should Rust be used, especially in the context of Data Engineering?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Understanding why Rust is hard for some (data) people.
To truly take a look at when we should, and should not, use Rust, we can talk about some of the ‚Äúthings‚Äù that make it hard for people to use and adopt Rust, especially in the Data space.
Let‚Äôs make some statements about Data Folk that generally hold pretty true.
Python people to the core
SQL people to the core
Many non-traditional software backgrounds
This means that Data Folk generally avoids statically typed languages, except for the few weird Scala people still roaming the edges of Data Engineering society.
Probably the number one sticking point for most people when first trying to learn Rust is the
Ownership and Borrowing model
which is
very unforgiving
. For Pythonistas, this is like a whack in the face.
Also, it‚Äôs simply
harder to write Data Pipelines in Rust
and many times doesn‚Äôt make sense.
When we would use Rust.
While Rust has a steep learning curve, it is popular for a reason and has many benefits that Data Engineers can glob onto.
Speed
Confidence that if it builds, it will run forever
Tooling that others will use
For Data Engineers MOST of the time it doesn‚Äôt make sense to use Rust for day-to-day use, over Python unless some of the following are true.
Speed
. Sometimes you need things to go faster, faster than Python will move.
Then it might be time to move to Rust
. For example,
I‚Äôve had to convert Python AWS Lambdas to Rust
and experienced a %60 increase in speed.
Tooling
. Rust is perfect for command-line tooling built for use by others. For example,
I built a CSV sniffer in Rust
that is easy to use, fast, and fun.
Confidence
. Sometimes when it comes to building a ‚Äúthing,‚Äù you might have something that you want to build once and have complete confidence that you can set it and forget it, that it will run forever until the end of time, with no problems.
Probably shouldn‚Äôt reach for Python.
A good example is
when I built reepicheep
, a complex pill management reminder binary that would run on some cloud server for long periods of time, sending text message reminders at certain times.
I honestly would not trust Python with this task. But Rust? Oh yes. I can sleep well knowing Rust won‚Äôt fail me.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Why NOT to use Rust.
As much as I love to find reasons to use Rust, there are many times when NOT to use it for Data Engineering. More often there is a reason NOT to use it.
Other engineers have to be able to write Rust, not just you.
You have to know your tech stack.
Some problems are ‚Äúeasy‚Äù and solved problems in Python and other languages.
Working with files in cloud storage, Dataframes, etc.
You need to move rather quickly for a POC or have a short timeline for a project.
You‚Äôre building ‚Äúnormal‚Äù data pipelines.
Doesn‚Äôt make sense to reinvent the wheel just because you like Rust.
For example, some time ago I was doing some DSA stuff for fun, I
decided to explore graphs in Rust
, including using Dataframes in Rust.
It‚Äôs complicated. Sure, it‚Äôs fun to use Rust to solve problems and learn things, but, if you‚Äôre working on a Production project, you should choose the right tool for the job, and make it easy on yourself and others.
Why do I still write Rust?
So the question remains, why do I bother to use Rust at all? Well, I think there are a few good reasons.
Writing Rust makes you a better programmer overall.
It‚Äôs good to be challenged and learn and grow.
You will eventually find a Production use case for Rust.
You can eventually build open-source tooling for other Data Engineers built on Rust.
The future of Rust will never be mainstream in the Data Engineering culture, Python and SQL will rule the land for the foreseeable future.
But, you will see more Python tools that are wrappers around Rust code bases doing fast and amazing things."
142609860.transitioning-to-senior-engineer,Transitioning to Senior Engineer,The skills and habits to get you there.,2024-05-06 07:30:58.338-05,"In the data space, probably one of the longest and hardest climbing that can seemingly take forever is the one to Senior Engineer. Why? Well, for one there is a lot of competition, everyone without question usually starts their career and after a few years is eyeing this coveted title.
How do you get there? How can you stand out from your peers? What technical skills are required? What soft skills do you need? That‚Äôs what we are going to unpack today. The hope is we can shorten that path for you ‚Ä¶
from Engineer to Senior Engineer.
For those of us who‚Äôve already scratched and crawled our way up the proverbial ladder of Senior Engineer, we can easily forget how daunting and long that climb can seem from the bottom.
Early on in our data careers we uniquely struggle with a number of obstacles that sit between ourselves
and our perceived glory
.
Imposter syndrome.
Learning many new technical skills.
Learning to work on teams.
Solving problems.
Self-starting.
Just to name a few.
What comes as old hat to Senior Engineers who have years of experience in the form of blood, sweat, and literal tears, is simply unknown or hidden from younger engineers.
Can you shorten that gap of knowledge?
The real question is can you short-circuit some of those years of lessons and hard work? Yes and no. Of course, we all want to read that one book, take that one class, and do that one project that will jump us directly to Senior-level engineer like magic.
While this simply isn‚Äôt possible, there are actually real lessons, mindsets, and habits that can absolutely make the overall journey easier, less fraught with pain and difficulty, and probably yes, shorten the time required to reach that top rung on the ladder to Senior engineer.
Key Skills and Habits to Grow Faster to Senior Engineer.
Without further ado, we are going to jump right in. The plan is to share habits and tips that if you adopt quickly and consistently early on in your career, will cause you to move more quickly to operating at a Senior Level.
What some people don‚Äôt realize is that most of the time what you need to do is perform at a Senior Engineer level before you will get recognized and promoted.
You want to showcase certain skills and habits beforehand, show that you deserve the promotion and attract attention in a good way.
So, what can you do to get better at your craft and stand out?
You have to write more code.
You have to learn consistently.
Stay uncomfortable.
Work on communication skills.
Written and verbal
Share your knowledge and mentor
Be a team player.
Go the extra mile.
Work on planning out your work and projects.
I specifically picked this list of habits because they are attainable and approachable for anyone at any level to start to work on, and will pay off in the long run.
Let‚Äôs get to boiling it down.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Hard Work.
I‚Äôm not sure when it became unpopular to tell people to work harder ‚Ä¶ or maybe I‚Äôm just becoming the old grumpy engineer I‚Äôve greatly feared ‚Ä¶ maybe it‚Äôs both?
Do you want to get to Senior Engineer faster than your peers? You want to stand out? You want a so-called ‚Äúshort-cut‚Äù to the top. Ok, you‚Äôve got it ‚Ä¶ here it comes ‚Ä¶
Work harder.
Well, maybe smarter too.
What do we mean by hard work?
Working hard can mean a lot of things, and most of it is toxic, in our culture, so let‚Äôs be clear about what we mean by hard work. We can also what it doesn‚Äôt mean.
Don‚Äôt work endlessly and mindlessly for your employer for 10+ hours a day and every weekend.
Find time to work hard for
yourself
.
I‚Äôm talking about working hard on yourself and on your skills. You have all day at work to work on your skills at work. That will be the easy part. The hard part will be making sure you leave time for yourself. Time to grow yourself and your career.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
More Code and Learning.
Probably the two biggest and most boring pieces of advice I often give to those new to the Data Engineering and developer space on how to grow are ‚Äú
write more code
‚Äù and ‚Äú
learn more.
‚Äù
This often makes people mad but is really at the core move into a Senior Engineer level.
There is no better learning than doing. Write more code!!
Learn more! Reading, studying, following industry.
Reads
Data Warehouse Toolkit
The Pragmatic Programmer
Staff Engineer by Will Larson
The Staff Engineer's Path by Tanya Reilly
Other Stuff
The Pragmatic Engineer Newsletter
- Gergely Orosz
High Growth Engineer Newsletter
- Jordan Cutler
Honestly, this often requires you to stay uncomfortable in your life, inside and outside work. It just means taking a few extra steps. Doing a few more side projects, and reading a few more blogs and books. It isn‚Äôt rocket science.
Team (human) Dynamics
The other part that a lot of engineers end up ignoring when trying to move into more senior roles is basically
team and people dynamics.
Some people focus all their time on being the best coder in the world. While that might help, they can get left in the dark wondering why they aren‚Äôt being promoted when they think they should be.
Work on your written communication skills
Work on your verbal communication skills
Mentor and upskill other people, and be known as a teacher.
Be a team player.
Learn to resolve conflicts peacefully.
You simply can‚Äôt get around the fact that human skills on every level play a big part in reaching a senior engineering level. You are expected to be a coder, that‚Äôs the baseline, but it isn‚Äôt enough, you have to be able to handle humans as well.
Project Planning
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Last but not least, and often ignored is the project planning and implementation skills. Understandably this is often not talked about because it is the most nebulous of the ideas and skills, with not a lot of great resources that teach these topics.
It‚Äôs hard to know where to start, so I usually try to ingrain the following skills and mindsets in younger engineers.
Think before you code.
Design before you code.
Make rough plans on paper.
Estimate complexity and time per task (roughly).
Many times what separates more junior engineers from more senior ones is the time spent on a project
PRIOR
to code or decisions being made.
Senior Engineers know that for every hour they put into planning it will save them 5 hours of wasted effort and time later."
142462489.default-values-thoughts-and-more,Default Values - Thoughts and More,Guest Post by Anonymous Rust Dev,2024-03-28 07:56:53.675-05,"Hello readers. Today I‚Äôm bringing you a guest post from an anonymous Rust developer. I hope to turn it into a series if you all approve.
This is someone I know personally, have been on car rides with, camped in the woods with, sat around a fire with. They are one of the smartest and most wonderful programmers I have ever met.
Enjoy.
Default Values - Thoughts and More
Initialization can be one of the most thought-provoking parts of our jobs. You know you need a thing, and you'll eventually end up plugging in most of its attributes, but sometimes you simply aren't ready to initialize a field with a unique value. Enter default values, which buy us a bit of time before we need to account for a field.
Default values are an important aspect of our everyday programming lives. Understanding their value and potential pitfalls can save us a lot of grief.
Intrinsics and Structs
You've worked with the intrinsic data types before. Integers, floats, strings (depending on your choice of language) - these and others tend to have baked-in defaults (generally either zero for numerics, or empty string where applicable).
In some languages, such as C or its kin, initialization for intrinsics and stack-allocated variables is implicit. For instance, consider the following C code:
#include <stdio.h>

typedef struct {
    int a;
} Bob;

int main()
{
    Bob tom;
    printf(""tom.a = %d"", tom.a);  // prints: tom.a = 0
    return 0;
}
Note that at no point when declaring the variable
tom
did I invoke a constructor, nor did I explicitly set the value of the property
a
on the
Bob
struct, yet when we instantiate our struct and ask for its member we get a legitimate value.
Well, our C example doesn't give us much flexibility, but taking our example over to C++, we gain the opportunity to override the default constructor:
#include <stdio.h>

struct Bob {
    int a;

    Bob()
    {
        a = 3;
    }
};

int main()
{
    Bob tom;
    printf(""tom.a = %d"", tom.a);  // prints: tom.a = 3
    return 0;
}
Another thing C++ (and assorted other languages) lets us do is to disallow the default constructor entirely, meaning it's impossible to initialize in its default state; we can do so by making the default constructor into a
private
method, or by otherwise providing non-default constructors while omitting the default. In this way, we can force programmers to deal with initialization differently.
Also, in some languages, such as Rust, there are no implicit default initializations. If you want a default value, you need to ask for it explicitly:
// should display: A=0
let a: i32 = Default::default();
println!(""A={}"", a); 

// won't compile; we never initialize b
let b: i32;
println!(""B={}"", b);
In that language,
Default
is a trait associated with intrinsics and several other common data types in the standard libraries. For structs you create, you need to opt in to the
Default
implementation if you want it to work for you:
struct A(i32);

#[derive(Default)]
struct B(i32);

struct C(i32);

impl Default for C {
    fn default() -> Self { C(0) }
}

fn main() {
    let a: A = Default::default(); // won't compile, no default available
    let b: B = Default::default(); // works, because we auto-wired defaults with the #[derive(Default)] statement
    let c: C = Default::default(); // works, because we manually implemented the `Default::default` trait method
}
Databases (and other schema-backed data)
Maybe you're coming at this from the DB side of things. Database DDL (""data definition language""), some schema-related languages (e.g. XSD, JSON schema), etc. often have some provision for default values. The
Postgres docs
offer a representative example of how this is defined.
Note, from the Postgres link, that the docs tell us:
If no default value is declared explicitly, the default value is the null value. This usually makes sense
because a null value can be considered to represent unknown data
.
See that last bit there? That's a lead-in to our next point of consideration:
The meaning behind your defaults
In part, this is a matter for your problem domain - what is the significance of your default value? Are you stubbing the value out for later replacement, does the initial value have specific meaning in your application, or is your use of a default value a ""code smell"" representing deeper issues in your code?
For that first question, if you know with certainty that you'll eventually be plugging in a replacement value, there likely isn't any meaning behind your initial value choice - it could be a 0,
NULL
, 42, doesn't matter.
In the second question, we're dipping our toes into the type system for your language of choice:
Nullability - the value that isn't
For instance, go back to that last bit in the Postgres discussion - the implication in the note is that the affected field is a nullable value. This isn't always a given, and in fact you should be careful when defining your schema to ensure that only those values that may not always be present could be represented with
NULL
. The same is true for code; modern languages (e.g. recent versions of C#) specifically provide for
clearer representation around nullability
.
If you were familiar with the language's evolution, you might already know that C# traditionally allowed all classes (anything that derives
System.Object
) were nullable, even if you in your code knew you'd never have a null representation for your data.
This creates problems for modeling data, and either you were on the hook for handling null checks; or, worse, you might ""know"" that it could never be null, only to run into an edge case down the line where you throw a null reference exception that you weren't ready for. Thus, the language in recent versions offers the
nullable annotation context
as a means of getting in front of this dilemma by requiring explicit opt-in for object nullability (when the correct context is chosen).
Another example is Rust - the type system explicitly defines whether something might not have a value through the use of the
Option
type. Because it's Rust, you're forced to account for every representable state in your code.
The above languages aren't an exhaustive list; several modern programming languages (and, as mentioned already, databases) allow you to model your data in ways that explicitly declare whether something may not always have a legitimate value. While null values are
the source of many woes in the programming world
, they're also sometimes a legitimate way of defaulting values that don't yet have meaningful content. Because, you see, it might be important to differentiate between a missing value and a zero.
Sentinel values
These are values that represent something special. A classic example would be the ASCII text format, which isn't restricted to visible text but also contains several control codes or values with other meaning.
Historically, these special values were used with more prevalence and have since fallen out of use (for instance, the
BEL 0x07
character, when ""printed"", would make your computer beep). Some have stayed with us, though, such as the
NUL 0x00
or
LF 0x10
. Any program that handles ASCII potentially needs to be aware of these special values and handle them appropriately; else, any number of problems can lurking that aren't visible to end-users.
Therein lies the danger of sentinel values - they have special meaning, but programs or people who lack the awareness of those values might fail to deal with them properly. A byte, for instance, might be a
signed
or unsigned number, and in the wrong hands one person's
-5
might be another person's
251
.
Unless you're a total nerd, you probably need to check the ASCII chart any time you do something dealing with ASCII codes.
Abusing your types
Regarding the third question (whether we're hiding fundamental problems behind our default value selections), it's worth first asking: is there a better way?
Consider those sentinel values - if you were to invent ASCII today, people would laugh you out of the room. Besides the fact that it's been superseded by Unicode, it's also mixing concerns of text, control mechanisms, and special values that only have meaning in specific contexts. In its day, this may have been appropriate, but we've come a long way since and have better ways of separating out concerns.
To drive the point home, consider one context in which sentinel values have collectively cost society untold amounts of money - string handling. A C-style string, for instance, is one in which text is represented as an array of characters, with a sentinel value (
\0
) at the end to terminate it. Consider what
OWASP has to say on the matter
:
Relying on proper string termination may result in a buffer overflow.
Want to guess how often this happens? Historically, unsafe string handling has led to countless bugs and intentional exploits, all because the early authors of languages like C trusted that their conventions around sentinel values would be honored.
And, guess what - that
\0
value is the default intrinsic value for a character. You might assume, when you define an array in C, that it fills the array with default values - and you'd be wrong, unless you specifically chose to use the correct initializer.
Defaults done right
Okay, now I'm baiting you - there isn't any single one right way to go about any of the above. However, if we're being smart about it, there are some important things we can keep track of as we write our code.
No magic numbers
First, an apology - if you are the author of
this Shadertoy app
, I mean you no harm. However, it helps me to illustrate a point; consider the following snippet of code from that program:
float time = iTime;
// ...
float index = mod(ceil(time/1.0),5.0);
I think the
time
variable is self-explanatory, but what's the deal with
index
? Why do we divide
time
by
1.0
? Why do we do a modulo of
5.0
? What we have here are
""magic numbers""
, one of the greatest evils of the programming world. Such a value is ""magic"" because it has no discernible meaning - you
might
be able to tease out some kind of meaning from context, but that's if you're lucky, and it's a colossal waste of your time. Unless the meaning is patently obvious to a broad range of readers,
such values should be extracted out to a named constant
.
Use the right types
If you have access to
tagged unions
, they can be great ways to explain the hidden meanings of your values. For instance, revisiting the ASCII problem, Rust might approach the range of values (
0x00
to
0xFF
) by classifying the codes by purpose:
enum Ascii {
    Null,
    Control(u8),
    Numeric(u8),
    Alphabetic(u8),
    Special(u8),
    Extended(u8),
}
Basically, you would wrap your value with the appropriate value from the
Ascii
enum, and when encountered the consumers of your code would know what kind of value they're dealing with.
In another approach, you could back every single value in the ASCII code space with a dedicated value:
#[repr(u8)]
enum AsciiCharacter {
    Null = 0x00,
    StartOfHeading = 0x01,
    // ...
    Space = 0x20,
    ExclamationMark = 0x21,
    // ...
}
While tedious, this is the most robust way to handle a finite (within a reasonable quantity) set of values. Every single value has its own meaning. Basically, the above enum is a namespaced range of constants; it could just as easily be accomplished by defining them as such:
mod Ascii {
    const NULL: u8 = 0x00;
    const START_OF_HEADING: u8 = 0x01;
    // etc.
}
Whether constants or enums, one or both approaches should be available to most modern programming languages.
Runtime considerations
The type system often isn't robust enough to handle every edge case. For instance, in our Rust wrapper enum, there was nothing to stop a user from shoving a printable character into the
Ascii::Control
wrapper in the type system. Part of our problem is that the data model depends on
u8
(byte), which offers access to the full range of ASCII values even for the wrong branch of our type.
Defensive programming
is crucial for any fields or variables that can accept a range of values that hold special meanings. Basically, you need to assume that your types will be misused or abused by neglect, exploit, or unexpected behavior. Consider the following C# function, for instance:
class Program
{
    public static string GetFileContents(string path)
    {
        return File.ReadAllText(path);
    }
}
Seems reasonable enough, right? We~ell, maybe not; what if your consumer looked like this:
class Program
{
    public static void DoThing()
    {
        string path = null;
        
        if (DateTime.Now.DayOfWeek == DayOfWeek.Friday)
        {
            path = ""C:\\friday.txt"";
        }
        GetFileContents(path);
    }
}
Yes, whoever wrote this code should be fired, and for multiple reasons:
It will crash if run on any day that isn't Friday (NULL value)
It will crash if
C:\friday.txt
isn't accessible (e.g. missing, file lock)
It will crash if you run it on a non-Windows filesystem (yes, you can do that in .NET).
It will probably crash for some reason I haven't even considered yet
Basically, if you're the author of
GetFileContents
, you need to be aware of every way that someone can misuse the parameters; even though it's called ""path"" you may still find that its contents aren't in reality a valid path. Be aware of Murphy's Law.
Also, can we talk about our
string path = null;
statement? Why would we ever want a
NULL
value for
path
? Well, depending on how you implement
DoThing
, it might be the right tool for the job, but you would be doing yourself a major favor if you avoided that initialization in the first place, and instead used the
if
block to populate it:
class Program
{
    public static void DoThing()
    {
        string path;
        
        // at this point in time, our variable `path` is uninitialized
        
        if (DateTime.Now.DayOfWeek == DayOfWeek.Friday)
        {
            path = ""C:\\friday.txt"";
        }
        else
        {
            path = ""C:\\other.txt"";
        }
        
        // now, our variable `path` is guaranteed to have at least SOME value,
        // and our program will compile.
        
        GetFileContents(path);
    }
}
See that? By leaving the variable uninitialized, we are then forced by the compiler to account for its value later (e.g. in the
if
/
else
blocks).
That's fine for code, but what about databases? For instance, suppose you have the following table definition:
CREATE TABLE student_grades (
    student_id INT NOT NULL,
    grade VARCHAR(10) NOT NULL
);
Let's say you're worried about what can be crammed into a
grade
field and want to protect against it being abused? You've already ensured it's not nullable, but
VARCHAR(10)
leaves a lot of room for abuse. Here, you could use a lookup table of valid grades, and your
student_grades
table would then reference that:
CREATE TABLE valid_grades (
    value VARCHAR(10) primary key
);

INSERT INTO valid_grades (value) VALUES ('Superb'), ('Excellent'), ('Meh'), ('FAIL');

CREATE TABLE student_grades (
    student_id INT NOT NULL,
    grade VARCHAR(10) NOT NULL references valid_grades(value)
);
Now, you'll be screamed at if you try to shoehorn any value into
student_grades.grade
that isn't already accounted for in the
valid_grades
table.
Conclusion
Default values can be great, but they can also be the shotgun that takes your foot off:
Values have meaning. Make sure that meaning is well-understood by future generations.
As we saw in our C# illustration at the end, it's sometimes better not to have a default value in the first place.
Leverage your type system. As much as you can, try to avoid modeling unrepresentable states into your data (e.g. avoid allowing things to be null when it makes no sense for them to ever be so), and back your sentinel values with meaning (constants, enums, lookups)."
142459736.microservices-for-data-engineering,Microservices for Data Engineering,Dream Come True or Nightmare?,2024-03-20 08:30:23.069-05,"You know, looking back on it, I kinda figured by now all of us Data Engineers would be building microservices for this and that, and Batch ETL would have long lain dead under a pile of leaves in the backyard.
I mean I remember when microservices became a thing. I mean it was literally all everyone talked about. You were labeled a knave of the worst kind if you didn‚Äôt at least pretend to be on board and be in the know.
But, here we sit. Still pounding on the keyboard, hacking out the same old scripts and batch pipelines day after day, like smashing rocks in gulag.
Could Microservices work in a Data Engineering context? Could they replace our Batch ETL pipelines? Would it make our lives easier? Is it our Savior?
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Can Microservices work for Data Engineering?
This is a very interesting question to ponder. It goes beyond the question of streaming vs batch to something much larger, who should Data Teams be approaching the architecture and solutions they build?
For the last many decades, with no end in sight, Batch ETL has been the bread and butter of most data pipelines. Today, %85%+ of daily data pipelines are still batch in nature.
What this usually looks like is mono-repo style designs.
Most all the business logic to do data transformations, of all kinds, is usually tied up into a number of utilities or class files and methods that are hopefully shared and imported through different ingest scripts.
# some utils.py file

def read_csv():
    ....

def write_parquet():
    ....
and used ‚Ä¶
from utils import write_parquet

# some ingest script
This sort of method usually works fine. Write methods and functions that can be unit-tested and shared throughout many different ingestion scripts and pipelines.
The only problem with this approach is that over time, especially on large and complex teams and projects ‚Ä¶ it starts to get overwhelming ‚Ä¶ simply because
TOO MUCH
code and logic in a single repo.
Even if you are good about creating many different Classes or Utility files ‚Ä¶ at some point you run into the same problem again. Too much stuff.
What‚Äôs the option?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
So then what? Some people start to make their own libraries ‚Ä¶ say Python pip installable packages hosted on something like
AWS artifactory
for example.
Could microservices be the answer?
This brings us to the question at hand. Why don‚Äôt more teams embrace the classic microservice architecture popular in other Software Engineering spaces?
I have a theory that many Data Engineering teams are made up of a lot of non-software-engineering background folks. Data Engineering in general has been well known to lag behind SE best practices. Maybe most Data Teams simply don‚Äôt think of microservices as an option.
Depending on your point of view ‚Ä¶ microservices could make things more complex or less. It‚Äôs probably more about hiding the
complexity pea
under a cup, which cup are you going to choose?
This sort of microservices approach to break up specific logic has become more of a reality with the adoption of Rust as an acceptable programming tool, with support from the likes of AWS for Rust-based Lambdas etc.
Example.
As a simple example, what if we had a common use case of converting fixed-width flat files to tab-delimited flat files as part of a raw ingestion?
Typically a very normal Data Team on Databricks for example, would just add some code to their mono-repo to do this simple work of reading fixed with flat file and writing back out some tab-delimited text files in processing the raw data in preparation for ingestion into a Data Lake.
But, what if we wanted to take a simple step towards a microservices printed architecture without jumping all the down the rabbit hole?
We could write a standalone Rust-based lambda that triggers on a file hitting a s3 bucket, does the work, and pushes the results back to s3 again.
Very small, encapsulated logic in one spot that does a very specific thing only. Easy to manage and debug.
Here is an example of doing just that in one of my GitHub repos.
Now there is nothing really earth-shattering about writing Rust and putting it inside a Lambda. But, we can probably safely say that there are very few Data Teams that are approaching classic ETL problems in this manner.
With this new design, we move from slow, long-running classic Batch ETL that isn‚Äôt very scalable probably ‚Ä¶ to ‚Ä¶ an AWS Lambda approach with Rust that is extremely fast, self-contained, and scalable.
In the same way, we could break up a lot of logic inside classic mono-repos into smaller ‚Äúservices‚Äù where our logic is ‚Ä¶
contained
scalable
fast
easier to debug and reason about
Of course, this means a complete mind shift and probably a whole new set of skills for many Data Teams. Shifting from batch SQL and Spark pipelines on Snowflake or Databricks managed in mono-repos to more of a broken up ‚Äúmicro-services‚Äù approach is probably not an easy or overnight change.
Conclusion
What do you think? Are microservices something that holds a future in the broader Data Engineering community? It‚Äôs hard to say. I would think it depends on the Data Team themselves.
Change is hard for everyone.
I think it will be some time before we see any movement away from Batch ETL in mono-repos, old habits die hard. But, I think it‚Äôs an interesting area to start exploring. Especially, with this infiltration of Rust and Rust-based Python packages.
With tools like AWS Lambda in combination with some creativity ‚Ä¶ it‚Äôs possible to throw the chains of the old off and try on something new."
142451707.apple-pie-angry-people-other-news,Apple Pie. Angry People. Other News.,Weekend Edition,2024-03-09 09:20:22.938-06,"Hello Readers! It‚Äôs a sunny Saturday morning and I just finished baking an apple pie, so I‚Äôm in a good mood. To break the normal cycle of weekly editions, I figured I would dip my toe into the waters of Weekend Editions.
I wanted to update you on a few non-Substack pieces of content I‚Äôve done lately ‚Ä¶ give you something to do while you lay around on the couch all day.
Angry People Of The Internet.
I broke the internet with an argument I had with a Databricks Architect this week about putting ‚Äú
Notebooks in Production.
‚Äù
You can
read more here
and
here
. I assure you, you will be entertained.
To be fair, I‚Äôve read none of the comments, not a one. After you‚Äôve been doing what I‚Äôve been doing for as long as I have ‚Ä¶ you learn to ride the waves and keep chugging along never looking back.
I couldn‚Äôt help doing a YouTube video on it though.
I also found out DuckDB has MAJOR memory issues, that seemed to deserve a YouTube video as well. Maybe it will become a habit.
I also wrote two other pieces ‚Ä¶
The Best Piece of Software Engineering Advice
https://www.confessionsofadataguy.com/the-best-piece-of-software-engineering-advice/
Why I Love Rust, but Deploy Python
https://www.confessionsofadataguy.com/why-i-love-rust-but-deploy-python/
I also dipped my other toe in the waters of
producing paid coding exercises
. I always get people asking me to do various Data Engineering courses ‚Ä¶ but can‚Äôt seem to muster the strength to do it.
It was a reasonable experience. I am working on similar Polars Practice Exercises that will be double the size, shooting for 100 problems.
We shall see.
%40 OFF paid subscriptions this weekend
For all you free-riders ‚Ä¶ here is a link to %40 OFF a paid subscription.
https://dataengineeringcentral.substack.com/weekend"
142309377.error-handling-for-data-engineers,Error Handling for Data Engineers,Exception.,2024-03-25 08:58:49.932-05,"Hmmm. I feel like error handling is a strange topic for most Data Engineers, more so than your average Software Engineer writing TypeScript all day.
We do different things, write different code, and therefore have different needs.
I‚Äôve working on Data Platforms running 1,000 nodes on Kubernetes. I‚Äôve worked on small-footprint Python scripts running on a server ‚Ä¶ and everything in between.
Exception Handling  is not an often thought-about topic, yet has major ramifications down the road ‚Ä¶ when you have to crack open Pandora‚Äôs Box to solve a problem. Then it matters.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
What are we trying to accomplish with Error Handling?
This is an interesting question and I think the answer is different for most Data Engineering teams than it is for say ‚Ä¶ front-end folks writing TypeScript all day.
Data Engineers aren‚Äôt worried about handling all errors in a graceful manner, we aren‚Äôt building user-facing applications.
If you are a normal Front-End engineer, you want everything to go smoothly, you don‚Äôt want your users ever to see some random Traceback or StackOverflow error.
For Data Engineers when it comes to Error Handling this is completely different.
Data Engineers want data pipeline errors to be easily traceable, researched, and discoverable, giving an exact idea of what and where the problem occurred.
This is where Error handling on Data Teams starts to diverge from most of what we would think of as ‚Äúnormal‚Äù Software Engineering. Let‚Äôs dive into Error Handling Data Engineering style.
Error Handling for Data Engineering.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Let‚Äôs list the main things that Data Engineers should be thinking about when they are creating Data Pipelines.
A central repository of discovery for all Errors.
Alerting around Errors.
Some sort of ‚Äúlogging‚Äù module.
Error handling ‚Äúmetadata.‚Äù
The core concepts are at the heart of what we want for Error Handling as Data Engineers.
Central Repository
We Data Engineers have been sent from above to tame the teaming masses of Data Pipelines that run every day on our watch. Unlike some Software Engineers who handle errors in a single application, we are dealing with many hundreds of Data Pipelines.
If we have errors coming from multiple different sources, we need a central place from which we can look, discover, and dig into those errors.
Amazon CloudWatch
is a good example of this. Having the ability to go to a single place where one can search and discover errors in a time series-type fashion is a game changer.
You don‚Äôt want to have many Data Pipelines producing and shipping errors to various spots in various ways. You need to be able to say ‚Ä¶ ‚Äú
Look, if there are Errors ‚Ä¶ the logs for them are always going to be in this one spot.
‚Äù
For example, you can read here about using Prefect for observability into Cloud Watch logs.
Sometimes half the battle of debugging can be finding the correct error and finding its root cause, taking half that battle away, and making it easy.
Alerting around Errors.
Another core tenant around Error messages in Data Engineering is alerting and notifications. Although Slack may be the bane of many Engineer‚Äôs existence, they do serve a purpose.
Data Teams typically have a number of pipelines that are extremely important to the business, for decision-making and customers. We need to know if things go wrong.
How else can you fix something if you don‚Äôt know when things have gone south?
For example,
many tools provide out-of-the-box Slack integrations to this common use case
. Making it extremely easy to be notified of pipeline failures.
from prefect_slack import SlackWebhook

SlackWebhook(url=""WEBHOOK_URL_PLACEHOLDER"").save(""BLOCK-NAME-PLACEHOLDER"")
Like it or not, having Slack on your phone can be annoying, but very helpful!
Logging Modules and Error Meta Data
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
You might be thinking ‚Ä¶ why talk about logging when writing about error handling? Well, any sort of templating and meta-data around error handling can make a huge difference when it comes to the business of actually making something of an error message.
What do I mean ‚Ä¶
Timestamps and dates
Logging from activities leading up to the error.
Templated meta-data about the pipeline itself etc.
For example ‚Ä¶
logging.basicConfig(
        format=""%(asctime)s %(levelname)s - Data Warehouse Staging Load - %(message)s"",
        level=logging.INFO,
    )
logging.getLogger().setLevel(logging.INFO)
logging.info(""Assigning and tracking job etl id."")
This sort of information combined with actual error logs can help hone in on exactly where the problem happened.
If you have never run across a good logging module, I recommend l
ooking into this one, for Python
.
Some specifics about Error Handling.
I wanted to take a little time to dig into some examples of Error Handling, mostly based around what you should
NOT
do. Personal opinion, but it does come with some experience.
Here is what I don‚Äôt like to see for Error Handling in a Data Engineering context.
try:
        spark.sql(sql_etl_cmd)
    except Exception as e:
        """"""Delta Lake throws random ConcurrentAppendException's randomly when using COPY INTO.
        Attempt to solve this by catching and retrying after a sleep.""""""
        if ""ConcurrentAppendException"" in str(e):
            logging.info(
                ""Encountered ConcurrentAppendException exception .. trying again.""
            )
            sleep(180)
            spark.sql(sql_etl_cmd)
        else:
            # kill script for unknown exception
            print(e)
            exit(1)
I really have a love-hate relationship with `
try: execpt
` logic used in a lot of spots to try and ‚Äúcatch‚Äù and deal with errors.
Why? Because it‚Äôs really easy to miss something and layer in so much logic that the real underlying stack track for the error can get hidden in the noise. It can make debugging harder than it needs to be.
It‚Äôs simply dangerous in my mind. Feel free to do it, but move wisely and cautiously.
For example, here is a raw error log dump from a production failure that is without any `
try:except
` logic.
[2024-01-02, 13:50:09 UTC] [2024-01-02, 13:50:09 UTC] {{taskinstance.py:1851}} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 175, in execute
    return_value = self.execute_callable()
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File ""/usr/local/airflow/dags/airflow_utils/sftp_transfer.py"", line 199, in fiserv_file_processing
    r.sftp_run_command(ssh, cmd=cmd)
  File ""/usr/local/airflow/dags/airflow_utils/sftp_transfer.py"", line 105, in sftp_run_command
    return stdout.readlines()
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/paramiko/file.py"", line 349, in readlines
    line = self.readline()
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/paramiko/file.py"", line 291, in readline
    new_data = self._read(n)
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/paramiko/channel.py"", line 1361, in _read
    return self.channel.recv(size)
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/paramiko/channel.py"", line 699, in recv
    out = self.in_buffer.read(nbytes, self.timeout)
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/paramiko/buffered_pipe.py"", line 160, in read
    self._cv.wait(timeout)
  File ""/usr/lib/python3.10/threading.py"", line 320, in wait
    waiter.acquire()
  File ""/usr/local/airflow/.local/lib/python3.10/site-packages/airflow/utils/timeout.py"", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: Timeout, PID: 29035
For me, it‚Äôs easy within a minute to hone down on actually what this bare exception is telling me, I don‚Äôt have to dig through other weird `
try:except
` logic that might have been triggered and buried the problem.
I can deduce that `
sftp_run_command()` with a `handle_timeout` error, not suprising in a SFTP context. Easy.
Wrapping Up
So what did we learn?
Have a central spot or technology that gathers all your logs and errors (like Cloud Watch)
Have good alerting and notifications on your errors (like Slack)
Have plenty of metadata to go with your errors, use a logging library.
Be careful how you handle errors.
Let me know in the comments how you handle your errors in all your pipelines!
Leave a comment"
142244982.udtfs-user-defined-table-functions,UDTFs (User-defined Table Functions) in PySpark. ,Introduction for Data Engineers,2024-03-18 08:04:44.07-05,"Probably one of the best parts of being in Data Engineering and data for a decade is the ability to sit back and watch new things come along. New things that is. It‚Äôs fun to see what the community dreams up.
This one I didn‚Äôt see coming.
UDTFs (User Defined Table Function)
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I‚Äôve been a hater of Python UDFs in the context of Spark for a long time. Most all the UDFs I‚Äôve seen can always be replaced by simply using the out-of-the-box functions provided by Spark. UDFs have bad performance on large datasets, especially poorly written ones.
I‚Äôm not sure what to think about UDTFs, but whatever, I guess in my old age I just let people do what they want, they want to foot-gun themselves, then so be it, a good way to learn.
UDTFs -
User-Defined Table Function
Python User-defined Table Functions (UDTFs) came about in Spark version 3.5, so it‚Äôs very new. Normal UDFs that have been around in Spark for some time are called ‚Äú
scalar functions
‚Äù they return single values.
Think about a PySpark Dataframe, then apply a UDF to each row, so the UDF returns a single value and is applied on the entire Datafarme row-by-row.
This differs from UDTFs. UDTFs can return an entire ‚Äú
table
‚Äù of data (
or can if you want
).
This is an interesting idea is it not?
I am curious what the performance is like, if UDTFs are as slow as UDFs, but whatever, I promised I would be nice and not complain about it.
I get it, the thing about UDFs or UDTFs is that they allow those organizations who are entrenched in Spark for their workloads, to be able to do things ‚Äúoutside the box‚Äù of what Spark usually is used for.
It‚Äôs flexibility.
Flexibility has its place and time.
Trying out UDTFs in Databricks.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
If you know me, you know I‚Äôm not a fan of theory, I prefer to actually
try
something out. Kick ye‚Äô old tires if you know what I mean. The best way to learn in Data Engineering is to DO code. Strike while the iron is hot.
So, in case you didn‚Äôt know,
Databricks offers a free Community Edition
, which I‚Äôm not sure why more people don‚Äôt take advantage of it. It‚Äôs the perfect way to test out things and learn new things.
Things to know about UDTFs.
I‚Äôm just going to rattle off some things you should know about UDTFs, but I assume you will learn more from looking at the one we build.
Import a UDTF from
spark.sql.functions
UDTFs are written as Python classes
Can use Python decorator
@udtf
UDTFs can be registered and used in the SQL context.
Trying out UDTFs in Databricks.
Let‚Äôs just try one out, I‚Äôve never used one, so this should be interesting, but I think we will learn the most doing this.
The whole point of using a UDF or UDTF is that we are reaching for some functionality that is outside Spark. So, let‚Äôs come up with a pretend scenario.
Since LLMs and AI are popular today, let‚Äôs do something along those lines. We are going to pretend we have a dataset/dataframe of company documents, including their text, and we have a pipeline built in Spark to massage the data in preparation for LLM work.
Let‚Äôs see if we can write UDTF to do some tokenizer stuff. During the messing around with data in a LLM context, text needs to be tokenized ‚Ä¶ and
HuggingFace has a PyPi Python package
that does this.
This is clearly work you probably can‚Äôt do easily with Spark out of the box. So our UDTF will use the Python package
tokenizers
to do this work on a Datframe.
First, let‚Äôs write the UDTF.
from pyspark.sql.functions import udtf
from pyspark.sql.types import Row
from tokenizers.pre_tokenizers import Whitespace

@udtf(returnType=""document_id: int, document_name: string, text: string"")
class PreTokenizerUDTF:
    def eval(self, row: Row):
        pre_tokenizer = Whitespace()
        yield (row[""document_id""], row[""document_name""], pre_tokenizer.pre_tokenize_str(row[""text""]))
Let‚Äôs explain what‚Äôs happening.
This UDTF is made up of a Python class
PreTokenizerUDTF
and also has the @udtf decorator. It has a single method called
eval
(required). It also takes a Row as an argument since we will be passing an entire SQL context table as an argument.
Note that we have to have a
returnType
specified, in our case, it‚Äôs all the columns in our dataframe/SQL table plus the new one our
UDTF
is calculating.
We
yield
the result which is row columns, including
pre_tokenizer.pre_tokenize_str(row[""text""])
which is what we are using the UDTF for in the first place ‚Ä¶ a
pre_tokenizer_str
method.
Let‚Äôs write a sample Dataframe to run this against.
dataframe = spark.createDataFrame(
        [
            (1, ""Company Goals"", ""We want to sell 10% more ACME Widgets into international markets this year.""),
            (2, ""Company Document"", ""Our Marketing strategy at ACME, INC is to be active on MySpace.""),
        ],
        [
            ""document_id"",
            ""document_name"",
            ""text""
        ]
    )
Running the UDTF
So now we can actually register our UDTF in an SQL Context and run it.
dataframe.createOrReplaceTempView(""company_text"")
spark.udtf.register(""my_first_udtf"", PreTokenizerUDTF)

output = spark.sql(""""""
            SELECT * 
            FROM my_first_udtf(TABLE(company_text))
            """"""
            )

output.display()
Sorta makes sense right?
Above code we register our UDTF, register our Dataframe as SQL Table, and then we can call the the UDTF in the FROM clause of our SQL and pass in the table.
Below you can see the results from my Community Edition of Databricks.
Something wrong with the text output, but hey, you get the point.
Thoughts
My thoughts on UDTF are surprisingly nice for once in my grumpy life. They are pretty easy to use once you do your first one. I find the fact that you can pass and return entire tables to be very nice and convenient, especially in the SQL context.
I think this will become very useful in the future with AI and LLM use cases that will be coming down the highway soon.
One of the problems with LLMs and the Data Engineering work that surrounds it is the scale of data operations to prep large datasets for use downstream.
Databricks and Delta Lake are the perfect toolset for this type of work that needs to scale. UTDFs in the context of Spark and Delta Lake doing custom transformations ‚Ä¶ well that‚Äôs incredibly powerful.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
142187662.40-off-sql-practice-problems,40% Off SQL Practice Problems,For Free Subscribers,2024-02-29 15:50:30.415-06,"As a thanks to all free subscribers to Data Engineering Central, use the code  ‚Äú40OFF‚Äù to get 40% off on the new
SQL Practice Problems
.
40OFF
Do you want to learn SQL or practice your SQL skills?
There are few more important skills than SQL today as a developer, programmer, or engineer. But, it can be hard to learn SQL skills without some real-world problems to solve!
This is a Docker and docker-compose-based code-base that uses Postgres (within Docker) set of 35 SQL questions.
SQL Practice Problems
There are three categories
Basic
Intermediate
Advanced
Each has its own set of questions. There are templates where your SQL answer to the question goes and prebuilt unit tests that can be run that will test if your SQL is correct or bad!! (
there is already preconfigured data ... all you have to do is write SQL to answer the questions
)
An example question is ...
#### Problem 35: Window Function - LEAD() and LAG()
Objective: Utilize `LEAD()` and `LAG()` window functions.
Table: stock_prices (columns: `stock_id`, `date`, `closing_price`)
Task: Write a SQL query to compare each day's closing price with the previous and next day‚Äôs prices for each stock.
Return `stock_id`, `date`, `previous_day_price`, `next_day_price`
Order by `stock_id` and `date`
As long as you can build a docker image and run a docker-compose command, you will be off to the races practicing your SQL skills.
Using this code base is very easy! You can practice your Docker skills and SQL!
First, `cd` into the directory you first want to work on.
```
- beginner-problems
- intermediate-problems
- advanced-problems
```
Then you must build the `Docker` image in the directory you're working on.
```
docker build . --tag=beginner
docker build . --tag=intermediate
docker build . --tag=advanced
```

Next read the questions.
Place your SQL to the questions in the `tests/sqlhereyouhobbit.sql` files ... Each problem is labeled.

To check if your SQL is correct, run the unit tests against them by running ...
`docker-compose up beginner` or `docker-compose up intermediate` etc.
Each problems unit test will either pass or fail.
Checkout them out here!
SQL Practice Problems"
142118007.iteration-vs-recursion,Iteration vs Recursion.,Data Engineering Style,2024-03-11 08:24:15.454-05,"I don‚Äôt know if I can remember back that far, back when I wrote Perl and PHP, far away lost in the swirling and misty years of yore. I don‚Äôt remember the first time I wrote a loop, but I do remember the first time I stumbled across that concept of recursion.
I would so most of the mindless masses of Data Engineering blindly shuffle their weary feet down the well-worn trail of iteration. It‚Äôs a natural way to walk.
I can‚Äôt say I blame ‚Äòem, I do the same. I don‚Äôt even think I have a strong opinion on which is better, two different ways to solve the same problem I suppose.
Don‚Äôt get me wrong, I‚Äôm sure there are
Lofty Ones
soaring high above us, pushed into the heavens by their giant brains and unstoppable programming prowess. They probably prefer recursion, simply because most other people don‚Äôt.
Today we will walk down that lane.
Doing the same thing with Things.
It‚Äôs funny how most of us turn our noses at DSA (Data Structures and Algorithms). I‚Äôm with you for the most part, even though I‚Äôve written a few posts on the topic.
For most developers, there are marginal benefits in the short term, although I would argue from the point of view that yes, most folk might never directly use many of the techniques taught, but I still think it‚Äôs helpful for your learning and self-confidence in the long term ‚Ä¶ leading to overall better code quality.
But, what you will notice about a lot of the ‚Äúbeginner‚Äù DSA stuff or even LeetCode problems, is that they center around Arrays/Lists
.
They are central to and at the core of a lot of problems and underpin a lot of the systems and tools we work on.
But what are they really? Just a collection of things we want to do things with.
This brings us to our topic for today.
Recursion vs Iteration.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
If you had to pick one I‚Äôm sure you would pick the iteration on the left. It's just what we do, it‚Äôs easier and it feels comfortable. I mean that‚Äôs what I pick most of the time.
But, if we want to join the cool kids club we are going to have to learn recursion eventually, like it or not. You want to be cool, don‚Äôt you?
Let‚Äôs start with a simple and obvious example that most Data Engineers have probably worked on. We have a list of s3 objects and we want to do something with each one.
(I‚Äôm going to use Rust, just to brainwash you.)
This is a simple example of
iteration
. You‚Äôve seen this and written this a million times probably.
We have an Array or List of thingys, and then we do something with those thingys using a
for loop
.
Iteration is obvious and easy to understand.
What about doing the same thing recursion?
If you‚Äôve never written any recursive code before, or thought about it much, it can be a little strange the first time. Basically what it boils down to is having code that calls
itself
.
A method calling itself, let‚Äôs do that with our iterative code to achieve the same results.
If you‚Äôre new to recursive functions, the crux of the matter is to note that within the definition of `
process_thingys
` there is a call to itself ‚Ä¶ `
process_thingys
`.
The Word on the Street.
Share Data Engineering Central
Now the
word on the street
that you will find is that recursion is more of a memory hog, but easier to read. I‚Äôm not sure I agree with that, I guess beauty is in the eye of the beholder.
I happen to think that simple while, for loops, etc, that are explicit are easier to read, but that‚Äôs probably just different for every person.
I mean recursion feels cooler, it seems fun to write and a little bit more fancy.
My guess is that the more junior the engineer or developer, the more likely it would be that a codebase full of recursion is going to be harder for them to debug in the beginning.
It seems like Stackoverflow agrees with me.
Recursion is seen as more elegant, especially for certain algos, but is slower than simple iteration.
What are your thoughts?
Tell me what you prefer and what use most in your Data Engineering work? Do you like one over the other? Comment below!
Leave a comment"
141997113.duckdb-vs-polars-thunderdome,DuckDB vs Polars - Thunderdome., 16GB on 4GB machine Challenge.,2024-03-04 08:18:51.15-06,"If you know me, you know I like to stir the pot, the big boiling and smoldering cauldron of Data Tools pot. Yes, that‚Äôs the one, blackened and burned pot from years of conjurers pouring myriads of Modern Data Stack tools into it, which have since bubbled and encrusted us all with the refuse of a thousand promises to be the cure-all for our ailments.
That pot.
What better way, me thinks, to unleash the hounds upon myself, while I run through the dark woods of the internet in the moonlight than to pit Polars and DuckDB against each other?
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
The Challenge.
What I want to do is compare Polars vs DuckDB in some interesting way, and see what they are capable of. I‚Äôm not really concerned about syntax, or features, they are both full-blown projects with lots of users and development going on.
What I am curious about is processing large datasets, at least ‚Äúlarger than memory datasets‚Äù on small commodity hardware. This is where a tool has a real chance to steal market share from the likes of Spark.
So what I want to do is pit Polars vs Spark on say a 4GB memory Unbuntu-based machine in the cloud ‚Ä¶ read say 16GB of data in s3, process it, and write it back to s3.
Standard stuff. Standard Thunderdome.
The Data
We will use the
free and open source Back Blaze dataset set found here
.
Using the aws cli it looks like we got about
16GBs
of data.
aws s3 ls --profile confessions --summarize --human-readable --recursive s3://confessions-of-a-data-guy/harddrives

2024-02-24 15:19:06    0 Bytes harddrives/
....
2024-02-24 15:24:57   97.5 MiB harddrives/2023-09-30.csv

Total Objects: 183
   Total Size: 15.9 GiB
Setting up the Compute.
Time to get stuff installed ‚Ä¶
root@localhost:~# pip3 install polars
Collecting polars
  Downloading polars-0.20.10-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.8 MB)
     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 26.8/26.8 MB 67.4 MB/s eta 0:00:00
Installing collected packages: polars
Successfully installed polars-0.20.10
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
root@localhost:~# pip3 install duckdb
Collecting duckdb
  Downloading duckdb-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.8 MB)
     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 17.8/17.8 MB 36.9 MB/s eta 0:00:00
Installing collected packages: duckdb
Successfully installed duckdb-0.10.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
Let the battle begin. 4GB Ubuntu machine, 16GB of data.
Polars reading 16GB from s3 on 4GB machine.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The first black mark against polars is that the lazy `scan_csv` doesn‚Äôt work against s3 yet.
See this currently open GitHub issue
, apparently, others would like this as well of course. The only thing left for me to do is resort to some
old code I wrote this past summer
where I combined pyarrow with Polars to give me the ability to read CSV files from s3.
import polars as pl
import pyarrow.dataset as ds
import s3fs
import os
from datetime import datetime

t1 = datetime.now()

os.environ['AWS_ACCESS_KEY_ID']=''
os.environ['AWS_SECRET_ACCESS_KEY']=''

bucket = 'confessions-of-a-data-guy'  
path = 'harddrives'
fs = s3fs.S3FileSystem(key=os.environ['AWS_ACCESS_KEY_ID'],
                           secret=os.environ['AWS_SECRET_ACCESS_KEY'],
                           config_kwargs={'region_name':'us-east-1'}
                                          )

s3_endpoint = f""s3://{bucket}/{path}""

myds = ds.dataset([y for y in fs.ls(s3_endpoint) if "".csv"" in y], 
                      filesystem=fs, 
                      format=""csv"")
lazy_df = pl.scan_pyarrow_dataset(myds)
sql = pl.SQLContext()
sql.register(""harddrives"", lazy_df)   
results = sql.execute(""""""
                      SELECT date, SUM(failure) as failures
                      FROM harddrives 
                      GROUP BY date
                      """""")
    
with fs.open(f""s3://{bucket}/{path}/results/failures.csv"", ""w"") as f:
        results.collect().write_csv(f)

t2 = datetime.now()
total = t2-t1
print(f""It took {total} to run Polars"")
The code isn‚Äôt very complicated, even with the addition of pyarrow to get the job done, it‚Äôs fairly straightforward. Note the use of scan_pyarrow_dataset, this allows the work to be done in a lazy manner, with a LazyFrame ‚Ä¶ until I call collect() at the end to pull the results to a Dataframe so I can write.
The LazyFrame has no options to write directly, so apparently you have to go to Dataframe and manually collect() at that point. Seems like this is something it should infer.
I find it extremely annoying at this point that polars has not built-in feature to scan remote s3 files in cloud storage ‚Ä¶ for a tool that‚Äôs supposed to beat out Pandas and be the next hot thing ‚Ä¶ come on.
Results ‚Ä¶
root@localhost:~# python3 polars-v-duckdb.py 
It took 0:03:43.186879 to run Polars
I can‚Äôt complain though 3 minutes and 43 seconds to process 16GBs of data on s3 on a 4GB machine.
That‚Äôs powerful and promising. The results looked fine.
Duckdb reading 16GB from s3 on a 4GB machine.
I‚Äôm looking forward to this one ‚Ä¶ hopefully DuckDB will be less headache to read and write data from s3. Again, it‚Äôs such a common use case these days. Polars is still in the back of my mind not being able to support that out of the box for CSV files.
I digress.
Let‚Äôs do this.
import duckdb
from datetime import datetime

def main():
    t1 = datetime.now()
    duckdb.sql(""SET temp_directory = 'temp';"")
    duckdb.sql(""""""
        SET memory_limit = '3GB';
        INSTALL httpfs;
        LOAD httpfs;
        SET s3_region='us-east-1';
        SET s3_access_key_id='';
        SET s3_secret_access_key='';
        CREATE VIEW metrics AS 
        SELECT date, SUM(failure) as failures
        FROM read_csv_auto('s3://confessions-of-a-data-guy/harddrives/*.csv', ignore_errors = true, union_by_name = true)
        GROUP BY date;
    """""")

    duckdb.sql(""""""
        COPY metrics TO 's3://confessions-of-a-data-guy/harddrives/results.csv';
    """""")
    t2 = datetime.now()
    print(f""It took {total} to run DuckDB"")

if __name__ == '__main__':
    main()
I have to say the DuckDB code is much simpler. Not much to worry about there.
Uh oh.
root@localhost:~# python3 duckdb-test.py 
Traceback (most recent call last):
  File ""/root/duckdb-test.py"", line 25, in <module>
    main()
  File ""/root/duckdb-test.py"", line 18, in main
    duckdb.sql(""""""
duckdb.duckdb.InvalidInputException: Invalid Input Error: Mismatch between the schema of different files
Bugger ‚Ä¶ I knew this was going too well. But, leave it to DuckDB ‚Ä¶ looks like they have a solution to that ‚Ä¶
ignore errors option
. Note I added `
ignore_errors = true
` back into the code above.
Lord willing the creek don‚Äôt rise, that will fix the problem.
Dang it.
root@localhost:~# python3 duckdb-test.py 
Traceback (most recent call last):
  File ""/root/duckdb-test.py"", line 25, in <module>
    main()
  File ""/root/duckdb-test.py"", line 18, in main
    duckdb.sql(""""""
duckdb.duckdb.InvalidInputException: Invalid Input Error: Mismatch between the schema of different files
Well, maybe this option will work ‚Ä¶ `
union_by_name = true
`
That worked, but what I greatly feared has come upon me. That which I dread. Like Job.
root@localhost:~# python3 duckdb-test.py 
Traceback (most recent call last):
  File ""/root/duckdb-test.py"", line 25, in <module>
    main()
  File ""/root/duckdb-test.py"", line 6, in main
    duckdb.sql(""""""
duckdb.duckdb.OutOfMemoryException: Out of Memory Error: failed to allocate data of size 32.0 KiB (3.0 GiB/3.0 GiB used)
And there was great weeping and moaning.
According to DuckDB‚Äôs website, it‚Äôs capable of spilling to Disk if it runs out of memory.
DuckDB is capable of operating both as an in-memory and as a disk-based database system. In the latter case, it can spill to disk to process larger-than-memory workloads (a.k.a. out-of-core processing). In these cases, a fast disk is highly beneficial. However, if the workload fits in memory, the disk speed only has a limited effect on performance.
After doing some reading, it appears we can enable this ability to spill to disk with something like `
SET temp_directory = '/path/to/temp_dir.tmp/'
`, otherwise DuckDB will NOT spill to disk automatically if not configured to use a ‚Äúdatabase file.‚Äù
I tried it a few times, and moved that command around all over, with the query, outside the query, didn‚Äôt matter. OOM every single time.
Traceback (most recent call last):
  File ""/root/duckdb-test.py"", line 26, in <module>
    main()
  File ""/root/duckdb-test.py"", line 7, in main
    duckdb.sql(""""""
duckdb.duckdb.OutOfMemoryException: Out of Memory Error: failed to allocate data of size 32.0 KiB (3.0 GiB/3.0 GiB used)
I read some more StackOverflow complaints about this issue, seems like maybe setting a memory limit, lower than whatever resource you are working on, to force DuckDB to stop at some point and then spill.
People are still opening issues like this as of December 2023 on GitHub but the issues simply get closed
.
And a similar issued close recently because of ‚Äúrepoduceability‚Äù
. Classic.
Clearly the DuckDB is sweeping this problem under the carpet.
I set the memory limit to 3GB, keeping some for overhead. That didn‚Äôt work either. Still OOM.
root@localhost:~# python3 duckdb-test.py 
Traceback (most recent call last):
  File ""/root/duckdb-test.py"", line 27, in <module>
    main()
  File ""/root/duckdb-test.py"", line 7, in main
    duckdb.sql(""""""
duckdb.duckdb.OutOfMemoryException: Out of Memory Error: could not allocate block of size 30.5 MiB (2.7 GiB/2.7 GiB used)
I‚Äôm pretty much out of options at this point, I‚Äôm no DuckDB expert and I‚Äôve tried everything listed in the Docs and from other ideas online, I‚Äôve pushed it about as far as I‚Äôm willing to.
It doesn‚Äôt seem to be a rare error, from what I Googled people randomly get this problem reading all sorts of large datasets from Parquets to CSVs. Go look for yourself.
Thinking about DuckDB and Polars for Larger Than Memory Workloads.
Subscribe now
The funny thing is both systems have their quirks. Goes to prove that no tool is perfect, and thinking any one tool is the end-all-be-all is kinda silly.
It‚Äôs clear though that Polars is your best bet to use commodity hardware to cheaply work on larger-than-memory datasets. DuckDB isn‚Äôt there yet obviously. Unless maybe you get lucky.
Tools are like people, you never know what you‚Äôre going to get until they are under pressure.
My other hesitation with DuckDB now is that it will apparently require large and expensive Cloud Compute to work on any dataset of real size. I‚Äôm not sure if you checked pricing recently, but it‚Äôs not cheap to have large Memory EC2 instances on AWS, for example.
Polars is the obvious choice here, we had to jump a little fence when we used PyArrow to scan the s3 CSV datasets, but it was pretty simple. Thinking about running pipelines cheaply and effectively, we would want the ability to use cheaper machines to run real-life datasets.
I‚Äôm sure I will get angry comments and emails from all the DuckDB zealots but I don‚Äôt care. I‚Äôm just trying out tools like any other average engineer would do, Googling problems and applying solutions from the documentation and other ideas.
Yet another disappointing experience with DuckDB.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
141912746.why-arent-you-filtering-more,Why Aren‚Äôt You Filtering More?,"Save Money and Time, and Money",2024-04-01 08:01:35.212-05,"A story as old as time, you and I‚Äôve been around the block plenty before eh? Yet, after more than a decade of doing Data Engineering work ‚Ä¶ I still forget sometimes. Old habits die hard.
If I think back even further, to my old SQL Server Data Warehouse days, the same thing applied. I spent years tuning queries, and even today, piddling around Spark pipelines looking for a little efficiency here and there.
It comes down to one thing ‚Ä¶ filter early, filter often.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Filter, filter, filter. Then filter again.
Sometimes I‚Äôm surprised, sometimes not, technology comes and goes, but I would say on average that the Core Data Engineering fundamental skills really haven‚Äôt changed that much.
Sure, Hive Partitioning is going the way of the dinosaurs, and
Clustering
(
aka invisible partitioning
) reigns supreme ‚Ä¶ but guess what ‚Ä¶ that‚Äôs just an abstraction.
No matter how smart computers and software get, no matter how hard you try to obfuscate away the mundane or obvious, you can only shuffle the pea back and forth under so many cups until you can‚Äôt anymore.
Filtering datasets is one of those unsung heroes ‚Ä¶ it‚Äôs a skill in and of itself, yet it seems so obvious no one talks about it. You just learn it by experience and heartache eventually.
I mean what is it we do all day long?
What we do at the core as Data Engineers
Now I‚Äôm sure you will argue with me about all the wonderful and cool things you do that aren‚Äôt this. Don‚Äôt worry, I do them too.
architecture
planning
mentoring
etc
We ingest data from somewhere, do what we do to it, and spit it back out the other end like some grumpy old middle school lunch lady.
But even those things my dear Watson are for a single purpose. To smash, cajole, massage, punch, and fight our data into submission.
The stuff in the middle.
Most of what the average Data Engineering does during the day is the middlebox. Who knows. Just a bunch of data wrangling, doing aggregations, transformations, business logic, blah, blah, blah.
Then we stick the result somewhere nice and go take our paycheck to the bank.
And this, my friend, is where data
filters
come into play.
Why Filter?
I imagine a lot of my readers, like myself, have moved into the Data Lake and Lake House era of Data Engineering ‚Ä¶ although this topic of filtering first applies to traditional relational databases like Postgres or SQL Server, it is of magnitudes more importance for file-based systems like raw CSV‚Äôs, Parquets, and tech like Delta Lake and Iceberg.
When working with Big Data technology ‚Ä¶ more data = more compute = higher cost = higher runtimes.
You MUST filter early and often.
The problem with filtering data in the context of Data Engineering is that you quickly wander into the land of Business Context.
To properly filter large datasets you need to have an understanding of what you are actually trying to accomplish and the consequences of what you filter.
You simply need to take the time to inspect the data you are working on and immediately throw out all the data you don‚Äôt need.
Working with time series-based datasets, filter.
Anytime you are working with aggregating data, the chances are there is some filter logic you can apply.
Understand dataset
partitioning
or
clustering
.
Yes, I understand some tech like Spark has query execution plans and tries to push back all applicable filters as far as possible ‚Ä¶ but guess what? When it comes to complex data pipelines, YOU know the data better than anyone else and should be applying filters early and often.
Why risk it?
Share Data Engineering Central
An Obvious Example of Filtering
Probably the most ubiquitous data format these days is Parquet .. and with the rise of tools like Delta Lake, backed by Parquet, these data files are typically stored in s3 or other cloud storage.
Especially when using tech like Delta Lake with hundreds of TBs of data, queries can become very expensive if you don‚Äôt what what you‚Äôre doing.
Enter predicate pushdown.
Predicate pushdown is a performance optimization technique used in query processing, especially relevant in the context of reading data from columnar storage formats like Parquet. Predicate pushdown plays a crucial role in improving the performance of data retrieval operations from Parquet files.
Here‚Äôs how predicate pushdown works with Parquet:
**Predicate Definition**:
A predicate is a condition used in the WHERE clause of a SQL query to filter rows, such as `columnA = 'value'` or `columnB > 10`.
**Without Predicate Pushdown**:
Without predicate pushdown, a data processing engine would read the entire dataset from storage into memory, and only then apply the predicate to filter rows. This approach can be highly inefficient, especially with large datasets, as it involves reading a lot of unnecessary data.
**With Predicate Pushdown**:
With predicate pushdown, the data processing engine communicates the predicate (the filter condition) to the storage layer (in this case, Parquet files). Parquet files are structured in a way that allows them to efficiently evaluate these predicates directly on the storage layer before the data is fully read into memory. This means only the rows or columns that meet the predicate conditions are read into memory.
Benefits
The main benefits of predicate pushdown are:
- Reduced I/O: Since only the relevant data is read, the amount of data transferred from disk to memory is significantly reduced.
- Increased Performance: Filtering data directly at the storage level can lead to much faster query times, as the amount of data loaded and processed is minimized.
- Cost Efficiency: Especially in cloud environments where data transfer and compute resources are billed, reducing the amount of data read can lead to cost savings.
In practice, using predicate pushdown with Parquet files can lead to significant performance improvements in data processing tasks, particularly for analytical queries that scan large volumes of data but only need to retrieve a small subset based on specific criteria.
Most modern data processing frameworks, such as Apache Spark, Apache Hive, and Presto, support predicate pushdown automatically when working with Parquet files."
141899616.new-sql-practice-problems-free-for,New SQL Practice Problems - Free For Paid Subscribers,Trying out something new,2024-02-21 11:24:40.357-06,"I‚Äôm trying something new.
I get a lot of questions from folks about getting into the Data Engineering space, how to get better, grow, learn, etc.
So I came up with a solution.
SQL Practice Problems.
Some moons ago I wrote a
Data Engineering Practice repo on GitHub
for free, and some 1.2K stars later it was a big hit.
Because that was so popular, I‚Äôve been using my precious weekends to write another one, this time SQL-based. A lot of folks have asked me when I‚Äôm going to do a course ‚Ä¶ and I always say no.
I‚Äôm a strong believer that you have to write code to get better and learn!!!
I want to test the appetite for paid practice coding exercises. It was a lot of work but I did it.
https://dataengineeringdan.gumroad.com/l/sql-practice
I‚Äôm giving it away free to all my paid subscribers. Use code
SQLRULES
to get 100% off!
These practice problems force learning by using Docker, writing SQL to questions you don‚Äôt know about, AND the results are automatically unit-tested for correctness!!!
I like this approach to learning vs just a course where I would talk.
Also, having the instant feedback loop of pre-build unit tests to check answers is great as well.
What you can do for me?
I would love for you to give it a try! Give me feedback, what do you like, or not like? Give me other ideas ‚Ä¶
I‚Äôm thinking about doing one with Polars and Spark maybe. I like the challenge of making people use Docker ‚Ä¶ and work a little! Please let me know what you think!!"
141761786.string-manipulation,String Manipulation,The Most Important Boring Thing,2024-02-26 08:02:40.284-06,"You know, I‚Äôve done my fair share of DSA (
Data Structure and Algorithms
) nonsense over the years. In fact you can read about some DSA on this very Substack ‚Ä¶
DSA For The Rest Of Us (Part 1)
- Linked Lists
DSA For The Rest Of Us (Part 2)
- Binary Search
DSA For The Rest Of Us (Part 3)
- Quick Sort
I honestly am ho-hum about the whole thing. If you are looking to learn mostly useless things and feel better about yourself, go ahead and spend your time in fantasy DSA land.
But, I have a different idea.
How about you learn something that you will actually have to do a million times over in your career ‚Ä¶ namely
String Manipulation
? I mean what could be more boring uh?
Plain old String Manipulation.
The problem is that anyone in Data Engineering, or the Data Space for that matter, spends more time than they would like to admit manipulating Strings in all manner of ways. You probably do it so much that you don‚Äôt even think about it anymore.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Strings, Strings, Strings.
If someone had told me before I started this all, when I was playing around with Perl, that low these many years and decades later, while having an illustrious career, designing Data Platforms and Machine Learning platforms ‚Ä¶ that to this very day, I would still find myself munging around Strings ‚Ä¶ well I don‚Äôt know.
What‚Äôs a person to do? The best you can.
So, let‚Äôs just have some fun today. Let‚Äôs do some typical String manipulation in both Python and Rust (
just because we can
), and hopefully help those Young Bloods get a taste what what their future looks like.
Sample Problems to Solve.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Let‚Äôs start with some of the most common problems of all with Strings in a Data Engineering context, and just see where it leads us.
One place we‚Äôve all been before is `s3` URIs, basically cloud storage file location. Oh, Lord knows how many times we‚Äôve manipulated cloud storage paths.
You would think we would have written general functions to do this stuff for us by now, but I guess we are gluttons for punishment.
Let‚Äôs just solve some problems and see WHAT it is exactly we have to do with Strings afterward.
Separating Strings ‚Ä¶ aka Splitting them.
Finding the last occurrence of a String in a String
Finding the first occurrence of a String in a String
Building Strings from other Strings
I mean that‚Äôs what it all boils down to, isn‚Äôt it?
How can we split the s3
bucket
apart from the s3
key
?
Easy stuff in Python uh?
The main feature is
splitting
a String. The concept of splitting a String is extremely common in most languages. In the case of Python, we get a
List/Array
back and we can take the first result in the List.
Basically, we are saying split(‚Äò/‚Äô) the string on the
/
‚Ä¶ give me the stuff on either side.
Split, split, split all day long, split, split, split while I sing my song. What, your Grandma never taught you that one??
What about Rust? I‚Äôm going to do this in a very verbose manner ‚Ä¶ no syntactic sugar.
I mean the cool kids (of which I was never one) in Rust would probably write ‚Ä¶
Again, this splitting a String concept is
pretty much
identical in Rust and Python. We can split on some specific char, and then grab either the front or back from the resulting
Array/List
.
Notice in both answers, Python and 2 Rusts, we also did a replace, of some part of the string with Nothing. This replace feature is something that is used consistently in Data Engineering when manipulating Strings.
Finding the last occurrence of a String in a String
What else can we do with Strings that happen often? How about the classic finding the last occurrence of a String in a String? The practical application would be getting the unknown filename out of a s3 URI.
In this case, all we know is we need to find the last /, and everything else afterward is the String ‚Ä¶ filename we are after.
(
There are actually shortcuts to this answer using split ‚Ä¶ but we will pretend that doesn‚Äôt exist for now
)
Here‚Äôs some Python.
Here‚Äôs some Rust.
Again, these are extremely similar answers. Both provide a
rfind()
method that is very helpful indeed. Also, once the location or index of the String within the String is found ‚Ä¶ we use that to ‚Äú
slice
‚Äù into the original string at a specific location and ‚Äú
take
‚Äù the rest.
Things of Note
I think it‚Äôs important to stop here, for those newbies, who maybe haven‚Äôt worked a ton with Strings and talk about
Strings
as
Arrays
or
Lists
.
This ain‚Äôt no Computer Science class, so take your comments elsewhere, but you can think of a STring as a List or Array of index locations, each location with one char, of which in total, make the entire String.
This is what is being done with the above solutions, we are finding the index of the last /, and then slicing into the array to take the
FOLLOWING
index until the end of the array.
Building Strings
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Probably the most mundane topic for today, and the easiest part,
building strings
. Those who‚Äôve been around the block have done this a million and one times ‚Ä¶ but let us not forget our younger counterparts.
When we are splitting strings, finding occurrences, or pulling apart strings ‚Ä¶ usually the next thing we do is put them back together again, like Humpty Dumpty.
Let‚Äôs say we want to pull the
date parts
out of an s3 URI because that‚Äôs our only option, let‚Äôs do that and put the date parts back together again.
This answer sort of combines some of the topics we‚Äôve already talked about ‚Ä¶ mostly we reuse the idea that Strings are just Lists/Arrays of chars at different indexes.
And the same thing in Rust.
Amazing how similar the two solutions are eh?
Thoughts on String Manipulation
Nothing much has changed all these years of Data Engineering, all the comings and goings of technologies, companies, whatever. And here I am, still piddling with Strings these many moon later.
I think in the beginning, when we first start working with Strings, things are unfamiliar. I remember those days, you do the best you can.
We forget that we can even interact with Strings as Arrays/Lists. We forget about all the nice built in functions ‚Ä¶ split ‚Ä¶ rfind ‚Ä¶ etc.
Let me know what you do with Strings!!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
141684906.unit-testing-for-data-engineers-43b,Unit Testing for Data Engineers,Do it. Or Not.,2024-02-19 08:45:09.545-06,"Maybe you've gone your entire career without writing a single test, or you're in a place that writes them but you can't quite figure out what value they bring.
Maybe you simply don‚Äôt like them.
Perhaps you've endured incessant battles about how much code coverage is enough or whether you really need to strive for 100%.
Maybe you‚Äôve taken a
7-hour road trip to the Great White North
sitting next to a TDD zealot who refuses to be brought into the light of day like some test vampire hiding underneath a desk sucking the life of unsuspecting victims.
If you're here, and you aren't clear on what the reasons (
and yes, there's more than one
) are for test-driven code, or not, I hope to offer you a path to enlightenment.‚Äò
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
What even is a test?
This question doesn't have a single
straightforward
answer, leading to some of the confusion people often experience around testing. Instead of talking about what a test
is
, it might be helpful to talk about
what a test
does
(when done properly):
Validates correct behavior
Documents your code
Protects against regressions
Justifies your code
We will call these our four pillars of
proper testing.
And, to be clear, ""
properly
"" means understanding TDD (""test-driven development"") and being disciplined about it, not simply creating a few aimless tests for the sake of it.
If you're not disciplined, the value of tests plummets rapidly, but it's hard to be disciplined if you don't appreciate the value of testing in the first place.
Whether you decide to embrace classic TDD as propounded about the aged pundits stuck in the past, or simply
better testing
than you have today ‚Ä¶ well that‚Äôs up to you.
Choose wisely and carefully.
Validating behavior
Subscribe now
I think this point is the most self-evident on the list. Basically, the
SUT
(""
system under test
"") is a function, and you want to demonstrate its correct working behavior by throwing inputs at it to see what kind of outputs you get.
Also, if you're not writing
pure functions
, you have some hidden complexities with
side effects
that also need to be addressed in tests. Thus, if when thinking of side-effects (
e.g. I/O, system clock, global variables
), it should be as if they are also inputs to the system.
For example ‚Ä¶ we have this function.
def add(x, y):
    """"""Add two numbers together.""""""
    return x + y
In this play example, we should ensure we are testing the function from multiple angles if you will.
import unittest

class TestAddFunction(unittest.TestCase):
    def test_add_positive_numbers(self):
        self.assertEqual(add(3, 4), 7, ""Should be 7"")

    def test_add_negative_numbers(self):
        self.assertEqual(add(-1, -1), -2, ""Should be -2"")

    def test_add_zero(self):
        self.assertEqual(add(0, 0), 0, ""Should be 0"")

    def test_add_positive_and_negative(self):
        self.assertEqual(add(-5, 5), 0, ""Should be 0"")

    def test_add_with_floats(self):
        self.assertAlmostEqual(add(0.1, 0.2), 0.3, places=1, msg=""Should be approximately 0.3"")

if __name__ == '__main__':
    unittest.main()
I think the point is you need to test a function fully, not just as a token sacrifice to the
Angry TDD Gods
.
Look here ‚Ä¶ I wrote a test ‚Ä¶ who knows if it‚Äôs any good ‚Ä¶ it‚Äôs there!
In case you're not already thinking of it in these terms, exceptions are outputs. Tests must not only account for the obvious outputs, but also provide for all the ways your code could break.
At the end of the day, you should be able to write a test for every possible way you can use your function.
Documenting your code
If done well (and in conjunction with
clean code principles
and other best practices as appropriate), your tests will serve as living documentation for your code.
For starters, if you're observing the clean code principles (
e.g.: naming things well, single responsibility
), your code may already be answering the question ""
what is it doing?
"" simply by existing.
As necessary, you might also have comments to explain ""
why do we do this?
"" where names alone don't make things clear. Where tests come in is in answering ""
how
do we do this?
"" or ""
what
can we do?
""
Every test is living proof that ""
if you give my function XYZ, it gives back ABC.
"" You could spend the time to write detailed documentation for your code to serve this purpose, but if you're already writing tests, you get this documentation for free.
Protecting against regressions
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
If we're being honest, writing the program the first time is the easy part. Maintenance down the road, such as bug fixes or adding features, is the hard part;
if you pull a thread you want to know with confidence you didn't just unravel the entire ball of yarn by mistake.
If you've been properly test-driving your code, you're also protecting yourself from code tampering down the road.
If you've been pursuing comprehensive test coverage, you should be able to fearlessly refactor your code and know whether you've made things
worse
in the process.
Justifying your code
To be clear,
if you decide to approach TDD as a discipline
, you are not simply bolting tests onto already-written code. You must write the test first, and any subsequent code is an effort to appease that test.
When letting tests dictate the needs of your codebase, you are giving your code an immediate reason to exist, and ensuring that you're only writing the code necessary to fulfill requirements.
If done right, you will have no code that exists without a purpose, and those purposes will be explained by the tests themselves. It's an extension of the ""
living documentation
"" point above, and it is also the key to 100% code coverage.
When done this way, you're ensuring that nothing is ""
slipping through the cracks
"" in your codebase and you should have few surprises.
What kinds of tests do I need?
Yes, that's right, there's more than one kind of test.
When discussing TDD, the most famous example is the
""unit test,""
one that minimally validates a particular behavior with a simple ""
if given this, XYZ happens
"" workflow. These unit tests are developer-centric - meaning, they exist for the purposes of facilitating development and documenting the codebase for other developers.
If you have a lead developer, you might be getting your specifications in the form of low-level requirements that translate directly to unit tests. In all likelihood, though, you'll also (or maybe even exclusively) be getting your requirements in high-level ""
I want a doohickey that does XYZ
"" language.
In this paradigm, you might start to see
BDD (""behavior-driven development"")
. Rather than TDD's approach of eliciting tiny incremental code changes to drive toward a working product,
BDD presents scenarios that your code must fulfill
.
Tests of this nature tend to produce more human-readable specifications than TDD, and serve as an excellent bridge between developers and other stakeholders such as QA or product owners.
Not everything you write will be an isolated function. It's also not always practical to use TDD to test complex functions that themselves are composed of other SUTs that have been tested elsewhere. For this, you'll find yourself reaching for
mocking
tools.
Also, you'll inevitably write code that pulls in external dependencies, which also leads to
boundary tests
or
contract tests
to ensure that you're using dependencies correctly and that they are what you think they are (e.g. ensuring that an API you depend on hasn't changed on you since you first started working with it).
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Choosing TDD
I won't get too detailed here, but there are some points I want to call out ‚Ä¶
For TDD to work, you need to have a goal in mind, lest you fall into an
underpants gnomes
trap.
Work in incremental steps. You're not writing tests that elicit entire programs at once, but instead are trying to tease out a working program from simple tests that gradually start to build a big picture.
As you're writing tests, they will initially be failing, and the test should then force you to go back to your code and write the least amount possible to make the test pass without breaking the other tests.
Choosing BDD
One of the biggest critiques of TDD is that it assumes that you can reason out a large system by first working through its smallest units of work.
For that to work, though, you need a handle on what those units are, and sometimes the problem you're tackling isn't so well-defined.
Also, with that level of granularity, it can be hard to ""understand"" what the program is supposed to be doing simply by virtue of the sheer volume of test noise. In many ways, it's a death-by-papercut approach to writing software.
BDD, on the other hand, looks at a function as a black box and is less dogmatic about how the function is written. With TDD, the developer is often trying to get a developer to write a specific line of code, and it can often lead to code that exists because the developer ""knows"" that it will be needed.
Note, it doesn't have to be either/or - BDD can complement TDD within a project, and can be a great way to give TDD a sense of direction or purpose.
For BDD, a DSL exists in the form of
Gherkin
- a human-readable specification for how a feature should be implemented. A feature comprises scenarios, each of which is responsible for explaining the different kinds of inputs/preconditions and how they translate to an expected outcome.
The Python library
behave
is able to parse these files and relies on developers to then flesh out how the different steps translate to actual test code. While the Gherkin files make for excellent documentation (far better than what comes out of TDD), the process of implementing the steps can create more cruft and work than the simpler TDD tests, and requires the additional testing framework to pull off.
Alternatively, Gherkin could serve as a rough structural template (given/when/then) for how to write tests, giving the tests a BDD-like pattern without the additional overhead of a framework.
Unit tests for beginners
So, you've decided you want tests but aren't sure where to start. First, let's put some lines in the sand, if only to make your life easier:
Don't write tests to fit the implementation - this is backwards, and you'll inevitably get something wrong
Don't try to force a specific implementation through janky tests; too many opinionated developers try to force a pairing partner to write specific code, and the resulting tests are hideous
Work incrementally; resist the urge to ""know"" in advance what you need to do (YAGNI). If it's needed, you'll eventually end up writing the test, so don't get ahead of yourself.
Ultimately, all this means you're not going back and retrofitting existing behavior with tests.
Example
Let's do something simple - a calculator. It's the perfect foundation for an incremental development workflow, but today we're just going to get some basics knocked out.
Strap in, we're in for a bit of a ride...
Our first test
Let's be as naive as possible. Start by asking what the most fundamental aspect of a calculator is -
my own answer to that is that it is a thing that gives us results:
calculator.py
:
import unittest

class CalculatorTests(unittest.TestCase):
    def setUp(self):
        self.calculator = Calculator()

    def test_initial_result(self):
        # SUT (system under test):
        result = self.calculator.getResult()

        # Validate expected result, giving a meaningful explanation:
        self.assertEqual(result, 0, ""initial result should be 0"")
Above, we have a ""
happy path
"" test that asserts that our not-yet-written Calculator gives us an initial result of
0
. The structural elements of this test are:
setUp
wires up dependencies for us to make an assertion; it's invoked anew for every test (we wouldn't want one test's behavior to mess with the results of another, after all)
test_initial_result
is the test function representing our scenario, in which we call the
getResult
method and assert that it initially spits out a
0
result
self.assertEqual(<actual>, <expected>, <explanation>)
asserts that the SUT result is as desired, and when it inevitably fails on our first go, we'll get a nice message explaining why it failed.
We run this using the
pytest
test runner from the command line:
$ pytest calculator.py
It should explode gloriously because we haven't given it a Calculator to work with yet.
This is expected, and the resulting failure from the test runner is the impetus needed to get us to write some code:
================================================== FAILURES ===================================================
_____________________________________ CalculatorTests.test_initial_result _____________________________________

self = <test.CalculatorTests testMethod=test_initial_result>

    def setUp(self):
>       self.calculator = Calculator()
E       NameError: name 'Calculator' is not defined

test.py:5: NameError
=========================================== short test summary info ===========================================
FAILED test.py::CalculatorTests::test_initial_result - NameError: name 'Calculator' is not defined
============================================== 1 failed in 0.03s ==============================================
Now that we have a failing test, let's address the immediate failure we're given - in this case, we need to create a
Calculator
class to satisfy the test runner; write the least amount of code possible to clear that error and move onward:
class Calculator:
    pass
I place this directly above the
CalculatorTests
class, and now get a different error:
>       result = self.calculator.getResult()
E       AttributeError: 'Calculator' object has no attribute 'getResult'
Okay, now we write just enough code to satisfy the need for
getResult
:
class Calculator:
    def getResult(self):
        pass
...
>       self.assertEqual(result, 0, ""initial result should be 0"")
E       AssertionError: None != 0 : initial result should be 0
... and our resulting error now forces us to give a result:
class Calculator:
    def getResult(self):
        return 0
Finally, we get a success message from the test runner. It may seem like a lot of work, but it does get easier from here; that first test led to a lot of infrastructure, but subsequent tests will require less effort.
More tests
Our second test will set us up for additional behavior (
the
add
function
). We're ultimately looking to create a function that lets us add to our accumulator with every function call. To start, let's add a new method to the
CalculatorTests
suite:
class CalculatorTests(unittest.TestCase):
    # ... skipping over the existing implementation

    def test_add_zero(self):
        # SUT (system under test):
        self.calculator.add(0)

        result = self.calculator.getResult()

        # Validate expected result, giving a meaningful explanation:
        self.assertEqual(result, 0, ""adding 0 to initial value should give 0"")
So, we're adding
0
to our initial
0
and expecting
0
back. Your first step is to run tests, and see the error that forces us to then define the function:
def test_add_zero(self):
        # SUT (system under test):
>       self.calculator.add(0)
E       AttributeError: 'Calculator' object has no attribute 'add'
So, just as before, you add the function:
class Calculator:
    # ... skipping over the existing implementation
    
    def add(self, value):
        pass
This should be enough to pass our test. Hopefully, you resisted the urge to do more, since that would be wrong. Instead, if we want to make our
add
function do more meaningful things, we'll need a new test to give us that functionality:
# We want random numbers for this next test:
import random

class CalculatorTests(unittest.TestCase):
    # ... skipping over the existing implementation

    def test_add_random_value(self):
        # given a random number to add:
        toAdd = random.random()

        # when adding that number to the calculator:
        self.calculator.add(toAdd)

        # then our accumulator should equal our added value:
        result = self.calculator.getResult()
        self.assertEqual(toAdd, result, f""adding {toAdd} to initial value should give {toAdd}"")
To make our test as robust as possible, we use random inputs. We could just write some tests with fixed values for input,
but if we're being TDD purists
, we'd end up with a
switch
statement covering just the fixed inputs. Here, we're forcing our implementation to account for any numeric value.
result = self.calculator.getResult()
>       self.assertEqual(toAdd, result, f""adding {toAdd} to initial value should give {toAdd}"")
E       AssertionError: 0.770880984964011 != 0 : adding 0.770880984964011 to initial value should give 0.770880984964011
Now, you're in for a treat: in order to make this test pass, we'll need to add some state to our Calculator to let it hang onto results, and also need to go back and retrofit our
getResult
implementation to supply that value.
Remember, we're making the smallest changes here that don't ultimately break previously-passing tests:
class Calculator:
    def __init__(self):
        # Give us some state, initialized to 0
        self.accum = 0

    def getResult(self):
        # Return the results we've accumulated thus far
        return self.accum

    def add(self, value):
        # Since we only add one time at most, we can get away with blowing out previous state:
        self.accum = value
As noted in the above comments, our tests aren't yet robust enough to force us to actually add; thus far, we've only asserted that we can get the same value back out that we passed in with
add
. As tedious as it sounds, this justifies another test:
class CalculatorTests(unittest.TestCase):
    # ... skipping over the existing implementation

    def test_add_multiple_values(self):
        # given a random number to add:
        toAdd1 = random.random()
        toAdd2 = random.random()

        # when adding both numbers to the calculator:
        self.calculator.add(toAdd1)
        self.calculator.add(toAdd2)

        # then our accumulator should equal our added values' combined total:
        result = self.calculator.getResult()
        expected = toAdd1 + toAdd2
        self.assertEqual(expected, result, f""adding {toAdd1} and {toAdd2} to initial value should give {expected}"")
Now, we're forced to go back to our
add
implementation and actually accumulate our results. In its entirety, the code should now look like the following:
import random
import unittest

class Calculator:
    def __init__(self):
        self.accum = 0

    def getResult(self):
        return self.accum

    def add(self, value):
        self.accum += value

class CalculatorTests(unittest.TestCase):
    def setUp(self):
        self.calculator = Calculator()

    def test_initial_result(self):
        # SUT (system under test):
        result = self.calculator.getResult()

        # Validate expected result, giving a meaningful explanation:
        self.assertEqual(result, 0, ""initial result should be 0"")

    def test_add_zero(self):
        # SUT (system under test):
        self.calculator.add(0)

        result = self.calculator.getResult()

        # Validate expected result, giving a meaningful explanation:
        self.assertEqual(result, 0, ""adding 0 to initial value should give 0"")

    def test_add_random_value(self):
        # given a random number to add:
        toAdd = random.random()

        # when adding that number to the calculator:
        self.calculator.add(toAdd)

        # then our accumulator should equal our added value:
        result = self.calculator.getResult()
        self.assertEqual(toAdd, result, f""adding {toAdd} to initial value should give {toAdd}"")

    def test_add_multiple_values(self):
        # given a random number to add:
        toAdd1 = random.random()
        toAdd2 = random.random()

        # when adding both numbers to the calculator:
        self.calculator.add(toAdd1)
        self.calculator.add(toAdd2)

        # then our accumulator should equal our added values' combined total:
        result = self.calculator.getResult()
        expected = toAdd1 + toAdd2
        self.assertEqual(expected, result, f""adding {toAdd1} and {toAdd2} to initial value should give {expected}"")
Are you willing to drink the kool-aid?
At this point ‚Ä¶ you probably get the point. And, one this is clear, you are either going to decide to head down the road of TDD or not.
For some people, the above work seems meaningless and they will just never put in the time or effort required to embrace a TDD testing approach that is pure. Humans will be humans.
You will generally find the following types of folks ‚Ä¶
No tests ever ‚Ä¶ they slow me down and I won‚Äôt do it.
I will write token tests here or there to make someone happy
I will die on the TDD hill of greatness above all else.
I will write good unit tests for all my code after I have written my code.
Conclusion
Hopefully what we covered today gives you an appreciation for what testing looks like (what types of testing is out there), why you want it, and some of the things to keep in mind as you put it into practice in your own code.
If you look into other online resources, you'll quickly discover myriad takes on testing. Some question its fundamental value, others will have opinionated takes that challenge everything above.
There is no one right way to test. It's a tool, like any other, and if done well should be an asset to your code.
Also, etiquette for many projects demands that you accompany your code with tests. Try to contribute to some projects, and you'll in many cases be expected to provide backing tests for your changes - consider
this example CONTRIBUTING.md file
, for instance, whose guidelines include the following point:
Include test coverage.
Add unit tests or UI tests when possible. Follow existing patterns for implementing tests.
In short, whether or not you believe in adding tests to your own code, you may find yourself having to do so regardless if your career in software development is to flourish."
141587653.introduction-to-ray,Introduction to Ray,Distributed Compute,2024-02-29 10:45:47.952-06,"I‚Äôve wanted to look into this for a long time. Time flies, too much going on, never have time. I heard about
Ray
many moons ago, heard nothing for years, then when LLMs started to become a thing, pop goes the weasel, there was
Ray
again.
I thought that thing curled up in a corner and wilted away to nothing. Guess I was wrong.
A few days ago when I was wondering when something interesting would come across my feed, something besides the same old data modeling, Python, or Spark cruddy crud ‚Ä¶
I saw a blurb about Databricks support for Ray Clusters and workloads
.
I figure if Databricks is working on Ray, I should probably pay attention, and you too for that matter.
Introduction to Ray
Let‚Äôs start by getting a high-level overview of
what Ray is
, what it is used for, and why you should care about it.
‚Äú
Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:
‚Äù -
ray-project
What does that mean in lay terms for us
Grug-brained developers
?
What it means is that Ray is a distributed compute engine for Python-centric code, with an emphasis on Machine Learning operations.
What do we mean by Machine Learning operations?
Data
: Scalable Datasets for ML
Train
: Distributed Training
Tune
: Scalable Hyperparameter Tuning
RLlib
: Scalable Reinforcement Learning
Serve
: Scalable and Programmable Serving
It‚Äôs clearly a tool built for pure Python Machine Learning pipelines, end to end.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What is so special about this???
Share Data Engineering Central
You have to understand something about Machine Learning, and its recent history to understand why something like Ray even exists and why it is a ‚Äúbig deal‚Äù in its own right.
I‚Äôm going to try to make this easy for you.
Back in the day when CatBoost, XGBoost, Scikit learn first got popular and were used for ML applications ‚Ä¶ before the days of Databricks popularity ‚Ä¶ Machine Learning happened on laptops, single nodes.
If things got crazy, you would spin up a massive Instance on GCP or AWS and let some model train for days on end, I‚Äôve even seen a week.
This was because
things like Dask
circa ~5+ years ago and more were just not what they are today. They sucked. It was extremely hard to run Machine Learning operations.
If you think MLOps tools and libraries are confusing and buggy now, think about what they were 7 years ago. Yikes. Everything was hard.
SparkML
was the first thing to ‚Äúmake it easy‚Äù to do Machine Learning with Python at scale ‚Ä¶ and by scale, I mean stuff that can‚Äôt be run on a single node because of data size.
That was a lot of blithering to make a point.
All this is to say that not much has changed in Python-centric Machine Learning at scale, especially for customized and specialty ML operations.
What it comes down to is that distributed compute on Python applications (ML data pipelines) is very hard, and still a new changing environment. Everyone is still battling Spark as the dominate force in the space.
Back to Ray.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Back to Ray. So what‚Äôs the deal with it?
You need to remember that the ML space is made up of Data Engineers, Data Scientists, and some unicorn ML Engineers. What do Data Engineers and Data Scientists have in common?
Python.
It‚Äôs the language of choice, even in the age of AI and LLMs, Python is what 90%+ of what the work is going to be done in. So, if you have some DS and DE working together ‚Ä¶ having them be able to use the same Python package to write ‚Ä¶
Data Ingestion
Training
Model Inference
etc.
And then be able to have that code scale from the laptop to a Cluster. That‚Äôs how you get an ML Project to actually finish. Unlike most that fail.
This is the gap Ray is trying to fill. If you have something that isn‚Äôt running on Spark ‚Ä¶ but it‚Äôs too big to run on a single machine effectively, Ray is here to save the day.
Trying out Ray the hard way.
One thing I‚Äôve always been a sucker for is trying something new out the hard way. I feel like you learn a lot about a tool by doing things yourself, you find the bugs ‚Ä¶ it tells you a lot about how stable a tool is.
We are going to ‚Ä¶
Setup a Ray Cluster manually
Read some data from s3
Transform data.
Save data back to s3.
Now Ray is supposed by ML centric so we should probably pretend like we are doing ‚Äúfeature engineering‚Äù work. I don‚Äôt really care what we do, this is just for fun.
I will turn to my trusty
Linode
to do this work. (
when setting up servers make sure you use firewalls like UFW etc and configure them correctly so only your nodes can talk to each other, and you, no one else
)
First, I ssh‚Äôd into each machine and ran ‚Ä¶
pip install -U ""ray[data,train,tune,serve]""
Let‚Äôs see if we can get this Ray head node started ‚Ä¶
root@localhost:~# ray start --head --port=6379

Enable usage stats collection? This prompt will auto-proceed in 10 seconds to avoid blocking cluster startup. Confirm [Y/n]: y

Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.

Local node IP: 172.232.25.170

--------------------

Ray runtime started.

--------------------

Next steps

¬† To add another node to this Ray cluster, run
¬† ¬† ray start --address='172.232.25.170:6379'
¬† To connect to this Ray cluster:
¬† ¬† import ray
¬† ¬† ray.init()

¬† To submit a Ray job using the Ray Jobs CLI:
¬† ¬† RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py
¬† See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html¬†
¬† for more information on submitting Ray jobs to the Ray cluster.
¬† To terminate the Ray runtime, run
¬† ¬† ray stop

¬† To view the status of the cluster, use
¬† ¬† ray status
¬† To monitor and debug Ray, view the dashboard at¬†
¬† ¬† 127.0.0.1:8265

¬† If connection to the dashboard fails, check your firewall settings and network configuration.
The next task is to see if we can get the two Ray worker nodes to connect to the head.
ray start --address='172.232.25.170:6379'
Well, look at that. We have a Ray cluster up and we can see our Nodes connected to it!
Maybe there is something to be said for Ray, after all, it works, and that‚Äôs usually half that battle in my experience.
I‚Äôm running out of steam, so let‚Äôs see if we can just make something happen at this point, run some code, any code, please!!
I put some data in my s3 bucket, flat files.
Let‚Äôs write a quick file to read some data and print the schema.
import ray
ds = ray.data.read_csv(""s3://confessions-of-a-data-guy/harddrives*.csv"")
ds.schema()
Now let‚Äôs try to submit that job. (t
his command is being run from the head node
)
RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python ray-test.py
That didn‚Äôt work.
root@localhost:~# RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python ray.py
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 169, in _new_conn
    conn = connection.create_connection(
  File ""/usr/lib/python3/dist-packages/urllib3/util/connection.py"", line 96, in create_connection
    raise err
  File ""/usr/lib/python3/dist-packages/urllib3/util/connection.py"", line 86, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused
Interestingly,
the connection was refused
. This is the exact command Ray printed out and told me to use when first starting the cluster.
I swapped the command over to use the actual IP address of the head node.
RAY_ADDRESS='http://172.232.25.170:8265' ray job submit --working-dir . -- python ray.py
Little something more, but no juice.
root@localhost:~# RAY_ADDRESS='http://172.232.25.170:8265' ray job submit --working-dir . -- python ray.py
Job submission server address: http://172.232.25.170:8265
2024-02-24 03:46:45,950	INFO dashboard_sdk.py:338 -- Uploading package gcs://_ray_pkg_0928dcf8aadccc97.zip.
2024-02-24 03:46:45,951	INFO packaging.py:530 -- Creating a file package for local directory '.'.
2024-02-24 03:46:45,960	WARNING packaging.py:405 -- File /root/.cache/pip/http/a/7/8/0/3/a7803f03ea53af03e3fa238b4e4e3d4850ed940522435f1303913537 is very large (17.40MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/root/.cache/pip/http/a/7/8/0/3/a7803f03ea53af03e3fa238b4e4e3d4850ed940522435f1303913537']})`
2024-02-24 03:46:46,031	WARNING packaging.py:405 -- File /root/.cache/pip/http/f/6/5/3/d/f653df7413c43b545d9c1d60ed4bd3b2de5c176614b533008f7d0bc6 is very large (61.86MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/root/.cache/pip/http/f/6/5/3/d/f653df7413c43b545d9c1d60ed4bd3b2de5c176614b533008f7d0bc6']})`
2024-02-24 03:46:46,205	WARNING packaging.py:405 -- File /root/.cache/pip/http/2/2/5/b/1/225b1db760c744f98502e5ac3f7f2bfc6af1e58e70cd9b8001d5e23f is very large (36.54MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/root/.cache/pip/http/2/2/5/b/1/225b1db760c744f98502e5ac3f7f2bfc6af1e58e70cd9b8001d5e23f']})`
2024-02-24 03:46:46,299	WARNING packaging.py:405 -- File /root/.cache/pip/http/9/0/2/3/9/90239c0dd9e2bb9aafc45a839ede0e329545be5148ece2bb510ad442 is very large (12.42MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/root/.cache/pip/http/9/0/2/3/9/90239c0dd9e2bb9aafc45a839ede0e329545be5148ece2bb510ad442']})`
Traceback (most recent call last):
  File ""/usr/local/bin/ray"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.10/dist-packages/ray/scripts/scripts.py"", line 2498, in main
    return cli()
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1128, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1053, in main
    rv = self.invoke(ctx)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1659, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1659, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 1395, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/usr/lib/python3/dist-packages/click/core.py"", line 754, in invoke
    return __callback(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/ray/dashboard/modules/job/cli_utils.py"", line 54, in wrapper
    return func(*args, **kwargs)
....
aiohttp.web_exceptions.HTTPRequestEntityTooLarge: Request Entity Too Large
That‚Äôs a weird error, no idea what that means ‚Ä¶
HTTPRequestEntityTooLarge: Request Entity Too Large
Interesting,
after reading this note
, and relooking at my command, I think Ray is trying to zip up the entire home directory on Ubuntu I am in while submitting the job.
So what if I remove the ‚Äîworking-dir arg from the command I used (the one Ray told me to use) ‚Ä¶
Progress! But still no juice.
root@localhost:~# RAY_ADDRESS='http://172.232.25.170:8265' ray job submit -- python my_script.py
Job submission server address: http://172.232.25.170:8265

-------------------------------------------------------
Job 'raysubmit_tSe5zsBr4sg1j8iL' submitted successfully
-------------------------------------------------------

Next steps
  Query the logs of the job:
    ray job logs raysubmit_tSe5zsBr4sg1j8iL
  Query the status of the job:
    ray job status raysubmit_tSe5zsBr4sg1j8iL
  Request the job to be stopped:
    ray job stop raysubmit_tSe5zsBr4sg1j8iL

Tailing logs until the job exits (disable with --no-wait):
/bin/sh: 1: python: not found

---------------------------------------
Job 'raysubmit_tSe5zsBr4sg1j8iL' failed
---------------------------------------

Status message: Job entrypoint command failed with exit code 127, last available logs (truncated to 20,000 chars):
/bin/sh: 1: python: not found
Bugger all. Let‚Äôs do Python3.
Ok, we move on to another error. Egads.
---------------------------------------
Job 'raysubmit_QSwjgBVUc3FtSqWx' failed
---------------------------------------

Status message: Job entrypoint command failed with exit code 1, last available logs (truncated to 20,000 chars):
    zip(
  File ""/usr/local/lib/python3.10/dist-packages/ray/data/datasource/file_meta_provider.py"", line 178, in expand_paths
    yield from _expand_paths(paths, filesystem, partitioning, ignore_missing_paths)
  File ""/usr/local/lib/python3.10/dist-packages/ray/data/datasource/file_meta_provider.py"", line 418, in _expand_paths
    yield from _get_file_infos_serial(paths, filesystem, ignore_missing_paths)
  File ""/usr/local/lib/python3.10/dist-packages/ray/data/datasource/file_meta_provider.py"", line 445, in _get_file_infos_serial
    yield from _get_file_infos(path, filesystem, ignore_missing_paths)
  File ""/usr/local/lib/python3.10/dist-packages/ray/data/datasource/file_meta_provider.py"", line 568, in _get_file_infos
    raise FileNotFoundError(path)
FileNotFoundError: confessions-of-a-data-guy/harddrives*.csv
Jezz ‚Ä¶ we are getting somewhere at least. The Ray read of remote s3 files seems straightforward in the docs. Maybe if I remove the *.csv portion.
Well, that almost got us there.
ray.exceptions.RayTaskError(ValueError): ray::_execute_read_task_split() (pid=3094, ip=172.232.30.171)
  File ""/usr/local/lib/python3.10/dist-packages/ray/data/datasource/csv_datasource.py"", line 56, in _read_stream
    batch = reader.read_next_batch()
  File ""pyarrow/ipc.pxi"", line 700, in pyarrow.lib.RecordBatchReader.read_next_batch
  File ""pyarrow/error.pxi"", line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: In CSV column #58: Row #4555: CSV conversion error to null: invalid value '100'
Seems like Ray is having some trouble with all the CSV files. Let‚Äôs just try to read one file I guess.
File ""/usr/local/lib/python3.10/dist-packages/ray/data/datasource/csv_datasource.py"", line 64, in _read_stream
    raise ValueError(
ValueError: Failed to read CSV file: confessions-of-a-data-guy/harddrives/2023-04-01.csv. Please check the CSV file has correct format, or filter out non-CSV file with 'partition_filter' field. See read_csv() documentation for more details.
Nope. I‚Äôve read these CSV a thousand times with tools like Polars, DuckDB, and Spark, so I‚Äôm not sure what the problem is that Ray is having.
I‚Äôm not going to keep pushing, enough for today.
Not Wasting More Time.
At this point, I‚Äôve seen what I wanted to see and learned what I wanted to learn. I‚Äôve seen these problems 1,000 times over in my career in Data Engineering, especially with Python Machine Learning tools.
The truth is that Python-based tools are always going to have these problems. I‚Äôve noticed things like this (Ray, Dask, etc) always have serious problems that most other JVM or Rust-based tooling don‚Äôt have.
But that‚Äôs nothing new for anyone who‚Äôs worked with Python for an extended period of time.
Sure, you can get them working if you have lots of time on your hands and no other options. But, this is why people stick with Spark ‚Ä¶ reliable, easy to deploy and run, no major bugs, and no hidden magic.
Overall I would like to poke at Ray again sometime, hopefully, its stability continues to improve. Next time we will have to try it with Docker and K8s or something and probably get the reliability we need to really kick the tires.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
141537859.batch-vs-near-realtime-vs-streaming,Batch vs Near-Realtime vs Streaming,Nothing New Under the Sun,2024-02-14 11:12:25.327-06,"I feel like if we all listened to the blithering pundits on social media for the last few years we would have gathered all batch ETL pipelines in a pile and burned them all till the flames reached the skies and we all danced around in the moonlight till we frothed at the mouth and collapsed into a pile.
I‚Äôve been hearing about the death of Batch Data Pipelines for years now ‚Ä¶ usually, it comes in conjunction with the release of some new streaming tool.
You would think people would learn their lesson by now. 90% of Data Teams will never need to use Streaming Pipelines, no matter how much you try to make them cool. They are simply unneeded‚Ä¶ for many reasons.
This is what we shall do today. Throw down the high tower from which those Streaming Zealots cast their fiery bolts. We will compare Batch, to Real-Time, to Streaming ‚Ä¶ the tools and architecture used in both, and most of all, the impact on Data Teams of this most important decision.
Batch vs Near-Realtime vs Streaming
I think on the surface sometimes it‚Äôs easy to think that Streaming vs Batch, and Near-Real time in the middle ‚Ä¶ is just not that big a deal when it comes to a Data Platform ‚Ä¶ we Data Engineers in all our Wisdom tend to think .. ‚Äú
no, problem, we got this.
‚Äù
This could not be further from the Truth.
Streaming vs Not changing everything.
The tooling changes
The architecture changes
Data stores are affected
Analytics is affected
Cost can be a big difference
Technical skills and maintenance changes
Real-Talk about Streaming vs Everything else.
Let‚Äôs be honest, sometimes the lines can start to blur between and arguments start to arise about the differences between Streaming, Near-Real-Time, and Batch.
It‚Äôs a matter of perspective sometimes. Just because you think something is not real-time, well, it might be to someone else. It has to do with the perception of how often data needs to be updated.
It‚Äôs true that most Data Teams do not need the overhead and complexity of running Streaming Systems.
Apache Kafka
Apache Storm
Spark Streaming
Flink
GCP Cloud Pub/Sub
Amazon Kinesis
Pulars
NiFi
Like it or not Streaming can be hard. Not that Batch ETL is any easier, but thinking about Architecture from either a Streaming/Realtime point of view is quite different from Batch ETL and Near-Real time.
The obvious proof of this is the different toolsets, not to mention the mental model and different through processes that surround Batch ETL and Streaming.
Trust me, I know tools like Spark Structured Streaming (
yeah, yeah, I hear you all yelling at me that Spark Streaming is micro-batches ‚Ä¶ whatever
) have tried to shorten the distance between Batch and Streaming.
But you and I both know a business model that needs Real-Time data is different from one that does not.
It‚Äôs More Than Just the Tool.
Share Data Engineering Central
The problem is that Streaming data insinuates that the Business needs to make decisions in real-time or have alerts in real-time. The chances of a single tool ‚Ä¶ say Kakfa ‚Ä¶ being able to meet ALL of these needs is close to zero.
You probably have. to think long and hard about
Analytics tooling
Monitoring and Alerting
Data Store
Cost
These things at a minimum can vary largely from Streaming to Batch.
Code and Skills are Different
The other part no one thinks about is the skillset and code, they go together. Do your engineers know and have they worked with Streaming before??
This is a serious consideration. What do I mean?
Create a topic named
iot-sensor-data
for publishing sensor data.
Implement a Producer
Implement a Consumer with Processing Logic
If you are unfamiliar with Producer and Consumer logic, and the nuances thereof ‚Ä¶ Streaming can be a challenge out of the box, and that‚Äôs just to get the code working. Let alone the maintenance of the system.
Real-time Processing:
Streaming pipelines process data in real-time as it arrives. This is ideal for use cases where immediate data processing and minimal latency are critical, such as in monitoring systems, real-time analytics, and online fraud detection.
Continuous Data Flow:
In streaming, data is processed continuously, 24/7, without interruption. There's no concept of starting and stopping the data flow, unlike in batch processing.
Event-by-Event Processing:
Data is processed one event or message at a time, or in small micro-batches, allowing for very low-latency responses.
Complexity and Overhead:
Streaming systems can be more complex to implement and maintain due to the need for fault tolerance, scalability, and dealing with out-of-order data or late-arriving data.
Resource Utilization:
Streaming processing can lead to more consistent resource utilization over time, as data is processed continuously rather than in spikes. But, keeping compute up for streaming processing for long periods can be more expensive.
Non-Streaming.
Share Data Engineering Central
Now we come to what we are all used to, this is what most of the world runs on. Batch ETL. This is where most of the work happens and most of the tools are built. Batch is the name of the game.
I think one of the biggest misconceptions is that Batch ETL is ‚Äúslow‚Äù or ‚Äúoutdated‚Äù or can‚Äôt provide insights for ‚Äúfresh data.‚Äù
Think about it, what separates a Streaming pipeline from a ‚Äúfast‚Äù Batch ETL system? Not much, and does anyone really care??
I‚Äôm sure there are some businesses that do, but based on the reality of the data landscape, *most folk use batch ETL, and works fine.
You got a complaint about your once-a-day ETL batch data pipelines? Well, why not do the obvious and schedule it more often? What would happen if you ran it once an hour?
How to Make Batch ETL Pipelines into Near-Real Time.
Again, some folk will probably take issue with this, but since when do I care? Never.
I‚Äôm convinced the world would be a better place if all Data Engineers spent time tuning their Batch ETL pipelines, making them as fast and optimized as possible, and then scheduled them to run every hour (for the data sources that make sense to do so).
All of the sudden, with just a little elbow grease and no big migration to a Streaming project, you have hourly updates available in your Data Lake, Data Warehouse, and Analytics, whatever those might look like."
141287154.are-data-contracts-for-real,Are Data Contracts For Real?,Or Just More Snake Oil?,2024-02-05 08:16:59.893-06,"I‚Äôm just as surprised as you that I haven‚Äôt gotten to them yet.
Data Contracts
. I felt like 2023 was the year of Data Contracts, at least for anyone in the data space cruising around
Linkedin
or
Twitter
, or other Substacks for that matter.
Sometimes I feel like the guy in the dark corner at the party around the witching hour of midnight, more interested in watching and listening to the blithering mumble jumble of the intoxicated masses screaming in each other‚Äôs ears about their new awesome gadget.
Sometimes you stumble onto an interesting topic, not sure what to make of it, maybe it‚Äôs the next hot thing, maybe not, hard to tell. That‚Äôs Data Contracts for me.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Just a quick word on the topic of tooling for Data Engineering.
We must sit down, look each other in the eyes, and tell the truth. Be honest with yourself about the Modern Data Stack. Real talk now.
Where do innovations come from in the Data Engineering field? Generally speaking ‚Ä¶
Springs up from the open-source community (sometimes from the overflow of a business ‚Ä¶ that wasn‚Äôt in that business).
Think Apache Airflow.
A for-profit company selling X product in the Data Space (could be on top of open-source, or brand new).
Think Snowflake, Databricks, etc.
It can be kinda hard to decifer where things come from and who the Wizard of Oz is behind the curtain.
Some companies can get it right, some can‚Äôt.
Does this matter? Yes and no.
Typically if you have something like Apache Airflow that gets super popular because it solves a real problem ‚Ä¶ you know that tool is legit because the community gets behind it and a wellspring of development and usage happens. You also then get companies improving on that model, and selling something that fills the ‚Äúgaps‚Äù in that product.
Also, you get brand-new things. Think
Snowflake
. They see an opportunity in the Data Warehouse and Data Lake world, maybe Redshift sucks, and they say, ‚Äú
We can do better
‚Äù, and they build something better.
This is where it gets complicated.
And then there is the last section of products that I try to be the most careful of.
That is, companies who try to sell something first, make a piece or part of that product open-source, push that new ‚Äúidea,‚Äù and inevitably their product, onto the wider Data community, wether they want it or not.
Does it work out sometimes? Yes. Does it not work out sometimes? Yes.
Who decides?
The Data community decides
. They either, regardless of marketing, find real value in the tool and ideas, and adopt them at large, or they do not.
Maybe just a small subset of glassy-eyed believers run around in circles frothing at the mouth and screaming ‚Äú
Do this or di
e.
‚Äù
Data Contracts in the Wild.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
This is where we get to the topic of Data Contracts. Hear me out.
I will try to give them a fair chance.
Let‚Äôs hear what
The Great Ones
have to say.
But, here is how I‚Äôve seen the Data Contract discussion come about.
One or two ‚Äú
popular
‚Äù people start bringing up the topic out of nowhere.
Start doing lots of footwork, meetings, talks, and content, all about this topic.
Start a company.
While still trying to act like the ‚Äú
topic
‚Äù is core to the success of the Data community, and hide the fact ‚Äúthey‚Äù are selling something.
But, who am I to throw stones? I like America. You can build what you want and sell to whoever you want. It‚Äôs the market that will decide.
I just can‚Äôt figure out if the Data Community has ‚Äúdecided‚Äù or ‚Äúhad the final word‚Äù on Data Contracts. ARE THEY IN, OR ARE THE OUT???
I took my curiosity to
r/dataengineering
to see what was up, are people talking about Data Contacts much? Not at all.
There are just a few posts over the last few years, and a lot of people wondering about them. Asking about them. No one really
using
them.
Also, places like LinkedIn have been unusually quiet on the subject. It‚Äôs kinda like sticking your finger in the wind, trying to figure out if it‚Äôs going to rain tomorrow. Anecdotally though, it appears the Data Contract buzz was small and short-lived.
Why are Data Contracts Dying on the Vine?
I‚Äôm going to do my best, to sum up what I see with the problem with Data Contracts. Although, I have to say, the idea of data having a contract and being trustworthy ‚Ä¶ who can argue with that?? Of course, we like that idea.
So, why are Data Contracts dying on the vine?
The concept can‚Äôt be fully explained in a few sentences.
Its content focuses too much on APIs.
Data Quality tools are seen as ‚Äúgood enough‚Äù in the Data landscape.
Cost matters now more than it did 2 years ago.
The ‚Äúimplementations‚Äù are too ‚Äúsoftware engineering-ish‚Äù for most Data Engineers and teams.
I
read through this content
to try and understand truly what a Data Contract is.
Well, of course, that sounds ok, also ‚Äú
Data Contracts cover both schemas and semantics.
‚Äù
The devil is in the details in this one I imagine. Here is a list of the main points the Data Contract folks talk about. These are quoted directly from the horse‚Äôs mouth.
Data contracts must be enforced at the producer level.
Data contracts are public.
Data contracts cover schemas
Data contracts cover semantics.
Again, these things are not bad in themselves, but they lack rubber meets-the-road pieces, and they lean heavily toward APIs, which is going to fail in the Data Engineering world at large.
I thought I could get real answers by looking into the ‚Äú
implementation
‚Äù of Data Contracts. Unfortunately, this is where the trail starts to turn cold.
There are two tools, both of which I am familiar with and have used, which are suggested for the actual implementation and encoding of Data Contracts.
Google's Protocol Buffers (protobuf)
, and
Apache Avro
.
It appears to me that Data Contracts are, at a simplified level, an idea of using existing tooling (they picked the wrong one‚Äôs), to enforce the ‚Äú
ideal
‚Äù way to ‚Äú
solve common data problems
‚Äù by ensuring nothing can ‚Äú
just change.
‚Äù
There is a real problem being addressed here.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
As someone who‚Äôs worked around data for decades, I can sympathize with this problem. It‚Äôs been around a long time. Data doing whatever it wants, things changing without notice, cascading problems going through the system.
This is true.
I‚Äôm just not sold that Data Contracts
(and apparently the rest of the Data Community as well)
, are going to be the silver bullet to kill the werewolf of bad data lurking at the door.
Why aren‚Äôt Data Contracts catching on?
From my point of view, this is very easy to spot. Call be biased or jaded, because I am, but people are people.
‚ÄúMost‚Äù of the average Data Platforms aren‚Äôt API-centric.
‚ÄúMost‚Äù or ‚Äúmany‚Äù producers of data are out of our control ‚Ä¶ either completely outside our company or might as well be.
Data Quality tooling appears to be a good enough way to deal with this problem. (this
Great Expectations
,
Soda Core
, etc.)
They picked technologies Data Engineers simply will not use. (protobuf and avro) (
both of which have been around forever and have limited adoption in Data Engineering
)
You need SQL and Python to win the hearts and minds of Data folk.
At this point, I‚Äôm just feeling bad for Data Contracts.
It‚Äôs true. I thought I was going to be able to deride and smash into dust the golden calf of Data Contracts. I can‚Äôt quite get myself to raise my heel and strike the blow.
I feel bad for them more than anything. They must be lonely.
I understand the problem that Data Contracts are trying to solve. I think the fumble was not providing a solution that people would get on board with. Simple as that.
Why did Databricks win? Because they did something simple. Packaged Spark and a Notebook in a way that literally anyone could use. Anyone who had used EMR in the past never looked back.
Do you want me to use Data Contracts to control my data? Fine. Sounds good.
You want to talk about APIs, Protobuf and Avro? You want me to chase down people I have no control over? Not going to happen Sunny Jim.
Sure, you will find your pockets of people and teams adopting Data Contracts, probably some massive Engineering teams where someone on the top decided it was a good idea and has plenty of Software Engineers to do the dirty work.
Times have changed, and Data Engineers have not. Is it SQL? Is it Python? You have a chance. Do you want to implement complex new processes, APIs, and technologies that no one uses like Protobuf and Avro?
Keep dreaming.
Good idea. Bad packaging.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
141276846.diving-into-data-types,Diving into Data Types,For Data Engineers,2024-02-12 08:38:56.085-06,"This is an interesting one, especially for Data Engineers who‚Äôve spent their life under the thumb of
SQL and Python
‚Ä¶ which is probably most of us. That‚Äôs our life.
Keep daydreaming about Rust, ain‚Äôt going to happen.
It‚Äôs very probable, for most data folk, that they think of
Data Types
in the context of SQL ‚Ä¶ say Postgres data types, and if they are heavy Python users, maybe a
list
or
dictionary
comes to mind.
Dare I say it,
when everything is an object in Python
, and you‚Äôve never learned a compiled language ‚Ä¶ you simply stop caring after a while. That‚Äôs ok ‚Ä¶ most of the time until it‚Äôs not.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
There are Data Types, and there are Data Types. You know what I mean?
When it comes to Data Engineering, those who spend their days pushing and pulling vast amounts of data over the hill and through the woods ‚Ä¶ to you know‚Äôs house ‚Ä¶ can get a little weary of the subject.
But, we also think about Data Types slightly differently than most classic Software Engineers I think.
They are worried about which memory structure takes up too much space in memory, and which is the best way to access some thingy in some other listy-array-thingy. Data Engineers worry about the data type of column in 300TBs of Parquet files stored in an s3 bucket.
The same, but not the same.
So what ARE we talking about? Postgres columns? File storage columns in Parquet files? Lists, Dictionaries, Strings in a Python program? Dare I say an array or struct in Rust?
We are talking about all of it.
The Goal
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What we want to do today is to simply raise ‚Äú
Data Types
‚Äù into the minds of those who don‚Äôt think about them much. We just want to give you a gentle pat on the head, like your dear old grandmother, give you a cookie and some tea, and say ‚Äú
There, there, now you don‚Äôt think it‚Äôs wise to completely ignore data types your whole life do you?
‚Äù
Data At Rest
Let‚Äôs start our little journey talking about Data Types we use at rest, in storage and common mistakes Data Engineers make who get lazy, old, both, or just don‚Äôt know any better.
And, we could ask why it matters, let‚Äôs just store everything as STRING or VARCHAR like most people do.
Disk space matters (data types impact storage size)
Reading data from disk only to convert it to another data type is wasteful and time-consuming.
Correct data types reduce errors in increase data quality.
Let‚Äôs take Parquet files as an example. Let‚Äôs list all the possible data types we could store ‚Ä¶ and you tell me how many you have actually used.
My guess is not many of them. I mean we could apply the same concept to Postgres and other storage systems as well. Typically they provide a wide range of very specific Data Types.
Yet, most Data Engineers when working on defining schemas or data models simply don‚Äôt think very hard about this part.
Questions you should be asking yourself.
Is this really a
Strings
?
Do I know the precision of this
Decmail
?
How big are the
Integers
in this column?
Should I use ‚ÄòYes‚Äô and ‚ÄòNo‚Äô or
Boolean
, 0 or 1?
Are these values actually
Timestamps
, or just
Dates
?
Is there a complex Data Type like Map or List that makes sense for this data?
Next time you are bored on a Friday and crusing around your codebase, look for all the `
CAST()
` calls in your code ‚Ä¶ SQL, Spark, whatever. And then ask yourself ‚Ä¶
why
?
Remember, this stuff (data) adds us over time. Let‚Äôs prove the point in a simple fashion.
I downloaded a bunch of free data from
Divvy Bike trips data set
.
We have about 571 MB of data in flat files (CSV).
Let‚Äôs take them and convert them to compressed Parquet files ‚Ä¶ once as all Strings ‚Ä¶ and another time with data types correctly converted.
import polars as pl

def main():
    dtypes = {""ride_id"": pl.String,
             ""rideable_type"": pl.String,
             ""started_at"": pl.String,
             ""ended_at"": pl.String,
             ""start_station_name"": pl.String,
             ""start_station_id"": pl.String,
             ""end_station_name"": pl.String,
             ""end_station_id"": pl.String,
             ""start_lat"": pl.String,
             ""start_lng"": pl.String,
             ""end_lat"": pl.String,
             ""end_lng"": pl.String,
             ""member_casual"": pl.String}
    data = pl.read_csv(""*.csv"", dtypes = dtypes)
    data.write_parquet(""pickles/data.parquet"")

if __name__ == ""__main__"":
    main()
That gives us about 151MB
What happens if we apply the correct data types?
correct_dtypes = {""ride_id"": pl.String,
             ""rideable_type"": pl.String,
             ""started_at"": pl.Datetime,
             ""ended_at"": pl.Datetime,
             ""start_station_name"": pl.String,
             ""start_station_id"": pl.String,
             ""end_station_name"": pl.String,
             ""end_station_id"": pl.String,
             ""start_lat"": pl.Float64,
             ""start_lng"": pl.Float64,
             ""end_lat"": pl.Float64,
             ""end_lng"": pl.Float64,
             ""member_casual"": pl.String}
We get a slight storage savings 151.3MBs, about ~1%, so not a ton. But we also get more accurate data! Having the right data types when data is ‚Äú
at rest
‚Äù or ‚Äú
on disk
‚Äù makes it easier when we transition the data into memory and reduces the antics we later have to deal with.
Sometimes diving into Data Types simply means actually looking at your data and applying the correct Data Type.
Data In Memory and Code.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
I suppose at this point we could move onto the slightly more alluring topic of Data Types in memory .., aka our code. This can be a little harder of a topic to tackle, simply because of the sheer size and breadth of what we could cover.
There are lots of opinions and tears that can be shed on this subject, but let‚Äôs simply try to give a very basic overview of Data Types in memory, say with Python and Rust.
We could all use a refresher at some point no?
Non-Python Data Types for Python Folk
My gut says you are coming here with at least some Python under your belt. Maybe that's your entire shtick. On the off-chance that you haven't witnessed what other languages bring to the table, I'd like to briefly talk about a few of them.
First, there's this little guy called C. It's kind of famous, especially if you've ever used a computer before. It sets the bar for many languages in use today, so it's a great starting point.
struct Thing {
    int a;
    int b;
    char c;
}
There it is. A well-defined data type with three fields, each of which has a known size in memory to the compiler. It serves only one purpose - to codify the structure of a
Thing
; no trickery regarding field visibility, member functions, polymorphism, etc.
Well, since C came along, we've had other languages take a stab at things. C++ lets you use that same exact struct in your code with no alteration and it just works, but they couldn't leave well enough alone; they also have all that baggage I mentioned just a moment ago:
Here, we included functions, started playing with visibility, and included the concept of inheritance. Also, as mentioned in the comment, we could have omitted the
b = 0;
statement, since it automatically initializes to
0
.
Some Rust
Then, there's Rust. It looks at the above, and says there's a ""better"" way. A Rust struct has all the same moving parts, but is a lot more formal about how things work:
Where Rust really differentiates itself from C++ is in the separation of concerns - the shape of the struct is independent of its behavior.
Also, unlike C++, there is no concept of inheritance; ThirdThing can't simply bolt onto another existing struct definition to get free fields, taking more of a cue from the earlier C example.
...And, if you don't like having to spell out defaults in your constructor (e.g. the b: 0 assignment shown above),
Rust allows you to bolt on some built-in behavior using its Default derivation:
And how about Python?
Traditionally, Python was very relaxed about defining structures.  Or, rather, you didn't; you simply threw things into one of its existing constructs and made do.
For instance, if you wanted an object with fields a and b as with our other examples, you'd probably reach for a dictionary:
thing = {'a': 5, 'b': 0}
Maybe, if you're a Python lifer, you see this and think this is an argument in favor of Python over the cruft and ceremony of the other languages.  And, you might be right, depending on your needs.
However, if the goal is to have a consistent and inviolable definition for what a ""thing"" needs to look like, you have no guardrails here, and bad data can derail your program.
If that were the end of the story, I'd simply tell you to jump ship and switch to Rust.  Thankfully, it's not game over for Python, thanks to
classes
.
class Thing:
    def __init__(self, a):
        self.a = a
        self.b = 0
Well, it's self-contained, at least, but compared to the other languages, it's still very sketchy - aside from the initialization of field
b
, there's nothing type-related here to tell us what a
Thing
actually is.
We'll need to dig just a bit deeper into the Python ecosystem to get what we need:
Dataclasses
Since version 3.7, Python has offered
dataclasses
as a way of defining the shape of a data structure. It depends on the use of type annotations (as seen in
PEP 526
) to wire up some default behaviors for us, and given those type annotations we also have a well-represented schema for the shape of our data.
Employing this technique, our
Thing
could be better represented as shown below:
from dataclasses import dataclass

@dataclass
class Thing:
    a: int
    b: int = 0
The
@dataclass
decorator, by default, brings a lot to the table, one of those things being that it autowires a
__init__
function for us that takes a single parameter to populate the
a
field - giving us feature parity with the previous example. Consider the following script (building off of our current definition):
from dataclasses import dataclass

@dataclass
class Thing:
    a: int
    b: int = 0

thing = Thing(3)

print(thing)

# Output: Thing(a=3, b=0)
We got a
__repr__
implementation for free, allowing us to print
thing
directly and get readable output. In fact, we get a lot of really cool things by default, which are described in detail in the earlier-linked Python docs page.
Unfortunately, we don't get any runtime constraints - nothing stops you from blowing out one of the fields with invalid content; for instance, after instantiating
Thing(3)
, we could follow up with a statement like
thing.a = 'x'
and get away with it, defeating the purpose of explaining the member types.
This can be compensated for, for instance by
using properties and hidden members
, but let's be honest and admit that it's heavy-handed and we're asking Python for something it wasn't originally designed to handle.
There are better languages out there for this kind of job, and trying to protect developers from themselves in one of the most flexible languages available is perhaps a bad use of your time.
Bringing value
So, we talked a bit about what data structures are, and gave a few trivial examples in different languages. As a data engineer, the primary benefit of having typed structures is that it establishes a clearly defined domain to work within. We're modeling important things, and we want to know what those things are when working with them.
First, we're probably dealing with collections of things.
Frequently, these collections are homogeneous lists, wherein the items in the list are of the same type. SQL tables, for instance, are consistent and strongly-typed collections that let us operate in batch rather than forcing us to deal with field consistency at the record level.
In a language like Rust, it's very easy to make it clear in code what we're up to:
struct Person {
    pub last_name: String,
    pub first_name: String,
    pub middle_name: Option<String>,
}
type People = Vec<Person>;
Now, any time we see an instance of
People
, we know with certainty that we're dealing with a list of entities that consistently have a last and first name, and may or may not have a middle name. You can easily project those columns because you know with confidence that they're present and are all of a consistent type.
What if your list isn't homogeneous? If the possible types are well-understood and of a reasonable quantity, Rust can still help you out with its
algebraic data types
:
struct Person {
    pub last_name: String,
    pub first_name: String,
    pub middle_name: Option<String>,
}
struct Business {
    pub name: String,
    pub doing_business_as: Option<String>,
}

// Our entity could be either a Person or a Business:
enum Entity {
    Person(Person),
    Business(Business)
}
type Entities = Vec<Entity>;
Here, we're dealing with lists of items that could be either a Person or a Business, and the language forces us to manage both any time we're dealing with the collection
// Get display names for our collection of entities
fn get_display_names(entities: Entities) -> Vec<String> {
    entities
        .into_iter()
        
        // Match statement requires comprehensive coverage of all arms:
        .map(|entity| match entity {
            Entity::Person(person) => format!(""{}, {}"", person.last_name, person.first_name),
            Entity::Business(business) => business.name.clone(),
        })
        .collect()
}
Above, you can see that it takes a bit of work to tease the two entity types apart, but because our language has the details of our data structure available, it can let us know when we've neglected to handle something.
For instance, if we add a new type to our
Entity
enum, but make no changes to the
get_display_names
function, we'll get screamed at by the compiler:
enum Entity {
    Person(Person),
    Business(Business),
    Other(String),
}

// Compiler output:
// .map(|entity| match entity {
//                     ^^^^^^ pattern `Entity::Other(_)` not covered
// non-exhaustive patterns: `Entity::Other(_)` not covered
// help: ensure that all possible cases are being handled by adding a match arm with a wildcard pattern or an explicit pattern as shown
With such a degree of type safety in place, we can fearlessly model our data in code, and know that the compiler is keeping track of many ways we might botch handling it.
Wrapping It Up
Yikes, we ran through the whole world there, didn‚Äôt we? Covered a lot of stuff. I can‚Äôt even remember where we started, my brain hurts. Oh yes, Data Types ‚Äú
at rest
‚Äù.
Data Types are one of those topics we all either kinda talk around, without actually talking about them. Or we simply ignore them and don‚Äôt think about them much.
We reach for the stuff we get used to, we are creatures of habit. Everything is just a String in the database. Or throw a bunch of stuff in a list in Python. Who cares? Or should we?
I will let you be the judge of that.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
141102064.a-primer-on-data-architecture,A Primer on Data Architecture,For Data Engineers,2024-02-01 09:07:52.262-06,"This is one of those tricky topics that doesn‚Äôt seem to get much conversation in the data world these days. Too many other fun things are going on, dbt,
polars
,
Rust
taking over, just to name a few.
I have a feeling most people think of Architecture like a glorified lego game. You have a bag of tools, Airflow, Snowflake, Databricks, whatever, and you simply pick a few that you like and move on with life.
Sometimes this approach works out, and sometimes it doesn‚Äôt. Probably says more about the organization or team that approaches Data architecture like this.
What is Data Architecture, and why does it matter?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
This is an interesting question, especially in the age of the over-done Modern Data Stack. But, not that much has changed even if we moved from Oracle and SQL Server to Spark and Delta Lake. Many fundamentals have not changed drastically, especially when it comes to Data Architecture.
I‚Äôm going to approach this topic from a high level, and we should probably just start with a picture.
My problem is that a lot of folks dive straight into Lambda vs Kappa architecture, streaming vs batch, this or that technology ‚Ä¶ well it just isn‚Äôt a prudent place to start.
Start high level before low level.
Start without picking ‚Äútechnologies‚Äù or ‚Äútools‚Äù
Problems are easier when you think big picture first.
Modern Data Stack has changed, but the basics have not.
Where would I start, or where do I start when doing some Greenfield brand new architecture or upgrade project?
This is where I start. I start with what I know and I try to match the business requirements and Engineering goals to these topics.
I know I have to store data somewhere.
I know have to process or use compute to do stuff to my data.
I know I will need a tool to orchestrate all the things that need to happen.
I know there are specialty things from an Engineering perspective I need to do.
Analytics might need extra help.
I need some CI/CD and DevOps to support the operations.
Machine Learning is a big part of our world that is complex.
The Hardest Part of Data Architecture
There are so many things to think about. It‚Äôs hard to know where to even start. We have no choice but to think about what the business needs, and what Engineering ‚Äú
wants
‚Äù and try to marry the two together.
This is one of the problems we must avoid. Engineers have their favorite pet toys and technologies, we start with our pets and force the business requirements into them, instead of the other way around.
Architecture Where The Rubber Meets the Road.
It‚Äôs hard to know where to start and where to end with Data Architecture, but at some point, we have to step away from theory into real life, where the rubber meets the road.
If I had to boil it down to something, this is what how I would do it.
Gather business requirements.
Understand the ‚Ä¶
Volume of the Data (how much)
Velocity of the Data (how often)
Business Needs (how fresh they need the data)
The Type of Data (what it looks like)
The Transformation of the Data (what they look like)
This will give you a good start.
I mean there are obvious implications like ‚Ä¶
Postgres vs Delta Lake
Spark vs Polars vs Pandas
Usually, these types of decisions are fairly obvious.
Examine the Data Closely
Once we understand what the data looks like, how much there is, how often we need to update and transform the data, and other such things we can start to pair technologies with those needs.
Some of the decisions are not that easy, not so obvious.
Redshift vs BigQuery vs Databricks vs Snowflake
This is where the intersection of the Business and Engineering needs come together. We have the bridge the gap between the tooling we can use and the results we need to produce.
Knowing what the data looks like, how much you have, and what the rough transformations are ‚Ä¶ helps you decide if you need a big distributed system like Spark to crunch hundreds of GBs and TBs of data.
Or maybe you need more real-time systems like Pulsar, Flink, and Kafka. But that‚Äôs not all, it‚Äôs just the beginning, we have to think about the other technologies needed to make the whole system work.
Thinking about Integrations.
Share Data Engineering Central
The funny thing is you can‚Äôt just pick your compute engine ‚Ä¶ say Spark, while ignoring other very important parts of the puzzle.
Data Storage
Orchesration.
These decisions have a major impact on the Data Architecture, both from a cost perspective and how the entire system works as a whole. We are talking about the integrations between your
Orchestration
tooling and your
Storage System
, and how they all interact with your compute platform.
Now don‚Äôt get me wrong, in today‚Äôs Modern Data Stack the lines have become very blurry.
If you take Databricks and Snowflake for example, they wrap up these entire set of needs, compute, storage, and orchestration all in one system. Although buyer beware, not all that shimmers is gold.
Most advanced Data Platforms and teams use a variety of tools that best fit their needs. Seldom is that the vendor giving you a ‚Äúcomplete end-to-end package‚Äù is an expert in every single area ‚Ä¶ one of their offerings is going to suck or stick you in a box you don‚Äôt want to be in.
Ok, so how do I actually architect?
So with all the humdrum in the back of your mind, and you‚Äôve got your first architecture project creeping up on you, the internet is full of this and that, how do you actually architecture a Data Platform in a reliable way?
Do these things.
Draw pictures to show different systems and how they work together.
a picture is worth a thousand words
Read the documentation for all sorts of tools so you understand their foibles.
Pour over the business requirements so you understand the basics of the data like ‚Ä¶
velocity
volume
types
Think about the entire picture, compute, storage, and orchestration.
Think about CI/CD and DevOps too, think big picture.
Come up with multiple options, list the pros and cons of each approach and each tool.
Understand the Engineering culture you work in, what they are good at, and what they are not good at.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
140949433.introduction-to-daft-vs-polars,Introduction to Daft ( ... vs Polars),Distributed DataFrames for Python (built with Rust),2024-06-17 07:37:02.209-05,"It‚Äôs been a while since I‚Äôve kicked ye ole‚Äô tires on something new. You know how much I love to pick and poke at things, I just can‚Äôt help it. But this one didn‚Äôt take any convincing on my part. I couldn‚Äôt even tell you where I ran into it.
Daft
that is.
Part of me said ‚Ä¶ seriously, another Python DataFrame library? How we could possibly add another one to the list? And, why have I never heard of it?
But, then I saw
it was built with Rust
. Well, that changes things. Maybe? We have
Polars
, but not Polars distributed ‚Ä¶ yet. Is this another takeout shot on PySpark?
Good luck with that.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
I think I‚Äôm getting ahead of myself. Nice and easy does it.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
High-Level Overview of Daft
Well, let‚Äôs start from the top.
‚ÄúDaft is a
fast and scalable Python dataframe
for Complex Data and Machine Learning workloads.‚Äù
Typical of pretty much every Python Dataframe package since the beginning of time.
Polars
Ray
Dask
etc ‚Ä¶
It‚Äôs always the same old story I guess. Everyone wants to create the next Apache Spark, can‚Äôt blame em`.
What does Daft say about itself?
Offers UDF operations on Dataframes
Integration with other ML/AI tools like Ray etc.
Goes from local to the cloud seamlessly
Can run on a distributed cluster
Built with Rust
Build on Apache Arrow
This sounds nice, and it also sounds like Polars, except for the distributed part. That‚Äôs probably one of the most annoying parts of Data Engineering.
If you want reliable distributed Dataframes the only option is PySpark, everything else people try to laud is typically just not that reliable and falls apart once you do something *crazy* ‚Ä¶ like read files from s3 <
insert eye roll
>.
Anyway, enough moaning. Let‚Äôs try Daft (
and compare it to Polars
)
Trying out Daft.
I‚Äôm not sure what I‚Äôm expecting to learn by trying out Daft, but if we don‚Äôt, then I guess that is learning something. First things first, when I‚Äôm doing work like this I usually set up a Git repo and a Docker setup that container that has what I need.
You can get that here.
The first thing you should always try.
I‚Äôm a firm believer in trying out the easy stuff on the bat, for a few reasons. If a tool can‚Äôt do something basic like read files from s3 using environment variables ‚Ä¶ then it‚Äôs probably not a good sign of things to come.
That being said, let‚Äôs see if Daft can read CSV from s3 using env creds. (
we are using Divvy Bike Trip open-source dataset
)
export AWS_ACCESS_KEY_ID=xxxx
export AWS_SECRET_ACCESS_KEY=xxxxxxxx
export AWS_DEFAULT_REGION=us-east-1

>> import daft
>> df = daft.read_csv(""s3://confessions-of-a-data-guy/*.csv"")
>> print(df.collect())
I‚Äôll take it. Pleasant surprise. You have no idea how many so-called great tools can‚Äôt even do something simple like this without many various gyrations.
If you are an astute observer, you might have noticed something about the above Dataframe printed out. Take a second look, do you see it?
It did a very good job of picking up on the DataTypes, aka it got them all correct. Impressive.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
You know what the real question is. Can Polars do this? Let‚Äôs give it a try.
>>> import polars as pl
>>> df = pl.scan_csv('s3://confessions-of-a-data-guy/*.csv')
>>> print(df.collect())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.12/site-packages/polars/lazyframe/frame.py"", line 1855, in collect
    return wrap_df(ldf.collect(callback))
                   ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: No such file or directory (os error 2): s3://confessions-of-a-data-guy/202401-divvy-tripdata.csv
That‚Äôs a no so far. Let‚Äôs try `read_csv` instead of `scan`.
Nope
.
>>> import polars as pl
>>> df = pl.read_csv('s3://confessions-of-a-data-guy/*.csv')
>>> print(df.collect())
ImportError: Install s3fs to access S3
Ok, so we can `
pip install s3fs`, no problem. Try again.
raise FileNotFoundError(
FileNotFoundError: [Errno %s not found. The URL contains glob characters: you maybe needed
to pass expand=True in fsspec.open() or the storage_options of 
your library. You can also set the config value 'open_expand'
before import, or fsspec.core.DEFAULT_EXPAND at runtime, to True.] confessions-of-a-data-guy/*.csv
Come on Polars, it‚Äôs not that hard, Daft can do it. Seems like with Polars we have to resort to `pyarrow` to help us get it done.
import polars as pl
import pyarrow.dataset as ds

dset = ds.dataset(""s3://confessions-of-a-data-guy/"", format=""csv"")
(
    pl.scan_pyarrow_dataset(dset)
    .collect()
)
Good on Daft for getting it done right.
Also, the smart ones among you would have also noted that I‚Äôve been using `collect()` by default with Dask because ‚Ä¶.
‚ÄúThis is because Daft is
lazy
and only executes computations when explicitly told to do so.‚Äù
Let‚Äôs do a simple little performance test of Daft vs Polars, and in the process learn how Daft handles simple queries on Dataframes like some GroupBy and Aggregation.
I‚Äôm thinking of making this Polars vs Daft a little interesting, make ‚Äòem both work at it, we should GroupBy year, month, and day (
forcing them both to do a little extra work
) ‚Ä¶ as well as member type, and then count the number of rides, and the Sort that output.
Simple enough, yet under the hood, enough work to make those engines sweat.
Start with Daft ‚Ä¶
import daft
from daft import col
from datetime import datetime

t1 = datetime.now()
df = daft.read_csv(""s3://confessions-of-a-data-guy/*.csv"")
df = df.with_column(""year"", df[""started_at""].dt.year())
df = df.with_column(""month"", df[""started_at""].dt.month())
df = df.with_column(""day"", df[""started_at""].dt.day())
df = df.groupby([""year"", ""month"", ""day"", ""member_casual""]).agg(col(""member_casual"").count().alias(""count""))
#df = df.sort(by=[""year"", ""month"", ""day""])
print(df.collect())
t2 = datetime.now()
# print how long it took
print(t2 - t1)
About 31 seconds, just to be clear, this data has about 1,084,749 records, not very much.
Let‚Äôs do this in Polars.
import polars as pl
import pyarrow.dataset as ds
from datetime import datetime

t1 = datetime.now()
dset = ds.dataset(""s3://confessions-of-a-data-guy/"", format=""csv"")
lf = pl.scan_pyarrow_dataset(dset)

lf = lf.with_columns((pl.col(""started_at"").dt.year()).alias(""year""))
lf = lf.with_columns((pl.col(""started_at"").dt.month()).alias(""month""))
lf = lf.with_columns((pl.col(""started_at"").dt.day().alias(""day"")))

lf = lf.groupby([""year"", ""month"", ""day"", ""member_casual""]).agg(pl.col(""member_casual"").count().alias(""count""))
lf = lf.sort(by=[""year"", ""month"", ""day""])
print(lf.collect())
t2 = datetime.now()
# print how long it took
print(t2 - t1)
Interesting, unsurprisingly Polars took 45 seconds, about 15 seconds longer than Daft. That doesn‚Äôt surprise me because we are mixing two libraries to make Polars do what Daft is doing behind the scenes reading the s3 CSV files.
Honestly, the syntax is about the same, although I think Daft is slightly cleaner.
Distributed Daft
One of the features that first drew me to Daft was the allure of being distributed. After doing this work, reading the docs, etc, I couldn‚Äôt really find any mention or quick guide to disturbed Daft, a little disappointing.
I feel like they tricked me.
Looking closer at what it says you
CAN
distribute Daft with other tools like Ray
. I see. I‚Äôm not sure if that‚Äôs the same thing in my book, but we can give them the benefit of the doubt.
This is interesting because you can
setup Ray Clusters on Databricks
, they have support for that, so if you‚Äôre interested you could in theory run something like Daft on Databricks, assuming you can get enough speedups to beat PySpark with Daft to make it all worth it.
That sound worth my time to test, but I‚Äôm running out of time, so let‚Äôs leave that till next time.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
140840392.why-duckdb-is-losing-to-polars,Why DuckDB is losing to Polars,There's always a battle raging.,2024-01-29 08:52:02.021-06,"In my never-ending quest to make
r/dataengineering
angry,
of which I‚Äôve been quite successful
, I‚Äôm always on the lookout for Golden Calves and other Idols that I can smash to bits, and wait for the angry masses frothing at the mouth to hunt me down.
Sorry, not sorry, after years of doing this, I know what clickbait is and isn‚Äôt. Is that what I‚Äôm doing here, pitting
DuckDB against Polars
?
Yes and no.
Sure, some zealots will come to me and say that Polars and DuckDB can live in harmony, that they have different use cases and can co-exist. Yeah, well, so can humans ‚Ä¶ expcept we don‚Äôt and Data Engineers are humans ‚Ä¶ so ‚Ä¶ let the battle begin.
I will attempt to be fair to both tools. But, I will be honest and say up front that I think
Polars
has won the battle or will win it soon enough. The battle that you may think doesn‚Äôt exist, yet still rages on in the shadows.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Why Polars vs DuckDB?
As someone who thinks about and practices the art of Data Engineering on a daily basis, I find the rise of new tooling extremely interesting and wonderful.
Most of the new innovations are born out of necessity, trying to solve some problem, and if it‚Äôs useful enough to the world at large, it gets popular.
What I also find interesting is when you have two technologies that are similar ‚Ä¶
I like to see how they ‚Äúrespond to each other.‚Äù
How they adapt to the community after time goes by.
How picks up more stream and who falls behind?
If you look at DuckDB and Polars on the surface ‚Ä¶ they appear to be very similar. Take for instance this high-level view.
But are they really that similar? I think not.
I would venture to say that the only ‚Äúsimilarity‚Äù that DuckDB and Polars share is SQL. Other than that, they are morphing into totally different tools.
DuckDB will always have a place in Data Engineering, I mean SQL is SQL.
Anything SQL always does ok.
What does the ‚Äúcommunity‚Äù say?
When I‚Äôm testing the waters between two tools, whatever they may be, I try to take some time to ignore the talking heads and lick my finger and stick in the air.
Taste what‚Äôs on the wind.
A healthy tool gets talked about in the open air. On Linkedin, Twitter, Reddit, wherever. You can‚Äôt hid a tool that‚Äôs being used.
Also, if it‚Äôs just marketing fluff being pushed ‚Ä¶ that never is the same thing as actual users talking about doing actual things with the tool(s).
What have I personally seen when I look at the community talk around DuckDB and Polars? I find people talking a lot about Polars, and how they are actually using it. With DuckDB I find a lot of marketing material, lots of benchmark and testing talk ‚Ä¶ but very little boots on the ground for day-to-day use.
I mean go look for yourself around Twitter and Linkedin.
All this content I‚Äôve seen around DuckDB isn‚Äôt bad. But it‚Äôs mostly just talk. I don‚Äôt see a lot of people building new tools with DuckDB, talking about it, using it in Production, solving problems, talking more about it.
Maybe I just run in the wrong circles or have been looking under the wrong rocks. But I simply just don‚Äôt see and find a lot of Data Engineers using DuckDB ‚Ä¶
at least not as much as Polars.
If you search Polars, you will find actual things.
Every day users are talking about using Polars. Also, it doesn‚Äôt hurt to take a look at both GitHub repositories, not for stars per se (Polars has double the stars of DuckDB), but for the number of
open issues
.
Open issues indicate how active the community is, and the more people that use the tool, you are going to have a lot of issues being opened.
Polars has 1.3K compared to 223 on DuckDB. That should tell you something.
Polars is turning into a Data Engineering juggernaut.
Share Data Engineering Central
You might accuse me of being biased, and I am, I‚Äôve already written my fair share of Polars in my free time.
I‚Äôve also put it into Production at work, to replace some Spark jobs.
Here is some of the stuff I‚Äôve done with Polars.
Polars vs PySpark.
Polars vs Pandas on AWS Lambda.
Polars with Delta Lake.
Polars DateTime Manipulation
So why do I think the winner for the future of Data Engineering is Polars and not DuckDB? Simply put,
because you can do a lot more ‚Äúthings‚Äù with Polars.
Polars is truly and end-to-end Data Engineering tool in every sense. It can, without problem, and with ease, be used to write entire end-to-end Data Pipelines.
I think you would struggle to do the same thing with DuckDB, I mean you can do what you want, but I don‚Äôt think that‚Äô show DuckDB even sells itself.
Being fair to DuckDB.
I know that there have to be a few people ready to ring my little neck by now. That‚Äôs fine.
DuckDB is trying to do one thing very well. Being the next SQLite. It‚Äôs extremely fast, lightweight, and made to do one thing and one thing only. Fast SQL analytics in memory, in process.
One can argue that is completely different from Polars. And it is. Yet it is not.
What do Data Engineers look for in tool(s)?
Data Engineers are human and follow human patterns when picking tools, and as a whole, using tools that become widely adopted and popular (think Apache Spark for example).
Polars just offers enough of everything to become the number one choice.
If you are building Data Pipelines for a top-tier Engineering organization, with a variety of use cases and problems to solve ‚Ä¶ you‚Äôre simply going to look for a tool that can solve all your problems.
As someone who picks technologies and designs architecture ‚Ä¶ I need a reason to add complexity, to add another tool.
Polars is based on Rust and is extremely fast. It can work with larger-than-memory datasets. Am I really going to design more complexity into my pipelines and add steps to use DuckDB so I can gain some milliseconds on a few SQL queries?
No, I will not.
The End
Now don‚Äôt put words in my mouth. I have nothing against DuckDB, it‚Äôs a fine tool, and probably will be very useful to many folks. But it‚Äôs going to lose the tool war to Polars, by a lot.
I think DuckDB is a niche tool. Polars is a tool for the masses.
Give a gift subscription"
140606485.intro-to-sql-indexes,Intro to SQL Indexes,For Everyone,2024-01-17 09:43:02.304-06,"Struggling to get the most out of that SQL query? Wondering why it takes so long for that simple query to run? Getting angry emails from users and tired of being blamed for every slow report or dashboard?
If you aren't indexing your data, or you're not using the indexes you have, you may be leaving massive performance gains on the table (
is that a pun? I'll let you decide...
).
Indexes are one of the pillars and foundations of what data teams have been working on for decades, yet, if you scroll the endless LinkedIn or Twitter tech universe, I would hazard a guess it‚Äôs pretty much impossible to find folk talking about indexes.
Today, we set the record straight, and crack the book open for those who‚Äôve been wondering what indexes are all about, what they are, and how they are used. Hopefully, you leave this article with a broader knowledge of indexes and their uses, that‚Äôs the plan!
This blog is brought to you by
Firebolt
. It‚Äôs a fast cloud data warehouse I‚Äôm currently exploring for low-latency use cases, and it‚Äôs able to deliver sub-second response times over TBs of data.
What even
is
an index?
For those old enough to remember, phone books were a popular way of letting you know how to get ahold of the people in your community. The ""
White Pages
"" gave residential numbers, while ""
Yellow Pages
"" provided a catalog of business numbers and related advertisements.
Consider what you might see in a typical white-page listing:
Last name
[Optional] First name(s) and/or initials
Street address
Local phone number (area code was already known by virtue of which book/section you were reading)
Before the internet, if you wanted to find a person's contact information, this was the tool. You would query the book by last name, and (fingers crossed) they had a distinctive name and were easily teased out. What you were dealing with is a table of people
indexed
by name.
What if, though, you were to start with a phone number and have a desire to know who it belonged to? With a normal phone book, this was a herculean task - the phone numbers weren't sorted numerically, and if you were to match a number to its corresponding owner you could probably spend days
scanning from top to bottom
until you eventually found the corresponding name. This technique, known in the database world as a
scan
, is the process of working top-down through your data to naively search for something.
So, to reiterate, when you're searching your data using
indexed keys
, you're going to get your data very quickly. Without that index, it's monumentally more difficult to track down the matching data.
This is why indexes matter in SQL and the ‚Äúdata world.‚Äù
Indexes are critical to operations in most SQL-based systems that involve any data of size. If you don‚Äôt understand and use indexes, and your data grows, it‚Äôs inevitable that troubles will quickly start to show in the system.
If you think about finding a piece of data, say a customer record, within millions or billions of records, how can you quickly find what you need? Your options are to either scan all the records till you find what you want, which can take a long time, or use an index to jump straight to the record you need to read.
Ok, so we can all agree indexes are critical to SQL operations if you need performance, and we all do. Next, let‚Äôs talk about some common types of index you‚Äôve probably heard of or will hear of, in your journey through SQL and indexes.
Clustered
Non-Clustered
These are the two most basic index types that you should understand in the context of most SQL-based indexes.
Clustered Indexes
Of course, there are nuances with different database systems, but there are two central factors that make clustered indexes very useful and useful as tools in the belt of data engineers and others working on SQL-based systems.
Sorting
Uniqueness
As for sorting, the rows of data are physically stored on disk
in the order of the index keys
. This means that the data is stored in a sequence that matches the indexed column. This is very helpful when you‚Äôre searching for a particular piece of data as it‚Äôs related to those around it.
Secondly
, most times clustered indexes will be unique or have a high degree of uniqueness. Again, many database systems will automatically create clustered indexes on the ‚Äúprimary key‚Äù of a table.
Share Data Engineering Central
Clustered Indexes In-Depth
In our white page example, we saw that the last name is the sort sequence and the data point we initially used to look up a person's record. Unfortunately,
some names aren't unique (
see below
)
, and since the first name isn't guaranteed to be present or otherwise unique, you might get many hits for a single lookup.
The situation's a bit better in the Yellow Pages. A local business name will be unique by virtue of a state's business registry, and so if you know a business's name
you're virtually guaranteed a 1:1 match
. As with the White Pages, records are sorted alphabetically by name.
You could store both White and Yellow Page data as tables, but only the Yellow Pages allow you to use the name as a primary key, which has the requirement of being a unique data point. Continuing with the Yellow Pages illustration, that business name functions as not only a primary key but also a
clustered index
.
What makes an index ""
clustered
"" is the fact that it is stored within the table. When you search a clustered index for a record, you're also looking directly at the record you're searching for. This index is created for you automatically when you define the table's primary key, and only one clustered index can exist per table.
Sidebar: Clustered indexes don't need to be unique, and don't require a primary key, but it's considered bad form to have non-unique clustered indexes.
Non-clustered indexes
What if you were to start with a phone number, and wanted to know who it belongs to? A traditional phone book wouldn't be able to help you with this, since it's only indexed by name. Still, phone numbers are unique and owned by a single account owner, so in theory, you
could
index by number rather than name.
A reverse-lookup service could tell you this information. These weren't commonly available (likely for privacy reasons), but with them, you could look up the owner of a phone number.
Supposing this lookup table contained a phone number and the account owner's name, you might then use that information to go back to the white pages to track down their primary listing and get a street address.
Stored separately from data.
There can be multiple non-clustered indexes.
Indirect data access.
This will add overhead to the database system
In this scenario, the reverse-lookup service is serving as a
nonclustered index
- one that is stored separately from the records and refers back to the original via
indirection
.
Because this lookup table is stored outside of the source table, there is some additional work involved in maintaining the lookup data any time the source record it refers to changes, and the two-step lookup process is also a bit more work to perform than a direct search as with clustered index queries.
Heaps
Finally, if we have a table that isn't backed by an index, it's considered a
heap
- an unsorted collection of records. Or rather, any sorting is at the discretion of the database engine.
If order isn't important and a primary key doesn't make sense, you might intentionally choose this data structure, but the only way to get a specific record out of this structure is to use the expensive table scan mentioned earlier.
When to (or why) index?
Ok, so we now know a little bit more about indexes and some of the most common types, but this doesn‚Äôt tell us when or why would need an index in the first place. One of the greatest reasons data professionals need to understand and use indexes is the sheer volume of data we deal with.
When it comes to complex analytics, think about all the complicated queries that get written, with many JOINS, and WHERE clauses to say the least. Indexes are important for several reasons.
Improved Query Performance
Efficient Data Access
Optimized for Specific Queries
Reduction in Resource Usage
Impact on the Execution Plan
Any data team knows well the importance of indexes when producing analytics from data warehouses, it‚Äôs almost impossible to provide reasonable performance without them. Indexes are critical to providing fast and efficient queries.
Let‚Äôs look at an example to help.
Suppose your resident
PHB
comes up to you and wants to know how many reciprocating dingle arms were shipped to Dubai in 2023 by Bob, one of your lazier salespeople. Turns out you're the first person to ask this question of the database, and your DBA (
who's out on vacation, but left you admin rights before he bailed
) never crafted a suitable index.
Your query churns for a long eternity since your company is one of the leading global suppliers of parts for turboencabulators, and there's a lot of data to comb through.
The offending query:
Do I need an index?
Whether you need to play with indexes is a subjective decision. If you're doing a one-off analysis and can afford to walk off and grab a coffee while your query runs, you're in a very different position than someone who desperately needs an answer on the spot.
Also, indexes aren't free - they need to be kept in sync with source data, and incur an up-front performance cost in addition to the additional storage needs involved. If you're writing often and querying infrequently, you might be creating more problems than you're solving by adding indexes.
You might also get away with alternative solutions, such as periodic report generation or other caching mechanisms when real-time accuracy isn't needed. This isn't a tool that should be reached for every time you lose patience with a query, as it potentially falls into several traps (YAGNI, bikeshedding, premature optimization).
For our current illustration, though, we've decided it's worth it. You're the DBA for the day, and in your official capacity, you've decided it's the right call.
What do I need in my index?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
First, let's distinguish between the two things that the query is doing:
Selection
- in the WHERE clause, we have constraints that limit
which
records are returned.
Projection
- the SELECT clause identifies the data returned by the query.
Both of these needs are addressed separately when crafting our index:
For indexing purposes, if it's already accounted for in the Selection columns, it doesn't need to be added to the Projection columns.
What we're effectively creating here is a logical table that contains everything we need to speed up our query. It's a bespoke index tailored to this particular query. Note how it corresponds to our original query's columns.
Implicitly, this index will also have a pointer back to the original record, so you basically get the primary key for free in your projection columns.
We could include additional columns in either the selection or projection portions of the index, and in cases where you believe this index might have value for other queries if it just included that extra bit of data, it might be the right move.
Get too greedy, though, and you add to the incurred penalties described earlier, and you might also convince the query planner that it's not worth using your index.
That's right - even after creating your index, the database engine might not even end up using it. It gets a lot of discretion in deciding the ""best"" way to query for your results. There are ways to improve this situation if it occurs, but there's also a good reason why DBAs are well-paid professionals.
That's fine and all, but my query's more complex!
Okay, so maybe you're doing some fancy joins and have three pages of constraints in your WHERE clause. The situation is still the same - you need to identify all of the selection criteria and the projection values in play. Consider the below query for a better-fleshed-out illustration (
this time using the infamous Northwind schema
):
Here, we have two datasets (
Customers and Employees
), each of which brings something different to the table. Let's break it down for each:
Customers
Selection
Address
City
Phone
Projection
ContactTitle
Address (but since it's already in Selection, we will ignore this)
Employees
Selection
Address
City
HomePhone
HireDate
Projection
LastName
FirstName
So, based on the above, we have two potential indexes:
Understanding Use Cases
Understanding the use cases and query access patterns is probably one of the most important steps when trying to come up with indexes to support a large setup of analytics and queries running on data warehouses.
Without understanding common and overlapping data access patterns, it‚Äôs almost impossible to be smart about designing indexes to support those needs.
What tables are used the most?
What datasets are joined?
Where are the common filterings applied?
One of the best things you can do is to inspect all the queries being run and compare them. Find the commonality between them, in many cases, there will be lots of overlapping datasets that are used to do similar joins and filters, this is where you should start.
Always remember though, too much of a good thing can be a bad thing. Indexes cause overhead and maintenance, one has to be very careful not to get carried overboard.
Trailing thoughts
Performance tuning is about so much more than indexes. Sometimes, all it takes is rethinking your query and whether you're asking the right questions of the database. However, when trying to leverage repeatability and performance in your querying, it is one of the most impactful tools at your disposal.
It‚Äôs critical to have a basic understanding of the most common types of indexes used across most different types of databases and tools, clustered and non-clustered indexes to begin with. It‚Äôs also clear that good index creation and management is half art and half science. Significant time must be put into understanding the data itself, and its common access (query) patterns.
Armed with this information, in most cases, it becomes very clear where indexes can help boost performance.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
140407708.llms-part-2-fine-tuning-openllama,LLMs Part 2 - Fine Tuning OpenLLaMA ,For Data Engineers,2024-01-16 09:15:32.701-06,"I figured I should probably take this chance before AI takes my job and I end up like Oliver Twist working in some dirty factory filling bottles full of snake oil. What chance you ask?
The chance to fine-tune an LLM to write like me. (
all code on GitHub
)
Some months ago I did a Part 1 on LLMs called
Demystifying the Large Language Models (LLMs)
. It was mostly to help Data Engineers who‚Äôve not worked around
Machine Learning
(ML) much to at least get their feet wet and run an LLM locally on their machine.
Take some of the scaries away.
I‚Äôve been personally sorta reticent to dive too deep into the AI and LLM
humdrum
. This might be due to the fact that I‚Äôve worked around ML for too many years to see the dark side. ML has always been the Wild West, even after all these years.
A landscape littered with little to no standards.
Lacks most good Engineering best practices in many aspects.
Besides the Data Engineering portion ‚Ä¶ can be wildly boring.
Especially with LLM worlds being so young, new models coming up all the time, a zillion different packages being released that are half-baked‚Ä¶ I know in my heart of hearts that trying to go a step deeper ‚Ä¶ say fine-tune an LLM ‚Ä¶ would be a heartache and headache.
Let‚Äôs Do It Anyway.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Well, I figured even if I didn‚Äôt want to do it for myself, I could at least find it in my Grinch‚Äôs heart to do it for you, my dear reader.
So, that‚Äôs what we are going to attempt to do.
In Part 1, we were able to download and interact with the OpenLLaMA model on our laptop
. Just to get a feel of things.
What we are going to do today, is fine-tune
OpenLLaMA
‚Ä¶ to be more like me ‚Ä¶ so it can write all my blogs for me while I galavant off into the woods to live like the mountain man I want to be.
I want to do this (
fine-tune an LLM
) for a few reasons.
Mostly to prove that LLMs can be a boon for Data Engineers ‚Ä¶ and increase job security.
I hope you will see that ‚Ä¶
Just like most Machine Learning ‚Ä¶ LLMs are data-heavy.
Data munging and wrangling are a major part of the game.
How important DataOps, DevOps, and CI/CD are critical to reliable and repeatable LLM operations.
LLM workflow is mostly a test of Data Engineering skills.
The boring Data Science part can be left to birds to fiddle with.
You need GPUs to do much fiddling with LLMs.
What is Fine-Tuning?
Good question, I‚Äôm glad you asked. Fine tuning is the process of taking a pre-trained model, like OpenLLaMA, see the last article, and then doing further training on a small specific dataset, relevant to some domain of interest.
It‚Äôs important to note the actual LLM fine-tuning (training) must take place on a GPU compute machine (
Probably with at least 24GBs of GPU memory
).
The outline of what we are about to do.
Get the model staged.
Get data gathered.
Massage data into format.
Get config and model defaults from someone else.
Set up Cloud resource(s) (GPU) to train the model.
Deploy code and data to Cloud Resource(s).
Retrieve results.
Combine base model with fine-tuned adaptor(s).
Replace me with the LLM to write the rest of the blog.
This is a super high level of what is going to happen. Some things I will skim over, others I will not.
I am no expert but you can pick up on what the steps are by simply reading a few other blogs and articles on the topic of fine-tuning, regardless of what the model is.
One thing you will notice when pursuing the interwebs for details on fine-tuning ‚Ä¶ is that most of the time, the little rats gloss over the ‚Äú
hard parts
‚Äù. I will not.
I would say the actual writing of the code to fine-tune a model isn‚Äôt the hard part (speaking to a Data Engineering audience), you can find many examples in GitHub of folks doing that for many different LLM models.
The hard part is ‚Ä¶
Gather a dataset to fine-tune on.
Figuring out what format to put the data in.
Putting the data in that format.
Figuring out WHERE and HOW the compute will look to fine-tune
Finding somewhere to do it.
Getting the GPU resource prepped with tooling.
Knowing what dials to twist for best results (leave that to the Data Scientist)
Making some decisions upfront.
So, we are going to just pick
HuggingFace
up front for the Python packages to use for training, and for the dataset format. Why? Mostly because people are very familiar with them as a leader in the LLM space.
Since having data to fine-tune is the most basic task, let‚Äôs start there.
Understanding data gathering and formatting for fine-tuning LLMs.
First, let‚Äôs get a crash course on gathering and formatting data for fine-tuning LLMs.
Unfortunately, we can‚Äôt simply just throw a bunch of data in a pile, point some model at it, and click run. Hence Data Engineers have a bright future in the LLM and AI future.
The data must be ‚Ä¶
gathered from some source(s) around a specific subject (the whole point of fine-tuning)
put into a format that makes sense to the LLM so it can be trained on it.
So what does that mean to a Data Engineer?
Taking unstructured data and putting it into a semi-structured format.
Take for example the dataset
wizard_vicuna_70k_unfiltered
on Hugging Face. Below is a screenshot.
It is basically in the format of ‚Ä¶
On the fly, during training, this semi-structured format will be again reformatted into a `
string
` needed by the model training, but, if we want to store the semi-structured text data in some useful format ‚Ä¶ we have to pick one.
Real Life example of gathering and formatting data for fine-tuning an LLM.
Ok, so all that is good and well, but how about a real-life example? I said at the beginning I would like to fine-tune an LLM to write my blog posts for me since you won‚Äôt know the difference and then I can retire to the mountains to live off the land.
So what data do I need?
I need some of my blog posts ‚Ä¶ AND, I will need to massage them into something like the above format that the LLM can understand during tuning.
Here is what I‚Äôm going to do. I will attempt to export my blog posts from my WordPress website
www.confessionsofadataguy.com
, pull the
Title
and
Text
from each blog, and input them into the below format.
My hope is that the pre-trained model is familiar enough with Data Engineering concepts, I‚Äôm sure it is, but what I really want is for the fine-tuning of OpenLLaMA to pick up on my voice.
Munging my data together.
Here goes nothing. I‚Äôm a Data Engineer and I can well guess that this is going to be the most painful part of the process. Data always is.
Below is me exporting my blog posts from WordPress.
And then my head exploded.
XML
. That‚Äôs what the mighty WordPress gave me. Go figure.
Bugger all. I really don‚Äôt feel like pulling out a bunch of crap from XML, time is money. Not cool. I have one other option. This Substack.
Well, the Substack export is a little more promising. The blogs are HTML files with the title as the filename. This is workable. Seen below are the download HTML files of the content with the filename being the title. Just what I need.
Ok, so we can deal with this. We just need to iterate the files and pull the title and the contents with
BeautifulSoup
.
Of course, while my script is doing this work, it‚Äôs also massaging everything into the needed format we saw at first.
It looks like a mess of code because it is. It‚Äôs really just writing out a JSON file with the messages we reviewed before in the correct format that the
HuggingFace
datasets library is ready to work with. What a pain.
See what we got. Not perfect, but decent enough. We get a title and a bunch of content.
(Below you can see the titles and content from the blog posts being pulled)
.
So we have 100 blog posts that we can extract. Let‚Äôs start with this, and get this data into the format we need. Which is below.
Seems to work ok, the text isn‚Äôt perfect, but neither am I, so who cares?
This is a screenshot of the file the above script writes out.
I then uploaded this JSON file to s3, so I could get it later on my GPU machine for training.
The Other Hard Part (the code).
So, we‚Äôve got our data sorta ready to go, or at least we‚Äôve learned a lot about what it looks like to prepare a common dataset into some sort of coherent format for ingestion by an LLM.
So. On to the next hard part (maybe)?
This is where we do what any good script kitty would do, and go looking at
other examples of people fine-tuning OpenLLaMA models
. Since this is such a popular thing, there are many many examples in GitHub.
The GitHub repo I focused on to learn what it looks like to actually write code to load the data and the model and do the actual fine-tuning training
is this one
.
Thank the Good Lord for the people that come before us. We all stand on the shoulders of others.
It‚Äôs hard to know what you don‚Äôt know, and a great way to learn is to emulate other codebases, read the general flow of what they are doing, and then adapt to your needs.
I‚Äôm going to link to GitHub for my code so you can inspect it
, but the general flow seems to be this from all the online tutorials I read.
Set configurations and parameters for model and training
Where model sits
What knobs to turn (just trust others on this)
Training data
etc.
Load up the
QLoRA
training
Save adaptor.
As an FYI, there are
ALOT
of decisions and important concepts to learn in this code. Things like
QLoRA
and why you would choose it (
reduce memory consumption
). But, we will save that for another blog post.
Anyway, we now have data and a code we need to fine-tune. On to the next task.
Getting GPU Hardware to fine-tune.
The next tricky part was finding someplace where I could rent some GPU power at a reasonable cost. I first tried my old trust
Linode
, but I had to put in a support ticket to get approved to spin up GPU nodes ‚Ä¶.? Strange, I‚Äôve been using them for years.
I also ran across
vast.ai
where you can bid/rent out GPU‚Äôs for reasonable money ‚Ä¶ say like
$0.50 per hour.
That‚Äôs cheap enough for a boy like me who grew up where I grew up.
I have no doubt my first few tries will probably fail. I started out running as much of the code locally on my laptop to try to debug all issues.
Mostly around the munging of the data and loading up the data into the correct format required for training by the Hugging Face datasets library.
Again, it‚Äôs very clear to me the data munging part is one of the most difficult. There are very few clear instructions online, and many competing ideas and solutions.
Your data needs to be exact, exactly what I needed and expected to be loaded and sent to the model ‚Ä¶ otherwise it simply work work.
Oh, and we can‚Äôt forget the tooling part.
Another ‚Äú
problem
‚Äù if you will, is making sure you can quickly stage all your code, model, data, and install all tooling onto whatever GPU machine you get.
I mean you‚Äôre paying by the hour, so you don‚Äôt want to dillydally.
Either containerize your entire setup into some Docker image.
Or, put your code in a repo, and data in s3, or be able to scp copy your data and files, and be able to quickly install the necessary tools to clone and copy everything onto the GPU machine (ex. be able to run git commands, have the AWS CLI etc).
I ended up renting this machine by the hour.
I took the hard route and simply used a blank Unbuntu container for this machine. So I had to of course do a few things ‚Ä¶
install a bunch of tools (pip install, few apt-get installs)
get my code on the machine (easy)
get my data on the machine (easy with aws cli)
(
BTW, you need
sudo apt-get install git-lfs on a Linux machine to get all the big model files)
After that it was simple enough to run my training script, it appeared be working fine.
Except till I checked on it later.
It had run out of Disk Space.
Added more, and then back at it.
Train, train, train.
Then whammy, training done, and adapter saved.
Not bad! Of course, I saved the adaptors into s3 for later use.
Of course, this was just my first time learning to train an LLM, and in the real world, we would harden the process a lot more and probably use a lot more data.
How do we summarize what we learned from a Data Engineering perspective?
Do we care if it was right? Not really. We got the general steps and flow to work, as well as the adapter to output. We learned a lot. That‚Äôs the idea.
Fine-tuning LLMS is mostly a class in ‚Ä¶
finding data
massaging data into formats
installing correct tooling (python packages etc.)
getting code correct for training
find a GPU instance to train on
I can tell from simply walking through this basic exercise, even if it‚Äôs half wrong, that just like normal ML workloads I‚Äôve worked with for years ‚Ä¶ LLM stuff is again a lot of Data Engineering.
Gathering and formatting data.
Setting up environments with tooling.
Automating a lot of tasks and workflows so it can be repeatable
.
This new age of ‚ÄúAI‚Äù and LLM‚Äôs is going to favor Data Engineers.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
140221912.why-python-always-breaks,Why Python Always Breaks,Long Live Python,2024-01-19 07:59:22.403-06,"Okay, so that's a bit fatalistic, but if you've spent any time in Python you've likely stubbed your toes on either the language, interpreter,
or that dreaded pip install
. There's no one perfect tool or language out there, and if there was then Python isn't it (
sorry, not sorry, speaking as someone who‚Äôs built a career in data with Python
).
Don't get me wrong... Python has many great applications and is a language both accessible and versatile. It‚Äôs the only choice right now for data and ML workloads ‚Ä¶ for 80%+ of the market.
When it comes to rapid iteration, prototyping, or one-off scripting,
it can be the fastest path to delivery
. Memory management and the type system have a strong focus on
getting out of your way
and allowing you to quickly express ideas in code. Also, its long-running success has created an enormous ecosystem of tools that cater to science, finance, data, and ML professionals.
Any good tool requires experience, diligence, and discipline. For instance, accuse a C++ developer of working with a foot shotgun (maybe even throw in a reference to
Keith, the ""official"" mascot
), and you'll get a retort to the effect that it's only a problem with developers who don't know the language well or aren't using the newest standards.
While that point's debatable, there are plenty of pitfalls in Python that can trip up developers of all experience levels.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
I believe that being honest with yourself, and your tools is an underrated skill. To become better, to use the right tool at the right time ‚Ä¶ we can‚Äôt walk around with blinders on. We simply must look ourselves in the mirror and say the hard truths.
Flexibility isn't always your friend
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Let's be honest - one of the strengths of Python is the ease with which you can crank out a working program,
it‚Äôs quite amazing if you stop to ponder it
.
It's simple enough for beginners, and powerful enough for veterans, and its dynamic typing means you don't need to fight-type systems along the way. However, the lack of guard rails is a two-edged sword.
For instance, consider the following function signature:
def calculate_total(items):
You might tease out a bit about how the function or its parameter is named, but you're still left hanging when it comes to
understanding that function
.
Does it return something? What side effects does it have? Somewhere,
a Haskell dev might be encountering that line of code right now and fighting back some bile
.
Unless that function is well-documented, you're forced to take a look at the code. Of course, you're not totally out of luck;
PEP 484
has your back,
but type-hinting is optional and does nothing to help you with legacy codebases that don't already take advantage of it
.
What if you didn't know about type hints? You might try to use a default parameter value to serve in its place:
def calculate_total(items = []):
Here be dragons.
The statement itself is innocuous enough and offers a hint to readers of your code that you're dealing with a list. However, if you didn't already know that the
items = []
initialization is evaluated only once, you might inadvertently assume it's reinitialized with every call of the function.
Above, we can see that unless a
discounts
list was provided as an argument, as was done in the
p2
declaration, every instance of the
Product
class shares the same list of discounts.
Beginners beware
As already mentioned, Python positions itself as a valuable tool for all calibers of developers. That said, it ends up being sold pretty hard to students and learners as a ""
beginner
"" language. So, what kinds of things tend to trip this crowd up?
Here are some things that a neophyte might try:
Any rookie mistakes I missed?
Leave a note of your favorite in the comments.
There are always two sides to every story. You can be like me and write Python for 20 years, and you‚Äôre still human. If you‚Äôre in a hurry, need to fix something, tests are covering ‚Ä¶ and you just push some quick fix PR ‚Ä¶ next thing you know ‚Ä¶ yup ‚Ä¶ a syntax error.
Scope
Seasoned developers are primed to understand scoping rules. For new developers, though, this is easily one of the more confusing topics they'll encounter regardless of their choice of language. Python is no exception.
Consider a trivial scenario where Python's scope allows for closures:
Pretty cool, right? A novice might see an example like that, and think that being inside the closure gives them full access to parent-scoped variables, inspiring them to write something like:
Behavior like that can make a person want to avoid using closure scopes, particularly if coming from another language such as JavaScript:
As mentioned in the comment, it's unambiguous what's going on here, since the omission of the
let
or any of its alternatives (
const
,
var
) means we're not trying to redeclare a variable but instead are accessing one that is already assumed to be in scope.
Not your speed? Look at how PHP does things (
trigger warning: we're about to see some PHP
):
In this language, there are now additional guards against explicitly defining what's coming in from a parent scope, as well as the fact that it's a direct reference (
the
&
before the variable
) rather than a copy. While it requires a bit more code than Python, it leaves no room for ambiguity and behaves as expected.
Fail fast
There's a mentality shared between programmers and entrepreneurs - ""
fail fast
"" or ""
fail early.
"" If something's not going to work, you'd rather know early on rather than tripping over a problem in a deployed product.
This is where compiled and
strongly typed languages
,
such as Rust
, really shine; they don't let you out of the gates if there's a
syntax
error,
typo
, missing variable declaration, ambiguous
typing
, etc.
def my_func():
    print(my_first_variable)
    
print(""Not calling the function for now..."")
The contrived example above shows how you can have a glaring issue (in this case,
my_first_variable
wasn't declared), and potentially not know about it even as the program otherwise successfully runs.
Supposing you're developing a shared library and your functions aren't often called within your application, you might be shipping all manner of show-stopping bugs without ever witnessing them for yourself.
The Importance of Unit Tests for Python
This can easily be remedied, of course, with unit tests. With 100% code coverage, ensuring that every branching statement is handled, you can preemptively identify and get in front of these kinds of mistakes. That said, can you guarantee that you or your colleagues have a perfect testing system in place?
Testing is of upmost importance in Python codebases. It can give some trustworthiness to things that otherwise a toss up.
Alternatively, your IDE or toolchain might catch such issues with linting. For instance, Visual Studio Code, with the Python and Pylance extensions enabled, will identify that usage of
my_first_variable
with a warning:
""my_first_variable"" is not defined
If this is your approach, you should make a point of knowing all of the warnings emitted from your codebase; unfortunately, many developers have a tendency to ignore warnings or otherwise fail to consider them.
The pain with the toolchain
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Everything to this point has been an
idiosyncrasy
of the Python language. However, there are considerations outside of just the grammar and syntax of a language that need to be considered.
Global Interpreter Lock (GIL)
Python's dirty little
not-so-secret
: it's forcefully single-threaded. It does this by design, using a mutex (
mut
utally
ex
clusive lock) to ensure only one thread is in execution at a time. In a nutshell, you could think of the mutex as a baton, and only the thread holding that baton is allowed to run.
You might ask why? Well, it's an effective tool for preventing concurrent access from multiple threads for the same resource. It's not the only way this could have been done, and other languages find ways to accomplish multiple concurrent threads, but it's been around for long enough now that getting rid of it would be extremely painful and almost certainly break existing code.
What it means for you is that, if you want to do multiple concurrent processes in Python, you'll need to reach outside of the interpreter (e.g. spawning new processes, relying on non-Python-based libraries). That said, you may take some comfort in knowing that your code is inherently thread-safe; languages lacking this mechanism often tend to run into some nasty issues (race conditions, deadlocks) that can create some of the most painful bugs imaginable.
The environments (pip, venv, conda, etc.)
A naive developer might start by unboxing the Python runtime and get to work with
pip
. Eventually, they'll find themselves with an installation riddled with dependencies for all the one-off tasks they were working on, possibly even tripping up over version conflicts. Thus, the a need for an environment wrapper to encapsulate a given program's dependencies.
If you haven‚Äôt been stabbed in the back by a requirements.txt file ‚Ä¶ you haven‚Äôt been working on Python code bases long enough.
If you're using virtualenv, you have a bit of rigamarole to go through as part of creating and working within a project (creating, activating, and deactivating), all of which is fairly painless.
If you have many projects, you might also find yourself with multiple copies of a given dependency occupying your drive, but disk space is cheap and this only really becomes a problem if you juggle many projects at once or are a packrat.
The strange third cousin ‚Ä¶ conda.
There's also conda, which has the added advantage of being useful for more than just Python. Rather than being directly tied to a single project, a conda environment is stored in a centralized location that can be used across multiple projects.
It'll even manage multiple versions of Python for you and bring several precompiled binaries to the table for popular dependencies like Scipy or toolchain components like GCC.
Be careful if you're not running a solo development act, though; their
TOS
can make this a potentially expensive venture if you have enough revenue or people on staff.
Whatever your solution, though, it's undeniable that developing in Python is more complex than just the code you write - you ultimately depend on tooling, system libraries, specific versions of Python, and assorted dependencies. For a beginner, understanding the options and their respective pros and cons can be overwhelming.
Performance
For a computationally heavy project, Python's performance may fall flat when compared to compiled languages, particularly system languages like Rust, C/C++, or Go. This likely won't be a deterrent for many people, and indeed often Python is ""
good enough
"" to be the first choice for new project development.
Want to know how Python stacks against the competition? Well, it's complicated - the workload and environmental considerations can drastically affect the relative performance between languages.
You'll find no shortage of people attempting to quantify the differences
, and some can be more flattering than others when trying to make a case for a particular choice.
That said, let's be honest: there is an obvious and unavoidable tradeoff when choosing Python or any other interpreted language over one that is compiled, even when allowing for technologies such as JIT.
Decisions with Python
Share Data Engineering Central
Also, much of the pain (or relief thereof) can be attributed to which dependencies you pull in. For instance,
Polars
is rapidly gaining popularity over Pandas because it performs dramatically better in many regards.
Often, choosing the right dependency will have a much larger impact on your project than your choice of project language, and language choices also potentially involve programming difficulties and issues with time-to-market.
Most of this is irrelevant to the beginning developer, but for greenfield development, there is an often painful and contentious battle over which language to choose and what the short- and long-term impacts will be.
In these cases, Python can be great for establishing a proof-of-concept or a skeletal high-level workflow, but could also lock you in with a worse-performing solution. To some extent, that last point can be dealt with if you allow for iterative refactors through
best practices
(e.g. good TDD coverage, clear separations of concerns, quality code standards).
Best practices
Whether you're in Python or any other language, you should approach a project with professionalism and discipline. You could perhaps afford to be sloppy on a one-off pet project, but for anything that requires collaboration or that might be picked up by another person (including future-you),
you should strive for clean, consistent, and maintainable code.
Because ""good"" code is a subjective topic, you're bound to butt heads with other people on what the right standards are. I'll say this now: a ""bad"" standard is better than no standard, so you might have to swallow your pride or preference when choosing standards as part of a team.
It is critical that the standards and expectations for a project are clearly defined upfront. At a minimum, a project should have code quality tools like
pylint
or
ruff
in place, and enforcement of these rules (git hooks, CI/CD restrictions) should be automated.
Testing is a must in Python projects.
Tests are your friend, but if you're going to do tests, you need to do it right. Decide with your team what needs to be tested versus what doesn't; establish coverage requirements (sometimes, 100% just isn't feasible, but you should justify yourself if you can't manage it); and decide what kinds of tests (
unit, BDD, integration, boundary/contract, etc.
) offer the greatest rewards for your project.
""Clean code"" is a somewhat loaded term, but the version popularized by Robert ""Uncle Bob"" Martin can be an excellent starting point. While his original work dealt with Java and its nuances, there have been efforts (e.g.
here
) to adapt the principles to Python.
Many of the principles described within may seem self-evident, but having a team agree on certain practices such as ‚Ä¶
a maximum number of statements per function
single-responsibility principles
naming conventions
when and how to use comments, etc.
All these decisions will go a long way in protecting your code from yourself or other developers.
What else should Python developers be doing?
Where possible, tools and automated processes should enforce rules, but you also want a strong peer-review culture. In addition to having reviewers provide perspective on all code that you may lack, you ensure knowledge sharing across development teams by having multiple eyes on all committed work.
At a minimum (where feasible), all code should be introduced through pull requests that cannot be merged prior to having unanimous approval from at least two others.
As for Python itself, you'll want to have good tooling to preemptively handle many of your problems. A quality IDE with refactoring and code-quality capabilities, such as PyCharm, will not only empower your development cycle but also aid in proper debugging (
and no, sprinkling
print
statements all over your code is not a ""proper‚Äù approach
).
Conclusion
Python's actually a great language,
dare I say the greatest
? It's not the best overall (if there even is such a thing), and in many aspects, it will lose to its alternatives, but at the same time, it is also a terrific first choice for assorted problems.
If you want to make the most of it, though, you need to put in the time to understand it and grow in your skills. What ultimately makes or breaks most projects isn't the choice of language, but the developers responsible for its creation.
Always look for opportunities to continue to develop yourself even as you are trying to ship code, and be aware of the ever-changing landscape of tools, frameworks, language evolution, and best practices."
140221010.new-years-discount-40,New Years Discount 40%,"Come one, come all",2023-12-31 08:04:37.876-06,"Thanks to all the subscribers for a wonderful year, all 8k+ of you! Looking forward to more great content coming in 2024, here is a link to a %40 discount if you are not yet a paid subscriber ‚Ä¶ and want to be!
Get 40% off for 1 year"
140091930.data-warehouse-analytics-latency,Data Warehouse Analytics - Latency,The Latency Problem,2024-01-10 08:47:24.487-06,"What‚Äôs the one thing that never changes in the Data World? Even with the rise of the Modern Data Stack, many new tools, like little chicks hatching from eggs, go on to glory, while others wither on the vine, the circle of life. One steady thing has been the same through the eons ‚Ä¶
analytics
.
It‚Äôs safe to say Data Warehouse Analytics is the workhorse of our age. Ever grinding and churning out aggregation after aggregation.
Working in and around the data space, analytics, and Data Warehouse low these many years, nothing has changed.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The business still needs dashboards.
A multitude of analytics still need to be calculated.
Analytics are still hard to get right.
Another thing that hasn‚Äôt changed much?
People want their data fast and Johnny on the spot. No waiting. If they wait, there must be something wrong.
Latency
. It‚Äôs still a big deal and a hard problem to solve at scale.
This blog is brought to you by
Firebolt
. It‚Äôs a fast cloud data warehouse I‚Äôm currently exploring for low-latency use cases, and it‚Äôs able to deliver sub-second response times over TBs of data.
Latency + Analytics + Data Warehouse
It‚Äôs a simple problem to understand that has been around for almost two decades, latency, that is, but not an easy one to solve.
Find yourself a Data Warehouse producing analytics, and you will find yourself analysts, data engineers, and business intelligence folk, all at some point fighting latency problems.
The real question is, do you have a consistent process and design to deal with it? Most likely, it is some haphazard approach, throwing gum at the wall to see what sticks.
The Concept of Latency in Data Analytics
We should probably stop for a minute and review more about what we mean by latency problems inside a Data Warehouse.
Most often the entire reason behind creating and accumulating data inside a Data Warehouse is to do analytics on that data, create dashboards, join different data sets, and generally give insights into large-scale datasets
that drive business decisions
.
Of course there is a lot of work that goes into data models, ETL, and business logic massaging of the data to get it to a certain point where it becomes useful.
But, that isn‚Äôt the end. Those insights have to be presented to the consumers, typically in the form of reports and dashboards. This is where the latency problem comes in, and it is a real problem.
Simply put, the consumers of analytics simply aren‚Äôt willing to wait around for even 15 seconds for a report or dashboard to load. They are busy, have a lot to do, and like it or not, will simply ignore or mistrust the data if it isn‚Äôt snappy and quick, easy to consume.
Users expect a seamless interaction with analytics.
They are not willing to wait more than a few seconds for data to load.
Analytics at their core, aggregating datasets, are time-consuming and resource-intensive.
To prove the fact that data latency is still a serious problem in analytics, I simply brought up the question of how people solve this problem on
LinkedIn
.
Not surprisingly, I received a flood of comments and interactions to this question within a single day. The comments made it clear this is a problem people struggle with, and one people struggle to solve. There were all sorts of ideas ‚Ä¶
If this still doesn‚Äôt make any sense to you, let‚Äôs look at a simple example.
This query is probably run millions of times a year in some form or fashion.
Simply calculating dollars or sales, but some product, over some time period.
There could not be an easier query, and that isn‚Äôt really the problem. The problem is when you have millions and billions of records over many years and someone opens a dashboard or report ‚Ä¶
and then nothing happens for a minute or two.
They close it out, get frustrated, and go tell the Data Platform team nothing is working and why are we spending all this money if we can‚Äôt get a simple answer like this in under a few seconds.
Definition of latency in the context of data processing and analytics.
I think it‚Äôs important to get a ‚Äú
lay of the land
‚Äù so to speak, of latency in the context of what Data Engineers usually are dealing with when it comes to analytics.
Types of latency
:
Data latency
Compute latency
Query latency
Network latency
We could probably argue back and forth about the nuances of latency when it comes to analytics. Still, generally speaking, these four main areas can summarize when most of the problems lie.
Of course, each one of these areas deserves its own article and deep dive, but we can cruise past these and get a 10,000-foot view that can be helpful in understanding what latency problems analytics solutions inside Data Warehouses usually struggle with.
Diving into latency
Data latency
Let‚Äôs start with data I/O latency, I will also refer to this as file I/O, why? Because it‚Äôs common for many of the current Modern Data Stack Data Warehouses to be built with file-based systems like
Delta Lake
,
Iceberg
, etc.
The differences between Data Lakes, Lake Houses, and Data Warehouses are more in the eye of the beholder and have a lot to do with data modeling and implementation, rather than the underlying technology.
When processing large datasets stored in some file system and file type, say parquet files, all the sudden file I/O becomes a major factor.
File size
Type of files
Number of files
Compute latency
Next on the list is compute latency, this simply refers to the latency required to utilize CPU and Memory to literally compute the analytics in question.
Most analytics, reports, and dashboards at some point are doing aggregations of many different kinds, this work must be done by CPU, and on large datasets simply calculating a SUM of sales over a large time series, like in our above example we mentioned, will take time, depending on the compute architecture.
Query latency
This is probably one of the most common issues in Data Warehouse analytics, simply put, bad queries.
It‚Äôs hard to overcome bad queries with hardware and other antics like caching data etc, bad engineering practices can overcome, in a bad way, the best-laid plans and architectures.
Understanding how to implement different query optimizations, like when to filter, when to join, indexes, and a plethora of other practices will have a major impact on the latency of Data Warehouse analytics.
Network latency
Last but not least, one of the hardest problems to overcome is network latency, simply moving all those bits and bites over the network and possibly between nodes in distributed computing environments.
This is a problem that has been hassling Data Engineerings working on producing analytics on large datasets for decades, and the solutions aren‚Äôt always so simple, often involving multiple approaches to solve.
Wait, there‚Äôs more.
Unfortunately, this isn‚Äôt the end of latency problems inside Data Warehouses, the list of critical paths leading to problems is long and varied.
Lest you think I‚Äôm ignorant of other major concerns, here is a list of more ‚Ä¶
Wrapping it up with possible solutions.
Why is latency a critical issue in data warehousing and analytics? Because like it or not, we data teams rise and fall with the trust business has in us, our data, and the analytics and products we produce.
Nothing will sour the mood and the way we are viewed as poor user experience and latency in what we build.
It‚Äôs not an understatement that latency seriously impacts business operations. Product and marketing teams, as well as C-suit stakeholders, rely on data to make decisions.
It‚Äôs also clear that the data world is headed towards using more near-real-time data analysis. Latency problems can make this impossible.
What is the solution to latency issues?
It‚Äôs hard to come up with a one-size-fits-all solution to latency since the problems are very context-specific. What type of Data Warehouse are you running, what technologies are in use, what are the front-end tools, and how much data is being processed?
There are two sets of possible solutions to the above problems. There are sets of engineering and software principles that can be applied to help alleviate as much as possible some of these latency issues. As well, there are several companies that‚Äôve done a good job of solving these latency issues on large datasets, such as
Firebolt
, Clickhouse, Rockset, and Imply.
What are some of the practical steps that you can put in place immediately?
Data modeling work
Advanced data caching strategies
Indexing
Use of faster hardware and scalable cloud solutions
Pre-aggregating datasets and rollups.
Technical debt deep dives and fixes
Better architecture
Query optimization
In future articles, hopefully, we can start to dive in-depth into more of the problems AND solutions around latency in Data Warehouses and analytics.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
140019216.semi-structured-data-the-challenges,Semi-Structured Data - The challenges.,Demystify semi-structured data.,2024-01-03 08:47:30.116-06,"If a new engineer started on your team and was assigned a task requiring some work around defining a data spec and the ability to store some messages from another system.
They seemed excited and buoyant, working hard all week on their grand design, bumbling and stumbling through standup meetings, you‚Äôre kind, so you let them do their thing.
I mean you want them to learn right? It will be fine.
You ask for an update at the end of the week on how far they‚Äôve gotten so you can give some feedback. And then it happens. They send you an example XML file.
Yes. That‚Äôs right.
XML
.
Your eyes glaze over. You see red. You can‚Äôt speak. How could they?
XML
? What is this 1995?
Later at home, you feel bad. Semi-structured data. You have never given it much thought. You‚Äôve even made fun of
JSON
. But it gets you thinking. Is there more to semi-structured data than meets the eye?
This blog is brought to you by
Firebolt
. It‚Äôs a fast cloud data warehouse I‚Äôm currently exploring for low-latency use cases, and it‚Äôs able to deliver sub-second response times over TBs of data.
Data and Software in the Real World.
In reality, modern software and data generally aren't very simple. Getting your data structured in the right format is a critical function in many applications, and your needs will vary depending on the needs of your software and business.
Of course, much of the data stored in Data Warehouses comes from business applications and isn‚Äôt very clean, clear, or structured.
Today we are going to talk about
semi-structured data
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
If you stop and think about it, most data that we all deal with, flying to and fro around our s3 buckets and Python code, probably falls into one of two categories.
structured data
(
think relational databases
)
semi-structured data
(
think JSON
)
If we are honest with each other, compared to structured data, semi-structured data gets a bad rap.
This is for various reasons, some good, some bad, some misguided, some not.
It‚Äôs important to note while diving into this topic that we can deal with semi-structured data in two ways.
storage on disk.
data types in memory.
You might think this is obvious, but as we will see later, the lines can get blurred when you store JSON inside Parquet or Postgres.
What is it then? Structured or semi-structured?
What does semi-structured data look like?
At this point we should probably at least try to define what semi-structured data is. I imagine there is some disagreement on the subject, but that‚Äôs the spice of life.
Semi-structured data is data that does not map to a rigid structure and schema, like traditional relational databases. Yet, it still contains some properties that make it more accessible to understand and use.
In the real world, this semi-structured data is represented in certain formal data types like JSON or XML, and in memory with structures like dictionaries and maps.
Examples.
For those new to semi-structured data, let‚Äôs look at two of the most common formats that are known as the Godfathers of semi-structured data.
JSON
XML
JSON
JSON (JavaScript Object Notation) is probably the most common data interchange format. It has some obvious advantages ‚Ä¶
easy for humans to read and write
easy for machines to parse and generate
JSON is most commonly used to send data in web applications (
e.g., sending some data from the server to the client, so it can be displayed on a webpage, or vice versa
).
Many applications omit logs and data in JSON format.
Here is an example of JSON.
In Python, you can use the json module to read and manipulate JSON data. Here's a basic example:
XML
XML (
eXtensible Markup Language
) is known as a
markup language
that is used for encoding files in a format that is reasonably human readable.
It‚Äôs common to see XML used for configuration files
If you‚Äôve ever seen raw HTML, XML is similar in the sense that it uses ‚Äútags‚Äù to define objects and the text in these objects.
But, XML allows anyone to define their own tags, making it a great option for various data representation needs.
Here is an example of an XML file.
Example of reading XML with Python.
XML is widely used in web services, configuration files, and in applications where data needs to be exchanged between different systems.
However, JSON is often seen as a good replacement for XML depending on the situation.
More Semi-structured data types?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Well, yes and no. We would be remiss if we skipped two data types that might be considered quasi-semi-structured, in the sense that these data structures are popular tools used to hold semi-structured data themselves.
As well, depending on the underlying technology in which these two data types are implemented, they will many times contain different data types which makes them semi-structured because one can‚Äôt rely on a contiguous set of data types. Let‚Äôs look closer.
Maps + Arrays
Most folk probably don‚Äôt think of
Maps
and
Arrays
as semi-structured data, but you should, for several reasons. They are commonly used and in languages like Python, you can mix data types, so the structure is much more ‚Äúunknown.‚Äù
Also, many data storage systems, like
Delta Lake
, and others, support
Maps
and
Arrays as data types.
Of course, with data types like Map, what was once a single column of known data type can change to be a mix, hence the data becoming only semi-structured.
Semi-structured data in systems like Parquet and Postgres.
Semi-structured data has become so ubiquitous across the data landscape that common file formats like Parquet, used extensively in big data, and databases like Postgres, directly support storing data in these formats.
For example, if you examine the
Parquet data type docs
you can find support for ‚Ä¶
JSON
BJSON
Lists
Maps
Here we use Python‚Äôs
pyarrow
module to write data to a JSON column inside the parquet file format.
It‚Äôs extremely common to even run across semi-structured data inside a Postgres database, and we all know there are few things in life more structured than a relational database.
Should I use semi-structured data?
Ok, so we‚Äôve probably beat that concept into the ground. By this point, we should all know what semi-structured data is, and we‚Äôve seen some examples of working with it in code.
‚ÄúOn the surface it‚Äôs apparent that working with semi-structured can be more complicated than structured data, to work with.‚Äù
So what gives?
If your data isn't flat, it gets complicated. It could be stored in an RDBMS with a technique known as ""normalization"" - breaking nested data out into separate tables and building relationships between parent and child entities.
It‚Äôs common to ‚Äúflatten‚Äù out semi-structured data as well.
The effort might be great enough to justify looking at other options, though, which takes us to the topic of simply learning to work with semi-structured data as a first-class citizen.
More Thoughts on Semi-Structured Data
The reality is that semi-structured data is a fact of life and a balancing act. Especially for those working in the Data world around analytics and Data Warehousing. It‚Äôs common to deal with large datasets made up of semi-structured data.
It could be in the form of s3 buckets full of JSON files, or a Delta Table with complex data types like Maps and Arrays. Either way, this can be overwhelming for Data Engineers who‚Äôve not worked much with this type of data.
Do you flatten and remodel the data? Do you find tools that can easily deal with complex data types? Do you find something in-between, keep some data semi-structured, and flatten out other parts?
Positive Aspects of Semi-Structured Datasets
It‚Äôs one thing to simply have a pile of semi-structured data sitting in a cloud storage bucket, but what happens when you need to add that data to a Data Warehouse, do
JOINs
,
aggregations
, and more?
Semi-structured data for Data Warehouses and Analytics
As previously mentioned, this is where the proverbial rubber meets the road. It‚Äôs one thing to learn about semi-structured data, to store it away in cloud buckets, or even embedded in systems like Postgres or Delta Lake, but what do you actually do when you need to use the data?
This is where Data Engineers will have to step into the breach and spend some serious time understanding the usage of this data, and designing data AND compute models that will deliver the results.
The options at a high level are ‚Ä¶
Flatten and normalize the semi-structured data.
Leave the semi-structured data as is.
Deal with ‚Äúunpacking‚Äù in memory with compute
Somewhere in between these two options.
We can visualize this in the following drawing.
What types of operations do we have to consider when working with Data Warehouses and analytics ‚Ä¶ and that may become challenging with semi-structured data?
Indexing
Joins
Aggregations
Filters
The question is ‚Ä¶ if we are dealing with semi-structured data,
say millions of JSON files
in an s3 bucket with a highly ragged structure ‚Ä¶ how do you deal with this?
Small file problem, network, and disk I/O.
How much of the data to flatten vs leave semi-structured
What data points will be used for
JOINS
or
Filters
?
What sort of functions may be needed to unpack semi-structured data?
If any one of these problems is not properly thought out and dealt with, the dataset will become unusable for downstream processing.
Pros
What makes semi-structured data so great is its flexibility - it can accommodate a wide variety of data needs, with a broader capacity for the things that make structured data a struggle.
The hierarchy of the data is negotiable, records can be heterogeneous or mixed in nature, and guard rails for data integrity are optional and can be pulled in when it's convenient.
Also, semi-structured data is considered scalable and rich, in the sense that more complex ideas and data can be expressed easily, something that is hard in structured and tabular datasets.
Cons
Ultimately, the disadvantages are varied, depending on which format you move forward with. Some notable examples are given below:
These formats are often tedious to edit by hand, particularly if generated by software that eliminates whitespace for file-size reasons (in particular, XML and JSON suffer from this). In some cases, your entire document might live on a single line of text, which aside from being difficult to read or edit often also has the side effect of bogging down or crashing your text editor.
They may not be the most efficient storage method available, particularly for binary data such as images.
Closing Thoughts
Today we hopefully gave you a gentle introduction to semi-structured data at a high level, what it looks like, the forms it takes, and a basic view of how it‚Äôs used in storage systems and in memory.
We also broached the topic of semi-structured data in the world of Data Warehousing and Analytics, talking about some of the challenges it presents in complex operations like Joins, Filters, and Aggregations.
Thinking through these problems with Data Modeling and in-memory unpacking prior to implementations is key.
In the future we hope to dive into more detail and examples around semi-structured data in Data Warehouses and Analytics, how to make data models, and use concepts like lambda functions to efficiently and effectively work with semi-structured data at scale.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
140014648.how-to-reduce-complexity,How to Reduce Complexity,Coding Super Power,2024-01-22 08:35:12.578-06,"Software engineers have lots to think about in the shower.
Let‚Äôs not play the fool, we‚Äôve all been there, sitting at the super table, pretending to pay attention but really we are thinking about that nasty little bug or problem we‚Äôve been working on for a week with no success.
We pretend it comes with the job, and some of it does. But then again, some of it doesn‚Äôt. Complexity. The Destroyer of Codebases.
Senior developers and engineers tend to learn a few things in their many years of misery, whittling away their lives on a keyboard. One of them is that not everything is as it seems. Anyone can write code.
But can they write good code? What is good code?
Thinking and code.
Not only do you need to think about your code, but you need to
think about how you write it
. What does that mean, though?
Just as with regular writing, when you code, you need to remember that your code will need to be maintained in the future. Not only that, but many people may maintain the code whom you don't even know!
So, your goal is to write good, readable code.
Now that doesn't mean that your code can't be efficient. You can write efficient code but it may take additional effort to keep it from becoming complex.
I would argue this is the hardest part about software development. Not solving a problem with code, that can be done. It‚Äôs solving a problem while reducing complexity to reasonable levels!
What's wrong with complex code, you ask? Here are a few of the problems:
-
Increases the number of bugs
- Increases development cycles
- Hard to troubleshoot
- Hard to add new features
- Takes longer to figure out
How do you fix issues of this sort?
Read on to find out!
Principles of Reducing Complexity
Software engineers have their own language, much like doctors and lawyers do. Engineers love their acronyms more than most, though. Engineers even have a couple that are directly related to reducing complexity!
Those two acronyms are
KISS
and
DRY
. Let's review those concepts and see how they can keep your code from becoming too complex.
These are something every new developer should write on the tablets of their heart.
KISS - Keep It Simple, Stupid
Many engineers discuss making code simple, but it's a tough business. In most cases, though, a longer piece of simple code is a lot nicer to work than a short, complex piece of code.
Here's a small example of an unnecessarily complex conditional statement in Python:
While this code works, if you were to write a lot of code like this, it would become more and more difficult to test or understand.
Instead, let's rewrite it to be more conversational and simple:
Four lines were reduced by fifty percent and this example is easier to read and test too!
Now imagine a large production codebase, every single function or method adds up upon eachother. Complexity stacks up over time and over more lines of code.
Complexity is a serious business!
Want some homework? Go into your personal or work codebase and just pick a random file and read it. Look for places where the code is unclear and you think you could make it clearer.
Chances are you could have a serious impact, for the better, on the code you are responsible for on a daily basis.
DRY - Don't Repeat Yourself
Another popular engineering acronym is known as
DRY
or
Don't Repeat Yourself
. As the name implies, you shouldn't write repetitious code.
When you have multiple copies of code that do the same thing in your code base and you discover that you have a bug, you'll need to go and fix ALL the copies too! When you find yourself copying and pasting, STOP! Make that code common by putting it into a reusable module.
This concept also applies to similar code. You'll often see large conditional statements that change one or two variables. Sometimes, they call different functions.
In Python, you can simplify that type of code by using dictionary dispatch.
Here's an example that is repetitious:
You can DRY out this code using dictionary dispatch like this:
Here you can use a single conditional statement without any branches at all.
The Foibles of Over-Engineering
Why do engineers like complex code? Part of it has to do with being human.
An engineer wants to justify their salary or their prestige by writing code that looks complicated.
They won't get laid off if they are the only ones who understand the code.
Sometimes, the code gets complicated for mundane reasons, such as poor coding and patching it until it works. The business wants to ship the code quickly with minimal testing.
That leads to poor code coverage, which means there are a lot of hotfixes
. Your code base will become a patchwork of bandaids and get complex.
fast is not always your friend
everything is a tradeoff (speed vs time)
There's nothing wrong with trying to get your product out on time. However, when companies do this, they need to realize that they are creating technical debt that will need to be paid someday. That debt will be paid by paying for more defect fixes, refactoring, or scrapping code and rewriting it.
You can avoid over-engineering by committing as a team that you want to write simple code and you can then enforce it with tools.
For example, Python has a tool called
radon
that measures your code's cyclomatic complexity and performs other metrics on your code. You can add a tool like this to your CI/CD system to reject overly complex code.
Basic Software Design Patterns
Subscribe now
One way to help reduce complexity is using
software design patterns
. A software design pattern is a general, reusable solution that you can use to solve common coding problems.
They are templates that you can use to apply to code.
Here are a few design patterns you may have heard of:
- Dependency Injection
- Prototype
- Singleton
- Adapter
- Decorator
- Facade
Some programming languages work well with some design patterns, but certainly not all. For example, Python has the decorator pattern built into the language; in others, you might have to write the code yourself.
Modular Design
Another strategy engineers use to reduce complexity is known as **
modular design
**. The key to the modular design is that it takes a complex problem and breaks it down into smaller, more easily digestible problems or modules.
By breaking a complex problem into smaller modules, the code becomes easier to test and understand.
Following the modular design, you can also make the code more reusable. For example, if you create a module that holds an algorithm, you can import it into other applications. If you have unit tests around that module, you won't need to write additional tests for it when you add it to the new code or, at worst, require minimal new testing.
Example of Modular Design
A good example would be if you were to create a calculator application. Let's pretend you want to do a scientific calculator. When you originally wrote the code, you would put all the code into a single module. But then, once you have it working, you should split it out.
You can split the regular mathematical operations into their module (i.e., for addition, subtraction, multiplication, and division). Then, you could have a different module for the scientific calculations.
If you included trigonometric calculations, those could also get their module. Then you can unit test each of these modules separately and ensure they work the way you intend them to.
Code Refactoring
What if you have a large, complex legacy code base though? Can you reduce complexity there as well?
Of course, but it takes more work!
It‚Äôs always common to have a hard time getting upper management to take code refactoring and tech debt seriously. It can simply be seen as unnessary and unneeded, a waste of time.
Legacy code may need to have tests added before you can start refactoring. That's a part of the technical debt mentioned earlier in this article. The reason is that you want to verify that your refactoring doesn't break your code. You can't prove that if you don't have tests that verify the current code.
But let's go back to the original question.
How does code refactoring reduce complexity?
Well, code refactoring is applying
KISS
,
DRY
, and
modular design
in your code. You are breaking your complex problems down into smaller, more reusable pieces! If you do the refactoring correctly, your code will become easier to read and much more testable.
Other Methods to Reduce Complexity
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
There are other ways to reduce complexity before you even commit your code.
In this section, you will learn about three different ways:
- Leveraging frameworks or libraries
- Code reviews
- Design reviews
Let's find out how these topics can affect the complexity of your code!
Leveraging Frameworks or Libraries
There are many frameworks and libraries out there that you can use in your code. Some are free and some you must pay for. But why use one when you already have engineers who can make whatever your business needs?
There are many good reasons, but here are a few:
- Opportunity cost
- Development cost
- Testing cost
Do you want to have your expensive engineers reinventing the wheel? Most popular frameworks and libraries have extensive tests behind them and they have already figured out all the edge cases. You can save lots of time by simply using them. The main thing you need to worry about is making sure the software licenses match what you want to create.
You only want to reinvent the wheel if the current wheel is not good or doesn't do what you want and your customers want a better one.
By using frameworks, your code isn't cluttered as it's usually just an import in your code. Your code doesn't become more complex and you don't need to add anything new to your CI/CD system beyond testing your code that uses that library or framework, but you'd have to do that anyway with your custom code.
Code Reviews
Code reviews are another good place to catch code complexity early. The people reviewing the code should be looking for complex code. If they see it, they should provide constructive feedback to help the author(s) know how to simplify their code.
You shouldn't approve complex code if you can help it.
Artificial Intelligence may help you out with code reviews soon. A new product called
CodeRabbit
is out now that claims it can do code reviews and catch many mistakes.
One benefit of using a tool like this is that it can read the entire codebase and consider how the code that's up for review may affect other files that aren't in the review.
This tool can always catch the trivial problems for you while the engineers can focus on the design issues.
Speaking of which, let's talk about that next!
Design Reviews
A design review is where engineers get together and review the overall design of the code that solves a problem. They may look at the code and decide it needs to follow the original design better.
The review could show that the design is too complex. The engineers must then brainstorm to develop a new design or modify the original to become less complex. These kinds of meetings are very helpful in preventing the design from getting buried in scope creep.
You need to trust your engineers to be able to distill their ideas and remix them to fix any issues that crop up.
Conclusion
Complex code is a choice. You can choose a better way. By encouraging simpler, more readable code, your engineers will spend more time working on new feature work rather than spending long periods re-learning old code.
The less complex and more readable the code is, the easier it is to maintain and test. You can take that to the bank!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
139934999.introduction-to-write-audit-publish,Introduction to Write-Audit-Publish Pattern,Getting Your ETL On.,2024-01-15 08:09:30.273-06,"Ok, we are going to do it. Talk about a topic that everyone agrees on but no one actually does. Kinda like going to the gym after New Year‚Äôs, losing ten pounds, or being nice to your inlaws. Keep dreaming my friend.
We all know you aren‚Äôt doing WAP (
Write-Audit-Publish
) in your data pipelines either. But, is the WAP pattern all it‚Äôs cracked up to be, can we accomplish the same thing in a different way? What gives?
Today that‚Äôs what we are going to do, talk about what WAP (
Write-Audit-Publish
) is, its benefits and drawbacks, and maybe look at some alternatives.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What is Write-Audit-Publish (WAP), and why should you do it?
The Write Audit Publish (WAP) pattern in a Data Engineering context is a pattern used for designing ETL and data pipelines, it‚Äôs made up of the following steps.
Write (Stage 1)
This is the first step where data is ingested or captured. Data can come from various sources such as databases, streaming data, files, etc.
The data is run through common transformations and prepared for its final state. We are assuming at the end we have ‚Äú
production-ready
‚Äù data.
We could argue at what point we ‚Äú
store
‚Äù the data, it‚Äôs up for interpretation, raw staging data, mildly transformed data, data ready for production, take your pick.
Audit (Stage 2):
After the data is written, the audit phase starts the most important part of the process, verifying and ensuring the integrity and quality of the data.
This is really the whole point of the
WAP
pattern. What happens during this step?
Validation
Deduplication
Anomaly detection
Business logic checks
Any other Data Quality checks.
The idea is to make sure the data is accurate, complete, and suitable for further processing and analysis, aka it‚Äôs production ready!
Publish (Stage 3):
Then arrives the part we‚Äôve all been waiting for, we give the data our stamp of approval and ship it off to production, wherever that might be, Data Lake, Lake House, Data Warehouse, who knows.
This is really where the rubber meets the road when it comes to WAP and why you would use it. You‚Äôre protecting production, doing due diligence ‚Ä¶ not just relying on your data pipelines to be ‚Äú
always right.
‚Äù
WHY should we use WAP in the first place??
Share Data Engineering Central
The WAP (
Write-Audit-Publish
) design style for data pipelines is especially helpful in scenarios where data quality and integrity are of top priority.
This pattern also helps to ensure that only high-quality, vetted data is made available for decision-making and analytics purposes.
I don‚Äôt think it‚Äôs hard to imagine all the benefits of WAP design patterns. Enforcing standards, less data bugs and problems making it production, one place to put DQ checks and validations.
It‚Äôs not rocket science, it‚Äôs just a good idea.
But, have things changed? Are there other ways to achieve the same result? What are the dirty little secrets hiding behind WAP and what are the real-life downsides?
Let‚Äôs steelman this WAP stuff, and take a look at other options.
Do we still need WAP?
Now before you go and throw me over a cliff for speaking hersey, hear me out. No one would argue that we still need what WAP provides, namely, protection for Production, and Data Quality checks to keep things running smoothly.
That‚Äôs not what I‚Äôm saying. I‚Äôm saying do we still need the WAP pattern itself? Times have changed, Data Quality tooling is now becoming more widely avaiable then it was in the past, think
Great Expectations
.
Besides, anyone who works around cloud bases storage on systems with hundreds of TBs of data and compute knows that ‚Ä¶ resources are expensive.
Let‚Äôs take the above example of an implementation of WAP. We have data incoming, maybe it‚Äôs transformed and massaged. We write to pre-production tables, do our audit (which requires reading and checking the data), and then finally push (write) to production.
What‚Äôs the common theme?
That‚Äôs a crap ton of data being read and written to disk.
writing data to disk costs money
storing data costs money
reading data from disk costs money
the compute to do it all costs money
everything costs money
$$$
Sure, maybe you work for some billion-dollar company that doesn‚Äôt care. Money is no object for the data team, no one asks questions, so you do WAP. But, there is a whole lotta teams who care what their compute and s3 storage costs are.
Rethinking WAP (Write-Audit-Publish) as TAP (Transform-Audit-Publish)
I think there is a much simpler way, design, architecture, call it what you want, to get the same result as WAP, without WAPing yourself to death. It‚Äôs called TAP (Transform-Audit-Publish).
This is made possible by new DQ tooling that is cropping up everywhere, like
Great Expectations
. We can do ‚Äú
audits
‚Äù and ‚Äú
data quality
‚Äù inline now.
For example, Great Expectations can work on Dataframes ‚Äúin memory.‚Äù
We are saving a lot of storage and compute costs, getting rid of two sections, the double-hop of reading and writing the intermediate step.
Let‚Äôs face it, our tools and lives as Data Engineers have changed and become more complicated and able to do more than we could 10 years ago.
Writing DQ checks inline on our tabular data is not a thing that brings sweat to the brow anymore, it‚Äôs old hat. And we get the same results. With the new tools we can still alert, block, quartine, and do whatever we please with bad data.
Closing Thoughts.
I‚Äôm a fan of WAP. I just want it to be TAP. That works for me better and saves me money and hassle in the long run. Either way, what we are trying to accomplish, protecting production and auditing our data is correct prior to publishing‚Ä¶ well, that‚Äôs just good business.
However, I tend to think WAP is a little outdated with our current architecture and tooling options in the era of Data Lakes and Lake Houses. These systems are built on top of
Delta Lake
and
Iceberg
for example.
These are file-based systems, typically backed up by cloud storage. Reading and writing from the file-based systems, and extra storage, all cost money.
Why not simply apply those audit checks while that data is still in memory from its previous transformations? Long Live
TAP
.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
139892102.learning-the-command-line,Learning the Command Line,Commands your mom should have taught you.,2024-01-08 09:20:29.107-06,"Do you think the command line is dead?! You are wrong, so wrong my fair-weathered friend! Even in the age of cloud computing, the command line is still going strong, stronger than ever before I wager. If you would only learn how to use the command line effectively, you can accomplish more faster, of this I am sure.
The command line is helpful in the cloud on AWS, GCP, or whichever cloud provider you work with. The reason? You will still need to remote into these services occasionally to work on your server. Not to mention the plethora of CLI tools they provide you.
You'll never look back once you learn the command line because it can be much faster than digging through the never-ending cloud UI.
So, that‚Äôs what we will do today, roll over the rock you‚Äôve been living under, drag you out into the sunlight, dust the moss off you, and teach you some command line stuff.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Benefits of learning the command line
Why learn the command line? Everyone is using tablets or phones to do their work, right? Not if you're an engineer! Engineers, unlike the teeming masses, use desktop and laptop computers to do their work. Don't put yourself into a cage where you only use the UI that is provided to you, there is nothing more stunting to an engineer‚Äôs career.
Some companies provide amazing, intuitive user interfaces, and take the overly complex but at the same time wonderful AWS Console.
It‚Äôs almost too easy to become reliant on a GUI or web browser to do a job that could be automated via the command line.
But even the best user interface is slower than using a command line.
Here are some of the benefits you'll get from using the command line:
Enables a faster development lifecycle
The command line is much easier to automate
If you can automate the command line, then you can scale it
You'll beat the pants off your peers!
But wait? Doesn't each operating system work differently? Let's talk about that.
Linux vs Windows vs Mac OSX
Each operating system has its own command line utility. Let's talk about each of these tools. First, before I talk sides (*cough* Linux), I started out in Windows. Guess what?
Learning to be comfortable on the command line doesn‚Äôt really have that much to do with Windows, Mac OSX, or Linux. It‚Äôs more about knowing the general concepts and what the possibilities are, the rest is just Googling.
Options, Options.
You can choose between cmd.exe or Powershell or Windows Terminal on Windows. You should pick Powershell or
Windows Terminal
because it can do a lot more and you can use Linux-like commands in it that you cannot use in cmd.exe.
Mac comes with an iTerm tool that you use as a command line or terminal. You should upgrade and install
iTerm2
because it looks much nicer and more feature-rich. You'll also be using Zsh in your terminal on Mac, which is a more feature-rich version of Bash
Linux's terminal usually defaults to something called Bash although you can also install zsh (AKA Z-shell) or other types of shells in your Linux terminal.
If you want to get fancy with your terminal, you can install custom prompts, such as
oh-my-zsh
or
Starship
.
What should we say to all those folks who say they don‚Äôt need to know the command line just to copy files around? There is a lot more that happens on the command line on the average day of a Data Engineer.
The command line assists using popular tools
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What are some cloud-related tools that you can use from the command line? Here are just a few:
Docker
Kubernetes
AWS
The reality is if you are going to be a senior engineer and work on high-performance teams where you can contribute, you are going to have to be comfortable around the command line.
Using Docker on the command line
You may not know it, but
docker
Desktop didn't always exist. When docker was first released, it was a command-line-only tool. Even now, you can use docker to do anything you can do in Docker Desktop and more!
For example, if you want to build a new image, you can do the following
Want to see what containers are running or stopped? Try the ps command:
If you don't know any docker commands, you can run docker all by itself or execute docker help to get a full listing.
Using Kubernetes on the command line
There are few infrastructure tools you are more likely to run across than Kubernetes. It‚Äôs used by many teams to deploy every sort of service and app you can think of.
The chances of you skating by your whole career never working with Kubernetes on the command line are slim. Consider yourself uncommonly lucky if you manage to get away scot-free.
Using AWS on the command line
I dare to say it‚Äôs hard to call yourself an engineer or developer in the age of cloud if you haven‚Äôt worked with cloud providers, especially AWS, via the command line.
If and when you work on AWS, more tasks can be listed that are done via the
AWS CLI
. It‚Äôs just another reason to learn the command line.
Introduction to command line basics and concepts
You've probably heard enough about the theory and are ready for more concrete examples. You've come to the right place!
In this section, you will learn the basics of the command line. Don‚Äôt worry about Mac vs Windows vs Linux. Focus on just understanding the basic command line options we are going to present here and learn how to do them on whatever system you‚Äôre working on.
Moving around directory structures
When you open a command line, you are in a directory. Usually, you'll start out in your user folder. You can type
pwd
to get your current location:
If you want to change your location, you'll use the
cd
command, which stands for change directory.
Here's an example:
Changing directories is all about navigating your directory structure, kind of like how you would do it in Windows Explorer or Mac's Finder application.
But you can do more than change directories!
Listing files
Once you have navigated to the directory you wish to be in, you'll probably want to check and see if there's anything in that directory.
On Windows, the most common command to use is
dir
. When you run
dir
, the command line will print out all the files and directories in your current location.
If you are on Mac or Linux, you can use the
ls
command instead to get the same functionality. You can also use
ls
on Windows if you use Powershell or Windows Terminal. However,
ls
is not supported by cmd.exe, so you have to use dir there.
Here's an example of running ls or dir in Powershell:
Directory listing is only the tip of the iceberg. Let's find out what else you can do!
Moving and copying files
Let's say you've found a file using the list command you learned about and you want to move it. You can do that too!
To move a file, use the mv command:
If you pass in the fully qualified path to the file, you can move it from anywhere:
Sometimes, you don't want to move the original. You want to copy it instead. You can copy a file with the cp command:
Note that
cp
and
mv
use the same syntax. Handy!
Creating and Deleting files
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
You can create or delete a file on the command line as well.
Linux and Mac both use
touch
to create an empty file. Windows has to be different. On Windows, you would use New-Item in Powershell.
Let's look at an example for Linux or Mac:
The command above creates a file in your current directory and names it my_file_name.txt. You can also pass in a fully qualified path to have
touch
create the file in some other location, as long as it's valid.
On Windows, you can simply replace touch with this command-let:
Now that you know how to create a file, it's time to learn about destroying them! In this case, Windows, Mac, and Linux all use the same command: rm.
Here's an example:
This command will delete the file if it exists. You'll receive an error message if you try to delete something that isn't there!
Creating and Deleting directories
All the operating systems provide ways to create and delete directories. Fortunately, the commands are almost the same across all the major platforms.
To create a directory, use
mkdir
:
The command you use will change if you need to create a nested set of directories (i.e., a directory tree). On Windows, you can use
mkdir
to create the directory tree the same way you create a single directory.
On the other hand, if you are using Linux or Mac, then you'll need to add a
-p
flag:
The
-p
tells
mkdir
to create all the directories in the directory tree.
Want to remove an empty directory? Then you can use the same command across all platforms:
If you want to remove a directory tree, you'll need to do something different. In Windows PowerShell, you can use the following command:
On Mac and Linux, the usual command is:
Be careful with these deletion commands. If you point them at the wrong folder on your machine, you will mess up your operating system. Fortunately, most operating systems won't let you delete important files unless you're running as administrator or root, but be cautious regardless.
Viewing files
The command line allows you to view text files too. You won't want to open a PDF or a Microsoft Word document here, though. Binary files aren't human-readable, so even if you did open them, they would be incomprehensible!
However, the command line provides a tool called
cat
, allowing you to view any text file, whether it's a log, a Python file, or any other text-based file.
Here's an example that will display what's in a Rust file in one of my projects:
If the file is longer than can be shown on a single screen, it will whip on by, and you'll have to scroll back up to read through it.
For large files, you can use more instead:
The more command will show a page's worth of data. You can then scroll down one line at a time using the Enter or Return key. For the most control, you should use the less command, but that one isn't available on Windows. You would need to install a tool for that or use Linux on Windows.
All of these commands are available on Mac and Linux.
Editing files with vim
Linux and Mac come with command-line text editors built-in. The most common ones you'll find pre-installed are:
vi
vim
nano
These command-line editors allow you to create or edit text files in your command line.
Windows does not come with a command-line text editor, but you can install them. For example, you can install a
Windows-compatible version of Vim
too.
To launch vim, you can type vim and it will launch an empty editor window in your command line where you can type or copy and paste your text into.
If you have a pre-existing file, do this instead:
This time the editor launches with your file open inside of it. Hit the
i
button to write in the file. The
i
button turns on
INSERT
mode.
When you're done editing, hit the ESC key to enter NORMAL mode and then hit the colon key (
:
). Next, type
wq!
or
x!
to save your changes and exit.
The colon tells
vim
that you want to do some type of command while the wq means ""write and quit"".
If you don't want to save your changes, do
q!
instead.
There are tons of other built-in commands. You should Google for a vim cheatsheet and find one that you like.
SSH into machines
Secure Socket Shell or SSH is a popular command line tool to connect to a remote machine. You will almost always use [
OpenSSH
] on Linux and Mac. Windows 10 and 11 come with an
SSH client built-in
that is based on OpenSSH. However, you may need to enable the client.
Open up your command line tool of choice and get your credentials ready for one of your servers.
Let's pretend that you have a server named
sample.ssh.com
. Here's how you could use SSH to login to it using your default credentials:
Pretty simple, right? However, you'll often want to specify which user to login as.
That's easy, too:
Here you are logging in as the user ""john"".
You can also send along some of the commands you've learned about in this tutorial and run them on the remote machine.
For example, you might want to log in as ""john"" and list the contents of a directory. To do that, you could run the following:
If you don't want to enter your credentials every time you log in to your server, consider creating an SSH keychain.
Here's an example using ssh-keygen:
This will create two files, one with a
.pub
extension and one without an extension but with the same name in
~.ssh
. You'll need to copy the public key over to your server to use your SSH keys.
Just copy the contents of the .pub file over to your remote machine and put it in
~/.ssh/authorized_keys
and you should be good to go.
Move remote files back and forth with SCP
You'll find that you often have files on your local machine that you want to copy to your server. Yes, there's a command-line tool for that tool
It's called
scp
(secure copy) and it's available on all the operating systems.
Let's see scp in action. To copy a file from your machine to a remote machine, you would do something like this:
This command will copy the specified file, login to the remote machine, and place that file in the /remote/directory location.
If you'd rather copy a file from the remote machine to your local computer, you would use this command:
Note the period at the end. That tells scp to copy the remote file to the current location on your command line (remember pwd?). You can provide a full path instead of a period.
Closing Thoughts
Well, I could go on but I think you‚Äôve probably had your fill. I don‚Äôt think I can overstate the importance of being able to work on the command line in a variety of environments, with the ability to do basic commands.
You know what‚Äôs hard? Being a new engineer, or onboarding a new engineer onto your team, you give a them a simple task ‚Ä¶ ssh into this AWS EC2 box and do x,y,z. You get blank stare.
They don‚Äôt know how to SSH, setup keys, or even move around and use basic commands. Not good.
The reality is that the command is easy. Unless you‚Äôve never done it much.
Work in your IDE all day long? Consider switching to
vim
for a week or so, just to freshen up your skills. Again, it‚Äôs not like you have to memorize every command line operation on Windows and Linux ‚Ä¶
it‚Äôs more about being comfortable on the command line so you can do what you need to do when the need arises.
Share Data Engineering Central"
139617130.the-age-old-problem-logging-and-monitoring,The Age-old Problem - Logging and Monitoring,Two Pillars of Data Engineering,2023-12-14 09:28:55.359-06,"Over the years of churning and burning in the Data Engineering space, there are a lot of things that have changed. Some tech is being pushed into the rusty scrap pile of the moldering and smoldering tools of yore.
Yet, there are a few things that haven‚Äôt changed at all, they are the stone pillars standing beaten and weathered over the eons, and refuse to change or go anywhere.
Logging and Monitoring are two of those wizened old oak trees. So boring that no one bothers with talking about them much, yet they are at the core of everything we do in this distributed age.
Today we shall sit and cross our legs under these forlorn and forgotten topics and see what comes to mind.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Logging and Monitoring - Useless or useful?
Some topics are evergreen and never leave, like SQL, other things are second-class citizens. I imagine it‚Äôs because they are ‚Äú
not that cool
‚Äù maybe?
After building many data platforms over the years, all the way from custom distributed systems on Kubernetes to simply using Databricks ‚Ä¶ there is one thing that has never changed.
Logging and Monitoring are undervalued and of upmost importance to anyone in the Data Engineering space. They can often make or break a system, make something either usable or unusable, loved or unloved.
Why, you ask? How can something so simple like Logging and Monitoring be so important?
It seems so straightforward, but I've learned over the years that it‚Äôs always the simple stuff that can make you sink or swim.
First, let‚Äôs talk about
Logging
.
Logging for Data Engineers.
Share
I suppose if you just wander around life writing code and not thinking about logging, you just might not be familiar with some common terms you can hear when people talk about logging.
Here are some general basics that will get you started in the right direction.
Generally speaking, we can throw logging into two different camps.
Diagnostics
Audit
Generally speaking, audit logging would be logs sprinkled in and around the business logic and flow, very helpful for bugs when things go wrong.
Secondly, diagnostic logs are more technical in nature. They may tell you stuff about database connections, queries, API calls, retries, etc.
Together, both these ‚Äútypes‚Äù or ‚Äúapproaches‚Äù to the ‚Äúwhat‚Äù of logging complex systems, can go a long way in understanding and troubleshooting problems.
Beyond these two general types of logs, there are different levels of logging, it‚Äôs common to run into these terms:
Debug
:
logs detailed information.
Info
:
logs general flow and events.
Warning
:
logs potentially problematic situations.
Error
:
logs for critical errors.
Logging in Python - with examples.
Probably one of the easiest ways to ‚Äúget into logging‚Äù is to simply start out with the
Python logging package
. It‚Äôs as easy as an import, and of course, there are a lot of configurations that can be done, but, getting logs to
STDOUT
has never been easier.
Easy!
Come on an imaginary trip with me, close your eyes, and away we wisk to a large and complicated codebase full of complex and large data pipelines.
You‚Äôre new to the codebase, you‚Äôve only been at it 6 months and have barely scratched the surface of most of the code. You are on call for the weekend, your boss is gone, and of course, you get a production failure alert for a pipeline you‚Äôve never seen.
There are two options ‚Ä¶
You have logs that tell you generally what was happening in the script and how far it went before the failure.
You have nothing and have to figure it out yourself.
That‚Äôs how you have to think about and approach logs.
Let‚Äôs take this play Polars data pipeline for a second, and pretend it is much larger and more complex.
You can see it‚Äôs easy to use
logging.info()
sort of log where in the script things are at, and possibly variables like ‚Äú
where am I writing data too.
‚Äù
Also, you can get fancy and log ‚Äúbusiness logic‚Äù type warnings. Things like this can make a huge difference in a large complex codebase. In the above example, I chose to large a WARN level if the dataframe is empty.
I hope this is getting your wheels spinning in your head, get those cobwebs blown out. Logging is a powerful underused tool in the belt of the Data Engineers.
It makes debugging easier and understanding new codebases easier. Without good logs, the developer is at the mercy of their own experience and a dash of good luck.
Ok, let‚Äôs move on and talk about monitoring.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Monitoring for Data Engineers
This topic can be a little more difficult to get our hands around, simply because it‚Äôs ‚Äúnot as easy‚Äù as logging. By easy I mean ‚Ä¶ more complicated. Which can make it hard to talk about it, but we will give it our best shot.
It can be two sides of a coin ‚Ä¶
If you‚Äôve never had good monitoring, you don‚Äôt know what you‚Äôre missing.
If you use good monitoring tools, you know how much of a game-changer it can be.
Of course, monitoring depends on the tools you are using, but nonetheless, it can be game-changing to know ‚Ä¶
Are things working well or not?
Resource utilization, where the money or compute is being burned.
A bird‚Äôs eye view into the data platform infrastructure.
When data pipelines start to grow in number and complexity, you can‚Äôt rely on some cron jobs on an EC2 instance to keep the lights on.
The key to monitoring is to have a visual inspection of what is running and what the status is.
source -
Prefect
Not only do Data Engineers have, and provide, high-level monitoring of the data systems, but they also have the ability to dig into problem spots and see where tasks are long-running, where something is always failing, and other issues are key to running a data platform in a healthy way.
There are a lot of options when you start ‚Äúlooking into monitoring‚Äù for data teams. It can be overwhelming at first.
I would suggest a few things to make your life easier.
Choose data tools,
like Prefect
, that have monitoring built-in.
It‚Äôs easier to use what you have than start from scratch.
Start simple
You probably need basic info like what is or is not running.
Or what is taking the longest to run, where are the bottlenecks?
If you can start with just getting a bird‚Äôs eye view into your resources and processes, be able to understand what is breaking, what is not, what is running well, and what is not ‚Ä¶ you can solve a lot of problems.
Now you don‚Äôt have to guess, you can save money by looking for over, or under-utilized resources. You can long for the 80/20 rule, what are the 20% that are running long and eating all the costs?
Heck, maybe your monitoring doesn‚Äôt work well and you realize you have an architecture problem because you can‚Äôt monitor your system well. That tells you something important as well.
Take this moment to think about your monitoring ‚Ä¶ what you are using today? Can you answer these simple questions?
What is running the longest?
What breaks the most often?
How many things are we running?
If you can‚Äôt answer those questions with your monitoring ‚Ä¶ what does that tell you?
Let us know in the comments what your best or worse experiences are with data monitoring in logging. What has worked well, what has not?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
139437789.kubernetes-for-data-engineers,Kubernetes for Data Engineers,The Age of Containers,2023-12-11 09:25:45.658-06,"We do live in the Age of Containers, it‚Äôs the world we all live in. Docker has become standard. Data has grown. Everyone and everything is in the cloud. The Modern Data Stack we‚Äôve all been working
low these many hard years
has only made the need for an agnostic and scalable container platform more real.
And this brings us to today‚Äôs topic.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
We want to give Data Engineers an introduction to Kubernetes. It‚Äôs a tool everyone talks about, but not that many folks get a chance to get their hands dirty with.
Hopefully, by the end of this article, you will at least know enough to be dangerous and break something. That‚Äôs the idea anyway.
We will start from ground zero and play around with minikube, and get our feet wet so to speak. Concepts first, then the fun stuff.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Introduction to Kubernetes
So, let‚Äôs start from scratch and pretend you‚Äôve been living in a Mad Max apocalypse and have no idea about what Kubernetes is, and does. We will start with the ‚Äúwhat‚Äù, and then the ‚Äúwhy,‚Äù concepts before we go deeper.
‚Äú
Kubernetes
, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.
‚Äù -
kubernetes website
How can we distill this down even more?
Kubernetes provides the ability to manage and abstract away a cluster of machines from which various applications can be served in a high availability and scalabable manner. - me
Let‚Äôs draw out this concept of Kubernetes, and then talk about the concepts we see.
While I‚Äôm sure my Kubernetes ramblings will gain the ire of many a seasoned user, I‚Äôm here just to impart the imperfect but hopefully helpful overview.
A Kubernetes cluster consists of worker machines called
Nodes
.
Nodes
can have one or more
PODs
on them.
PODs
are where
Containers
run.
And this is all virtual.
I‚Äôve used Kubernetes myself over the years for several things, the description of which may help the reader who is unfamiliar with Kubernetes where it can help in a Data Engineering context.
What‚Äôs the best way for a Data Engineer to think about how Kubernetes can help in a data context, and in general?
Deploy distributed systems like Spark inside Kubernetes.
Deploy data apps inside Kubernetes
Deploy works from tools like
Prefect onto Kubernetes
.
General compute for various workloads.
What is Kubernetes good for? It‚Äôs a place to run and deploy ‚Äúthings‚Äù.
It‚Äôs the melting pot of the data world. It can be what you want it to be. But it‚Äôs not a joke to run and learn.
Options in the real world.
So in the real data world, I‚Äôve worked with two different types of Kubernetes deployments.
Managed Kubernetes much like
GKE from Google Cloud
.
Self-hosted Kubernetes clusters.
In option 1 you can easily with code spin up and down Kubernetes clusters with a little code or a click of a button. In option 2 you typically have a host of DevOps and Platform folks, a team, dedicated to supporting and running Kubernetes.
Typically option 2 is for very large teams, with hundreds or thousands of engineers deploying a wide variety of applications and services. It‚Äôs a pain. More than you can imagine.
I‚Äôm in favor of hosted Kubernetes clusters like GKE or others, simply because of lower administrative overhead. Kinda like using RDS instead of installing and managing your own Postgres instance.
Ok, enough lofty talk, it‚Äôs hard to really learn more without poking around our fingers on the keyboard and doing something useful. While most of us don‚Äôt want to spend the money or have access to a Kubernetes cluster, we play pretend with something called
minikube
.
‚Äú
minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. We proudly focus on helping application developers and new Kubernetes users.
‚Äù -
docs
A few notes before we begin.
Before we start playing on the command line I should mention a few more specific topics and concepts about working with Kubernetes, things you will hear and read about once you start digging in.
Services -
Exposing an application running on a POD to the network (other PODS). Think Postgres.
Persistent Volumes -
since we are working in a virtual environment, we need a way to have permanent storage.
Helm Charts -
Tool to build with configuration complex Kubernetes applications (think multiple PODs, storage, services, all working together).
Learning with minikube.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
What do you need?
Docker installed.
minikube installed
.
helm installed
.
This will allow us to pretend like we are playing with a Kubernetes cluster, I highly recommend this for playing around and learning about Kubernetes.
To just poke around and get something deployed locally into Kubernetes, let‚Äôs mess around
Arroyo, a new Rust-based streaming SQL tool
.
Let‚Äôs try to do the following ‚Ä¶
Deploy this tool using a Helm chart into local minikube/Kubernetes.
Look at the services, PODs, and other things we will specify.
Learn what we can if we are new to this.
First step, start minikube you hobbit.
Next, we save this
YAML
file somewhere, which is a helm chart that tells minikube how to install Arroyo into the cluster.
Now, we can add the Arroyo repo to our Heml installation, and deploy it.
Now I get a bunch of messages that apparently it worked.
Checking our Kubernetes application install.
Now we have done that, maybe you can learn something and solidify some of these concepts we talked about earlier. The first step is going to be to start to use our command line to describe and inspect what we have running in our ‚ÄúKubernetes‚Äù local cluster.
First, remember those PODs we talked about before, we should be able to see a bunch of Arroyo PODs running using
kubectl
commands.
Ah, now we can see a bunch of PODS, many of them with Arroyo in their name. It‚Äôs obvious what some of them are ‚Ä¶ we can see things like Postgres, and Prometheus being listed.
What else could we do? How about looking at all the services that Arroyo is exposing and what ports they are using?
Hopefully, this is starting to bring some concepts to bear that we talked about earlier, if you are new to Kuberentes.
We can deploy applications onto PODs, expose them on a network to talk to each other as needed, etc. We can even look at the volume (storage) claims being used by the PODs.
What do you think?
I hope if you are new to Kubernetes and have never had the chance to work with it, this has been helpful to at least introduce some of the basic and core concepts.
I personally think it‚Äôs hard to grasp new tools without playing around, and minikube + helm is the perfect way to learn Kubernetes on the cheap, on your computer.
If you are a Data Engineer and want to learn more about Kubernetes, try a project like writing a deployment for Prefect, and see if you can get that running.
Kubernetes is an interesting tool that isn‚Äôt used that much in Data Engineering. It‚Äôs seen as more of a tool for platform teams and others to run different applications. Sometimes you will find folks that have deployed Spark, or maybe their orchestration workers with Kubernetes.
How I‚Äôve used Kubernetes
I‚Äôve personally used Kubernetes for a few different production tasks.
Customer geospatial data pipelines for distributed processing.
Running orchestrator tools.
Deploying small ML models and pipeline components.
To me, Kubernetes is just a way to save time and make deployments easy, if it‚Äôs managed correctly and doesn‚Äôt get out of control. But, many times you can simply find managed services and other tools like EC2 instances or Fargate to solve the same problems.
Kubernetes is great at providing fault tolerances and other features if you‚Äôre just a team that needs to deploy a TON of random data components.
Do I miss working on Kubernetes? No, not that much. Did I learn already using Kubernetes, very much so.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
139374134.do-a-project-learn-something,Do a project. Learn something.,Rust + SQLite + Twilio,2023-12-13 09:27:24.36-06,"I‚Äôm not sure why people don't do it more, learn that is. It‚Äôs a cornerstone to growing as an engineer over the long term. I would say it‚Äôs fairly easy to be an engineer for a good 3-4 years, maybe 5 at tops if you are lucky, without learning anything new.
Anything longer than that, you are probably starting to stagnate. Some people will probably have a problem with that, but that doesn‚Äôt make it untrue. You do the same thing over and over, and slowly become some little piece of moss on the dark side of a rock.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Doing projects is a surefire way to learn. It is probably one of the best ways to learn.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Sure, at work sometimes you get the chance to work on a greenfield project, brand new, nice smelling, and all, but that doesn‚Äôt happen often.
Nothing will teach an old dog new tricks like ‚Ä¶
finding a project that means something to you.
building out that project with some new to you stuff.
project
reepicheep
I‚Äôm going to give you a real-life example, perhaps extreme, but something that gives you an idea of what I‚Äôm talking about. You know the best part of working on a project that ‚Äú
means something to you
,‚Äù or that you ‚Äú
have skin in the game.
‚Äù
In a recent upturn in my life,
my wife Crystal was diagnosed with a rare type of blood cancer
. It has upturned my life in many ways, and coding, as it always has been, has been a helpful outlet for me in dealing with such a thing.
One of the hard parts, of many, of this cancer journey with my wife is the management of the medication and pills, which can be quite complex.
many cancer patients get treated in cycles
cycles can consist of complex and confusing days to take medicine.
it can be extremely hard to remember ‚Äúwhat‚Äù to take ‚Äúwhen.‚Äù
So I decided to help my wife, and like any good engineer automate away the problem. The project needed to be able to meet a few criteria.
Something fast, reliable, something I can kick off and run and forget about it, confident it will not flake out?
Rust was the only answer for me.
That‚Äôs when
reepicheep
came to life. Here is a rough sketch out of what I wanted it to look like in the end.
What‚Äôs some Rust stuff I‚Äôm going to have to learn either for the first time, or the first time in awhile?
Want to practice moving data from
JSON
to a
struct
.
Find a way to use an
enum
a few times for logic.
Use
match
to deal with logic.
Find out how to send text messages via
twilio
.
Do
date
and
datetime
with Rust.
Figure out what it‚Äôs like to use
SQLite
with Rust.
Find out what it‚Äôs like to do ‚Äúbusiness logic‚Äù with Rust.
You can see the
final product here
.
Tidbits from the code. Things I practiced.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I‚Äôm just going to give you a quick and dirty of this projects, code snippets, and how I put it ‚Äúinto production.‚Äù Keep in mind these are the types of projects you should look for, something that makes you do something you don‚Äôt normally do. That‚Äôs the key to learning and growth.
The code.
Here is my main Rust entrypoint.
What was new for me off the bat in Rust? Remember, new things don‚Äôt have to be earth shattering, they can just be small things you add to your toolbox of knowledge.
Running an infinite loop.
Using a break to get out of the loop.
Using SQLite with Rust.
Here is an example of a few Rust functions doing some SQLite stuff. It was much easier than I thought and actually enjoyable to combine Rust with SQLite.
What were some other things I wanted to learn and practice in Rust?
Using match statements.
Do some date and datetime manipulation.
Use an enum.
Use structs.
Do some HTTP stuff.
Here are some of the examples of each. First, some random functions that I was able to use match logic in.
Enums?
Structs?
HTTP?
And there you have it. It was enjoyable to do some stuff I don‚Äôt often get to do in Rust, especially in combination with each other. For me I haven‚Äôt been one to just match that much, and it‚Äôs wonderful to combine match with enums in Rust.
The simplification of complex logic is wonderful.
Also, I was surprised at the
chrono
cargo crate to use with date and datetime manipulations. It can be a pain working with date and datetimes, very tedious, I expected something very unpleasant in Rust, but was surprised at how easy it was.
I think one of the best parts about doing a ‚Äúreal‚Äù project that actually interests you and you have skin in the game for, is that the learning comes easy. You want to do a thing, so you figure out how to do that thing. You hardly even realize your learning and developing your skills.
Putting it in production.
So in my case I needed something bullet proof that I could set and forget. I trusted the Rust code I wrote, unlike Python, it would compile and after running a few tests I could be sure it would be reliable and that binary would run till the end of time without failure.
I decided to use the
Linode cloud
and spin up a tiny 1GB shared CPU instance for $5 a month, running Ubuntu.
I setup the firewall on the instance using UFW to lock it down, installed rust and cargo on the machine as well. Then I
simply cloned my Git repo for the project
. Next I set the .env file with the creds for twilio.
This allowed me not to have to cross compile a linux binary on my mac machine.
After that I simply ran a nohup command and specified output to a file. This will allow the program to run in the background forever.
Then it was off to the races!
And to top it all off, I scored some husband points. Writing some code for something truly useful doesn‚Äôt hurt either!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
139065302.black-friday-sale-40-off,Black Friday Sale - %40 off.,Come one come all,2023-11-22 13:26:55.561-06,"Thanks to all my loyal readers who make this newsletter possible. I find it quite amazing you all put up with my weekly ramblings.
Believe it or not, it takes a lot of time to plan, research, and write a constant stream of uninterrupted (
interesting?
) content.
I'm offering a 40% discounted rate for Black Friday that will run through the weekend.
Instead of giving Amazon your money, they have enough, use it to support the Data Engineering content you love!
Click the link below!!
https://dataengineeringcentral.substack.com/blackfriday"
139001651.sqls-having-clause,"SQL's ""Having"" Clause ",The little engine that could.,2023-12-18 09:17:47.912-06,"SQL gets enough love from all you fanatics. Doesn‚Äôt really need anything extra from me. I mean, I‚Äôm the guy who wrote ‚Äú
You Need To Break Up with SQL
,‚Äù and I haven‚Äôt changed my tune much since then.
But, because all you data people just can‚Äôt seem to let go of your SQL Mother, I will give you a little something to keep you happy. Don‚Äôt let it go to your head.
Do you want some SQL? Fine, open wide.
Today we talk the not often used, small but mighty,
HAVING
clause.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
HAVING clause - a diamond in the rough.
Before you judge me for being so hard on SQL, you have to understand a little about my background. I wrote SQL every day for years, and years, and more years after that.
I breathed it, slept it, woke up to it, dreamed about it. And then one day I finally decided I had enough. ‚ÄúNo more,‚Äù I said, time to move on to greener pastures. I needed a break from it.
But, just like any long-lost lover, I can‚Äôt help but take a peek over the fence every once in a while. Just to see what‚Äôs going on. Gotta keep a tab on things.
Today, we are going to celebrate the underdog, the last one in line. Imagine a teaming mass of bytes standing in line inside an SQL Server. Queries running hurriedly here and there, deadlocks roaming the hallways looking for some unsuspecting fool.
At the front of the line, there is a DMV-like clerk. You must do things as the clerk says, everything in its order. And there, aha,  cast your eyes to the bottom and see the poor little blighter over in the corner. Ragged shoes, hair a mess, clothes all rustled and tussled.
There sits poor old
HAVING
clause.
Is that why we always forget to use HAVING? I mean when was the last time
you
used it? Don‚Äôt lie now. When was the last time you even saw it in a query??
There must be some use for it I warrant, or it wouldn‚Äôt be here ‚Ä¶ maybe.
HAVING
I have an even better question. Without cheating ‚Ä¶ give me your best academic answer to ‚Äú
What does a HAVING clause do ‚Ä¶ exactly
?‚Äù
The poor little
HAVING
clause can filter data coming out of a
GROUP BY
. Didn‚Äôt you know? This can‚Äôt be done with a
WHERE
clause.
Think of it as a filter that is used in an aggregate function, like
SUM(), AVG(), COUNT()
, etc.
Your WHERE clause would happen BEFORE data is grouped, the HAVING is done AFTER.
So useful I think. I would wager many a SQL statement has been written with subqueries, CTEs, and other such things simply because the person writing that SQL totally forgot about poor ol‚Äô HAVING.
I would guess this probably happens 8 times out of 10.
HAVING Example.
Let‚Äôs give a simple example to show what I‚Äôm talking about. Let‚Äôs ask a simple question. ‚Äú
Find the departments with sales greater than $10,000.
‚Äù
This is what most folk would do.
Also, we know that most production SQL queries are much more complex so we would see more and more complicated subqueries and CEs etc.
But, you could easily reduce the complexity of that above query by adding the small but mightily HAVING clause.
It‚Äôs always the little things in life, isn‚Äôt it? The
HAVING
clause makes the whole thing easier on the eyes and makes the business logic of the SQL jump out at you.
Anyone who‚Äôs written SQL for years can understand how important these little changes are. They add up over time in a codebase.
They can make any complicated query less complicated.
Why? Typically using a subquery is the approach most people take and when queries get large and complex, reducing the number of subqueries will make it easier to read and reason about.
What else makes the HAVING clause so powerful?
The HAVING clause can include multiple conditions combined with logical operators like AND, OR, and NOT.
I‚Äôd be curious to hear if you are a regular user of the HAVING clause, or never. Do you see it much in the queries you‚Äôve seen throughout your career?
Let us know in the comments below!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
138805222.the-double-edged-ai-coding-sword,The Double Edged AI Coding Sword,CoPilot and ChatGPT will make you stupid,2023-12-04 09:31:26.451-06,"I will preface this article by saying that I use CoPilot and ChatGPT pretty much daily to help me with coding. I make it write my unit tests, that‚Äôs the best way I can think of to punish the robots trying to take my job.
I also think that avoiding using those tools, in the long run, is going to make you less efficient and put you behind the general flow of developers who are broiling and roiling down the road of life.
We can‚Äôt put the little fairies back in Pandora‚Äôs box now that it‚Äôs been opened. Best to use these tools to your advantage and accept them as part of your life and future.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
But, heed my warning all you mindless masses, ignore me at your peril. CoPilot and ChatGPT will make you stupid.
First, we are going to talk about why you SHOULD use AI to do your coding. Then we will talk about why you SHOULD NOT use AI to do your coding. Then you should find the middle ground.
Using CoPilot and ChatGPT to write code.
Even with all my general pessimistic views of too much AI code writing, I cannot ignore certain truths and wonders about this new frontier that spreads itself before us.
You should be using something like CoPilot or ChatGPT in your daily workflow, if you are not, you are slowly dying on the vine. Don‚Äôt be like those old times sitting on their dusty horses throwing rocks at the great iron trains going by. Recognize the future is here and it‚Äôs time to get on board.
AI coding can help you in many ways.
Without a shadow of a doubt I‚Äôve used AI to produce code much quicker than otherwise ‚Ä¶ it‚Äôs essentially increased my productivity by probably 30%.
I‚Äôve used ChatGPT to stub out all my unit tests and capture some test edge cases I wouldn‚Äôt have thought of or was too lazy to write myself.
It‚Äôs also great at giving me ideas on how to approach solving a problem. I can ask the same question from different angles and get different answers and possible solutions. I then use this output to make a decision ‚Ä¶ which might be one or the other of solutions, or maybe both.
Let‚Äôs try to give ChatGPT a try with some Data Engineering stuff and see if what I say is true.
So I ask ‚Äú
I have a large CSV file that won‚Äôt fit into memory, I need to convert it to a parquet file stored in s3. Can you solve this problem with Python?
‚Äù
I mean I guess if you are new to Data Engineering this would give you a good idea of what to do.
Pandas
,
PyArrow
, and
boto3
are all well-used tools.
But, what is the major problem with this?
The problem with this is that we who‚Äôve been around the block know that if we ‚Äú
have a large CSV file that won‚Äôt fit into memory
‚Äù Pandas is probably not the tool to use.
We can simply tell it ‚Äú
I don't want to use Pandas, I would like to use Polars lazy frame, please provide me a solution.
‚Äù Let‚Äôs see what it does.
Well, it tries. It apparently doesn‚Äôt know about Lazy frames and larger-than-memory computing with Polars. I don‚Äôt know, will we ever get to the point where these AI tools can pick up on new features a little better, models trained on Data Engineering specifically? Probably someday.
AI Coding will make you stupid.
I‚Äôve also come to believe after using these tools for a while now, that while there is a great propensity for CoPilot and ChatGPT to make you into a 3x productive engineer, as with any great power comes responsibility.
The AI Coding sword cuts both ways.
Leaning on any tool too much is not going to be beneficial for you in the long run. This applies to pretty much every part of life, not just coding.
You probably won‚Äôt learn what you don‚Äôt write (code).
If you let AI tools provide you with solutions, you will put yourself in a small box, leaving solutions on the table.
Fundamental knowledge of data structures and engineering basics like system design etc are what make a 10x engineer.
You should use AI to make you better, not the other way around.
The human experience isn‚Äôt easily reproduced by AI.
Context is king, and AI isn‚Äôt good at that yet either.
If you lean on AI too much, you will wilt and falter.
All that to say.
What it really boils down to is this.
You should use AI every day while doing your coding job. It‚Äôs the future and it will make you more efficient. It doesn‚Äôt always have to be a bad thing. Writing code faster, testing for you, giving you ideas, writing documentation, it‚Äôs like having a personal programming assistant.
But, that‚Äôs the key for right now. It‚Äôs an assistant. Who knows what the future holds, we can cross that bridge when we come to it, don‚Äôt let the fearmongers scare you. It‚Äôs probably going to be a long while before you‚Äôre replaced by a robot.
Use AI like a tool, and learn to wield it.
Share Data Engineering Central"
138666313.reducing-memory-consumption,Reducing Memory Consumption,A Data Engineers Guide,2023-11-27 08:56:00.902-06,"I was working on a
Polars
data pipeline recently, one in which a ‚Äú
larger than memory
‚Äù dataset was being processed. This data pipeline was extremely fast and enabled the processing of a large dataset on a small instance with not much memory. It got me thinking about
streaming data
and
memory consumption
.
This concept of reducing
memory pressure
is an important one in Data Engineering. To build cost-effective and scalable data processing pipelines, memory consumption plays a big part.
It doesn‚Äôt matter if you‚Äôre using Python or Rust, writing big code or little code, I think at some point we should all stop and think about how we are writing our code that processes data about memory usage.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
In most instances, it seems to be a wise choice to reduce memory consumption as much as possible while maintaining performance. Such code will have a longer shelf life and scale much better.
Diving into memory consumption.
I think the best of all my truly lovely and dear readers, but perhaps some of you may not have
thought about your code in relation to memory usage
. It‚Äôs about having a mind shift, changing how you look at the data that is flowing through your code.
Let‚Äôs draw a picture to make the point clear.
You will often hear this concept referred to as ‚Äú
streaming
‚Äù the data. We reduce the memory footprint by simply not processing all of the data in memory at once.
Instead of loading the entire parquet, CSV, Postgres table, or whatever ‚Ä¶ into memory and working on it, which requires a larger instance or machine size to provide enough RAM to load all the data into memory and work on it, we do it in a ‚Äúlazy‚Äù or ‚Äústreaming‚Äù manner.
Streaming data for a smaller memory footprint has benefits like ‚Ä¶
Reduce the size of machine/instance needed
Reduce costs.
Ability to run multiple processes on the same machine/instance
Reduce costs.
Makes the data pipelines more scalable.
They can scale up to more data without changing scaling machines up.
Helps Engineers advance their skills.
Python streaming examples.
Pretty much every language provides access to these sorts of streaming operations on data. You will find that many of the languages will use common concepts to implement these sorts of features.
Maybe you‚Äôve used them without even knowing.
Let‚Äôs play around with some data in Python and explore the idea of reducing memory size when writing data pipelines. Then later we can switch to Rust.
Starting simple.
Let‚Äôs start with a simple Python example that shows processing data all at once vs doing it in chunks. How about reading data from one CSV file and writing it to another?
You will see this concept of ‚Äú
chunks
‚Äù in many different Python packages. As you can see above this simply allows you to work with a ‚Äú
chunk
‚Äù of the total data you‚Äôre working on.
If you‚Äôre writing a data pipeline that works on CSV files, and it‚Äôs possible those files will grow in size over time ‚Ä¶ it‚Äôs important to work on the data in a streaming matter so as not to have future
Out-Of-Memory
errors because you just assumed the entire dataset would always fit into memory.
With Python, you can find ways to reduce your memory footprint by doing lots of different common tasks. For example, reading a file from an s3 bucket.
Heck, you can even do the same thing when reading data over HTTP.
It‚Äôs really about approaching problems with a different mindset. Instead of ‚Äú
doing everything at once,
‚Äù we simply try to do the work in chunks of memory, rather than all at once.
You can apply this concept to a lot of tasks in Data Engineering since we are mostly moving and transforming data from point A to point B.
Applying the concept across languages.
The good thing about learning and thinking about processing data in this manner is that it isn‚Äôt just confined to a single language, no matter what language you use, you can probably find a way to reduce memory usage.
For example in Rust, we could do a similar thing as above when we were reading files.
Working to reduce memory is more of a way of approaching a programming problem, and not really about the language you use.
What does it look like in the real world?
Another way to look at how important and useful streaming data is by looking around us in the open-source world ‚Ä¶ to see how and if this sort of feature is offered.
In the new hot Dataframe tool
Polars, there is a concept of Lazy Evaluation
.
‚Äú
One additional benefit of the lazy API is that it allows queries to be executed in a streaming manner. Instead of processing the data all-at-once Polars can execute the query in batches allowing you to process datasets that are larger-than-memory
.‚Äù
-
pola.rs
In the Python API for Polars it would look something like this.
Data Engineering tools that can reduce memory pressure by providing ‚Äú
streaming
‚Äù capabilities are truly the next generation of tools and the future of development.
I‚Äôm very curious if you‚Äôve implemented this mindset and streaming-type processing of data in any of your day-to-day work. Comment below and let us know!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
138525378.delta-lake-liquid-clustering-vs-partitioning,Delta Lake Liquid Clustering vs Partitioning,New School vs Old School,2023-11-20 07:57:02.986-06,"Some days I want to throw my hands up in the error and give up. I‚Äôm not a young man anymore. I feel my age. Can an old dog learn new tricks? He probably can ‚Ä¶ but does he want to?
One of the biggest challenges of being a Data Engineer is keeping up with all the new features and shiny rocks that are pushing and shoving for their moment in the spotlight. How do you keep on top of it all?
I will be the first to raise my hand and say ‚Äú
focus on the basics first,
‚Äù but at some point, you become obsolete if you don‚Äôt know which way the market and the tools are moving. At the risk of making my readers mad ‚Ä¶ oh what the heck ‚Ä¶ do you really want to be a Redshift expert for the next decade, or should you move on to Snowflake and Databricks?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
How do I keep up with it all? Well, I read ‚Ä¶ a lot. What other way is there? I watch r/dataengineering, Linkedin, and various and sundry blog posts. I attempt to keep tabs on what new features or tools are released, and at least have a cursory understanding of what‚Äôs available and becoming a thing.
Hench, I‚Äôm writing about
Liquid Clustering
for Delta Tables.
Liquid Clustering in Real Life
Today I want to give a cursory introduction to myself, because I know nothing about it yet, of
Liquid Clustering
. I also want to do some testing, and some benchmarking on
Liquid Clustering
vs classic
Partitioning
of Delta Tables.
Mostly because I have some outstanding questions about how Liquid Clustering works, in the beginning, and over time.
I pray, give me a moment to avail myself of my own misgivings, much like anything else when I run across something new to me. I‚Äôm always going on witch hunts, and I‚Äôm never satisfied until I‚Äôve burned something or other, even if just a token.
So you know about Partitions right? Right? Right?
It could be hard to understand or reason about Liquid Clustering if you are not familiar with ‚Äúclassic‚Äù data partitioning, like Hive-style partitions.
If you have a million parquet files holding a few TBs of data ‚Ä¶ and you need something from that data ‚Ä¶ what do you do? Read all million files to find what you want. Not a good idea. This is where partitions come in. They help you ‚Äú
narrow in
‚Äù on what you‚Äôre looking for.
For classic data partitions, say when you‚Äôre making a new Delta Table you have to sit down and
think
for a while. You have to figure out what your major query patterns are, and how the data is laid out physically. Then base your partitions on this information.
Maybe you working with time-series data so you partition by year, month, and day. Maybe it‚Äôs by the customer. Maybe something else.
It can be half-art and have science, trying to divine all the use cases beforehand. And, you have to understand how the data is physically laid out and its volumes. Otherwise, you get ‚Äú
skew
,‚Äù that is all the data in 1 or 2 partitions, and none in some others. This will kill query performance.
Liquid Clustering ‚Äúsolves‚Äù all these problems.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
So, back to our new Liquid Clustering kid on the block. It appears Databricks is going all on in on this one. They recommend all new tables use Liquid Clustering and are very ‚Äúbroad‚Äù when listing their use cases, which is pretty much every use case.
‚ÄúDelta Lake liquid clustering replaces table partitioning and
ZORDER
to simplify data layout decisions and optimize query performance.‚Äù
What are some things you should know about Liquid Clustering?
You need a fairly new version of DBR runtime to do all read/write stuff with Liquid Clustered Delta Tables.
Only up to 4 columns can be clustered.
Only columns that have statistics collected can be clustered.
Not all DML (merge, etc) clusters data on write ‚Ä¶ so OPTIMIZE frequently.
What questions do I have about Liquid Clustering?
So, it‚Äôs all rainbows and glitter falling from the sky? I doubt it. I find there is a dark side to every new feature, especially the ones that are trying to do us the most good. You have to be careful not to shoot yourself in the foot.
Test everything carefully, trust nothing. That is a good rule when it comes to production.
The problem is that all our use cases are different and you don‚Äôt know how yours will react without testing things out first.
Should we OPTIMIZE more frequently with Clustered tables?
How quickly does data shift to and fro from clustering keys based on usage?
Some guidance on this would be super helpful from Databricks.
If you have a day or two with some specific one-off queries, will data be clustered differently, and then next week‚Äôs first pipelines will grind to a halt because of this?
I‚Äôm mostly concerned about performance. Partitions and data locality have a major impact on Big Data workloads and performance. Call me old school, but that‚Äôs why I like to have control.
Do I want every SELECT statement run on Production data by every Analyst and Data Scientist to affect my workloads that run all week long? Just because they happened to run some query a few times and a OPTIMIZE happened to kick off that ended up clustering data in such a way that will kill the next production workload?
I‚Äôm sure someone will tell me it‚Äôs all too smart for that ‚Ä¶ mmmhmmm. Heard that before. They told me Photon was going to solve all my problems too. Forgot to mention availability and the effects on cost they did.
Sometimes the real world is different from the ideal one.
Let‚Äôs try some Clustered tables out vs Partitioned tables.
To do this test we are going to use the free Community Version of Databricks which provides a free 2-core and 15 GB Spark cluster, along with a Workspace and Notebooks.
This will be perfect for our testing. Of course, we need data and can use the
open-source Landsat (satellite) data hosted on AWS s3
. There is a csv.gz file that lists all the ‚Äúscenes‚Äù taken by the Landsat satellite as it circles this world of ours.
The truth is we probably need a few TBs of data to do a true test, but whatever, we do what we can these days.
Let‚Äôs create two Delta Tables, one with Liquid Clustering, and the other with normal partitions, heck, maybe one with no partitions or clustering at all. Just to see how it goes.
Here are the normal partitions as we are used to, lo, these many years.
We will also OPTIMZE and ZORDER this above table by entityId.
And another for the Liquid Clustering.
and just a plain ol‚Äô table.
And now there is nothing for it except to get our data loaded into these Delta Tables and run our
OPTIMIZE
commands, along with the
ZORDER
for the partitioned table. (
note I only ran OPTIMIZE a single time on each table
)
Of course, I ran this code for the other two tables, the no partitions and the Liquid Clustering Delta tables.
Now I‚Äôm just going to run a query doing some stuff per year and month. I will run the query 5 times on each table.
On the Liquid Clustering table, I will run an OPTIMIZE command after each query.
This is probably the most unscientific testing we could do, but it will still be interesting to see what we see. I‚Äôm hoping by running each query 5 times in a row we can sort of throw any weird caching out the window and get an idea of what is going to happen.
run query
optimize (for liquid table)
repeat 5 times
I would expect that the partitions table would stay the same through the runs, notwithstanding caching of data and the like, if that is happening on the Databricks side.
I would also expect the Liquid Clustering to be slow in the beginning and faster for each query and optimize that is run.
Well, now I am very curious indeed! I wonder if these results will scale with the data!! The Liquid Clustering tables are twice as fast to query as the manual Partitions table!
It‚Äôs hard to say why, other than that Liquid Clustering combined with the Databricks Runtime is just been designed to be magical together. I mean it‚Äôs hard to say at this small scale if this not scientific test really means anything or not.
But, we must give credit where credit is due. Begrudgingly even. I guess I have no excuse now but I must upgrade the DBR and Delta Table versions and try this in a production environment!
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
138312203.spark-vs-polars-real-life-test-case,Spark vs Polars. Real-life Test Case.,A Brave New World.,2023-11-09 07:16:40.74-06,"Sometimes I think people misunderstand things ‚Ä¶ like a lot. The Spark vs
Polars
topic is one of those topics that can be heavily shrouded in hand-waving arguments and other general mud-slinging. I mean, I like to throw mud too, who doesn‚Äôt?
Spark vs Polars. Is it a thing? Which one should a Data Team use in their stack?
I venture to step on the old soapbox and give my two cents. I will yell my beliefs from the rooftops for all to hear. I have a prediction, nay, I have a wish and a hope for the future, a desire for what Data Engineering with Spark and Polars
could
look like.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
‚Äú
I believe that 80% of Spark workloads don‚Äôt require Spark after all. Polars with it‚Äôs Lazy nature, will, and can, easily replace a great many expensive and heavy workloads ‚Ä¶ contributing directly to a reduction in costs.
‚Äù -
me
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Let me spin you a tale and a story.
This sums up most Datat Teams that I have worked on over the years. Absolutely we need Spark. Many datasets are billions+ and PBs+ of data that need to be aggregated, joined and crunched. Spark is the only way.
But, there is another reason we all use Spark ‚Ä¶ everywhere. Because the barrier to entry is low (Databricks, EMR, Glue), and it‚Äôs easy to use (SparkSQL and Dataframe API). Anyone can do it with a day or two of practice. Maybe not well, but they can do it.
It‚Äôs like Python. It‚Äôs too easy not to use. So we ended up with Spark running everything.
It‚Äôs convenient to be on a single tool.
We can use SparkSQL.
It‚Äôs offered as a managed service by everyone.
It‚Äôs good for job prospects (it‚Äôs expected).
But then we look back and find ourselves in a spot we might not like that much.
What spot is that? It's an expensive spot. Like ‚Ä¶ really expensive. We found we are creatures of comfort. We ran everything with Spark, for convenience, kept adding, tacking on the side, every new job, every piece of data, all Spark.
And now we are paying out the nose for it.
Testing if Polars is up to the task.
And this my friend, is where the rubber meets the road. Can we prove this theory of mine right or wrong? I mean I have put Polars in production before ‚Ä¶ but it was replacing a Pandas-type workload.
Can it do more? Is it capable of more?
Polars
has a few things on its side.
SQL Context.
Lazy evaluation
Query optimizations
These are Spark-like features. Do they give Polars the ability to step into the breach? This is what our Brave New World in Data Engineering should look like.
Ok, enough chitter chatter, time to get to work. Today‚Äôs order of business ‚Ä¶
27GBs of flat files stored in s3 ‚Ä¶ Spark vs Polars on a Linode (cloud) instance. (
data from Backblaze open source hard drive data set)
First, we have to find something to do with the
27GBs
of data, write some simple data transformation, and deposit the data into a Lake House.
I‚Äôm going to copy all the data down from s3 over to the local drive on this Linode machine I‚Äôm using. I‚Äôm curious how both Spark and Polars compare without adding complexity like network overhead, although that is an interesting question in itself between the two.
Read
27GB+
of CSV data.
Transform data.
Write data to a Parquet Data Lake.
Once with a Spark on a cloud instance.
Once with Polars on a cloud instance.
Here is what the hard drive data set looks like.
So first we will convert the CSV files to partitioned parquet files, so we can more easily work with the raw data in the future. Next, we will read those partitioned parquet files, do some aggregation to get some metrics and write those metrics to a partitioned parquet dataset. Pretty normal stuff.
What we need to find out is ‚Ä¶ will Polars step up to the plate?
Spark Pipeline.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
First, we need a machine to test this all one.
We will use Linode, an Ubuntu 6CPU and 16GB RAM machine
.
This should be an interesting test.
This isn‚Äôt meant to be some perfect side-by-side test. We just want to prove that, generally, can Polars deal with
27GB
of data with reasonable performance without dying?
We know Spark can do this, that‚Äôs why we use it.
This 27GB data size is good, it represents a big enough dataset that Pandas would most assuredly puke, and at that point, most folk would simply reach for Spark.
Again, we are asking, can we reach for Polars, and what is the performance like?
To set up Spark on my Linode, I followed
my own instructions
, using Spark 3.5.0. It looked something like this.
Let‚Äôs start with our baseline data pipeline in PySpark, a pretty normal script that would be written a million times all over the world.
All code on GitHub.
Nothing too crazy about this code ‚Ä¶
Read raw CSV files.
Write to partitioned parquet (Data Lake).
Read data from parquet (Data Lake).
Do analysis.
Save results back to parquet (Data Lake).
This first run was a failure with Spark.
23/11/04 21:20:14 INFO CodecPool: Got brand-new compressor [.snappy]
23/11/04 21:20:26 ERROR Utils: Aborting task
java.lang.OutOfMemoryError: Java heap space
So I tried with this ‚Ä¶
spark = SparkSession.builder.master(""local[*]"").config(""spark.executor.memory"", ""12g"").getOrCreate()
Still out of memory. Next this ‚Ä¶
spark = SparkSession.builder.master(""local[4]"") \
    .config(""spark.executor.memory"", ""12g"") \
    .config(""spark.driver.memory"", ""2g"")
That was the ticket! Pretty long runtime, about
17
minutes. But, that‚Äôs the power of Spark. We processed
27GBs+
of data on machine
with 16GB of memory
.
More than anything we just wanted a few normal type steps, aka, ingesting raw data in a few Data Lakes and doing some aggregation.
Let‚Äôs open the metrics results parquet dataset to ensure this code actually worked.
Good enough.
Polars code.
Not going to lie, I‚Äôm jittering with anticipation about how this whole thing will or will not work with Polars. Can its lazy nature truly come through for us?
What I want to know is ‚Ä¶ can Polars replace Spark for some workloads ‚Ä¶ Polars can replace Pandas, we know that. But does it have a brighter future as well, can it replace large workloads on single nodes? Can it give us cost savings?
Imagine replacing some Databricks job with a Polars job on an Airflow worker. That‚Äôs real money that is.
We should be able to reproduce this script easily enough in Polars. It will be interesting to see the rough performance of each as well.
Here is my Polars code.
Positives of Polars code.
It‚Äôs simple and easy to read
Mirrors the PySpark syntax we are used to.
Offers Lazy Evaluation options to work with larger-than-memory data.
Downsides of Polars code.
Here are some complaints about this Polars code. Using a LazyFrame to `
sink_parquet
`, unlike the
normal Dataframe parquet write
does not appear to offer
pyarrow
options to write with partitions. What am I missing?
Why would I want to write parquet files without partitions? Bugger me if I know.
Also, I had to pre-make the sink_parquet directory for writing the parquets, if it didn‚Äôt exist the code would error. Not very helpful.
Polars Results.
Well, there is none. Sorry. It ran for hours, I tried again, and let it run all day. It tried. CPU would spike, Disk I/O would spike.
I got my hopes up, then down.
The surprising thing was with the Polars code it could never even finish streaming the CSV‚Äôs to Parquets. That‚Äôs a letdown. I know this because the script would get as far as creating that write directory.
Also, later when I removed that part of the script to see if I could get Polars to at least stream-read the CSV files and do the aggregation, without the intermediate Parquet step ‚Ä¶ the script failed with the syntax errors at a spot right after the intermediate parquet write.
I never thought Polars with its ‚Äúlazy‚Äù way of reading a CSV file and then streaming and sinking it to a parquet ‚Ä¶ without even doing any partitions, would fail.
The Strange Part.
You know the strange thing? I removed the intermediate parquet write and had Polars stream read the CSV file and string sink the groupBy results. And it worked. Fast.
It ran in a little over 30 seconds.
That‚Äôs amazingly fast, 27GB+ of data. Dang. But what the heck?
Time to run polars pipeline : 0:00:32.158017
>>> df = spark.read.parquet(""hard_drive_failure_metrics"")
>>> df.show()
+----------+----+-----+---+--------------------+--------+
|      date|year|month|day|               model|failures|
+----------+----+-----+---+--------------------+--------+
|2023-03-23|2023|    3| 23|       ST14000NM0018|       0|
|2023-06-16|2023|    6| 16|HGST HUH721212ALE600|       0|
|2023-04-23|2023|    4| 23|TOSHIBA MG08ACA16TEY|       0|
|2023-05-12|2023|    5| 12|      ST1000LM024 HN|       0|
|2022-10-19|2022|   10| 19|       ST12000NM0008|       2|
|2021-08-03|2021|    8|  3|HGST HUH721212ALE600|       0|
|2023-01-19|2023|    1| 19|HGST HMS5C4040BLE640|       1|
|2023-02-28|2023|    2| 28|HGST HUS728T8TALE6L4|       0|
|2023-01-26|2023|    1| 26|HGST HUS728T8TALE6L4|       0|
|2023-01-01|2023|    1|  1|HGST HUH721212ALE600|       0|
|2022-11-09|2022|   11|  9|Seagate BarraCuda...|       0|
|2023-01-25|2023|    1| 25|HGST HUH721212ALE600|       0|
|2023-02-27|2023|    2| 27|       ST16000NM005G|       0|
|2022-11-02|2022|   11|  2|TOSHIBA MG08ACA16TEY|       1|
|2023-03-06|2023|    3|  6| WDC WUH721816ALE6L4|       0|
|2022-10-18|2022|   10| 18|       ST10000NM001G|       0|
|2023-04-07|2023|    4|  7|Seagate BarraCuda...|       0|
|2023-05-26|2023|    5| 26|TOSHIBA MG07ACA14TEY|       0|
|2023-04-16|2023|    4| 16|      WDC WD5000LPVX|       0|
|2021-08-20|2021|    8| 20|HGST HMS5C4040BLE640|       0|
+----------+----+-----+---+--------------------+--------+
only showing top 20 rows
Now I‚Äôm wondering what I did wrong. Is Polars still just new and has some bugs and nuances that need to be worked out? There must be something wrong with my two simple functions, one
scan_csv,
and one
sink_parquet.
def read_csvs(path: str):
    lazy_df = pl.scan_csv(path)
    return lazy_df

def write_parquets(lz, path: str) -> None:
    lz.sink_parquet(path,
                    compression='snappy',
                    )
I mean that‚Äôs all the first step was, and that‚Äôs what Polars would choke on. If you think about the difference between these two things ‚Ä¶ the final working Polars code is simply doing aggregation, it‚Äôs still scanning CSVs and sinking parquets. Just not twice like the other one.
It made me curious, what if I changed the code to do a simple SELECT on the subset of columns used for the aggregation, and put that method between the scan_csv and sink_parquet of the original code that wouldn‚Äôt finish?
lz = read_csvs(read_path)
lz = lz.select([""date"", ""model"", ""failure""])
write_parquets(lz, write_path)
I did this above. Simple change. You wouldn‚Äôt believe the results.
Time to run polars pipeline : 0:00:34.843670
I had to double-check the intermediate results, just to make sure my eyes didn‚Äôt deceive me. Heck, how is this possible? Crunching
27GBs+
of data in 34 seconds? Inconceivable.
>>> df = spark.read.parquet(""parquets"")
>>> df.show()
+----------+--------------------+-------+
|      date|               model|failure|
+----------+--------------------+-------+
|2023-01-01|HGST HDS5C4040ALE630|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
|2023-01-01|HGST HMS5C4040BLE640|      0|
+----------+--------------------+-------+
only showing top 20 rows

>>> df.count()
82181678
Above are the intermediate results of that Polars pipeline with the select added. Spark took 17 minutes!!
But, we should revisit that. To be fair we should add the SELECT to our PySpark pipeline, up the cores in use from 4 to 6, and see what happens.
Wow, big difference! 2 more cores (we gave it 4), and putting the SELECT to reduce the intermediate data size.
Time to write raw data to Data Lake: 0:01:23.223409
Now that‚Äôs more like it. Makes me feel better about the Polars results.
What should we take away?
Here is what I take away from this test. Spark is clearly the gorilla in the room. It can process pretty much anything with ease. But, as someone who‚Äôs used it for years ‚Ä¶ many times we are using a sledge hammer to put in a in trim nail.
Combine Spark with Databricks for an entire Data Platform and costs add up.
Polars is the real deal. If I can, with a little troubleshooting, process 27GB+ of data in 34 seconds on a machine with 16GB‚Äôs of memory ‚Ä¶ well, that should make you pause for a moment. 82 million records in 34 seconds.
My goal was not to do some perfect benchmark, the goal was to test if Polars is able to process ‚Äú
real amounts of data‚Äù and ‚Äúactuallly replace some production Spark workloads.
‚Äù That answer is yes.
All code on GitHub.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
138312190.data-engineering-with-match,Data Engineering with ... match,Replacing if:else with match,2023-12-25 09:08:15.654-06,"If you‚Äôre still using
if:else
statements it‚Äôs official. You‚Äôre an old little hobbit, stuck in your hobbit hole of comfort, sitting there eating your danties and smoking a pipe.
Well, I‚Äôm Gandalf and I‚Äôm here to shake things up a bit. You‚Äôve been complacent for far too long. You‚Äôre getting stale. Comfortable. You‚Äôre in danger of becoming predictable.
It‚Äôs time to set out over the hills and through the woods to new lands full of promise and sunshine.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Don‚Äôt get me wrong, I sympathize with you. I‚Äôve been writing
if:else
since some of you were chasing your crush around on the playground. But we must change with the times, and embrace new things.
Today, we embrace the match statement.
We need to have a talk ‚Ä¶ about if:else
This conversation has been a long time coming. You have a problem. I have a problem ‚Ä¶ with
if:else
. We‚Äôve gotten so used to it, now it‚Äôs like a third arm.
Sometimes it seems impossible to break old habits or imagine doing something different. But, at the same time, it‚Äôs fun to ‚Äúlearn‚Äù something new, or solve problems using a different approach.
Even if it only means adding a new tool to the old tool belt, it‚Äôs nice to have another way to solve problems.
I use
if:else
statements all the time, and I have some complaints about my own code.
The logic gets complex very fast.
It‚Äôs hard to read after a point.
It‚Äôs not obvious what‚Äôs going on all the time.
It‚Äôs easy to miss edge cases.
Just out of curiosity, I did a search of my personal GitHub account looking for all the
if:else
logic I could find, sure enough, there was plenty.
For example, in
tinytimmy my open-source data quality
framework for Dataframes, there
are many examples of Python
if:else
statements ‚Ä¶ with lots of else‚Äôs
!
I guess folk are probably used to seeing logic like the long
if:else
above. It‚Äôs just easy to write, something we default to. I don‚Äôt know, I suppose the more complex the business logic becomes ‚Ä¶ the harder the
if:else
becomes to read.
Then, fortunately, or unfortunately, I‚Äôm not sure which, I had the idea about a year ago to start working with Rust. That‚Äôs where I met match for the first time ‚Ä¶ face to face ‚Ä¶ I can thank enums for that.
Introduction to match statement - Python and Rust.
First, if you are unfamiliar with match statements, we should probably start from the basics. What are they
(we will start in Rust, and then later check them out in Python)
?
‚Äú
Match statements in Rust offer a powerful and expressive way to handle pattern matching, similar to using switch statements in other languages. They allow you to compare a value against multiple patterns and execute specific code based on the match.
‚Äù
- Google‚Äôs Bard
In code ‚Ä¶
What can it look like in real life? Here is an example of a match statement in Rust used in conjunction with an enum, from a recent project I wrote,
reepicheep
.
In this example, the code matches one of two options,
Morning
or
Evening
, and does something different in each case.
Readability and other benefits of matching.
I guess it might be a personal opinion, but it seems like match statements are what I would call ‚Äúmore precise‚Äù when written out in code, at least when trying to understand code that is trying to react to multiple conditions.
If you do a little Googling there are a lot of high and mighty claims about how matching is better than if:else, which I don‚Äôt disagree with, but I see them as stretching a little.
I mean, if you are purposeful with your
if:else
I suppose it‚Äôs just as good as anything else. But, since when are we all perfect? I think the readability argument is the best one, even if it is the least technical reason to use matching.
Other claims for matching over
if:else
are often summed up as ‚Ä¶
Errors:
Explicitly matching patterns reduces logic errors.
Decomposition:
Extracting components from matched pattern(s) within the code block.
Exhaustiveness:
All patterns are covered, preventing code gaps.
Python‚Äôs match-case (>= 3.10)
Lest you think we forgot about Python, never fear. It appears in a slightly different form, but starting in Python version 3.10 and greater there is now
match-case
functionality.
Hey, and improvement on
if:else
eh?
Let‚Äôs look at the example I showed above from my own code, convert it to a
match-case
statement, and see what we think.
I mean, doesn‚Äôt seem like much, but I suppose when we are dealing with larger statements that are more complex,
match
might our lives easier, especially when understanding and debugging new codebases.
Always good to have another tool in the belt anyway.
What about you? Have you used match statements, why or why not? Let us know in the comments.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share"
138305998.python-vs-rust-memory-usage-and-speed,Python vs Rust. Memory Usage and Speed.,Data Processing,2023-10-30 07:59:58.46-05,"The plethora of new Rust-based, Python interface data tools flooding the proverbial rolling hills of
Data Land
is too hard to miss. This new set of tech, like
Polars
,
Datafusion
,
ruff
, and all the rest has solidified Rust‚Äôs spot in the Data Engineering space. But, did you ever stop and wonder why?
It really boils down to
memory
and
speed
. After all, what are we doing besides pushing and pulling bytes around?
Aggregating
,
transforming
,
reading
,
writing
. Is Rust really worth it? Is it
that
much faster and memory-efficient than Python?
Why are people building data tools with Rust?
That‚Äôs what we are going to look into today. Writing some code in Python and Rust, find out the real reason folks are building Data Tools for Python with Rust.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
When you're working with large data, it can be difficult to hold it all in memory, and potentially expensive to ingest if you rely on the ability to do so ‚Ä¶ say with Pandas. Such an approach requires larger compute than otherwise, not to mention speed and longer run times.
We want to find the best way to take in data in a single pass with tight resource constraints, as this is likely to be both the fastest and least expensive route to completion.
Don‚Äôt get me wrong, I get it. We all have to use Python because it‚Äôs the language of choice for data processing in the world we all live in. Machine Learning and AI in this age are still all about Python. But, that doesn‚Äôt mean we can‚Äôt start chipping around at the edges.
When bottom-line costs like compute bills come into play, it‚Äôs hard to argue against the bottom-line improvements Rust-based tooling can give us.
Example problem.
To aid in this exploration, we'll be looking at
a synthetic medical claims dataset from Kaggle
, for which the schema can be found
here
. Below in a sample.
Reading through the author's comments, it is the result of merging multiple related datasets from the CMS, and so has a fair amount of opportunity for exploration. Though synthetic, it does offer ""
representative
"" data (
e.g. identifiers appear more than once, allowing for groupings
). A fairly typical dataset.
The test rig used in this article is a Proxmox-based VM with 2GiB RAM, 32GiB of SSD storage, and 4x Celeron J4125 cores, running Debian 11.4.0-amd64.
We won't be particularly rigorous or scientific in our testing and are instead just looking to get some
rough idea of relative performance implications for our assorted efforts
.
We are trying to show in a simple manner, why Rust-based tools like Polars and Datafusion are probably the future of Data Engineering.
For this illustration, the goals of analysis will be to produce the following:
Claim line counts by county
Claim line counts and total Medicare reimbursement by beneficiary
Timing and Speed (tracking Python and Rust performance)
We will use the
\time -v <...>
command to evaluate run duration; due to the low memory allocation for the VM, we will almost certainly be punished for being inefficient with memory, but we won't spend much time troubleshooting this as we ultimately only care about how fast we can make it to the finish line.
The naive approach - what not to do (Python)
To start with, let's first establish the need for efficient processing by showing what not to do. Just to keep things simple, our naive approach will use Python to ‚Ä¶
read the dataset into memory
and loop over the records to generate dictionaries of aggregate results
import csv

def main():
    claims_file = open(""MedicalClaimsSynthetic1M.csv"", ""r"")
    claims_data = list(csv.DictReader(claims_file, delimiter="",""))
    claims_file.close()

    claim_line_counts_by_county = {}
    claim_line_counts_by_beneficiary = {}
    medicare_reimbursement_by_beneficiary = {}

    for rec in claims_data:
        beneficiary_id = rec[""DESYNPUF_ID""]
        county_id = rec[""BENE_COUNTY_CD""]
        medicare_reimbursement = float(rec[""MEDREIMB_IP""])

        claim_line_counts_by_county[county_id] = claim_line_counts_by_county.get(county_id, 0) + 1
        claim_line_counts_by_beneficiary[beneficiary_id] = claim_line_counts_by_beneficiary.get(beneficiary_id, 0) + 1
        medicare_reimbursement_by_beneficiary[beneficiary_id] = medicare_reimbursement_by_beneficiary.get(beneficiary_id, 0.0) + medicare_reimbursement

    print(""count claim_line_counts_by_county: {}"".format(len(claim_line_counts_by_county)))
    print(""count claim_line_counts_by_beneficiary: {}"".format(len(claim_line_counts_by_beneficiary)))
    print(""count medicare_reimbursement_by_beneficiary: {}"".format(len(medicare_reimbursement_by_beneficiary)))

main()
And the test results.
# \time -v python3 naive-approach.py
Command terminated by signal 9
	Command being timed: ""python3 naive-approach.py""
	User time (seconds): 15.25
	System time (seconds): 26.10
	Percent of CPU this job got: 23%
	Elapsed (wall clock) time (h:mm:ss or m:ss):
2:55.24
...
	Maximum resident set size (kbytes): 1896948
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 163144
	Minor (reclaiming a frame) page faults: 734259
	Voluntary context switches: 169238
	Involuntary context switches: 41481
	...
...Well, uh, that isn't very helpful - we didn't even finish execution, and a quick look at
dmesg
tells us that our script ran out of memory. Granted, this was intended to prove a point about materializing large datasets, so let's not pretend that this isn't expected, but take note of how much time was spent getting to this answer:
2:55.24
.
Python is the tool that can do it all ‚Ä¶ to a certain point of course.
Python revisited: streaming read
With that said, we can make a tweak to our program so that we don't materialize the entire list prior to evaluating the results
import csv

def main():
    claims_file = open(""MedicalClaimsSynthetic1M.csv"", ""r"")
    claims_data_reader = csv.DictReader(claims_file, delimiter="","")

    claim_line_counts_by_county = {}
    claim_line_counts_by_beneficiary = {}
    medicare_reimbursement_by_beneficiary = {}

    for rec in claims_data_reader:
        beneficiary_id = rec[""DESYNPUF_ID""]
        county_id = rec[""BENE_COUNTY_CD""]
        medicare_reimbursement = float(rec[""MEDREIMB_IP""])

        claim_line_counts_by_county[county_id] = claim_line_counts_by_county.get(county_id, 0) + 1
        claim_line_counts_by_beneficiary[beneficiary_id] = claim_line_counts_by_beneficiary.get(beneficiary_id, 0) + 1
        medicare_reimbursement_by_beneficiary[beneficiary_id] = medicare_reimbursement_by_beneficiary.get(beneficiary_id, 0.0) + medicare_reimbursement

    claims_file.close()

    print(""count claim_line_counts_by_county: {}"".format(len(claim_line_counts_by_county)))
    print(""count claim_line_counts_by_beneficiary: {}"".format(len(claim_line_counts_by_beneficiary)))
    print(""count medicare_reimbursement_by_beneficiary: {}"".format(len(medicare_reimbursement_by_beneficiary)))

main()
And the test results.
# \time -v python3 python-approach-revisited.py
count claim_line_counts_by_county: 318
count claim_line_counts_by_beneficiary: 289413
count medicare_reimbursement_by_beneficiary: 289413
	Command being timed: ""python3 python-approach-revisited.py""
	User time (seconds): 15.36
	System time (seconds): 0.23
	Percent of CPU this job got: 99%
	Elapsed (wall clock) time (h:mm:ss or m:ss):
0:15.60
...
	Maximum resident set size (kbytes): 61988
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 13125
	...
Clearly, hanging onto each row of input data for only as long as absolutely necessary is a big deal. Let's remember that going into our Rust approach since the same should hold true there as well.
Naive approach with Rust
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
So we shouldn‚Äôt give up on the materialized list idea yet; Rust's memory overhead should be less than that of Python, and I am convinced we can still pull that full dataset into memory and work with it. Let's try it, just to get a feel for what we're getting into.
First, let's scaffold a new app and pull in the
csv
and
serde
crates.
And the code.
use std::collections::HashMap;
use csv::StringRecord;

#[derive(serde::Deserialize)]
struct Claim {
    #[serde(rename = ""DESYNPUF_ID"")]
    pub beneficiary_id: String,
    #[serde(rename = ""BENE_COUNTY_CD"")]
    pub county_id: String,
    #[serde(rename = ""MEDREIMB_IP"")]
    pub medicare_reimbursement: f32,
}

fn main() {
    let mut reader = csv::Reader::from_path(""../MedicalClaimsSynthetic1M.csv"")
        .expect(""should open file"");

    // `.flatten()` will filter against `Result::Ok(val)` and give us just the `val`:
    let claims: Vec<Claim> = reader.into_deserialize().flatten().collect();

    let mut claim_line_counts_by_county: HashMap<String, i32> = HashMap::new();
    let mut claim_line_counts_by_beneficiary: HashMap<String, i32> = HashMap::new();
    let mut medicare_reimbursement_by_beneficiary: HashMap<String, f32> = HashMap::new();

    for item in claims {
        *claim_line_counts_by_county.entry(item.county_id.clone()).or_default() += 1;
        *claim_line_counts_by_beneficiary.entry(item.beneficiary_id.clone()).or_default() += 1;
        *medicare_reimbursement_by_beneficiary.entry(item.beneficiary_id).or_default() += item.medicare_reimbursement;
    }

    println!(""count claim_line_counts_by_county: {}"", claim_line_counts_by_county.len());
    println!(""count claim_line_counts_by_beneficiary: {}"", claim_line_counts_by_beneficiary.len());
    println!(""count medicare_reimbursement_by_beneficiary: {}"", medicare_reimbursement_by_beneficiary.len());
}
This one is a bit more effort to write than our Python equivalent
, but functions similarly. We did need to define the struct up front, which differs from the string dictionary approach in Python; this comes down to the implementation details of the csv crate, which didn't offer an obvious way to do the same.
This time, when we run (
built using
cargo build --release
):
# \time -v ./target/release/rusty-csv-app
count claim_line_counts_by_county: 318
count claim_line_counts_by_beneficiary: 289413
count medicare_reimbursement_by_beneficiary: 289413
	Command being timed: ""./target/release/rusty-csv-app""
	User time (seconds): 2.89
	System time (seconds): 0.39
	Percent of CPU this job got: 99%
	Elapsed (wall clock) time (h:mm:ss or m:ss):
0:03.29
...
	Maximum resident set size (kbytes): 161952
	...
... you can see that it did complete successfully without running out of memory. Technically, because our struct doesn't hold all of the input fields from the .CSV file, we're not doing an apples-to-apples comparison in this effort, so let's be fair and flesh that struct out all the way:
#[derive(serde::Deserialize)]
struct Claim {
    #[serde(rename = ""DESYNPUF_ID"")]
    pub beneficiary_id: String,
    #[serde(rename = ""BENE_COUNTY_CD"")]
    pub county_id: String,
    #[serde(rename = ""MEDREIMB_IP"")]
    pub medicare_reimbursement: f32,

    BENE_BIRTH_DT: String,
    BENE_DEATH_DT: String,
    BENE_SEX_IDENT_CD: String,
    BENE_RACE_CD: String,
    BENE_ESRD_IND: String,
    SP_STATE_CODE: String,
    BENE_HI_CVRAGE_TOT_MONS: String,
    BENE_SMI_CVRAGE_TOT_MONS: String,
    BENE_HMO_CVRAGE_TOT_MONS: String,
    PLAN_CVRG_MOS_NUM: String,
    SP_ALZHDMTA: String,
    SP_CHF: String,
    SP_CHRNKIDN: String,
    SP_CNCR: String,
    SP_COPD: String,
    SP_DEPRESSN: String,
    SP_DIABETES: String,
    SP_ISCHMCHT: String,
    SP_OSTEOPRS: String,
    SP_RA_OA: String,
    SP_STRKETIA: String,
    BENRES_IP: String,
    PPPYMT_IP: String,
    MEDREIMB_OP: String,
    BENRES_OP: String,
    PPPYMT_OP: String,
    MEDREIMB_CAR: String,
    BENRES_CAR: String,
    PPPYMT_CAR: String,
    CLM_ID: String,
    CLM_FROM_DT: String,
    CLM_THRU_DT: String,
    ICD9_DGNS_CD_1: String,
    PRF_PHYSN_NPI_1: String,
    HCPCS_CD_1: String,
    LINE_NCH_PMT_AMT_1: String,
    LINE_BENE_PTB_DDCTBL_AMT_1: String,
    LINE_COINSRNC_AMT_1: String,
    LINE_PRCSG_IND_CD_1: String,
    LINE_ICD9_DGNS_CD_1: String,
}
Yikes! No wonder the Python program crashed on us; look at how many fields we're holding onto. Let's see if we can elicit the same out-of-memory response from our Rust friend:
# \time -v ./target/release/rusty-csv-app
count claim_line_counts_by_county: 318
count claim_line_counts_by_beneficiary: 289413
count medicare_reimbursement_by_beneficiary: 289413
	Command being timed: ""./target/release/rusty-csv-app""
	User time (seconds): 13.75
	System time (seconds): 12.85
	Percent of CPU this job got: 40%
	Elapsed (wall clock) time (h:mm:ss or m:ss):
1:05.27
...
	Maximum resident set size (kbytes): 1884004
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 85838
	Minor (reclaiming a frame) page faults: 1366523
	...
Well, so much for that theory. As I originally predicted, we were able to fit that payload into memory and do everything we needed to with our fully materialized dataset. So, just to recap everything we know so far:
So far, Rust has Python beat on speed by condensing the width of information stored in memory, but loses out on working memory. What happens if we give Rust the streaming treatment?
Streaming reads with Rust
If you look at the examples shown on the csv crate's readme, you'll note that they don't materialize data like we did, but instead stream the read-through iteration over the reader. Let's do likewise, and see what happens -
to start, we'll keep our wide struct definition from the prior run, just to see what kind of difference our read methodology makes to the worst-case scenario in Rust:
fn main() {
    let mut reader = csv::Reader::from_path(""../MedicalClaimsSynthetic1M.csv"")
        .expect(""should open file"");

    let mut claim_line_counts_by_county: HashMap<String, i32> = HashMap::new();
    let mut claim_line_counts_by_beneficiary: HashMap<String, i32> = HashMap::new();
    let mut medicare_reimbursement_by_beneficiary: HashMap<String, f32> = HashMap::new();

    for item in reader.deserialize::<Claim>() {
        if let Ok(claim) = item {
            *claim_line_counts_by_county.entry(claim.county_id.clone()).or_default() += 1;
            *claim_line_counts_by_beneficiary.entry(claim.beneficiary_id.clone()).or_default() += 1;
            *medicare_reimbursement_by_beneficiary.entry(claim.beneficiary_id).or_default() += claim.medicare_reimbursement;
        }
    }

    println!(""count claim_line_counts_by_county: {}"", claim_line_counts_by_county.len());
    println!(""count claim_line_counts_by_beneficiary: {}"", claim_line_counts_by_beneficiary.len());
    println!(""count medicare_reimbursement_by_beneficiary: {}"", medicare_reimbursement_by_beneficiary.len());
}
And the results.
# \time -v ./target/release/rusty-csv-app
count claim_line_counts_by_county: 318
count claim_line_counts_by_beneficiary: 289413
count medicare_reimbursement_by_beneficiary: 289413
Command being timed: ""./target/release/rusty-csv-app""
User time (seconds): 5.59
System time (seconds): 0.39
Percent of CPU this job got: 99%
Elapsed (wall clock) time (h:mm:ss or m:ss)
: 0:05.99
Average shared text size (kbytes): 0
Average unshared data size (kbytes): 0
Average stack size (kbytes): 0
Average total size (kbytes): 0
Maximum resident set size (kbytes): 58372
Average resident set size (kbytes): 0
Major (requiring I/O) page faults: 0
Minor (reclaiming a frame) page faults: 10239
Voluntary context switches: 5
Involuntary context switches: 9
Swaps: 0
File system inputs: 388088
File system outputs: 0
Socket messages sent: 0
Socket messages received: 0
Signals delivered: 0
Page size (bytes): 4096
Exit status: 0
By changing from materialized data to streaming reads, we bring the time down from 1:05.27 to a much nicer 0:05.99.
Okay, you know where this is headed, right? Let's boost our performance by stripping out the fields we're not using in our struct, going back to the original example, and see what the impact is:
# \time -v ./target/release/rusty-csv-app
count claim_line_counts_by_county: 318
count claim_line_counts_by_beneficiary: 289413
count medicare_reimbursement_by_beneficiary: 289413
	Command being timed: ""./target/release/rusty-csv-app""
	User time (seconds): 3.03
	System time (seconds): 0.22
	Percent of CPU this job got: 99%
	Elapsed (wall clock) time (h:mm:ss or m:ss):
0:03.26
...
	Maximum resident set size (kbytes): 58440
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 0
	Minor (reclaiming a frame) page faults: 9725
	...
Let's revisit our comparison of our scenarios with this latest batch of results:
Well, that's... not as exciting as I'd hoped. Yes, we've achieved the best results by combining our methods, but the only real gain in switching to a streaming read in Rust is that we use less memory; the duration between that and the materialized approach is far too close to call.
Wrapping up
I think this basic overview of simple data processing shows why Rust-based tools like Polars and Datafusion are probably the future of Data Engineering and taking more root.
Performance in terms of memory and speed can save real money to the bottom line when processing data over the long term.
On a lark, just to see if there is in fact a way to do this faster, I crafted a similar workload in
DataFusion CLI
to run the same data through its paces, and came up with the following result:
# datafusion-cli
DataFusion CLI v32.0.0
‚ùØ WITH
    TerseClaims AS (
        SELECT
                ""DESYNPUF_ID"" AS beneficiary_id,
                ""BENE_COUNTY_CD"" AS county_id,
                ""MEDREIMB_IP"" AS medicare_reimbursement
            FROM '../MedicalClaimsSynthetic1M.csv'
    ),
    ByBeneficiary AS (
        SELECT
                beneficiary_id,
                SUM(medicare_reimbursement) AS total_medicare_reimbursement
            FROM TerseClaims
            GROUP BY beneficiary_id
    ),
    ByCounty AS (
        SELECT
                county_id,
                COUNT(*) AS claim_line_count
            FROM TerseClaims
            GROUP BY county_id
    )
SELECT COUNT(*) AS ""count ByCounty"" FROM ByCounty
UNION
SELECT COUNT(*) AS ""count ByBeneficiary"" FROM ByBeneficiary;
+----------------+
| count ByCounty |
+----------------+
| 289413         |
| 318            |
+----------------+
2 rows in set.
Query took 1.453 seconds.
Clearly, we're leaving something on the table from a performance standpoint with our Rust implementation.
DataFusion
is definitely doing a lot more than we are to optimize performance, but with less than a two-second lead on us I'd say we're at a point of diminishing returns for our trivial problem.
What do you think? Is there an obvious next step to try on either the Rust or Python side to eke out more gains?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
138198287.to-exist-or-not,To Exist ... Or Not,If or If Not,2023-11-06 09:30:13.154-06,"The true question of life and Data Engineering is it not? Anyone who's worked in a Python world has seen, and been smitten by the seemingly simple question of ‚Ä¶ does something exist?
One would assume that such a simple thing couldn't possibly be that important, yet it's usually the things rattling and banging around the ‚Äú
simple
‚Äù and
fundamental
level that end up being what bites your behind.
Today we explore this idea of something existing, in Python, and maybe we can sprinkle in some Rust to talk about alternatives to this problem.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What do we really mean?
Okay, so what do we really mean by if something exists or not? This can be a complicated question to answer in itself and takes many forms in most code bases. Some of it depends on the context, and some on the code, usually Python in a Data Engineering context.
I mean some of what you see above are probably the most common things to see in a Data Engineering context. Things like ‚Ä¶ does this list even exist ‚Ä¶ is it empty, is there a String, or is the String empty?
In Python, the Data Engineering language of choice, all this indecision, and questions about what ISSS something, or is it even there or not can cause all sorts of trouble.
Let‚Äôs just go find a Python package available on PYPI, some open-source things, and crack it open to make a point. I‚Äôve contributed a few times to the
mack library
, Delta Lake helper package. Let‚Äôs look at the source code of a random function inside the package, like
drop_duplicates_pkey
.
This is how the function starts. That‚Äôs a lot of
ifs
. This brings up an important part of existence in Data Engineering.
Does something exist at all ‚Ä¶ or is it
None
.
Does something exist inside something else?
Does something exist exactly as a ‚Äúthing‚Äù, or is it the wrong thing?
And on and on it goes. Yet again this brings up another thing Data Engineers must think about when designing data systems and pipelines. Can we trust some input or output? Does this method need to be strict? What happens if I assume something exists, and it doesn‚Äôt?
Examples of existence in Python.
Let‚Äôs be practical about it. When we are writing Python data pipelines and transformations we are going to take one of two paths. Either we assume everything or assume nothing, or something in between.
The reality is that code that doesn‚Äôt assume anything is probably going to run better and be more resilient than on that does not.
The simple fact of deciding to deal strictly with existence in Python code will drastically change what that code looks like. Let‚Äôs take an example program that takes 4 inputs.
Do you simply assume that everything is passed and valid? A lot of do. What if we wanted to find out if each parameter is valid before using them for logic?
The `
config
` is probably straightforward.
We can test pretty easily if the config is
None
or
{}
‚Ä¶ empty, doesn‚Äôt matter which, we would raise the same error.
Now
bucket
and
key
are more interesting. In theory, if we were doing some s3 work, or whatever, we might want to or end up combining the two strings. And we could have weird stuff happen if one or both of them were None or even a blank string. In our case, we would want to assume a blank string ‚Äò‚Äò as
not exist(ing)
.
One might be tempted to do something like ‚Ä¶
Have you looked long and hard? What would the result be?
This is not what we want. It‚Äôs only half of what we need. A blank string is not None? Yup. What if we change the method slightly and simply ask
if bucket and if key
?
And the result?
One would think that
if something
and
if something not None
would be the same thing, wouldn‚Äôt you? Especially if you are debugging or writing code under a crunch and quickly moving along. Apparently
‚Äò‚Äò
issss
something. Has existence. Depending on how you ask the question.
What about boolean values?
Again, this is interesting to think about. If we pass
False
for
production
‚Ä¶ it acts as expected, and prints nothing. Also, if production is
None
, it does the same thing.
But is it really the same thing?
Explicitly passing a
False
for production is
NOT
the same thing as forgetting to pass something and passing
None
. Some might argue it is the same, but it really isn‚Äôt. Don‚Äôt we want to know if an argument was forgotten? That could be a different code path.
Thoughts
Again, this is all simple and obvious stuff, but it‚Äôs easy to see how a complicated code base the way we deal with existence in its many and various forms and definitions will have a major impact on how and what our code actually looks like.
And probably how easy it is to debug and how reliable it is.
Existence with Rust.
I‚Äôm a newbie to Rust. I‚Äôve been writing it for
pretty much exactly 1 year
. Here and there. So basically I don‚Äôt know anything. I can write things that run, but it isn‚Äôt pretty.
Since more and more tools are Rust-based, like Polars, are making their way into our Data Engineering lives, and I haven‚Äôt thought or messed with existence in my basic Rust programs, I thought it would be fun to examine it a little, especially after our Python discussion.
So how do we handle something or None in Rust?
Option
.
‚ÄúType Option
represents an optional value
: every Option is either Some and contains a value, or None , and does not. Option types are very common in Rust code, as they have a number of uses: Initial values. Return values for functions that are not defined over their entire input range (partial functions)‚Äù -
Rust docs.
This sort of forces the programmer, in Rust, to be explicit if a thing can be
None
or not. I actually ran into this recently when working on a side project involving working with AWS s3.
It probably has to do with my own poor intelligence about representing ideas in Rust, but anyway, here I have a struct, representing an object in s3.
At least in my design, when I first created this object, I might know the
bucket
and
key
, but maybe I don‚Äôt have credentials yet, and therefore no client is available. Hench
Option<Client>
, could be None.
Honestly, working with values, or the non-existence of something is pretty straightforward.
For example, if I have to
impl
for my S3Object struct, and I am setting that
Option<Client>
‚Ä¶ You can see I‚Äôm wrapping my Client option in
Some
.
‚Äú
Some()
encapsulates a value of type
Client
, indicating that there is a value present. In contrast,
None
indicates the absence of a value.‚Äù
Also,
as you can read here
, there are some nice ways to check for existence when considering an
Option
.
_.is_none()
(Returns
true
if the option is a
None
value.)
_.is_some()
(Returns
true
if the option is a
Some
value.)
Very handy indeed. I have a lot to learn there.
Closing thoughts on Existence.
I know we didn‚Äôt really get to dive in deep, not like I wanted. But, it was fun to think about something we probably all don‚Äôt do that well. Thinking about existence.
I personally need to do better in this area, trusting and thinking about parameters and values, assuming existence and not. It‚Äôs a tricky subject with varied approaches.
Let me know in the comments if truthiness and existence is something you code for, and how you do it!"
137868237.config-driven-pipelines,Config Driven Pipelines,Dante's Seventh Ring of Hell,2023-10-23 09:35:42.707-05,"When discussing Data Engineering there are few things I can think of fraught with more bitterness and vitriol as
config-driven data pipelines
. Or maybe I‚Äôm just jaded. It‚Äôs probably me.
That‚Äôs what the first person you dated in high school said uh? It‚Äôs me, not you. Yeah ‚Ä¶
Lauded by the teaming and raving masses of Data Engineers for flexibility and scalability, these ‚Äú
config-driven
‚Äù pipelines are nonetheless riddled with ‚Äúhard stuff‚Äù that often remains hidden beneath the sparkily and glittering allure of easy configurations.
Nothing is ever as it seems. There is always The Dark Side. Today, I'd like to shed light on some of these challenges from a Data Engineer's perspective.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What the devil is a config-driven data pipeline?
While we could probably argue up and down about this very point, for our purpose, we will define a config-driven data pipeline as ‚Ä¶
‚ÄúData pipelines who‚Äôs logic and arguments are controled by configurations external to the actual code running ‚Äúthe data pipeline.‚Äù
I‚Äôm pretty sure you will know one when you see one. There will probably be some giant JSON or YAML file, full of strange and wonderful things, some reading and unpacking of that config, and then logic tied to those data points. Kinda hard to miss.
The Upsides of config-driven pipelines?
Before we bemoan the evils that have befallen us due to these config-driven pipelines, we should probably be fair and laud their praises like good little peasants.
To be fair, I‚Äôve written, and even very recently written some large and complex config-driven pipelines, all the while, crying over my cursed keyboard at the monster that was taking shape beneath my wicked fingers.
Why did I create these warped and wonderful children?
Config-driven pipelines can introduce great flexibility.
A single config-driven pipeline can take the place of many other pipelines. The one for the many.
Less or no code changes are required to make more updates or changes.
Less technical folks are able to interact with pipelines more. Like Data Scientists and Analysts.
Decoupling of concerns ‚Ä¶ code vs params.
I‚Äôm going to stop there. I could be nicer, but since I don‚Äôt like them I will stop before you grow to connected and see all the bright spots.
Example of config-driven pipeline.
Here is an example of a simple config-driven data pipeline, just to give you an idea if you‚Äôve never run into one before. Now this is going to be simple in the extreme, but keep your mind open.
Let's consider a simple data pipeline that reads data from a source file, performs some transformation on the data, and then writes it to a destination file. The configuration file will determine:
Source file location
Destination file location
The type of transformation to be applied
Here is the JSON config file.
And here is the simple Python script.
Of course, in real life, many config-driven pipelines are large JSON or YAML files with many nested configurations for larger and more complex things. But, you get the drift I think.
There are lots of BOOLEAN switches in config-driven pipelines. The logic that says ‚Äú
If this is something or another ‚Ä¶ do this thing or that.
‚Äù
Where are config-driven pipelines useful?
I find in reality config-driven pipelines are useful in two different major spots.
Filters, inputs, lists.
BOOLEAN logic switches
Okay, let‚Äôs get to the dark side.
The evils that befall config-driven pipelines.
It‚Äôs time to talk about evils and what to watch out for in config-driven data pipelines. These things are not made up, but what I‚Äôve experienced and seen firsthand. They are the reality.
We know that all things in life are colored with both good and bad, and it‚Äôs good to know what we are dealing with on both sides of the coin.
Without good documentation, config-driven pipelines are a mystery to behold.
Decoupling configs from code comes at a price ‚Ä¶ harder to reason about logic.
All possible states of code and acceptable configs are hard to divine.
Debugging can become more complex and difficult.
It‚Äôs probably because I‚Äôve been burned too many times as a programmer trying to fiddle with some obtuse and obscure configs with little to no documentation.
It‚Äôs not just about knowing what the configs do, but what they control, HOW they interact with the code, and affect it in different ways.
Complexity Concealed, Not Eliminated
We‚Äôve talked about complexity before, about how a lot of complexity can simply be shuffling the pea underneath different cups, the question is ‚Ä¶ can you really remove the pea, or are you just moving it from one place to another?
I conjecture that config-driven data pipelines are trying to solve a problem ‚Ä¶ namely, inherent complexity. Most likely. Otherwise, there would be little use for them.
One of the main selling points of config-driven pipelines is that they abstract away a lot of the complexity inherent to data processing. While this is true in some aspects, it's a bit of an untrue statement.
The complexity doesn't disappear; it's merely concealed or moved. As configs grow, maintaining and understanding them can become as daunting as managing the codebase they replaced.
Error Handling and Logging Ambiguities
Config-driven approaches often provide very basic and ‚Äúnormal‚Äù error handling and logging. The nuances of the business logic and the particulars of the integration points between the code and configs will require more customized error handling.
Knowing what configurations were active during the processing and logging is going to be key to unwrapping what is actually happening, since the logic many times is tightly coupled to the configs.
A config-driven approach can lead to ambiguities where the pipeline's response to errors and logs may not always be predictable or appropriate.
Let‚Äôs take for instance this example of a config-driven pipeline that is transforming CSV data. Again this is simple, but you get the point. Imagine that there are many complex and nested configurations.
These configurations cause different code paths and executions. Say a problem happens and is logged. How do you map between what the config was that initiated the pipeline, and the code that failed at some transformation?
How can this be known during stressful production debugging in a simple and succinct manner? Walk through the code again below, does it do this?
Steep Learning Curve
This is a topic that is often overlooked in Engineering circles, scoffed at by tenured and wizened old software Wizards who think people should just figure it out.
The reality is that new people, junior engineers, Data Scientists, and the like are going to have to be able to unwind and understand what is happening.
Although config-driven data pipelines are marketed as user-friendly, they have their own learning curve. Engineers need to understand the underlying framework's nuances, the specifics of the config schema, and the implications of each configuration option and how it interplays with the code, affecting data flows. T
This can be just as time-consuming as learning a new programming language or library. Especially if the learning is happening at the same time as a production failure.
Other Options? Event-Driven Pipelines?
I‚Äôve also been thinking about other options than config-driven pipelines ‚Ä¶ the problem is that they are solutions to complex business-driven needs and it isn‚Äôt always easy to find another solution.
Event-Driven Pipelines
are probably the newest form of technology we deal with that can handle the scale and type of complexity that our static config-driven pipelines deal with today.
Streaming tools have become much more approachable and easy to use these last years, making the rearchitecting of static config-driven pipelines to more event-based a real possibility.
In Conclusion
I don‚Äôt know. Maybe I talked myself back into config-driven data pipelines after all. Maybe not. I‚Äôm still torn.
They are very handy in many instances, I‚Äôve designed some true wonders of the Worlds, with documentation and all. They are flexible, allowing various complex pipelines to run and be triggered by many non-technical folks."
137815762.my-journey-as-a-data-engineer-visually,My Journey as a Data Engineer - Visually,The Long Slog,2023-10-23 09:19:18.15-05,"Recently I was having a discussion with another Data Engineering type about where we are today, as people in Data, and where we came from in the past.
It made me stop and think. It brought back memories and things from the not-so-good old days. Ah ‚Ä¶ what a journey. So, I took it upon myself to make a little chart. A life chart. A Data Engineering life chart.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
It does not cover everything in my Data Engineering life, what it does is tell a story. Don‚Äôt get caught up in the details, get caught in the big picture, and try to read between the lines.
What you should take away from this visual.
So I‚Äôm not really going to beat around the bush, I‚Äôm going to tell you how it is. I‚Äôm going to give you the bullet points that color my Data Engineering Worldview. Take em‚Äô with a grain of salt, or seriously, whatever you prefer.
From analyst or newbie to Senior Engineer is a journey.
Starting easy and progressing upward is the path of least resistance.
Technologies build on top of each other.
Some things will serve you well over a long period of time like Python and Data Modeling.
If you make a chart for yourself and you aren‚Äôt growing or learning, okay, but know the consequences.
It‚Äôs hard to learn things on the job only. You have to learn and grow with some of your own time.
There is a lot more I could say ‚Ä¶ and probably should say, but I would rather leave the rest to your imagination.
Every journey is different, but I highly recommend if you want to grow in your Data Engineering career ‚Ä¶ that you make some sacrifices and work harder than those people around you.
If you insist on having more detail.
My Journey from Data Analyst to Senior Data Engineer"
137706216.simplify-delta-operations-with-merge,Simplify Delta Operations with MERGE and COPY INTO,for Delta Lake users.,2023-11-13 09:20:38.659-06,"Simplicity is the name of the game. I know, some engineers just can‚Äôt get over their love affair with complexity, it‚Äôs like a drug, easy to take and hard to get off of. But, we all know better in the end, our sins will find us out.
Death to complexity. Long live simplicity.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Today we are going to talk about the simplification of Lake House workflows with Delta Lake + Spark features
COPY INTO
And
MERGE
. And how they fit into the overall data pipeline picture.
You can build an entire stack of data pipelines on the back of these two Atlas ‚Ä¶ holding the proverbial Lake House world on their shoulders. How do I know this? Because I‚Äôve done it ‚Ä¶ like in real life.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Introduction to Delta Lake
Before we dive into the specifics, let‚Äôs briefly recap what
Delta Lake
is. Delta Lake is an open-source storage layer that brings
ACID transactions
to Apache Spark‚Ñ¢ and other big data workloads. It‚Äôs the new and upgraded Data Lake.
Delta Lake gives us the power of DDL, DML, ACID, CRUD, and all the rest ‚Ä¶ at a massive scale. It‚Äôs not compute, it‚Äôs storage and data representation.
Thinking about Data Pipelines with Delta Lake.
It‚Äôs common when Data Engineering Lake Houses, which hold, many hundreds of TBs or more of data, go through different data pipelines and stages through which our bits and bytes pass ‚Ä¶ to store that data in some representation that is meaningful to the business or to the Engineers.
This creates a very close relationship between our ‚Ä¶
ETL and Data Pipelines
Storage Layer
Compute
These three now can be closely knit together if so desired. Some might complain about
the separation of concerns
, and that is true, but simplicity at scale is also just as important.
This is where the two topics for today fall,
MERGE
and
COPY INTO
, they enable simple data pipelines in single functions, that can in themselves support complex use cases. We have to pick one, so let‚Äôs start with
COPY INTO
.
The Power of COPY INTO
Above is a visual of the simple and common use case, especially for Lake Houses. Say we have an s3 bucket that gets filled with CSV files daily or hourly.
We need to get that data from its raw format in our Lake House for downstream processing, all while doing some simple processing of the data. Our source is s3, our target is a ‚Äústaging‚Äù Delta Table.
Well, in the old days that would probably require a bunch of steps like read, transform, and write. Delta Lake‚Äôs COPY INTO does this with a single statement. Including idempotency!!!
Below is an example of doing just that.
This is one of the best-kept secrets of Data Pipelines with Delta Lake. These operations are idempotent, meaning they will only copy a new file once. Of course, what you see above is only a small part of the configurations that can take place.
You can read
more on the documentation here
. Above you can see all the following steps happening in a single command.
Copying any new files from source to target.
Doing transforms like casting to timestamps and creating hashes, etc.
Loading data from a remote cloud bucket with pattern matching.
Imagine the possibilities!! The
COPY INTO
command is perfect for staging type loads of raw data going into a Lake House. It can be idempotent, it can apply transformations, it can pattern match, and it can be an all-in-one load.
Simple!
The Power of MERGE
MERGE
, often called upsert (update + insert), allows you to merge a source table with a target Delta table. With MERGE, you can:
Insert new records
Update existing records
Delete records that match given conditions
This is a great feature as you can even tackle SCD / history recording tables with the
INSERT
,
UPDATE
, and
DELETE
capabilities of a
MERGE
statement.
Let's take a look at an example:
Scenario:
We have a products Delta table that we want to update based on the new inventory data coming in.
In the above SQL code:
If the product exists, it updates the quantity.
If the product doesn‚Äôt exist, it inserts the new product into the
products
table.
There are very few use cases when moving data between Lake House logical storage layers that can‚Äôt fit into the concept of the MERGE statement.
Now ‚Ä¶ think about combining both
COPY INTO
and
MERGE
into a single pipeline.
Benefits of Using MERGE and COPY INTO in Delta Lake
Efficiency
: Both commands are optimized for performance. Rather than writing custom logic, these commands handle most use cases with a simple SQL statement.
Simplified Data Operations
: No need for complex logic. Just express what you want to do in a simple SQL statement.
Wrapping Up
Delta Lake has transformed the world of data engineering by bringing ACID transactions, scalability, and simplicity to big data workloads. Commands like
MERGE
and
COPY INTO
further simplify data operations, ensuring that data engineers can focus on providing value rather than wrangling with complex operations.
Give them a try in your next project and experience the difference!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
137621717.thoughts-on-data-in-the-cloud-s3,Thoughts on data in the Cloud (S3).,Cloud Storage for Data Engineers.,2023-10-16 09:02:28.832-05,"It‚Äôs one of those subjects that‚Äôs sort of taken for granted, working with cloud storage like s3, that is. Yet, it‚Äôs probably one of the most common and, many times, one of the first tasks newly minted Data Engineers, fresh off the assembly line, work on during their first year of agony and confusion.
Files in s3. It seems like such a simple topic,
yet it‚Äôs such a fundamental piece of pretty much all Data Platforms
. You would think we would be experts by now.
That‚Äôs the plan for today. To explore the wind-swept and ravaged shores of s3 buckets with seas of data. With code. With the CLI. Forward and onward!
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
How Data Engineers do s3.
We are going to explore cloud storage from the viewpoint of s3, the most ubiquitous storage of them all. Amazon Simple Storage Service (S3) is a scalable object storage service provided by AWS (Amazon Web Services).
The simple idea of storage, with something like s3, has changed a lot since the days of yore, turning what once used to be a boring part of a Data Engineers job ‚Ä¶ into one of the most important. Things are simple anymore, infact, they can be very complex.
In a strange turn of events, it almost becomes impossible to cover all aspects of the topic of ‚Äúcloud storage‚Äù in even a few articles. We almost have to pick and choose.
Think about that little graph above listing just a few things Data Engineers do when it comes to cloud storage. Yikes.
To keep things down to earth and our feet in the mud, I think we will pursue at a high level the main points of cloud storage, how we should think about the topic, and from there move into actual examples using CLI‚Äôs and code to interact with s3.
This approach may jog the memory of some, introduce a new idea to others, spring up new ideas, and hopefully be all-around helpful.
Let us begin this clouded journey and see where it goes.
In the context of Data Engineering s3 plays a critical role in several areas:
Data Storage:
Raw Data Lakes:
S3 is frequently used to store vast amounts of raw data, which can be processed and transformed as needed. This allows businesses to decouple storage from compute.
Processed Data Storage:
Once data is processed, transformed, or aggregated, the results can be stored back into S3 for further analysis or consumption.
In summary, s3 is the garbage heap of our digital selves. We dump, dump again, and dump more. It‚Äôs a habit, it‚Äôs easy to do, so we do it.
This dumping and ‚Äúnot taking seriously‚Äù s3 or cloud storage is the first pitfall many Data Engineers make. And let me tell you, it‚Äôs a hard one to back out of once things are a mess.
You should apply rigor and thought to cloud storage in your Data Stack. Apply Engineering best practices to everything you do. For example.
Be consistent with the naming conventions of s3 buckets.
Understand the configurations of s3 buckets.
Be consistent with directory structures.
Understand what data partitioning is, and how to apply it in a s3 bucket.
Understand how data compression affects the costs of s3 storage. Be wise, compress.
When thinking about cloud storage systems like s3 we also can‚Äôt forget they provide benefits that we rarely think about. Mostly because they work so well we take it for granted.
Scalability & Durability:
Highly Scalable:
You can store any amount of data in S3, from a few bytes to petabytes, and it can handle large concurrent workloads.
Data Durability:
Amazon S3 is designed for 99.999999999% (11 9's) of durability. It replicates data across multiple systems in a given AWS region.
And that‚Äôs not all. One thing we think about a lot in Data Engineering is interfaces and how systems work together. A lot of Data Engineering pipelines and environments are made up of multiple tools working and talking together.
The nice thing about cloud storage, like s3, is that many tools offer out-of-the-box support. It just makes life easier.
Integration with Data Processing Systems:
Big Data Frameworks:
S3 easily integrates with big data frameworks like Apache Spark, Apache Hadoop, and Presto.
AWS Native Services:
AWS services like Amazon Athena, Amazon Redshift Spectrum, and AWS Glue can directly interact with data stored in S3.
Event-driven Processing:
S3 Event Notifications:
You can configure S3 buckets to send notifications (like Lambda function triggers) when specific events (such as PUT, POST, or DELETE) occur, facilitating real-time data processing workflows.
Share Data Engineering Central
s3 - code and CLI.
In the end, the life of an
average Data Engineer
ends up being code. Code, code, and more code. Working with cloud storage like s3 is no different.
There are endless ways to interact with data in the cloud, let‚Äôs examine some of the ways we can directly interact with files in the cloud. I‚Äôm going to approach it from the perspective of s3 and narrow it down for us.
boto3 with Python
(code package/API).
CLI (
aws provided command line interface tool
).
Using code or bash to call the CLI.
Via a tool like Spark (we will leave this one for later).
My personal interactions with cloud storage like s3 usually come from within a tool like PySpark or PyArrow. But, of course, there are many times when we are munging around raw data two and from s3 buckets.
Looking for files, getting files, filtering files, putting files. Always something. Honestly what I want to do is just introduce a variety of common tasks.
How to do common s3 tasks as a Data Engineer.
This isn‚Äôt supposed to be an exhaustive list, just things that I‚Äôve done many times over during my career, and I‚Äôm assuming you also, if not yet, then soon to come probably.
I will give you the ‚Äúwhat,‚Äù like maybe what this would accomplish and why, and then the ‚Äúhow.‚Äù
Without further ado and in no particular order.
sync two locations, one of them being s3,
delete something that doesn‚Äôt exist in the source but is in the target.
To sync two S3 buckets while excluding all
.parquet
files
Sync two S3 buckets, excluding all .parquet files and only including files that have ""2023"" in their path or name
To copy all
.gz
files from a local folder (including its subdirectories) to an S3 bucket
Of course, you could also list buckets with the CLI using the same combination of ‚Äîexclude ‚Äîinclude ‚Äîrecursive or whatever. It‚Äôs the little things that make a difference, and learning the nuances of searching a s3 bucket comes in handy.
Also, it‚Äôs important to note that more complex workflows with the AWS CLI tool can be written with bash. Being able to have bash scripts that do certain common tasks, take input, and that can make certain small logic switches ‚Ä¶ well ‚Ä¶ that‚Äôs very powerful.
For example ‚Ä¶
For every bucket in the AWS account, if the bucket name contains the word ""backup"", sync all
.log
files from a local directory (
/local/logs/
) to a folder named
logs
inside that bucket.
The sky is the limit, along with your imagination.
Code + s3 (boto3).
This is one tool I‚Äôve been using most consistently over the years as a Data Engineer. At some point, we are typically working on a pipeline or project that requires more ‚Äúfine‚Äù work with cloud files.
If you‚Äôre using Python in an AWS environment then boto3 is the most logical choice to do that work. It‚Äôs truly amazing the things you can do with boto3. It‚Äôs impossible to cover all the features of boto3+s3, but we can give a quick overview.
Before you can start using boto3 with S3, you need to set up authentication credentials. These can be set in a configuration file or in the environment variables.
Create an S3 client or resource instance.
Create or list buckets.
Upload, download, delete, or list files.
Directly read or write from S3 objects using Python's file-like interface.
I mean think about it ‚Ä¶ boto3 combined with Python logic can do many strange and wonderful things. Like getting the latest file from an s3 bucket after paging through results.
Again, the sky is the limit with boto3, it‚Äôs a powerful tool. Being able to combine Python and s3 into a single script, you can pretty much do anything your mind can dream up.
I feel like even after blowing all this hot air we have barely scratched the surface of cloud storage for Data Engineers. That means we should probably do a mini-series on this and dive deeper into some of these more complex topics and code use cases.
What it really boils down to.
Take your cloud storage seriously. Use CLIs, they are powerful when combined with bash. Never forget the power of Python+s3=boto3.
Don‚Äôt treat your storage buckets like the junk drawer."
137501753.the-truth-about-databricks-vs-snowflake,The Truth about Databricks vs Snowflake,For Data Engineers,2023-10-10 07:50:44.885-05,"It‚Äôs the same old story since the beginning of time. Them vs. Us. The endless battles that rage on through the ages. I remember the days of Postgres vs. MySQL, SSIS vs. Data Services, and Kimball vs. Immom, it just never ends ‚Ä¶ especially in tech.
I feel like the battle of this decade is Snowflake vs. Databricks. The endless benchmarks and nitpicking about query speeds. When honestly, most of the arguments miss the point of Data Engineering entirely.
We are going to solve this once and for all, set all things aright. No more wondering about which tool to use. Things are not all doom and gloom, always pitting this against that, leave that for the weak-minded Reddit genius always perusing the putrid offscourings looking for their next victim.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
The tech battle of the decade.
Well, you might accuse me, rightly so, that maybe I‚Äôm late to the argument. Better late than never. Both Snowflake and Databricks are touted for their extensive capabilities,
yet they serve slightly different purposes in the data pipeline and platforms, even if some overlap exists
.
As a data engineer, diving deep into the comparative analysis of Databricks and Snowflake can equip you with the knowledge to make informed decisions that align with current or future project requirements.
I believe the truth about Databricks vs. Snowflake is shown in the figure above. It‚Äôs really what it all boils down to.
Databricks is for those programming and Machine Learning centric workloads and teams, those in the Data Lake or Lake House world. Snowflake is for the classically SQL comfort of a Data Warehouse team.
Yes, of course, they overlap, as with any set of tools, if you really want, you can pit them against each other and accomplish some of the same things by going different routes.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Can you do Machine Learning with Snowflake? Yes, if you want to. If your focus is
ML should you use Databricks
? Of course.
Can you build a
DBT-based
Data Warehouse in Databricks? Sure, if you want to. Should you use Snowflake to do this? Yes.
Databricks is both ETL and Data Storage, half-and-half solution. Snowflake is more Data Storage, and can be ETL if all SQL driven.
Think about it this way.
If you walk into a bar (
GitHub repo
) filled with Databricks users, what would you expect to find?
Lots of functional PySpark code.
Machine Learning Pipelines.
Some Delta Lake stuff.
Very programming-centric tasks.
If you walk into a bar (
GitHub repo
) filled with Snowflake users, what would you expect to find?
Lots of SQL.
Probably a tool like dbt.
Lots of Data Warehousing concepts.
So, should it always be Databricks vs. Snowflake? Probably not. What type of data team you are, or want to be, and the problems you are solving should dictate which tool you choose!
That being said, in case you live under a rock, or inside a SQL Server in some musty cubicle farm, let‚Äôs cover the Databricks and Snowflake basics. The ‚Äú
what they are
.‚Äù
Databricks: The Unified Data Analytics Platform
Databricks, a unified data analytics platform, is designed to be collaborative and integrated, offering a multitude of services from data preparation to analytics and machine learning. Its collaborative notebooks feature facilitates seamless interaction among data scientists, engineers, and business analysts, making it a suitable choice for organizations looking to foster a collaborative culture closely tied to complex Machine Learning and Data Pipelines.
Key Features:
Unified Analytics:
Databricks brings together data engineering, data science, and analytics on a single platform.
Delta Lake:
The Delta Lake feature provides reliable data lakes and fast analytic solutions.
Machine Learning:
Offers an interactive workspace that enables users to build and train machine learning models effortlessly.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
A very simple idea of how Databricks workflows take place.
Set Up Databricks Workspace:
Sign in to your Databricks workspace.
Create a new cluster: Choose a Databricks Runtime version that supports Delta Lake.
Create a Delta Table:
Create a new notebook in your Databricks workspace.
In the notebook, execute the following commands to create a Delta table:
Perform Operations on Delta Table:
You can now perform update, delete, and merge operations on your Delta table which are normally not possible in a regular Parquet table. Here's an example of an update operation:
Query Delta Table:
You can use Spark SQL to query the Delta table as you would with any other table:
Visualize Data:
You can create visualizations of your data directly in Databricks. Just click on the bar chart icon in the toolbar above the cell with your command. This will automatically create a bar chart, but you can adjust the settings to create other types of visualizations.
Schedule Notebooks:
You can schedule your notebooks to run on a regular basis, right within the Databricks UI. This can be useful for updating your Delta tables with new data on a regular basis.
Snowflake: The Data Warehouse Built for the Cloud
On the other hand, Snowflake, a fully managed cloud data warehouse, provides a robust solution for data warehousing, data lakes, data engineering, data science, modern data sharing, and more. It's built to handle a multitude of data workloads via its unique architecture known as a multi-cluster, shared data architecture.
Key Features:
Performance
:
Snowflake's architecture allows for instant, infinite scalability, ensuring exceptional performance even with concurrent workloads.
Data Sharing
:
Enables seamless and secure data sharing with a simplified approach.
Zero Maintenance
:
Being fully managed, it requires no management or tuning, making it a hassle-free solution for data engineers.
What would an extremely basic dbt + Snowflake project look like?
Create a Directory Structure:
First, you'll need to have a directory for your dbt project. In your directory, create subdirectories for your models, analysis, and tests. Here's an example directory structure:
Create a dbt Profile:
Create a
profiles.yml
file in the
~/.dbt/
directory with your Snowflake connection details:
Create a dbt Project File:
Create a
dbt_project.yml
file in the root of your dbt project directory:
Create a Model:
Inside your
models
directory, create a model file, for example,
my_first_model.sql
:
Thinking about it all.
I‚Äôm sure you can spot the subtle differences. Of course, my example for each case is simple and the reality can vary widely depending on the Data Team, but at the core/basic level, it does give a ‚Äúfeel‚Äù for what it‚Äôs like to work with Databricks or Snowflake.
In the end, they have some overlap because doing a thing with data is inherently open to interpretation, but I do believe that the business use case and the team dynamics are going to drive you to one of the two tools in an obvious manner.
The truth about Databricks vs. Snowflake is that they don‚Äôt really have to be pitted against each other. There are enough companies focused on being programming-centric with ML use cases. Just as there are many companies that are more SQL-centric with Data Warehousing at their core.
Not everything has to be a fight.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
137458794.introduction-to-uuids-for-data-engineers,Introduction to UUIDs for Data Engineers,Those nasty (not so) little strings.,2023-10-09 07:15:56.791-05,"I have seen it both ways in my day. I‚Äôve seen Postgres tables polluted and primarily made up of 20 different UUIDs. Blah. I‚Äôve also seen supposed table counters in SQLServer counting up .. giving uniqueness ‚Ä¶ who‚Äôs context is quickly lost.
What to do? What are they good for? WHY ARE THEY EVERYWHERE???!!
Love em‚Äô and hate em‚Äô. The good ole‚Äô UUID. They ain‚Äôt going anywhere, anytime soon. Let‚Äôs explore their uses in the Data Engineering world.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What‚Äôs a UUID .. what do they taste like ‚Ä¶ precious?
If we are going to be sneaky little hobbitses, then we need to try everything, including UUIDs. Give them the old taste. See what‚Äôs what. I can‚Äôt imagine there are many of you left out there who haven‚Äôt run across them at some point. Some of us are probably tired of them. But, hey, that‚Äôs life.
Explain UUIDs to me like I‚Äôm 5.
Alright, let's imagine you are having a huge party with lots of friends from different places, and everyone is bringing their favorite toys. Now, you want to make sure no two toys get mixed up or taken by the wrong person, but some toys look very alike!
So, you have a cool idea. You decide to give each toy a special sticker with a unique name on it, a name so unique that no other toy in the world has the same name. This way, no matter how many toys are at the party, every toy has its own special sticker and can always be found by its owner.
In the world of computers, when programmers are dealing with lots of different things (like data, files, or users) and they want to keep everything organized and separate, they use something called
UUIDs
, which stands for
Universal Unique Identifier
.
It's like a special name sticker for each item, so even if there are millions or billions of items, each one gets its own unique name, and there‚Äôs no confusion.
So, that is a UUID.
UUIDs in a Data Engineering context.
In a Data Engineering context, UUIDs can be very helpful, and indeed in the age of Data Lakes are a must-have. With the rise of Big Data and Data Lake systems like
Delta Lake
,
Apache Iceberg
, etc., UUIDs have become very important.
In a move to confuse and gloss over the issue of uniqueness, both Delta Lake and Iceberg on the surface appear to offer
primary_keys
, but if you read the fine print ‚Ä¶ they are not enforced!
You tell me the point of a
primary_key
that is not enforced. That‚Äôs right, there is none.
UUIDs in Data Engineering solve the unique identification problem of tracking rows of data though different systems.
One of the biggest problems when moving from RDBMS (
Relational Database Management Systems
) like SQL Server or Postgres, to the Big Data space of Data Lakes with tools like Delta Lake, is the idea of keys.
Many of us are used to the idea of primary and foreign keys in the RDBMS space, being able to set uniqueness on data tables, via primary keys, and being able to trust that uniqueness implicitly in most cases.
This is a hard problem to solve in Big Data. If we have billions of records split across many partitions, being able to uniquely identify a row, for some business purpose, is no small task.
Luckily, this is a common enough problem that most Big Data tooling from Databricks to standalone Apache Spark provides the ability to generate UUIDS on demand.
I‚Äôm a fan of UUIDs in Data Lakes, and you should consider them because ‚Ä¶
You can uniquely identify rows on ingestion/creation in a data store.
UUIDs can follow records for their entire lifetime/lifecycle.
UUIDs can make debugging and troubleshooting data problems much easier (data lineage becomes easier).
At the early stage of Data Modeling it can seem like an extra expense, a large string, calculating it, tracking it, is it all really worth it? It can seem questionable in the beginning but usually pays off in the long run.
Your poor cousin‚Äôs UUID.
Closely related to the topic of
UUIDs
, aka officially and guaranteed uniqueness across space and time ‚Ä¶ is simply the related idea of hashes.
The honest truth is when ingesting millions and billions of records, and trying to decide ‚Äú
what makes something unique
,‚Äù this can‚Äôt be solved without business content. You can‚Äôt blindly apply a UUID to every row and walk away.
Life doesn‚Äôt work like that.
Many times I have some business context driving what I think should make a unique row, and I still want a UUID-like string to tell me that, for a particular row.
Luckily, tools like
Apache Spark have some nice md5 hash functions
, and other such things, that can do what we want.
For example, I have plenty of production code where something like the following suffices for my need for a ‚ÄúUUID.‚Äù The overall problem space is small, and the chance of duplicates is higher than normal.
So passing some business-context agreed-upon columns and hashing them. Well ‚Ä¶ it works.
What I don‚Äôt like.
What I really hate seeing, even in an RDBMS like Postgres, is some
INT
or
BIGINT
that is supposed to be autoincrementing, which at once point served a different purpose, but invariably starts to be used as some sort of key or uniqueness, which is is not!
Even in
Spark, it‚Äôs common to see something like monotonically increasing IDs
abused in this same manner!
Someone says ‚Ä¶ ‚ÄúOh the records in this Dataframe or Table or for sure unique .. and always will be, let me just apply a row_number or something silly
INT
ID and call it good.‚Äù
Hogwash
.
It‚Äôs always a fine line we walk. But, Data Engineers, more than most folk, know how to keep the business context front and center, thinking about the big picture.
What do I need a unique ID for?
How should it be calculated?
What makes this record unique?
Is this ID unique?
Again, and to finish up, what are the academic reasons and ways UUIDs in all their forms, are helpful?
Uniquely Identifying Records
-
In big data systems, each piece of data, like a record in a database, needs a unique identifier so it can be found and accessed easily. UUIDs are perfect for this job since they are unique across time and space.
Data Lineage and Provenance
-
When dealing with complex data workflows, it's crucial to trace where data came from and how it's been processed. UUIDs can be used to tag data and processes uniquely, helping to track data lineage and provenance.
Merging and Synchronizing Data
-
Sometimes, data from different sources or systems needs to be merged or synchronized. UUIDs can help ensure that each piece of data retains its unique identity, making it easier to manage and prevent duplication.
Distributed Systems
- In distributed systems, data can be spread across many different machines. UUIDs can be generated independently on each machine without coordination, making them a convenient way to identify data uniquely across the system.
Error Tracking and Logging
- When things go wrong, logs and error messages are crucial for figuring out what happened. Using UUIDs to tag operations or data flows can make it easier to trace errors through a system.
In all these scenarios, UUIDs provide a straightforward, standardized way to uniquely identify data and processes, making it easier to manage, track, and work with data in complex data engineering environments.
Your Thoughts?
I‚Äôm very interested in others solving the problem of uniqueness and their use of UUIDs and hashes in Big Data storage systems. Please share your thoughts below!!"
137326647.thoughts-on-abstraction-and-complexity,Thoughts on Abstraction and Complexity,Interfaces and Hiding the Pea,2023-09-25 09:00:20.051-05,"Recently the indelible
Mathews Powers
made a comment, on my comment, that got me thinking.
I am thinking about interfaces, complexity, and the idea of hiding the pea ü´õ under some rug. The age-old complexity problem. Can complexity be done away with, or just hidden, or a little bit of both?
The longer I do Data Engineering, the more I realize how foolish I was in those beginning years. I listened to the modern-day
Johann Tetzel
s peddling their gods of complexity, worshipping them and offering up the coins of my hard-earned mental capacity on the alter of ‚Äú
It is Complex Therefore I Am Smart
.‚Äù
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Can complexity be removed, or just hidden?
All this thought and wondering started with a simple post on my
LinkedIn page
.
‚Äú
The longer I do
#dataengineering
and write
#code
, the more I realize the hard part is making things simple and reducing complexity, figuring out how to write less code, or at least code that is readable, logical, and straightforward.
‚Äù - Me
I‚Äôve reached a point in my programming life where I‚Äôm stuck between two very hard rocks.
I have to accomplish complex tasks.
I know the dangers and pitfalls of complexity.
There is complexity at multiple levels. Complexity at the architecture level and complexity that resides inside the code at the data pipeline level.
I want everything to be as simple as possible. Everyone except the crazies knows that the road to perdition is riddled with complexity. Debugging and troubleshooting become harder, feature additions and changes become harder, onboarding engineers become harder, and tech debt deepens.
But I‚Äôm also a realist. Complexity in and of itself isn‚Äôt the devil. Some things are inherently complex.
There are two types of ways I personally try to deal with complexity in Data Engineering systems.
Write clean, mostly functional, readable, and straightforward code at all times.
When something starts to spiral or become too complex, abstract away the ‚Äúthing‚Äù into an interface of some sort.
What do I mean in real terms by that?
My methods and functions are as small as possible and do a single thing. No, or few side effects.
I break complex tasks and code down into smaller modules that are easier to reason about.
And that begs the question ‚Ä¶ Am I really reducing complexity or just shuffling the pea around underneath different rugs?
In Python for example ‚Ä¶ if you just pip install 50 different packages that allow your ‚Äú
code to shrink
,‚Äù have you reduced complexity or just moved it?
Obvious examples of complexity ‚Ä¶ and reduction.
Complexity reduction and hiding the pea is an impossibly big topic. It‚Äôs also fraught with engineers who still worship complexity, so it must be done, so get on with it. Engineers that are full of themselves, reveling in their own glory and imaginations.
The best and the brightest, at least until the next smarter person comes along.
Even with all that said, I think the easiest way for the average developer to tackle complexity is to simply start thinking about it in their code.
Let‚Äôs take a simple example problem.
Reading or writing multiple CSV files stored in s3.
Some complexity is tooling based.
Some complexity and code-based.
Here is an example of getting some CSV file data from s3 using Pandas and boto3, probably a very common problem AND solution.
The problem is, as any good engineer will know, the more tooling we add, the more steps we take, the higher the likelihood of future bugs, harder to extend and modify the code, and harder to reason about and debug. That‚Äôs just life. The more complex, the more problematic.
What do we see about the above code? Of course, it is less complex, for a number of reasons, some seen, some unseen.
The first bit of code has a bunch of logic going on all in the same place.
The second bit of code has small abstractions taking place in the form of functions.
The original code must iterate through the files. The second bit uses functionality to simply scan a bucket for the files.
All of these small decisions, added up over time in a large codebase will eventually mean either a complex and not very fun to work in the codebase, or a reliable and well-built, easy-to-debug codebase.
Complexity is many times the cumulative effect of good or bad code, and architecture, building up over time.
More examples.
Let‚Äôs go back to what
Mathews Powers
said, especially related to abstractions for OSS and the greater community deciding if something is worth abstracting or not.
This is an interesting concept.
Open source is the ultimate in complexity reduction. If the ‚Äúcommunity‚Äù of people goes through enough pain and suffering, someone decides to solve that and build a tool ‚Ä¶ an abstraction to solve some problem (probably related to complexity).
It either lives and breathes into itself a new life (aka the complexity was there and the tool provides a needed solution), or it dies on the vine ‚Ä¶ the problem might be com
plex but not enough to warrant a solution.
Here is an example in the Python package
mack
. Built to
Mr. Powers
its purpose is to ‚Ä¶ ‚Äú
provides a variety of helper methods that make it easy for you to perform common Delta Lake operations.
‚Äù
What is a more common Data Engineering problem than duplicates?
mack
provides a solution for that.
Pass a Delta Table and columns, and kill the duplicates in the Delta Table. Abstraction solving a very common problem for others.
We can do that very same thing every day in our codebases.
Closing it up.
I still wonder a lot about complexity and hiding the pea. Can you really program or design complexity away? Probably sometimes if ‚Äúthings are bad.‚Äù
I would guess a fair amount of time we are simply moving the peas around under different rugs though. Sure, pip installing some new package is an ‚Äúacceptable‚Äù way to reduce complexity.
Sometimes it‚Äôs simply using more features of the tools that are available to us. Writing less code and a more efficient manner reduces complexity. Making modules and abstractions, even if they are OOP, can reduce complexity.
Tell me your thoughts on complexity. When have you reduced complexity, how did you do it? Do you think complexity can be reduced? When is too much abstraction too much? Don‚Äôt be shy, let‚Äôs talk about this!"
137197857.a-treatise-on-database-connections,"A Treatise on Database Connections, Cursors, and Transactions.",For Data Engineers,2023-09-26 07:18:12.729-05,"Today I‚Äôm feeling a little nostalgic for the old days. The days in the beginning when I was enthralled with Python and Postgres, entangling them together like some medieval monk spinning and winding my web of mysticism and superstition on an unsuspecting audience.
To me, there is nothing more classically Data Engineering from back in the day, and I suppose people still do it now, than connecting to Postgres with some code, probably Python.
So easy, yet fraught with so many problems.
Transactions, Cursors
, and
Connections
, all hail and bow before the Great Three. Let‚Äôs explore these topics around
Python + Databases
(Postgres for our examples) to give us the refresher we all need.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
When Code talks to Database.
When, as Data Engineers, we start to write code that interacts with some relational database, there are a few important topics that we should get a good grasp upon to avoid problems and broken databases.
As we all know, at least some of us, from experience, it doesn‚Äôt matter if you‚Äôre working on an SQL Server or Postgres instance,  these relational database systems are reliable, but at the same time, they can be very fragile.
They are susceptible to bad programming practices that arise from Engineers who
possibly don‚Äôt understand the implications and importance of the actions they take against a database
from their code. It‚Äôs easy for things to go wrong, and hard to troubleshoot those problems.
So, let‚Äôs cover the basics,
Connections
,
Transactions
, and
Cursors
. We will approach them in the order they are used in the code, as shown below.
First, create a connection, then a cursor, then a transaction
.
Database Connections.
Connections refer to the connection objects that represent a connection to the (
PostgreSQL
) database. When programming in Python, you establish a connection to the (
PostgreSQL
) database using a library (like
psycopg2
or
SQLAlchemy
) to send SQL statements and receive results through that connection.
In the Python script, a connection is often created using a connection function with the necessary parameters such as database name, user, password, etc., and it returns a connection object.
You generally close the connection after performing the necessary database operations to free up database resources.
Here is an example of creating and closing a connection in Python using the
psycopg2
library:
I‚Äôve personally seen the (non) management of database connections by code cause serious problems. Here is what you should think about and keep in mind.
Creating database connections via code for every request is expensive (to the code and the database)
You should consider reusing connections in your program, rather than throwing them away and getting new ones.
Connection pooling is an important topic, learn it.
Always close connections when you are done with your work. Clean up after yourself.
Connections are the gateway to the database, treat them with respect and use them wisely and quickly.
Database/Code Cursors
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Cursors in Python and databases like PostgreSQL are database objects that allow you to retrieve rows from a result set one at a time. When using Python with PostgreSQL, you often use a cursor object to interact with the database, sending SQL commands and receiving results through the cursor.
The ‚Äú
idea
‚Äù of a cursor when interacting via code with most any relational database is a central concept.
Here is a simple example of using a cursor to execute a SQL query in Python using the
psycopg2
library:
For example, when working with cursors you can fetch results in a few different ‚Äúways.‚Äù
fetchall()
fetchone()
fetchmany()
For example ‚Ä¶
Cursors are your entry point into database results. They allow your code to access and iterate results from a query.
Transactions
Database transactions are one of those things that a lot of new developers gloss over, and which come back to bite them in the
you-know-what
later.
Understanding database transactions at least on the surface, is a very important skill to have.
In the context of databases, a transaction is a single unit of work that is performed against a database. Transactions in PostgreSQL are used to manage and maintain the integrity of the database by ensuring that a series of operations either all succeed or all fail as a single unit.
Transactions generally adhere to the ACID properties which stand for:
Atomicity
: Ensures that all operations in a transaction are completed successfully; otherwise, the transaction is aborted.
Consistency
: Ensures the database remains in a consistent state before and after the transaction.
Isolation
: Ensures that transactions are securely and independently processed at the same time without interference.
Durability
: Ensures that the results of the transaction are permanent and survive future system and database failures.
In Python, when working with PostgreSQL (often through a library like
psycopg2
or
SQLAlchemy
), you initiate a transaction with a
BEGIN
statement and can end it with either a
COMMIT
(to save changes) or a
ROLLBACK
(to discard changes) statement.
This can be important when doing tasks that require multiple steps that are tied together very closely, where you want an ‚Äú
all or nothing
‚Äù approach. Either everything works, or nothing goes.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Pitfalls of working with Databases via code.
Working with Python and relational databases is common in a Data Engineering context, but there is always a gloomy side to most things, this included. When doing so, several ""
gotchas
"" or pitfalls can emerge. Here's a list of some of these pitfalls:
SQL Injection
:
One of the most critical vulnerabilities in web applications is SQL injection. If you concatenate strings to create SQL queries, your code may be vulnerable.
Use parameterized queries or ORM tools to avoid this.
Charset and Collation Mismatch
:
Make sure the character set and collation settings of your Python environment match those of your database, especially if you're dealing with non-ASCII characters.
Handling of NULLs
:
Remember that in SQL,
NULL
is not the same as an empty string or zero. Operations with
NULL
can have unexpected results.
Fetching Large Result Sets
:
Fetching huge amounts of data into memory can cause performance issues or even crash your application.
Use paging or limit the number of rows returned by queries.
ORM Overhead
:
ORMs (like SQLAlchemy, Django ORM) can make database interactions more Pythonic but can add overhead.
Be aware of the ""N+1 query problem"" when accessing related data.
Hardcoding Configuration
:
Hardcoding database configurations (like passwords) in your code is a security risk.
Use environment variables or configuration files and make sure these are kept secure.
Datetime Issues
:
Always ensure time zones are handled correctly. This can be a source of many subtle bugs.
Use libraries like
pytz
for timezone handling.
Error Handling
:
Not handling database-related errors can lead to data loss or inconsistencies.
Catch and handle errors like connection failures, constraint violations, etc.
The list goes on as well. While on the surface code + database = power and easy, the more complicated the systems and interactions become, the more problems arise and the harder debugging and problem-solving can become.
Do you have stories of nightmares involving database + code? Share them in the comments!!"
136819114.life-updates-working-hard-and-rest,Life Updates - Working Hard and Rest.,and a 20% Off Sale,2023-09-07 09:56:36.556-05,"Get 20% off Paid Subscription!
I‚Äôve been doing two things the last few weeks.
Preparing for a 7-day backpacking trip.
Working a lot.
Writing a lot.
Onboarding a new Sponsor for the newsletter.
It‚Äôs been a fun and exciting ride this year, moving slowly from
www.confessionsofadataguy.com
over to this Substack. Landing some sponsors, and writing more and more and more.
I‚Äôve learned a few things so far.
Consistency is hard.
Make room for rest.
Make room for fun and family.
Don‚Äôt listen to the noise, do what you love.
I made a conscious effort this year to spend a lot of time outdoors with friends and family, doing what I love most ‚Ä¶ staring at trees.
Trout Fishing and Camping
Part1
and
Part2
Fishing
and
Fishing
Boating and Floating
Camping
Family Stuff
I‚Äôve also got an upcoming backpacking trip in Washington State and then in October with the family going to Colorado and Utah.
Doing all this stuff keeps me grounded and not burned out. Getting away from screens and code helps me be more creative while keeping the content pumping out, and have a good attitude about it which is important!
Doing tech stuff.
As you probably know, I‚Äôve been
writing Rust
on and off, and even started my own open-source
Data Quality project called TinyTimmy
, which has already picked up some contributors!
I always try to keep close to the real-world Data Engineering stuff going on in the wild. Sometimes is hard to bounce between high-level content and programming stuff in the weeds.
Let me know what you like!
As always, please let me know what type of content you like and want to hear more about.
Get 20% off Paid Subscription!"
136726687.json-parquet-python-vs-rust,Json -> Parquet. Python vs. Rust.,We know who's going to win (or do we?).,2023-09-12 09:30:10.132-05,"We know all good developers love a battle. A fight to the death. We love our files. We love our languages, and even when we know what the answer will be, it‚Äôs still fun to do it.
While this topic of
JSON
‚Üí
Parquet
may seem like a strange one, in reality, it was one of the first projects I worked on as a newly minted Data Engineer. Converting millions of JSON files stored in s3 to parquet files for analytics, so maybe it isn‚Äôt so far-fetched after all.
When dealing with massive datasets, the storage format can significantly impact performance. While JSON is an excellent medium for data interchange, Parquet, a columnar storage format, is more suitable for big data analytics due to its efficiency and speed.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
In this article, we'll compare converting
JSON
to
Parquet
using Python and Rust, highlighting their performance and idiosyncrasies.
Every pythonista secretly wishes they were writing Rust instead, right?  Think of all that performance, the strong typing, braces around blocks, and most importantly the street cred.  Except‚Ä¶ is it really true we‚Äôll see performance gains if we make the switch?
Python: Using PyArrow and Pandas (and maybe Polars)
First, we are going to start with Python. Python is one of the most popular languages for data manipulation and has a vast ecosystem of libraries.
The next best thing to use in the Python data world is
PyArrow
, a cross-language development platform for in-memory analytics, that provides an easy way to convert JSON to Parquet.
Apache Arrow
powers other popular libraries
like Polars
.
For our sample dataset,
we will be using a credit card fraud table that can be found on Kaggle
.
We will be reading this JSON file and converting it to a Parquet file.
One thing to note is the utter simplicity of this Python code. It runs a little slow at over
4 seconds
.
This is probably cheating, but what if we just used
Polars
(the Python bindings for the
Rust library
) to do the same thing, will it be simpler and faster than
Pandas + PyArrow
?
Indeed it is simpler and much faster, almost down to 3 seconds.
On a side note, this shows the power of Polars and how it might change the Data Engineering space. A task that once required Pandas and PyArrow can now be done quickly and more simply with Polars.
Advantages of Python:
Direct integration with Pandas and Polars, a primary tooling for data manipulation in Python.
A mature ecosystem with vast documentation.
Very terse and readable code
Drawbacks of Python
Might be slower compared to low-level languages like Rust for vast datasets
This is just yet another reason to switch from Pandas to Polars. No excuses.
Rust: Using Arrow and Parquet Crates
This should be interesting. We already saw we could move from Pandas in Python to Polars (Rust) and got a good little speed-up bump with some very simple code.
What will Rust look like? No doubt the code will be more verbose and complex, but will the performance make it worth it?
Rust, known for its memory safety guarantees and blazing-fast speed, has libraries to handle Parquet and Arrow operations, making it a strong contender for large-scale data processing tasks.
It was discovered during this experiment that the JSON reader for
Rust‚Äôs Arrow implementation
expects a text document where each line is a row of JSON, rather than a single JSON array of objects.
From the [
crate‚Äôs documentation
]
>>
This JSON reader allows JSON line-delimited files to be read into the Arrow memory model. Records are loaded in batches and are then converted from row-based data to columnar data.
Going into this blind, this slowed me down, and should factor into the total effort involved.  This can probably be circumvented by parsing the JSON document using some other mechanism, but perhaps with a significant performance impact depending on how that mechanism works.
It‚Äôs a little daunting to see the complexity of the Rust required to do the same job Python did.
It was also as slow, and slower than the Polars Python example. Part of the reason for the complex Rust was the inability of
json_serde
to read an Array of JSON objects with no newline characters.
So the above file had to be preprocessed into a file that could be handled better.
Rust Advantages
Typically faster due to Rust's efficiency.
Memory safety, ensuring that there aren't unwanted surprises during execution.
Rust Drawbacks
Steeper learning curve if you're new to Rust.
The ecosystem is less mature compared to Python's, but it's growing fast.
Rust‚Äôs implementation of the JSON reader is fickle and expects newline-delimited files rather than a conventional JSON array.
The Truth about Rust vs Python for JSON ‚Üí Parquet.
Let‚Äôs be honest. It wasn‚Äôt what I or anyone else probably expected. Are there some Rust savants who could probably eventually write code that is faster and better? Of course.
But, that‚Äôs not the point. We learned something important.
If you can write 4 or 5 lines of Python code that runs faster than spending a TON of time writing Rust that ends up being slower ‚Ä¶ well, sometimes it‚Äôs about more than just the obvious.
Having less code to manage and debug, in Python‚Äôs case, can make it a clear choice for some. Especially if we can just use Rust via something like Polars in Python.
It‚Äôs important to know when to use what tool for the job, that‚Äôs for sure. Of course, there is only one way to learn this stuff, by trying it out!"
136496781.project-planning-and-implementation,Project Planning and Implementation Data Projects,Introduction for Data Engineers,2024-01-01 08:11:24.701-06,"What?
Project planning
? We are just engineers mucking away in the dark recesses of our attics and basements, closing JIRA tickets and putting in PRs, scrolling
r/dataengineering
and laughing at all the poor souls still using SSIS.
What do we know about project planning?
Unfortunately for us Data Engineers, other people exist in this world. And those people require things of us.
We aren‚Äôt measured by just our code anymore.
In the age of
GPT
and
CoPilot
, our human skills seem to bubble up to the surface more often, setting us apart from the mindless robots trying to take our jobs.
Probably one of the hardest parts of Data Engineering, and one far from the grasp of our AI overloads is project planning and implementation in a data context.
This could be migrations, implementation of new tools, processes, or complex business requirements. How do you plan and implement multi-week/month to multi-year data projects?
Read on, I will lead you to the Promised Land.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Big Projects in the Data World.
Running projects and implementations in a data context is hard to learn and easy to fail. Most Engineers simply want to ‚Ä¶
engineer
.
Especially when it comes to Developers, we think code is the end all be all, when in fact, typically Senior Engineers are good coders but have the ability to deliver. That is key. Deliver.
So that‚Äôs what we are going to talk about today. How to deliver a data project. No more dropping the ball or unexpected surprises, it‚Äôs time to get the job done.
If you‚Äôve never delivered a multi-week, month, or year-long project, taking on such a task can be very daunting. The engineer inside you wants to simply jump in
fingers first
without giving a second thought to anything but the code you‚Äôre about to write.
Bad idea.
How to plan a big project.
I don‚Äôt have an exact science to how I run my big projects, even though I‚Äôve done my fair share of year+ long implementations. Everything from migrations to greenfield.
Honestly, for me, it doesn‚Äôt change that much, migration or greenfield, as long as I cover the basics, things tend to progress in a control chaos matter.
I‚Äôm sure others have fancy ways of doing things, but this has worked well for me. I would say that I have a few main goals I‚Äôm always trying to juggle when trying to implement a complex project or migration.
Have an accurate timeline for the project give or take a month.
Minimize the number of surprises (zero show stoppers).
Have a fairly clear understanding of the steps between start and finish.
Have a fairly deep understanding of technical details before starting the project.
Know what is going to be the hard part(s).
Have the project broken up into steps and pieces that are manageable.
Get buy-in on the whole plan from the business and engineering.
You have to be flexible of course, the troubles will come here and there, that‚Äôs part of working with technology. Mostly one should have done enough planning and research to avoid show-stopping surprises and have a fairly clear plan and set of steps to get you from point A to point B with a timeline for each of those steps.
Pushing for delivery over perfection.
I‚Äôve also seen a few pitfalls in my days. Especially with data people who love to die on hills, hills they think are worth dying for, which are usually not.
When it comes to any multi month or year+ long projects, that are complex, you must push for delivering what you said you would, focus on delivery, not perfection.
Your business partners will rarely care about perfection, they leave that up to the Engineers to make
reasonable
and
informed
tradeoffs
. They don‚Äôt want a peice of crap, but they want something that works ‚Ä¶ on time.
This is one of the key difference makers. Having Senior level engineers who are capable of understanding tradeoffs and making them. Getting buy-in from other engineers who might be actually writing the code or doing the work.
Know what features and steps are ‚Äúmust-haves.‚Äù Know what is critical to the operation of the system as a whole.
Be willing to bypass lesser things, trading evils, with a focus on delivering quality on time.
At the end of the day, you will be seen as someone who gets the job done if you deliver a quality job on time, but even with a list of ‚Äútech-debt‚Äù from an engineering perspective that you would like to fix.
A lot of engineers don‚Äôt like this approach. They demand perfection. Hence they rarely rise the ranks, able to complete large and complex projects that require sacrifice and tradeoffs.
Understanding where the hard part is.
There is another thing I‚Äôve noticed over the years as well. There is one spot in the tracks where everyone runs the train off the road, down burning into the ditch never to return.
It‚Äôs the beginning. At the start everyone is happy. Lots of easy code is just rolling off the assembly line. The end isn‚Äôt too bad, sometimes stressful but mostly small details and cleanup.
It‚Äôs the middle that gets people.
Learn your lesson. The middle is what gets people. Folk get sidetracked, and engineers get tired and cranky. Features get added or changed.
If you are going to get lost in the woods, this is the spot.
This is why planning is so important, keep everyone and everything on task and headed due north. Without a clear plan, it's human nature to wander to some places from which there is no return.
Closing thoughts.
I could ramble on with tales of war and travail, but I will desist. I feel like this is one of those topics that is more art than science.
Everyone thinks it's easy or they can run a multi-year data project until they fall on their face trying.
Take my suggestions with a grain of salt. I would say they are worth what you pay for them, but some of you are actually paying for them. So in that case they are gold. You're welcome.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
136254334.loop-vs-map-vs,Loop vs Map vs ...,Data Engineering Conundrums ,2023-08-28 08:30:13.292-05,"The infinite loop. Nothing screams the beginning of a programmer‚Äôs journey more than the classic stack overflow. Loops. I still remember writing some of my first loops in
PHP
and
Perl
. Wait. Did I say that out loud? I just dated myself to a specific decade probably.
There are probably few things more common to generic Data Engineering tasks than ye ole‚Äô
for
loop. We‚Äôve all looped a CSV a few times in our lives have we not?
Funny, leave it to a bunch of programmers with decades with nothing better to do, wa-la, now we have a million ways to loop.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
map
filter
reduce
while
for
something else?
Ultimately, we have an iterable, and then we must do something with that iterable. That‚Äôs life.
Today I want to talk about iteration. Simply for fun. Python. Rust. Maybe you will try something new. That‚Äôs the goal.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Programming Style, or more?
Who decides what to use, a
for
loop or a
map
?
It is just the way we were born? What language we first used? Who taught us? Blind luck? Our loosely held opinion on clean and readable code? What do we think is the fastest? Probably all of the above.
I still remember when list comprehension became a thing in Python. Boy, did I go overboard. I could write list comprehensions 3 layers deep with a lambda thrown in just for fun. Those were the days.
I‚Äôm personally not a big stickler when it comes to how people write code. As long as it‚Äôs legible, has been run through something like
ruff
or
black
(for Python), and functions don‚Äôt contain more than 20 lines of code max ‚Ä¶ I won‚Äôt complain.
Just for fun.
Let‚Äôs solve a problem, and try a few different ways of iterating. Simple problem. Let‚Äôs convert a CSV file to a Tab flat file. Iterate the rows, but do it a few different ways, in both Python and Rust.
Let‚Äôs just simply get a feel for what are options are and how they fit ‚Ä¶ like a pair of shoes, sometimes you just know if you‚Äôre going to like something or not.
We are going to use a single file from the
Divvy Bike Trips open data set
. Pretty typical, looks something like this. The file I have has about
770K
records.
Let‚Äôs say we work in some place old and crusty. They want to move everything backward in time, they enjoy the old stuff. As such, we get CSV files on a daily basis they want to be converted to
tab
delimited so their old mainframe can consume them.
So be it.
With Python I say.
First, we are going to do the task with Python. Let‚Äôs be creative and find a few different ways. Let‚Äôs time the results as well, just for fun.
I mean it‚Äôs Python. There are probably 50 ways to solve this problem with it. Writing Python is like the twilight zone, anything and everything is possible, all at the same time.
Let‚Äôs start with a simple
for loop
.
It works perfectly fine, and converts the file, although at quite a snail‚Äôs pace
0:00:03.925561.
This is probably the code we are all used to seeing. Simple for loop, very easy to read and reason about.
Let‚Äôs try to do this with
map
.
Well, we can say this is less legible, takes a second glance to figure out what is happening to someone seeing it for the first time probably.
It‚Äôs also notably slower
0:00:04.148699
.
Just because we are gluttons for punishment, and Python is the language of the masses. Let‚Äôs do this with a filter. Strange eh?
We are on a roll, getting slower every time,
0:00:05.695858
. This is expected having to call our naked filter function on every row.
As if we aren‚Äôt twisting the laws of Pythonic Python enough, let‚Äôs do this with a
reduce
function.
Reduce
is faster than the
filter
, at
0:00:04.002285
.
And if you thought we were done yet. Think again. We have the
while
loop.
And who would have thought, the slowest one yet!
0:00:06.743144
I do have to say, the filter, reduce, and map functions are probably the most confusing simply to read and understand, that is if they were in the context of a larger codebase, it would take some mental overhead when you run across code like that.
This is sort of sad to all of us people who like to act smarter than we are. Guess we are stuck with the old
for
loop.
I suppose this simply means that trusty
arrays
, bare-bones style, are just good for the fast stuff, syntactic sugar is fun, but incurs a penalty, especially when applied incorrectly.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Doing stuff in Rust.
Let‚Äôs do some of the same loops in Rust, and see what happens. We shall again start with a simple
for
loop. This looks very similar to the Python.
Gandalf‚Äôs Beard! That Rust is fast.
Duration { secs: 0, nanos: 787616000 }
Well under a second.
Makes you wonder why we don‚Äôt use Rust for more Data Engineering tasks. Easy to read, even for those who aren‚Äôt familiar with Rust, hard to miss a
for
loop.
While we are at it, let‚Äôs try the
map
function in Rust.
I don‚Äôt mind the map function here, it doesn‚Äôt ruin the readability that much. It‚Äôs still apparent what is going on. Little slower than the
for
loop, but still blazingly fast compared to Python.
Duration { secs: 0, nanos: 851033000 }
Thoughts on complexity and loops.
For some reason I think, as humans and engineers, we can fall into the trap of falling in love with complexity. Especially early on in our careers, we can equate complexity with genius. This is far from the truth.
It‚Äôs usually the opposite. And we find this truth played out in our examples of both Python and Rust.
We all loop. All the time.
Don‚Äôt you feel dirty simply writing a simple ‚Äúfor‚Äù loop sometimes? Like you should be doing something more fancy or complex?
Let this be a lesson for us all. In the words of Led Zeppelin, ‚Äú
Not everything that glitters is gold.
‚Äù There ain‚Äôt nothing wrong with a good ol‚Äô
for
loop, don‚Äôt let anyone else tell you otherwise.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
136229619.demystifying-the-large-language-models,Demystifying the Large Language Models (LLMs),For Data Engineers,2023-09-04 08:10:13.471-05,"This is one of those funny times in the tech world where you can get many folks probably starting to feel left behind, especially with AI and LLMs (Large Language Models).
As someone who‚Äôs worked in and around MLOps for about 5 years now, I can relate to the feeling. But, I can also relate to the realization when looking ‚Äúbehind the curtain,‚Äù the feeling of surprise that it isn‚Äôt black or dark magic. It‚Äôs just code.
Today I‚Äôm going to prove that to you. I‚Äôm going to show you that LLMs might be one the greatest advancements of our programming age, but that at the same time, they are remarkably not different than other Machine Learning flows from the Operations standpoint.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What are LLMs?
Ok, so if we were Machine Learning Engineers or Data Scientists who actually cared about the internal workings of our ML models maybe this wouldn‚Äôt be so easy. But, thank the good Lord, we are Data Engineers.
We don‚Äôt worry about such fuss.
We worry about the MLOps part of the story. How can we automate the building and exercising of Machine Learning workflows? This is what concerns us.
Where are here to take our Engineering best practices and knowledge and apply it to the problem at hand. In this case Large Language Models.
Being that this area of AI and ML is surrounded by such a haze of mystical fog and fairy dust, it‚Äôs time we bring it all back down to earth. We are going to pull back the curtain on The Wizard of Oz, and peer at the person pulling the strings behind all the fizz and pops happening on social media.
Demystify the LLM.
First, we are going to demystify the LLM by treating it like just another ML model.
Sure, it‚Äôs different from a lot of previous ML models we are used to, and if you‚Äôre new to ML, it can seem like the devil‚Äôs work, but I think by the end of this article you can feel like you have a better grasp at what it takes to be an Engineer working on LLMs.
First off, yes, I‚Äôm going to boil down LLM and the toolchain that goes with it
A LOT
. One thing you have to remember is that we have a few basic concepts going on.
Many parts of the toolchain you might hear and see about on YouTube or Linkedin are actually ‚Äúancillary‚Äù technologies to help improve the human experience of interacting with LLMs and to make things ‚Äúresponsive and fast.‚Äù
What are the simple basics you need to actually understand, how you are probably experiencing it as an end-user, and therefore how companies try to build a system around LLMs to make products? Let‚Äôs try to answer these questions.
Take a quick look at the figure above. What are the components?
LLM model object (
yes, a physical file for the model
).
Code to ‚Äúexercise‚Äù the model.
You need code that understands the model and can give input to a model and get a response.
Some way to interact with those two things above.
Is this simplistic? Yes. Is this true? Yes.
Prove it.
Instead of getting into more theory. Let‚Äôs SHOW you an example of actually ‚Äúdoing‚Äù or ‚Äúrunning‚Äù a LLM model. This will give us a better baseline to talk about challenges for MLOps and Data Engineering in an LLM context.
First, things first. A model. And some Python.
OpenLLaMa - trying it out.
When we want to start learning LLMs and how generally you can work with and access them via code, we need to actually get our hands on a model we can use.
Enter
OpenLLaMA
.
You can read about
this project here on GitHub
in more detail. Many times when you start reading about LLMs or looking into them you will start seeing numbers
7B, 13B, 30B, etc
. This for lack of better terms and easy understanding is the ‚Äúsize‚Äù of the model, the number of input parameters it was trained on.
Obviously the larger the number, probably the more accurate but more complex and probably more costly to run inference on.
Smaller number = smaller model = quicker and faster.
Most of the open-source LLM models you will hear about are based on different papers, models, and trained on different data, and fine-tuned differently. Some of them are for specialized things like ChatBots etc.
Notes for Data Engineers running LLMs for the first time.
Refer a friend
There are a few things I want Data Engineers to note if this is your first time exercising an LLM via code yourself. It‚Äôs important to note there is no magic here, just a different set of tools.
We will discuss some of this more in detail a little later.
You will need the ‚Äúmodel‚Äù code.
You will need a few simple
pip
installs.
An important part of LLM‚Äôs is the input.
An important part of LLM‚Äôs is the output.
It‚Äôs slow.
It‚Äôs a resource-intensive task.
It‚Äôs more about user interaction and experience than most other ML applications.
Everything we discuss might slightly vary depending on the model you are running.
With that being said, how easy or hard is it to run an
OpenLLaMA
3B (smallest) model and ask it what a Data Engineer is?
You will need a minimal set of Python installs.
After that, in our case, we need to git clone the HuggingFace model
OpenLLaMA
. You will have to ensure the code you run, is run from the directory containing this cloned code.
Once we have that code cloned locally, we can create a Python file with the below code. Put your file somewhere in the vicinity of your previously cloned git repo, and reference it in your script as shown below.
And running that Python script gives the below output.
It‚Äôs important to note that this simple question and script for our
OpenLLaMA
3B model
takes a few minutes to produce a response
! On a healthy M1 MacBook Pro.
No magic to be found here.
Just like with most Machine Learning applications, there is no magic. It‚Äôs just knowing things specific to the ML problem you are trying to solve.
As Data Engineers many times our job is to ‚Äúproductionize‚Äù ML pipelines, working closely with Data Scientists. Do we really need to know the inner workings of every ML model including a LLM? No.
The inner workings of LLMs might seem like black magic to you. I get it. But the point of the above code was to show you that it‚Äôs not out of your reach.
Just like the rest of ML, it might be surrounded by a cloud and cloak of fog, but ‚Ä¶ if I can write and exercise an LLM ‚Ä¶ so can you.
I also wrapped up this code and requirements inside a Dockerfile and put it on GitHub. This gives you no excuse to play around with an LLM yourself.
What did we need to interact with a LLM? These things give you hints at what Data Engineers might work on.
An environment or system setup to handle LLMs.
Some properly installed dependencies like pytorch, transformer, and the like.
A downloaded model to actually exercise.
The ability to write code to interact with the model.
The ability to take input from a user and give it to the model.
The ability to do something with the model output.
Of course, we are just scratching the surface of a real-life production LLM toolchain. There would be more tools, caching, etc. Setting up all these systems, making them work together nicely, and the like will keep some engineers busy.
But, that code is easy, isn‚Äôt it?! Give it a try.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Ok, Real-Talk about Data Engineering LLMs.
Ok, so now that I have brought you to the mountaintop and you are ready to become an LLM and ML wizard, it‚Äôs time for me to tear you down.
It‚Äôs not that easy. Sure, we just saw that LLMs are no black magic, but there are some serious hurdles to overcome when starting to tackle LLMs for production use cases, as Data Engineers.
Training and Finetuning LLM models is another beast.
The data required to do the above is different from tabular data most Data Engineers work with.
LLMs are slow without catching and have large resource requirements.
The user experience to get input is important.
Returning the response to the user is important.
Both of the above require ‚ÄúWeb Dev‚Äù UI work. It's not something usually in the domain of Data Engineers.
The toolchain used to enhance LLMs is large and complex.
There isn‚Äôt a ton of documentation or clear how-do-this guides, it‚Äôs a new domain.
MLOps around LLMs is quite new and unknown at this point.
On the one hand, we saw how easy it is to get ahold of an open-source LLM and run some simple inference, but we know that getting ticked by easy ‚Äúhello world‚Äù examples is far from the reality of doing things correctly in production.
You have to manage models. Versions. Training data. Track metrics. Create input and output UIs or prompts. Serve models with extensive resources. Automate everything.
If only it were easy.
I hope I opened the door a little for you with LLMs with this simple example and exercise. It‚Äôs easy to think something is out of reach just because it is unfamiliar and we don‚Äôt know where to start. But, the best place to start is somewhere.
Please comment if you have some experience with LLMs from a Data Engineering perspective. We all want to hear about it!"
136090956.bytes-for-data-engineers,Bytes for Data Engineers,and Buffers ... and Streams.,2023-08-21 08:30:29.976-05,"One of the most underutilized pieces of code I‚Äôve seen in all my many years of Data Engineering is Bytes and Streams. I‚Äôm not sure why. It just never appears.
I see
Strings, Ints, Floats
, I see everything, but never a plain old
Byte
. Poor little bugger. I don‚Äôt know if people think it‚Äôs too complicated, in fact, it is less, less to go wrong, less complexity.
What‚Äôs more computationally expensive than Serialization and Deserialization? Especially in a Data Engineering context. Lots of data moving around, coming from this place and going to that place. Does it really need to be a String all the time? No.
Let‚Äôs take a look at
Bytes
,
Streams
, and
Buffers
in Python and Rust.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Bytes Basics.
Where to start? Not sure. I feel like maybe we should start by talking about why I personally use Bytes whenever I can, and ponder why others don‚Äôt do the same thing.
I use Bytes whenever I can for a few reasons ‚Ä¶
Reduce the complexity of what I‚Äôm working on.
Reduce the mental burden of developing and debugging.
Make things run a little faster (especially Python).
Pushes me toward buffers and streams.
I know to someone not used to working with Bytes, Buffers, and Streams, the thought of reducing complexity with them in a Data Engineering context might seem strange, but it is true.
I think working with Bytes can reduce the cognitive overburden and detritus of working in complex data pipelines simply because there is ‚Äú
less to worry about.
‚Äù When you boil the movement of some data around to pushing and pulling some
Bytes
here or there, quite simply, there is less ‚Äúfluff‚Äù to distract from what the data pipeline is actually doing.
Looking at Bytes
For those unfamiliar, let‚Äôs give the easy guide to Bytes because frankly, that‚Äôs all I‚Äôm good for. Nice and easy. It just so happens that in a recent
Pandas vs Polars inside AWS Lambda
work I did, I used some BytesIO in Python.
For better or worse I‚Äôm going to call Bytes this ‚Ä¶
‚Äú
Raw as you can get data without context sitting in some contigous spot in memory. Of little use or meaning to a human, but to your machine, nector of the Gods.
‚Äù
I‚Äôm guessing you're used to having your data ‚Äú
have meaning.
‚Äù Aka you could print it to your console and you would get text, or numbers, or whatever. In Python ‚Ä¶
Not so with Bytes. You will get nothing of meaning to yourself. It‚Äôs raw. In Python the easiest way ‚Äúinto‚Äù Bytes is typically around Files ‚Ä¶ reading and writing them.
Notice the difference in output, if you can catch it.
And Bytes.
Bytes can be useful in all sorts of areas, files, data over the wire (network).
In a Python context, you will usually be using something called BytesIO ‚Ä¶
Binary I/O (also called
buffered I/O
) expects
bytes-like objects
and produces
bytes
objects. No encoding, decoding, or newline translation is performed.
Python Docs
So, instead of working directly with text and Strings, or numerical data directly in our data pipeline, we would simply work with the raw Bytes representation of that data.
Since there is no serialization or deserialization required, as noted above, say actually converting some Bytes you download over the wire from s3 into its String format for example, there is less to go wrong, less to reason about, and it‚Äôs quicker overall.
What does something similar look like in Rust? This code is from
some graph work I was doing in Rust
.
A cheaply cloneable and sliceable chunk of contiguous memory.
Bytes is an efficient container for storing and operating on contiguous slices of memory. It is intended for use primarily in networking code, but could have applications elsewhere as well.
Rust docs
Also a
Buffer
‚Ä¶
‚Äú
buffer
provides safe, write-only and generics-free byte buffers that can be used without initializing them first.‚Äù
Rust docs
and a
ByteStream
‚Ä¶
‚Äúconvenient way of reading and writing bytes to a buffer that implements the standard
Read
or
Write
traits.‚Äù
Rust docs
What has always jumped out at me about directly accessing and using Bytes in both Rust and Python ‚Ä¶ is actually the simplicity of it all. It takes away the complexity and really hones in on making sure I design some data flow to do exactly what it‚Äôs supposed to do.
What I‚Äôve used Bytes for most in Data Engineering
I‚Äôve used Bytes in both Python and Rust in a production context to keep runtime usage down on a Lambda for example, also avoiding DiskIO is another great reason to use Bytes, Buffers, and Streams in memory to simply make things run fast and avoid disk writes and reads.
I‚Äôve always approached burning and churning bytes like I would a file.
Fortunately or unfortunately, one of the first things you learn to do when getting into programming is reading and writing files, and that has a way of sticking with you.
Instead, opt for Bytes when you can, either write entire data sets to a Bytes/Buffer, then on to its next destination avoiding disk, or even better, reduce memory and use a Stream, ‚Äúbuffering‚Äù in ‚Äúx‚Äù amount of Bytes before flushing those Bytes downstream and filling the Buffer again.
As I was perusing my
website
for old code I‚Äôve written in the past I found an interesting example that shows the power of using BytesIO with Python. It shows the power AND the simplicity.
The post was titled
Concurrently Download Large Files from GCS.
It played on Pythons
BytesIO
, the
Google Cloud Storage client
, and a
ProcessPoolExecutor
to download different bytes and chunks of a file in parallel, then reassemble those bytes later into a single file.
Now it seems totally overkill and a little crazy, but it does make a point.
This is the power of Bytes, even with Python. You can abstract away the ‚Äúwhat‚Äù of the data. CSV, Parquet, Avro, or something else? It doesn‚Äôt matter.
Bytes is Bytes is Bytes.
It‚Äôs just a spot in memory holding data in its rawest form. There isn‚Äôt much you can do with those raw bytes, and that is good! Nothing to mess up, no UTF-8 conversion, no floats to convert.
When you‚Äôre free to just think of data as a pile o‚Äô bytes it gives you more mental freedom as a Data Engineer. You can be more creative, and faster with the way you deal with moving data from point A to B‚Ä¶ which let‚Äôs be honest, that‚Äôs most of what we do.
In the future let‚Äôs unpack some more usefulness around Bytes in a Data Engineering context, maybe even do some of those benchmarks that make everyone mad!"
136010654.duckdb-for-data-engineering,DuckDB for Data Engineering,Analytics or Pipeline?,2023-08-14 11:10:32.675-05,"DuckDB
and
Mother Duck
now seem to be a common part of the Data Engineering commentary. It‚Äôs probably here to stay. Say what you will about the
Mother Duck
thing and what it can and cannot replace, the future will decide.
Knowing that it‚Äôs here to stay has got me thinking more. It‚Äôs obvious that DuckDB scratches an itch on the Data Engineering back that has been itching. It‚Äôs fast, it‚Äôs SQL, it‚Äôs easy to use. But, I also know that people use things however they want, and it will probably end up being different from what most people think before it's all settled.
What‚Äôs been on my mind about DuckDB? Is it just a new single-node Analytics Engine? Is it an in-application data store? Can it be used for just normal Data Transformations and Pipelines? All of the above, one of the above?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Just because you can do something doesn‚Äôt mean you should. Let‚Äôs talk about it.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
DuckDB for Data Engineering
Ok. DuckDB.
DuckDB is an in-process SQL OLAP database management system. All the benefits of a database, none of the hassle.
- DuckDB
It isn‚Äôt hard to figure out how DuckDB is so popular. SQL is popular and everyone can use it. This is where the simple genius of DuckDB comes into play.
Let‚Äôs say you‚Äôre a Data Engineering team who is full of people who are SQL-heavy in experience. Most of your data resides in some RDMBS like SQL Server, Postgres, Oracle, MySQL, or whatever. Also, you‚Äôre trying to move your tech stack forward and make better data pipelines and transforms like a good little hobbit.
What tool do you use? You have few options probably.
Some canned semi-GUI tools like SSIS or Talend or some other junk.
Dive into the deep end of custom pipelines with Python or PySpark, or something else.
Neither of these options might be the best fit for a lot of teams. They want to graduate from stuff like SSIS because of its inherent inflexibility. But, you might not be ready or have the technical chops to drive straight into PySpark for example.
Enter DuckDB.
Simple. Easy to Use. SQL. Database without the database.
SQL for Analytics vs Transformations.
We‚Äôve probably come full circle in the last decade. From too much SQL, to more Dataframe centric and code, back to SQL again. The infinite loop.
When it comes to DuckDB, the so called OLAP database to rule them all, the question is, is it just for analytics (OLAP), or is it an end-to-end Data Engineering tool. Can it, and should it be used for data transformations all along the way?
Thinking through it.
What I wonder about while laying awake in bed at night is, whether is it worth inserting DuckDB into your Data Engineering pipelines, and how much and where it takes over other tools.
No one using Snowflake, BigQuery, or Spark is going to replace those things with DuckDB (
unless they shouldn‚Äôt have been using those tools
).
People using Pandas, Python, Postgres/MySQL, Polars and other non-distributed systems could probably insert DuckDB as a replacement.
DuckDB is in process, it‚Äôs only temporary, no long-term storage.
That‚Äôs helpful to start seeing where DuckDB can and might go. There are a lot of companies who want to use something from the Modern Data Stack, but simply don‚Äôt have the data size to justify things like Snowflake/Databricks/BigQuery.
What else makes DuckDB so attractive?
The thing is, you can simply `pip` install DuckDB and start using it in your pipelines immediately to transform and aggregate data ‚Ä¶ with SQL. People like easy, and people like SQL.
DuckDB for Data Pipelines
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Here are some of my worries with DuckDB, although they are not specific to DuckDB per se, but more to SQL-heavy pipelines and transformations.
The hard line it takes to make small and reusable transformations.
Unit testable
Knowing you can‚Äôt scale past a single machine easily.
SQL-only options for data transformation.
I‚Äôm simply saying the ability to apply best practices. This doesn‚Äôt have to do with a particular tool as much the culture of a particular Engineering team to do the development correctly. I‚Äôve found that purely SQL based teams struggle in this area.
This is interesting. Recently I wrote two simple Data Pipelines, one in DuckDB and one in Polars.
https://github.com/danielbeach/TheBearVsTheDuck
And then Polars.
I mean is there really any big difference to the naked eye between these two? No.
Are there differences ‚Ä¶ Yes.
For me, I‚Äôm not so sure how I would go about making small testable functions and methods to methodically pipeline transformations together into a single reusable and approachable pipeline.
I don‚Äôt want to just write extra large SQL statements that make up multiple steps in a pipeline, that simply would take us back to the days of old.
What can DuckDB pipelines look like in the wild?
I thought I would go searching to find some larger and more real DuckDB pipelines in the wild. I ran across a
blog post and some code for some benchmarking with dbt+DuckDb
.
Which led me to this repo
https://github.com/datamindedbe/tpcds-dbt-duckdb
This led me to some sample SQL that people would write and run
https://github.com/datamindedbe/tpcds-dbt-duckdb/blob/main/dbt/dbt_duckdb_tpcds/models/normal/tpcds_q02.sql
This is where my beef comes into play. How would you like to have more complex, and many more pipelines built like this? I wouldn‚Äôt. This harkens back to the old days when everyone used
stored_procs
and various and numerous massive SQL files to do everything (I lived that life for years).
There was a reason people rebelled against that travesty.
Inflexible
Becomes extremely over-complicated quickly.
Logic becomes disparate and impossible to manage.
Debugging and troubleshooting time and difficulty go through the roof.
I get it, this has a lot to do with Engineering culture and practice. We have to draw a line in the sand and say ‚Äú
We will not write spaghetti SQL and call it a pipeline.
‚Äù
But we are human, we take the easy pass, we try to get stuff done quickly, and we have deadlines. Can we say no when all our friends are doing it?
I‚Äôm not so sure.
Why DuckDB is still awesome.
Even with all that said, and I feel like I had to say it. I still think DuckDB is awesome and a much-needed tool.
It‚Äôs dead simple to use.
It‚Äôs lightweight on our data infrastructure.
It‚Äôs SQL.
It‚Äôs fast.
It has tons of integrations with other tools and file systems.
It makes data exploration and analysis dead simple and easy.
It can replace other heavy and slower data transformation technologies.
I think DuckDB is the perfect tool to enhance and replace SOME parts of the Data Engineering pipeline tech stack. It‚Äôs SQL. Fast. Lightweight.
It can make our pipelines better and less complex. Just like with any tool, don‚Äôt go overboard, slow, and methodically, use best practices.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
135943647.thank-you-for-5k-subscribers,Thank You for 5K Subscribers!,Get 50% off for a year.,2023-08-11 14:23:36.844-05,"Get 50% off for 1 year
Thanks for subscribing to Data Engineering Central, free or paid it means a lot. I just hit the 5K mark for subscribers and look forward to more growth! I will continue to bring you hopefully entertaining and snarky Data Engineering content for a long time to come.
I‚Äôve been pretty regular with getting out a newsletter every Monday. I‚Äôm also doing a 50/50 split on free vs paid content. I try to switch between every other one, free, paid, free paid, and so forth.
If have piles of moldy money and nothing better to spend it on, consider upgrading to paid, it‚Äôs 50% off right now in celebration of hitting 5K subscribers.
Get 50% off for 1 year"
135753997.a-primer-on-data-architecture-design,A Primer on Data Architecture (Design),For Data Engineers,2023-08-14 09:00:18.331-05,"There are a few skills you can just learn by practice, like programming, you can simply just write more code. Want to be better at SQL? Write more SQL. Data Architecture on the other hand ‚Ä¶ is a difficult one to learn, especially for noobs.
You can‚Äôt simply read a book, watch a video, and know how to design and implement Data Architecture from scratch. In fact, it‚Äôs simply hard to get the opportunity to do so.
Most people who start at companies are coming into some existing technologies. If there is some sort of large migration project that actually occurs, it can be the domain of the most senior Engineers, leaving little room for others.
Is it possible to ‚Äúteach‚Äù Data Architecture and Data Platform design? Yes and no. I will try.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below
Share Data Engineering Central
Learning the Unlearnable
Probably one of the most enjoyable parts of Data Engineering, to me, over the years, has been the ability to do my own architecture and data platform planning.
I‚Äôm not sure if it‚Äôs just been a bit of luck, or being at the right place at the right time, either way, I‚Äôve landed in spots a number of times where I‚Äôve had the opportunity to do data architecture and Data Platform planning and execution ‚Ä¶ from scratch.
I‚Äôm not talking about switching from MySQL to Postgres ‚Ä¶ I‚Äôm talking about complete architecture changes, and new technologies from the ground up. Complete replacements and builds.
Doing such work gives you perspective. Probably one of the worst things you can do in your Data Engineering career is stagnant like some brown and milky puddle baking in the hot afternoon sun. It has unfortunate side effects.
One track mind about how to solve problems.
Very technology specific without variation.
Hard time understanding how the tech landscape is changing.
Working solely on maintenance and generally uninteresting problems.
The thing is, when you start to work on actual Data Architecture and Data Platform migrations or additions, you work on and have to solve problems that are completely different from the ‚Äúday-to-day‚Äù Data Engineering work that is taking place all around the world.
It flexes different muscles.
Boiling Down Data Architecture (Design).
That was a lot of hot air. How can we boil it down for say some junior to mid-level engineer who wants to take their skills to the next level?
What is different about Data Architecture Design from everyday Data Engineering work?
The difference between everyday Data Engineering and being able to design Data Architecture is about pulling away from the mundane code details and having the ability to plan and understand high-level tradeoffs and concepts.
It‚Äôs a totally different skill set than being a JIRA task pusher. Just closing tickets one after another, on to the next one. No, it requires different skills and a different mindset.
And this my friend, is probably why in general Data Architecture and design is one of those skills that you can‚Äôt simply pick up over the weekend, like just writing more Rust to be better at Rust.
Data Architecture requires different and many times specifically non-technical skills.
Hard to learn and develop these skills? Yes, it takes experience.
Impossible to do things to learn and prepare yourself for data architecture tasks? No.
Practical Steps Toward Learning Data Architecture.
This is a hard one to reason about, architecture can widely vary depending on the type of company and the types of tech they use. What do the practical steps and thoughts around
how
to approach architecture design look like?
I see most Data Architecture as just using common sense and applying it in the tech space you‚Äôre working in. It‚Äôs about big-picture stuff. It‚Äôs about asking big-picture questions, and solving big problems,
thinking in a big way
.
Data Architecture is about integrations.
Fitting pieces together.
Thinking how ‚Äú
things will fit together.
‚Äù
Thinking about cost in
installation
,
usage
, and
maintenance
.
Thinking about how the flows between pieces of the stack.
Thinking about the data
Sources
and
Sinks
.
Understanding
features
between tech stacks.
Of course, this is a simplification, but that‚Äôs the point with good architecture. Stay out of the details too much, you have to paint a big picture first before you get caught up on a single feature.
Let‚Äôs take the Modern Data Stack for example.
source:
Datafold
So you‚Äôre running on SQL Server and SSIS currently, the stack is out of data and the CTO wants to move to some newer technologies and adopt a Data Lake.
Basically, it‚Äôs all new. What do you do? What do you evaluate? Where do you start? That is Data Architecture in a nutshell.
Someone who is going to architect a new Data Platform is going to understand where the business is at today, where they want to go, what technologies could be used to reach the new north star, and how they would fit together.
That architect has to have enough context to understand the data currently, understand technology options and how it would fit that data in the future, how it flows, and what the end state would look like.
They have to understand the business and the Engineering team, and what can they handle and support. For example ‚Ä¶
Databricks or Snowflake? SQL only, or some sort of Dataframe and Machine Learning Pipeline. Which one should we pick, what fits our culture? What are the tradeoffs?
The ability to understand the big picture, the different features, and tradeoffs is the key.
That being said ‚Ä¶
But, to be a good architect you need to have the details too. I personally don‚Äôt think anyone can build good Data Architecture without having worked on multiple data teams and experienced the ups and downs of doing that.
Some of the most important parts of Data Architecture is the ability to understand tradeoffs and features. You can‚Äôt do that well without experience.
This is why I think it‚Äôs difficult to teach someone how to architect and design data platforms. It takes some experience, which is why it‚Äôs probably more senior engineers who take on that role.
But to reinforce what‚Äôs already been said, what skills can a more junior engineer work on to build those muscles that will be useful for an architect?
The ability to see the ‚ÄúBig Picture‚Äù
The ability to think how pieces of tech fit together.
The ability to make and see high-level tradeoffs.
The ability to think about costs and maintenance.
Having broad real technical abilities.
Extra Reading
Here are some resources I threw together on the subject generally for extra reading.
4 Ways To Setup Your Data Engineering Game.
Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems
Pragmatic Programmer, The: Your journey to mastery
Uber Engineering Blog
AirBnB Engineering Blog
Netflix Big Data Blogs"
135691031.why-some-data-people-love-rust,Why (some data people) Love Rust?,Hype or Reality?,2023-08-07 07:32:54.943-05,"Love is a hard thing. Love requires work, patience, longsuffering, and the ability to stick through the hard times, the bitter times. That‚Äôs what it‚Äôs like to love Rust.
Rust is a jealous lover, whenever you find yourself writing Python or some other language ‚Ä¶ well, life just seems dull. There is no spice and excitement. But, we have to be careful. We can‚Äôt just fall head over heals with infatuation, not love, and end up wasting our time and energy on a lover who doesn‚Äôt give us anything in return.
This is where I find myself with Rust. I picked it up just because The Primagean seemed impressed with it. I struggled for a day or two, and since then have slowly started using it more and more.
I've found myself becoming angry with my plain old lover Python. Angry about packing, random import errors, cursing it, asking it why it can‚Äôt be more like Rust.
Today I want to talk about Rust in a Data context. Where is it being used, can it actually be used for Data Engineering, and what does the future look like?
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Let‚Äôs dive in.
A Rusty Love Story.
As many of my long-time readers probably already know, I‚Äôm the eternal skeptic when it comes to anything programming and Data Engineering related. I think everything has its time in the sun, things come and go, rise and fall. Everything has its place, and there is a place for everything.
Rust has been a ‚Äúhot‚Äù topic of late in every tech circle, and Data Engineering is no exception. Everyone was righly skeptical until tools like Polars took the DE community by storm.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Rust is one of those languages that takes time. Time to learn. Time to like. It has a little bit of a learning curve to it, and honestly, right up front, that limits the reach that Rust has and ever will have in the Data Engineering community.
In a world where Python reigns supreme, Rust is a hard sell to 90% of the everyday Data Engineers writing pipelines on a daily basis.
We are going to talk about why, but first, here are a few articles I‚Äôve previously written on the subject of Rust + Data Engineering.
Using Rust to write a Data Pipeline. Thoughts. Musings.
Ownership and Borrowing in Rust ‚Äì Data Engineering Gold Mine.
AWS Lambdas ‚Äì Python vs Rust. Performance and Cost Savings.
Exploring Graphs in Rust. Yikes.
DataFusion courtesy of Rust, vs Spark. Performance and other thoughts.
Delta Lake without Spark (delta-rs). Innovation, cost savings, and other such matters.
Dataframe Showdown ‚Äì Polars vs Spark vs Pandas vs DataFusion. Guess who wins?
Working with Cloud Storage (s3). Golang vs Rust vs Python. Who shall emerge victorious?
Thoughts on Saint Augustine, Rust vs Golang. Complexity, verbosity, and other matters.
Now since you‚Äôve read all my wonderful articles .. em‚Ä¶ yeah, you‚Äôre now a semi-expert in Rust in a Data Engineering context. But, let‚Äôs talk about
Reasons (data) people love Rust.
Reasons Rust will not be (Data Engineering) Mainstream.
Let‚Äôs dive in.
Why (data) people Love Rust.
Love is a hard thing to define. What exactly is it about Rust that people love, and more specifically, Data people? If you have Golang, Scala, Java, and all the rest, what does Rust have that has turned it into a hot topic?
I mean we‚Äôve started to see some serious traction pickup with Rust when comes to tools like
Ruff
,
delta-rs
,
Polars
, etc. It kinda makes you wonder what‚Äôs coming next. Maybe it isn‚Äôt all just hype, there could be something to the Rust bandwagon. Or maybe not.
As someone who‚Äôs used Rust off and one for 6 months in a Data Engineering context, here‚Äôs what I think. I want to present this information in such a way that someone who‚Äôs never written a line of Rust can grasp why some (data) people are choosing it.
Here are the top few reasons.
cargo (packaging and dependency management)
fast (blazingly fast)
not terribly verbose (learnable)
memory model, immutability, and static typing
Let‚Äôs unpack each one.
cargo (depends + packaging)
Refer a friend
Probably one of the least appreciated parts of any language by newcomers to programming, in general, is the packaging and dependency management of an ecosystem.
For example, in Python, with tools like pip, you have the allusion of being in control and easy to use, which it can be in some sense. But, it‚Äôs clear to anyone that has used Python, for example, for an extended period of time, that there are massive problems with Python‚Äôs packaging system that can have serious production repercussions.
a tangled web of package versions
the ability to not be specific with versions
things stop working randomly at some point in the future.
When you start writing Python for production use cases that you depend on, you will inevitably at some point start fighting versions, things WILL break somewhat randomly, and even backing packages into things like Docker can and will eventually bite you.
Is it possible to do everything perfectly and keep things from breaking, sure, if you have a team of Ops Engineers whose sole responsibility is to prevent such things (I‚Äôve worked in that environment and that still fails sometimes).
Enter
cargo
, the Rust package manager. What can you do with Rust, or what would you do with Rust that makes it simply a pleasure to work with?
These commands may seem simple, and they are ‚Ä¶ that‚Äôs the point. Want to create a new project with everything you need `
cargo new {project}
` will do the trick.
Need to add some crates to support your requirements? Good ol‚Äô `
cargo add {crate}
` will be your go-to.
Ready to build and run your project, simple as `
cargo build
` or `
cargo run
`.
Honestly, the simple ease of use and bullet-proof ness of cargo makes working with Rust a pleasure and a breath of fresh air in a Python world (I say that as someone who still writes Python on a daily basis).
Rust is (blazingly) fast and not verbose.
I‚Äôve done my fair share of
Python vs Rust blog posts
that get everyone angry and yelling at me. Yet, at the same time, everyone really knows that there is no comparison.
Besides a few angry hold outs on the internet, most folks are not going to argue with you about how is faster at most things than most other languages.
The thing about Rust being blazingly fast is that it isn‚Äôt like you‚Äôre writing C or C++ to get that performance, you can write code that is legible and understandable, and yet at the same time is faster than fast.
It‚Äôs like you get your cake, and you get to eat it too.
For example,
in my GitHub
I have some code where I compared Python to Rust inside an AWS Lambda. Let‚Äôs say I want to download the contents of some file(s) from AWS s3.
There is no magic about that Rust code. It‚Äôs fast, it‚Äôs readable, and it‚Äôs pretty clear what‚Äôs going on. Sure, like any language it‚Äôs going to have its initial learning curve, but it is approachable and doable.
The proof is the fact that if I can write Rust that works, so can you.
Rust is fast, that‚Äôs why some data people love it.
Memory model, immutability, static typing.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Ok, before you roast me, I know not everything listed is unique to Rust, but the point is ALL these things exist together as a unit, the crates go with the memory model go with the static typing. It‚Äôs a package deal, my friend.
Probably one of the biggest learning curves with Rust is the Ownership and Borrowing memory model. It still catches me up. But, it also protects me from myself, and I need that, let me tell you.
Folk who‚Äôve been around the data world, fought production bugs, and tried to make life easier, these people will understand the importance of ‚Ä¶
static typing
memory safety
immutability
Two of the three aren‚Äôt specific to Rust, immutability and static typing can be found in many other languages, but Rust does shine in the memory safety (ownership and borrowing) model.
Simply knowing something is immutable or not makes programming and debugging large and complex code bases easier.
Static typing, like in many other languages, helps to prevent errors and hardens the development process to reduce errors that can happen at runtime with languages like Python.
The below code has a memory bug that will not let the code compile. Rust will throw an error when trying to build.
Did you find it? Keep looking.
Borrowing and ownership
make Rust bulletproof and very scalable. Rust allows you to be specific in your code if some method or function ‚Äúowns‚Äù the thing it‚Äôs using, or if it is ‚Äúborrowing‚Äù it ‚Ä¶ it‚Äôs a very powerful paradigm.
Why do (some data people) love Rust?
Rust is an easy choice for things Data Engineering folk who are looking to build fast, scalable tooling for others to use. Is it helpful for the everyday Data Engineering task? Not really.
Yes, it will make you a better programmer, and you will have fun, and create cool things, but most of the data-to-day work will never be done by Rust per se.
The future is probably more like the Polars way, some smart Engineers build tooling in Rust, then wrap it with Python for others to use.
I think you should learn and use Rust.
It‚Äôs not that hard to learn (after the first two days).
It‚Äôs (blazingly) fast.
It‚Äôs starting to be used on the periphery of Data Engineering.
People will think you‚Äôre smarter than you are.
It will make you a better engineer.
I dare you to try Rust for one week. You will fall in love."
135402735.my-love-hate-relationship,"My Love, Hate Relationship.",... with AWS Lambdas.,2023-07-24 12:47:20.954-05,"Have you ever had one of those things in life where you just aren‚Äôt sure what to do with it? It seems like a good idea and you have visions of grandeur in your head, daydreams of the good times to be had. But, then the reality is somewhat different than you expected.
That‚Äôs how I feel about Data Engineering with AWS Lambdas. Yes, I have a handful of AWS Lambdas I‚Äôve written running in production, yes they are nice, but sometimes I hate them as well.
They never seem to do just exactly what I want, sometimes I want to use them for everything, force them into something they were never meant to be, other times I hate them and migrate them away to some
underutilized Airflow worker
.
Alas, such is life.
With that being said, I realize AWS Lambdas are probably somewhat under-utilized in a Data Engineering context, so today
I want to simply talk about the technicalities of using AWS Lambads in a Data Engineering context.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
All Hail our benevolent master ‚Ä¶ AWS Lambda.
I get the feeling that AWS Lambdas aren‚Äôt used that much in the Data Engineering context, at least in the circles I run in. Not sure why. Well, maybe I do.
In the area of Apache Airflow and Databricks/Snowflake, AWS Lambdas are probably thought of as the domain of backend engineers and other such rats. But, I think that‚Äôs just because people need to have a better imagination, that‚Äôs all.
I want to give a very high-level overview of AWS Lambdas, and some of the technicals I think everyone should know about them, as well as what it‚Äôs like to deploy and manage them. This should give us a decent baseline to then discuss where and how AWS Lambdas might be useful in a Data Engineering context.
AWS Lambdas, what you need to know.
I‚Äôm going to try to give you the 10,000-foot download on AWS Lambdas.
‚Äú
Run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes.‚Äú
I mean that‚Äôs really what it boils down to in my opinion.
AWS Lambdas are, or should be, small bits of code running on small bits of data, in a serverless manner
.
That‚Äôs part of what makes AWS Lambdas so attractive, they are a lightweight answer to some problems, requiring minimal effort and overhead to manage. Honestly, not sure why they aren‚Äôt embraced more for Data Engineering.
What else?
Lambdas can max out at 10GB of memory in size.
Lambdas can max out at 15 minutes of runtime before timeout.
You pay for both the size and runtime of a Lambdas.
The best way to deploy and manage an AWS Lambda in my opinion is the use of the `
Container image
` option. This consists of using a ‚ÄúBase‚Äù Docker image provided by AWS and layering on whatever you need. The following languages are provided support for base AWS Lambda images.
Node.js
TypeScript
(uses a Node.js base image)
Python
Java
Go
.NET
Ruby
Most of the time after you‚Äôve built an image, you can store it in AWS ECR (Elastic Container Registry), and then reference that image when creating a Lambda.
What does a Python Lambda image look like?
Something like this.
Does it look simple? That‚Äôs the point. AWS base images provide everything you need, you simply must layer in your code and any dependencies yourself.
How does an AWS Lambdas work?
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Pretty much across the board, the default way to interact with an AWS Lambda is to write your code and have an entry point called `
lambda_handler
` that the Lambda will look for by default and exercise, although you could change this.
Three of the major concepts of an AWS Lambda are
EntryPoint, Context, and Trigger.
There is a single entry point in the code called
lambda_handler
.
There is a ‚Äú
context
‚Äù passed to every lambda when exercised.
You can attach ‚Äú
triggers
‚Äù to Lambdas that exercise the lambda one some event (s3 file, schedule, whatever).
Below is an example entry point for a Python lambda which has a
trigger
that exercises the lambda whenever a file creation event happens in s3. That event is passed to the `
context
` of the lambda and certain information can be pulled from that
context
and taken action on.
https://github.com/danielbeach/PythonVsRustAWSLambda/blob/main/pythonLambda/python_main.py
How do you deploy a Lambda?
If you are taking the Docker route, there is a set of steps you would take to deploy a lambda. Of course, as you will see, these steps could be automated and built into CI/CD pipelines very easily.
This is not all-inclusive but gives you a general idea of a workflow.
Build the image.
Tag the image.
Push the image.
Exercise the lambda.
Something to that effect would do the job and can easily be automated.
Also, lest ye be afraid of Docker and using something like Rust, which has no official provided AWS image, it‚Äôs still possible to build Lambdas and use
binaries
to do so, no Docker needed! See the below links for an example of building a Rust lambda.
https://github.com/danielbeach/PythonVsRustAWSLambda
https://www.confessionsofadataguy.com/aws-lambdas-python-vs-rust-performance-and-cost-savings/
Lambdas for Data Engineering.
Ok, so maybe you know a little something more about AWS Lambdas than you did before. Maybe not. Either way, let‚Äôs talk about the intersection of AWS Lambdas and Data Engineering. The challenges, the realities, and real-life use cases.
Real-Life.
I think there are a few reasons that AWS Lambdas don‚Äôt really play a large part in many Data Engineering teams.
Require small datasets.
Short runtimes.
Don‚Äôt fit easily into architecture.
Logging and monitoring.
Uncomfortable with pure coding solutions.
Simply use other tools like Airflow.
Well, I can say that many of these reasons are valid. Do you really want to add something else to the tech stack? Are the group‚Äôs coding skills really that good? How can we log and monitor the lambdas? We don‚Äôt have good DevOps, CI/CD or Docker, can we really handle the deployments of lambdas?
Valid valid. But I think there are real benefits and use cases for AWS Lambdas. I‚Äôve written a number of them in my life, and have many running in Production as I write this.
So why would a Data Engineer choose an AWS Lambda?
If you want a cheap solution that embraces serverless and reduces complexity, than Lambdas are your friend.
Cheap.
Easy to use.
Drives better CI/CD and DevOps practices.
Fits well into an event-driven problem space.
Do you have smaller-ish CSV files, or any files at all, that you store in an AWS bucket that requires some processing? AWS Lambda is perfect for that.
Do you have some data quality checks you want to run on files stored in the cloud? AWS Lambda is perfect for that.
Do you have some small task that needs to run a schedule and execute some quick and easy logic? AWS Lambda is perfect for that.
Trust me, you can always find use cases once you start looking. Technically most things you run on an Airflow Worker can probably fit and run on an AWS Lambda ‚Ä¶ and it‚Äôs serverless!
Basically what it boils down to for Data Engineering teams is that if you want to reduce architecture complexity, and cost, and drive better DevOps, CI/CD, and coding skills ‚Ä¶ AWS Lambdas are your friend. They are like the little engine that could.
Of course, they will not fit every case. Nothing over 10GB unless you are streaming, and runtime no longer than 15 minutes. But, there are plenty of niche use cases in Data Engineering where files are being manipulated, bash scripts are being run, and little scripts kicking off to do this or that. All those things are perfect for the AWS Lambda, give it a try!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
134871671.using-rust-to-explore-distributed,Using Rust to explore Distributed Systems,Doing is Learning,2023-07-17 10:16:21.001-05,"One thing I find has been an advantage for myself over the years, especially early on in my career, is the ‚Äúdoing‚Äù part of learning. Sure, read a blog post, watch a video, whatever, but at some point, you have to pick up the proverbial keyboard and do something. Anything.
Doing is learning.
It doesn‚Äôt have to be fancy, you don‚Äôt have to build the next Apache Spark, in fact, I think this sort of mindset holds so many people back. It‚Äôs like going to the gym, the hardest part is putting your shoes on and taking the first step. It‚Äôs mental.
Today, we are going to take a page from our own book, and do just that. Learn something by doing.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Refer a friend
Distributed Systems.
So, how is it, someone like you and me, average Engineers walking through the data wastelands, could possibly learn something about Distributed Systems by ‚Äúdoing‚Äù something with them? With code. Sounds impossible or daunting? Not so fast.
When we want to immortalize some concepts in our minds, concepts that help us understand, not perfectly, but in essence, some topic, there is no better way to learn than to do. So what‚Äôs a fundamental piece of Distributed Systems ‚Ä¶ of all sorts. How about ‚Ä¶
communication between nodes
.
Node Communication
We know that any MPP or other Big Data Distributed system relies on a cluster or set of computers/servers/nodes that must talk to each other and be coordinated in some manner of fashion.
This is a core tenant of working with and understanding systems like Apache Spark, probably the premier Data Engineering distributed system.
Unfortunately, or fortunately, depending on your viewpoint, new tools like Databricks and EMR have made Spark so easy to use ‚Ä¶ you can almost forget about all those poor little nodes running around doing the work for you!
I think it‚Äôs important to not overlook the complexity that is all around us, hidden away over the years, stuffed under the bed. Sure, we don‚Äôt need to be an expert, but we should at least acknowledge what‚Äôs going on.
Since you‚Äôre probably tired of me espousing my ideas, let‚Äôs do a ‚Äúsimple‚Äù thing ‚Ä¶ with Rust.
Node Communication with Rust.
Ok, so if we want to start out simple, as simple as possible ‚Ä¶ what do we need?
We need at the most basic level we need some sort of ‚ÄúNode‚Äù that can listen for a message, and then process it. I suppose it‚Äôs not all that dissimilar in theory to a streaming system. We want to be able to send a message from ‚Äú
outside
‚Äù a process, and that process needs to listen and recognize a message was sent and take action.
Imagine the complication of such a simple thing in Spark! Data and queries being sent, processed, status and messages going back and forth ‚Ä¶ dang! Gives you some appreciation doesn‚Äôt it?
Where to start with Rust.
Share Data Engineering Central
In
Rust
I always start with a struct. This is no different. I simply need a
Scheduler
, which will be a
process
that runs on some machine.
This little thingy will in my simple case need an
address
and
port
to listen on, and name, just because.
Ok, easy enough. Let‚Äôs implement a `
new
` to create access into our new
struct
.
Ok, so it‚Äôs all been super easy so far, but now we actually have to write something.
What we need now is to enable the ability to listen for TCP messages on a port we would have defined as our address to listen on.
What a wonderful tool Rust is! It lets us do this sort of work fairly simply. What‚Äôs going on in the code above?
create a
TcpLister
and bind it to that address and port.
start to
loop
through that
lister
waiting for something to happen.
unwrap that
stream
into a
Buffer
.
If we get anything fill the
buffer
into a
Vector
or results.
Convert results into a
String
and print the results.
Of course, we need a small method to run this whole thing.
And putting it all together!
Here we try to bind to local host on
port 1024
, and then run. Now all I do is
a cargo run
.
I can now see my
Scheduler
is listening on that port.
Now what I need to do is send a message from just another terminal to that address and port, and see what happens inside our
Scheduler
once we do this.
Here is the command I sent.
And what happens on the
Scheduler
when we do this?
Well, it worked! Who would‚Äôve known? We were able to send a message over to our node that was waiting and listening, and it processed the message we sent.
Thoughts on Rust and Distributed Systems.
I think exercises like this are incredibly important for Data Engineers to take at some point. Sure, none of us are going to write an Apache Spark replacement in Rust, although that would be nice.
But, we can surely spend some time thinking about Distributed Systems, since we work with them day in and day out for years. Having some fundamental knowledge, even if theoretical, of what‚Äôs going on behind the scenes is a wonderful thing.
It helps us appreciate those tools and their complexities a lot more. We would probably stop complaining so much.
I‚Äôm looking forward to extending this code and exploring more concepts of Distributed Processing!
https://github.com/danielbeach/skein"
133995104.most-data-engineers-are-mid,Most Data Engineers are Mid,Stuck in the doldrums? Read on.,2023-07-19 08:32:57.632-05,"Well, you might not think it‚Äôs a very nice thing to say. But that doesn‚Äôt mean it isn‚Äôt true. I‚Äôve worked as a Data Engineer for a long time, worked with many of them, and interviewed many of them, and I think my sample size is just right.
Most all the Data Engineers that I met are mid, at best.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
It‚Äôs actually not hard to stand out these days. I‚Äôve experienced endless piles of electronic resumes washing over me with boredom and single-minded sterileness, like the endless crashing of the ocean on some desolate island shore.
I‚Äôve stared in wonder at many bulbous and deformed creations like some Frankenstein electrified in a gloomy dungeon, code, SQL, and ideas that have spewed forth, bubbling rancid cauldrons that devour the souls of Data persons unhappy enough to step into them.
All the while, their creators, smile and grin like some youth devoid of understanding, completely unaware of the monsters their own hands have made, that are about to turn and tear them apart.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
No one wants to be mid. At least I don‚Äôt think so. Is it my old age getting to me? Maybe I‚Äôm becoming the old grumpy person in the neighborhood, sitting on my digital porch of sorts, bemoaning the state of things.
What does a mid Data Engineer look like?
Mid Data Engineers are easy to spot. Why? Because they are everywhere, all around us. The reality is you will rarely meet an Engineer who doesn‚Äôt think they are a gift to humankind. There must be something in the Data water we drink, so strange sickness that takes hold of people. That seems to be the only explanation for the heinous crimes I see perpetrated against our precious Data.
I‚Äôve always caught that path that is closest to the edge, against the grain. This rambling will be no different, I will call out what I see, and exhort others to abandon their evil ways and come to the light.
Contrary to popular belief, a mid Data Engineer is very easy to spot in the wild. You don‚Äôt even need your binoculars. I can sum that mid Data Engineer up with a few bullet points.
Never Act
Poor Quality
Never Learn
Lone Wolf
So what about those of us who don‚Äôt want to be mid? Well, we can just train ourselves in the reverse. Let‚Äôs go through each of them.
Never Act
The mid Data Engineer never acts. What does this mean? It means constant hand-holding, never-ending questions, and the ability to NEVER make a decision or just try something. Stagnant. Sitting. Never moving.
There is a difference between careful planning and not doing anything. There is a difference between thinking about a problem and being unable to move forward. Code is dirty, Data is dirty. Working in data is a master class in flexibility and divining the opaque.
You can‚Äôt rely on a mid Data Engineering to ‚Ä¶
Just make a decision.
Use their past experience to inform the decision today.
To be a self-starter.
To plan a project.
Just write the code.
Unblock themselves.
See the big picture.
I could go on. Right or wrong, as an Engineer who wants to grow, you‚Äôre going to make mistakes. But the only way you will ever make a mistake is if you are actually doing something.
Failure is learning. Getting yourself through hard spots, even when it‚Äôs frustrating builds the muscles that allow you to move and act in Senior level roles.
It might seem like a small thing, but having an engineer who can deal with ambiguity, make decisions, and just get something done without a lot of fanfare ‚Ä¶ well, that‚Äôs worth something.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Poor Quality
What‚Äôs the next worst thing after a mid Data Engineer who can‚Äôt seem to make the decision to write code? How about poor quality at every level?
Honestly, it doesn‚Äôt have to be code either. It‚Äôs everything. Poor Quality ‚Ä¶
Documentation
Design and Architecture
Code
Testing
Communication
PRs
When it comes to Data Engineering poor quality is a killer. It creates more problems than it solves. Better to not do it, rather than spew out streams of half-done code that either doesn‚Äôt work or only sometimes works. Why? It creates frustration, bugs, and on-call problems, and increases exponentially the difficulty of future development.
Poor quality is hard to deal with. Mostly because I suppose we should be happy ‚Äúsomething is getting done.‚Äù Yes and no. Of course, there are times when we have to move fast and get things done. But, there is a line drawn in the sand somewhere.
Poor Quality is a habit, it‚Äôs a way of life.
It could be simply someone new to a field, maybe, or it could be laziness or simply not caring, who knows? The worst part of Poor Quality? Many times it‚Äôs context-based. It depends on the work environment and expectations around you.
One person‚Äôs crappy code could be the best thing ever in another context! What does it boil down to?
Is the work you are doing lifting the ship or sinking it?
Never Learn
This one isn‚Äôt that hard to guess at. A mid Data Engineer will not learn. Happy to sit like a frog, stewing in some mucky dark pond. It‚Äôs probably the worst place to be, it‚Äôs a hard pit to dig yourself out of.
I‚Äôve heard endless complaints about not having enough time, wanting to have a life, don‚Äôt work so hard, etc. Sure, I get all that.
Spending your weekends on Leetcode is not what I‚Äôm talking about. I‚Äôm talking about an attitude of learning that is natural to your way of life.
Learning means you care. Care enough to simply spend a minimal amount of time over your entire life simply trying to improve yourself by various means in your vocation of choice.
Read a technical book.
Read blogs.
Read the documentation of tools.
Be interested in new things.
Pay attention to the macro-environment around you.
Teach yourself something new here and there.
Notice I didn‚Äôt say to read 15 books a month, contribute to open-source software, write your own blog, read 15 others, and the list goes on.
No, I‚Äôm talking about simply an approach to life in which you want to keep up and better yourself.
If you can‚Äôt find it inside yourself to simply read a book or two a year, read a few blogs, or try something new ‚Ä¶ then I can‚Äôt help you, nor can anyone else.
Mid data engineers don‚Äôt take learning seriously. Things are what they are, they settle in for the long term.
Lone Wolf
Last but not least, the Lone Wolf. Well, I have to be honest with you. I love me a good Lone Wolf. I have a perpetual desire to be a Lone Wolf. I prefer the Lone Wolf.
But, in the world of the
eternal Grug
‚Ä¶ Lone Wolf bad.
The problem with the Lone Wolf mid Data Engineer is that team cohesion and the efficiencies that come along with that ‚Ä¶ are impossible to foster and generate.
mid Data Engineers work by themselves, for themselves.
Lone Wolfs are bad for long-term team happiness.
Lone Wolfs can hog knowledge that needs to be shared.
I‚Äôm not sure what else to say about Lone Wolfs. I love them and hate them. Because I find myself slipping into Lone Wolf status myself.
But don‚Äôt call me a mid Data Engineer.
I will find you.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
132898655.data-modeling-101-part-2,Data Modeling 101 - Part 2,Relational Databases vs Lake House,2023-07-10 09:00:20.81-05,"An astute reader pointed out recently that I had failed to follow up on
Part 1 of Data Modeling
. Much to my chagrin, I must return to this turbid and opaque topic of Data Modeling. Part 1 was just a general brain dump on Data Modeling, how to approach the problem, and other high-level thoughts.
While I think the high level is important, knowing the ‚Äúwhy‚Äù of doing something ‚Ä¶ I think it is time we delve into the innards of the beast, and see what fancies we can tease out.
Like most things in Data Engineering, Data Modeling is one of the topics full of intrigue, grey lines, and chaos, with no clear direction in sight.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Relational Databases vs Lake House
We can break down Data Modeling into two major areas. Big Data and Not So Big Data. Otherwise known as RDBMS (Postgres, MySQL, SQL Server) vs Lake House (Delta Lake, Iceberg, Hudi, Parquet Files, blah).
There is, of course, bleed over between the two, but, generally speaking, there are fundamental differences that are large enough between the two underlying technologies, that they both really require their own set of methodologies and Data Modeling to be truly effective.
What is the same about RDBMS vs Lake House Data Modeling?
Personally, as someone who started out designing RDBMS systems for years, both Data Warehouses at scale in both SQL Server, MySQL, and Postgres, and now for these last few years, Data Models in both parquets ins s3 to Delta Lakes with hundreds of TBs of data ‚Ä¶ there are many base Data Modeling concepts that apply in both situations.
Ok, so get to it already. What‚Äôs the same?
Data Modeling Baseline
Let‚Äôs start with some Data Modeling baseline ideas and concepts that will ensure you can tackle a large range of problems. I would also like to insert a comment here, that may be controversial.
Most of Data Modeling, of all types, boils down the the 80/20 rule. 80% of general Data Modeling will get you in a good spot, the last 20% is specific to the technology you‚Äôre working with. (RDBMS vs Lake House).
What are those simple baselines that are the same between RDBMS and Data Lake modeling?
logical normalization
business use case/access patterns
physical data localization
data types
constraints
‚Äúgrain‚Äù of data
These items listed above are concrete steps that have to be taken when starting to unravel some mystery to solve a data model. You can‚Äôt skip them no matter if it‚Äôs Postgres or Delta Lake.
Let‚Äôs see if we can sus out some rubber meets the road Data Modeling ideas from the bullet points. We will eat the elephant one bit at a time.
Logical Normalization.
Normalization, ah, that curse word that has started arguments and stirred up bad blood for years in the old days when Oracle, SQL Server, and Kimball Data Warehouse Toolkit roamed the earth. It was always a tricky and fraught subject.
A part of me is actually sad I don‚Äôt hear about this topic much, normalization that is. It‚Äôs incredibly important, doesn‚Äôt matter how deep you jump into the normalization pool. If you‚Äôve been around classic RDBMS databases for any amount of time you‚Äôve heard of probably at least
Third Normal Form
. Let‚Äôs ask ChatGTP what it thinks normalization is.
‚ÄúNormalization is a systematic approach to decomposing tables to eliminate data redundancy and undesirable characteristics like Insertion, Update, and Deletion anomalies in a relational database. It is a multi-step process that aims to simplify the data structure while maintaining data consistency and integrity. The main objective of normalization is to divide a large table into smaller (and less redundant) tables and to establish links among these tables.‚Äù
Well said my unfeeling AI electrons friend. That sums it up fairly well. If you‚Äôre still confused, the below diagram shows you on the left not-very-normalized, and on the right, very-normalized.
Of course, there are official 1-5 normalization levels with their own definitions for those who care about such things. I think it‚Äôs important to understand the different levels of normalization if you are not familiar with them, but in the end, I think you should stick to ‚Äú
Logical Normalization.
‚Äù This applies to both RDBMS databases and Data Lakes in a Delta Table.
Break up a data set(s) up logically.
Many times files or OLTP systems skew towards ‚Äúeverything together.‚Äù
Normalization reduces data redundancy
Think of the problems with duplicate data and updates for example.
Normalization focuses on the use case of a table, reducing complexity.
The normalization process forces understanding of data relationships (keys etc.)
You think about primary, foreign, or composite keys.
In essence, breaking down and bucketing data into logical tables with normalization is a key part of starting to
understand your data
, which is a grossly under-practiced art.
Business use case/access patterns.
Share
Very closely related to the last topic of Logical Normalization, is the theory or concept of Business Use Cases or Data Access Patterns. This is typically the point in the process where the ‚Äúhard‚Äù questions are asked.
This is where ambiguity goes to die.
The good part about what has happened in the Data Modeling process up to this point has all been conjecture and theory rooted in the data lying before you. But, at some point, reality has to set in. We have to ask the question of ourselves, and our end users ‚Ä¶ ‚Äú
Please kind person, pray tell, what do you want from this data set?
‚Äù
This is probably the most important question, and least often asked on any Data Team. Silly kinda.
Typically the answer to the question, if really asked to the end-users and business, is often different than we imagine or think. Why? Because they simply approach life from a non-engineering viewpoint, unlike us, and have a very different idea of what is possible, what they want, and what is of value.
Imagine the real use cases and understanding you can derive from these questions. Of course, they will affect the data model!
You may logically normalize your tables in a different matter to support the data access time-series problems that your end user said is the whole point of the data set.
Physical Data Localization
The next step in the Data Modeling process, be it Data Lake or RDBMS table in Postgres, is the idea of the locality of the data in relation to itself.
It‚Äôs a big deal. Have you tried to do some queries on a Postgres table with a few million records and no indexes? How about a Delta Lake query with a few billion rows and no partitions?
Now, some might argue that indexes don‚Äôt really co-locate data together, it depends on the RDBMS in use. But it‚Äôs typically the underlying question that really matters. You have a bunch of data that you are modeling, you found out what the use cases and access patterns are ‚Ä¶ what next?
You have to think about how to locate data together in a way that supports, not hinders analytics and queries.
Think about indexes in RDBMS.
Think about Partitions and ZOrder in Delta Lake.
Think about scanning 10‚Äôs of thousands of files looking for the needle in the haystack, or you need to gather all data from a certain time period. Anything that can reduce complete scans. When working on a data model, use every tool possible in relation to your technology, RDMBS, or Data Lake, to ensure data is modeled locally together whenever possible, in theory, and in where ‚Äú
data meets the disk.
‚Äù
Data Types and Constraints
I‚Äôm going to lump the next two, together, for the sake of brevity, it‚Äôs getting long already. Another basic tenet of Data Modeling, which is often skipped, and left for later, is the thought process around Constraints and Data Types.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
But, they are each in their own right, extremely important to the Data Model being thought out. Why? Because Data Types affect things like JOIN speed, lookups, aggregations, and how much downstream processing is required (do you have to cast some STRINGS to INTs all the time?).
Constraints are also critical to the Data Model because they require you to actually know something about the data you are modeling. If you don‚Äôt know what the constraints are of the data, you haven‚Äôt done your job.
CHECK constraints (say, is this value in this set).
NULL vs NOT NULL
BETWEEN and RANGE
Simple constraints enforce the integrity of the Data Model, that it is valid!
In the same way, Data Types also enforce our view of the Data Model in the real world, and have a big impact on performance. Running a JOIN on 10 million records on columns that are INTEGERS vs STRING will make a difference at scale.
Grain of Data
The last, least known, and most important step in Data Modeling is understanding the ‚Äúgrain‚Äù of the data. I‚Äôve seen it happen many times, someone spends tons of time on, weeks, on developing a large data model, of many tables, only to be found out.
The grain of the data was wrong.
Someone tried to answer a question, drill down, and ‚Ä¶ oops, no can do.
‚ÄúThe grain of the data describes what is the lowest level of detail that a indivudal record describes, in a table‚Äù
This is critical to a good Data Model. Being able to describe at what level an individual record exists in relation to its counterpart records, is very important.
Why?
Because you must know what makes each and every record unique. You also must know how this record relates to others, and most importantly, how you can, or cannot roll up and provide analytics based on this data set
.
Agreeing on the level of detail provided by the Data Model might drive, for example, more rollup or summary tables to be added to the Model to support Analytics and necessary use cases for Analysts that are unencumbered by confusing nuance.
Closing Thoughts
And here I end my tail of Data Modeling basics. It‚Äôs a strange world we live in. One day a person can be modeling some tables in Postgres, another day in Delta Lake. Are they the same? How do they differ?
I mean the underlying technologies between an RDBMS and a Lake House could not be any different. They are complete opposites. Yet, Data Modeling after all these decades, has it really changed much? Sure, we have vendors telling us to do this and that, mostly for their own good, not ours.
Having designed many Data Models of all different shapes and sizes, over both technologies, I have found that it‚Äôs strikingly similar. Covering the basics as described above is a must.
But, at the same time, each technology does provide its own nuances that must be understood, or chaos will ensue. Delta Lake is not Postgres, and Postgres is not Delta Lake.
As I continue to flesh out this series on Data Modeling, I hope to dive into examples of both RDMBS and Lake Houses, say Postgres and Delta Lake, and examine them each in depth."
132816662.merica-celebration-50-discount-for,'Merica Celebration - 50% Discount for Life!,Do it. Now.,2023-07-03 12:57:39.444-05,"Thanks for subscribing to Data Engineering Central. It‚Äôs the American Dream getting to write content for you all!
Here‚Äôs a 4th of July Gift from me. 50% off subscription forever!
Get 50% off forever"
132661657.sql-joins-where-clauses,SQL Joins + Where Clauses. ,Beowulf and Grendel. Perfect Match or Monsters?,2023-07-03 12:45:10.863-05,"Have you ever read the story of Beowulf and Grendel? It‚Äôs a tale of two ancient titans locked in a battle of wits and death. You almost feel bad for the evil Grendel, wanting him to win. Beowulf is the hero, prideful, and in the prime of life. It sort of reminds me of SQL Joins (Beowulf) and Where Clauses (Grendel).
Each is wonderful in its own way, capable of helping us out of inevitable bubbling and Moorish quagmires of data we find ourselves in. But combined, we find a peat bog of sinking sand and watery traps of seduction waiting to pull us down into Sheol.
SQL Joins and Where Clauses. It‚Äôs what we cut our teeth on years ago when meticulously playing around on Postgres or SQL Server, tuning our queries like stringed harps, and sending our beloved compositions into the heavens. Each is easy enough to use, together a blade sharp enough to draw blood on the most experienced Data Engineer when they make the foolish decision to relax their mental hold on the tasks at hand.
What tale will we spin today? Tales of using SQL Joins and Where Clauses together, and the evil that can lurk down that dark path, devouring the unsuspecting.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Let‚Äôs dive into our topic.
Combing two SQL ideas while keeping the faith.
SQL is one of those tools that is like the broadsword on the side of every living Dark Ages person, it‚Äôs practically and appendage. As Data Engineers in the age of SparkSQL, Snowflake, and BigQuery, it‚Äôs barely even mentioned nowadays, it‚Äôs taken for granted. It‚Äôs part of life.
If you don‚Äôt write SQL, you're worse than nothing practically, and liable to be burned at the stake as a heretic.
But, the funny thing is, although there may be many an infographic floating around telling you the Order of Execution, or the difference between an INNER JOIN and a LEFT JOIN, there is the grey middle ground that gets passed over.
That middle ground is the intricate way SQL ideas can be combined and mixed with a seemingly unending swirl of madness until the result is hardly recognizable and requires grand powers of intellectual all-seeing wizardry to sus out the true meaning of ‚Äúa thing.‚Äù
Examples of SQL Joins and Where Clauses.
You still might think I‚Äôm beating around the bush, and being long-winded, which I am, but let‚Äôs talk about the data where the rubber meets the road.
You
will
see what I‚Äôm getting at.
First, I will have you know, I‚Äôve spent a decade writing the most mind-numbing SQL statements even to be imagined, like some Alice in Wonderland living nightmare. I gladly left those troubled shores some years ago to find greener pastures elsewhere. I still write my fair share of PySpark and Polars SQLContext, but many battle scars still remain that ache when I write too much SQL.
Let
me
ask
you
a question for once.
I have two tables, any tables, make them up. What happens if I
LEFT JOIN
Table 1 with Table 2 on Column A, with a
WHERE
CLAUSE asking for records WHERE Table 2, Column A is
NULL
?
What is the answer?
What if I do the same thing ‚Ä¶ expect with a
RIGHT JOIN
? What is the answer? What about
FULL OUTER JOIN
? Then what?
Tricks in your head.
How‚Äôs that ole‚Äô head of yours holding up? Were you able to answer all those questions right away? When I said
RIGHT JOIN
, then
FULL OUTER JOIN
, did you stop to think? Were you coming up with a mental model, thinking it was right, then re-checking it in your mind?
Isn‚Äôt this supposed to be easy? It‚Äôs SQL after all.
But, I‚Äôm sure you know as well as me, the Devils in the Details. One little miss-step is all that is needed. One moment of slippage and your whole set of analytics comes crashing down around your head. You will soon be the target of some angry user and feel foolish yourself that you made such a ‚Äúsilly mistake.‚Äù
Examples.
Let‚Äôs do something easy. Castles and Knights.
So many ways to answer questions with SQL is there not? Sometimes life has more than one path, just as with SQL queries. Maybe something that starts out simple, turns out to be complex.
How many of our mighty castles have no brave knights of their own? Poor doomed castles.
Why
Foe Hammer
castle of course. What if we changed the query to a RIGHT join, what happens, can we get the answer?
No, we cannot. Not possible. RIGHT JOINs only return rows where there is a match on the right side. I know all of this is elementary SQL, stuff you learn when first cutting your teeth in data.
But, honestly, most of the ‚Äúproblems‚Äù or ‚Äúbugs‚Äù that I see people struggle with, even those who‚Äôve been writing for years is the classic ‚Äú
Scope Creep
‚Äù of SQL statements.
Most data folk are over-worked. They write this or that statement, there is jumble and bumble of requirements, they change three times over, someone asks to add this or that. Before you know it, someone has forgotten ‚Äúwhat‚Äù the user is really looking for. They get caught up in the details, the complexities, trying to solve the problem without stepping back.
Let‚Äôs look at a simple example of how this mismatch of SQL and ‚Äúwhat‚Äù a user is looking for becomes muddled in clarity. People talking past each other, and a seemingly simple request and SQL query ends up being ‚Äúwrong‚Äù to someone.
In our
castles
and
knights
example, the Thane of Castles comes to us and says ‚Ä¶ ‚Äú
On pain of death, I demand to know which one the castles in my Kingdom are ready to fight the bloodthirsty invaders. I know I have a few Lords, Dukes, and good Sirs running the show. Tell me where they are located and how many?
‚Äù
Easy you say. I could do this in my sleep. But, I don‚Äôt want to be thrown in the dugeon and torchered for doing something wrong, so I will do a
FULL OUTER JOIN
just to make sure I get everything.
Also, since you want to be sure, you had the query to your lowely court Page who adds some stuff to the
WHERE
clause to ensure we are capturing the Knights who meet the requirements of the Thane of Castles. You unwittingly say, ‚Äúlooks good!‚Äù
But, truth be told you unwittingly dropped the castle Foe Hammer, a large castle, because at the moment no one was stationed there. The question is, will you get your head lopped off by the Thane of Castles?
Probably depends what kind of day he‚Äôs having. You might have given him exactly what he‚Äôs asking for ‚Ä¶ but after all, he lives a life of luxary and ease ‚Ä¶ does he really know what he wants? He probably wants to know about ALL castles before the great invasion, and won‚Äôt be happy he didn‚Äôt know about a large castle he owned, sitting empty.
So what‚Äôs the answer? A query that starts out one way, answering a specific question, gets modified, added too, messed with, all the while that original ‚Äúwhy‚Äù and ‚Äúreason‚Äù for that SQL gets farther and farther away from the truth.
Simple changes in JOINS or WHERE clause addition or subtraction can and will, change answers. And, most of the time those pesky business and product folk only half know what they are asking for.
Before you loose your head.
The examples we looked at reminded us of how easy SQL is to write, how well versed we all are in it, but also the trick way that all the different parts of a SQL statement, especially the
JOINS
and
WHERE
clauses work together.
I‚Äôve been doing SQL for decades, and most SQL for the business is way more complex. Many more tables, dirty data, a myriad of filters, call combining into one evil spirit that is looking for a person to destroy.
Business changes their mind, or the business doesn‚Äôt know what to ask. Either way all the little decisions we make to tweak this or that ‚Ä¶ well, they mean nothing unless we can marry that logic back to
the original question that was asked
.
Are we still answering the same thing in a helpful way that isn‚Äôt hiding data from the users, things they would never know or think about as the non-SQL experts. Asking these questions and thinking about what we are doing as Data Engineers is important. It‚Äôs when we take our eyes of the prize and something becomes mundane and easy that we do something inadvenrant, and get our proverbial heads lopped off."
132019227.invite-your-friends-to-read-data,Invite your friends to read Data Engineering Central,Referral Program!,2023-06-29 20:11:48.267-05,"Thank you for reading Data Engineering Central ‚Äî your support allows me to keep doing this work.
If you enjoy Data Engineering Central, it would mean the world to me if you invited friends to subscribe and read with us. If you refer friends, you will receive benefits that give you special access to Data Engineering Central.
How to participate
1. Share Data Engineering Central.
When you use the referral link below, or the ‚ÄúShare‚Äù button on any post, you'll get credit for any new subscribers. Simply send the link in a text, email, or share it on social media with friends.
Refer a friend
2.
Earn benefits.
When more friends use your referral link to subscribe (free or paid), you‚Äôll receive special benefits.
Get a 1 month comp for 3 referrals (plus my Ebook)
Get a 3 month comp for 5 referrals
Get a 6 month comp for 25 referrals
Visit the leaderboard
To learn more, check out
Substack‚Äôs FAQ
.
Thank you for helping get the word out about Data Engineering Central!"
130308691.mlops-101-model-training,MLOps 101 - Model Training,For Data Engineers.,2023-06-26 10:57:35.661-05,"I‚Äôve been looking forward to coming back to my series on MLOps, the basics, the stuff everyone thinks is rocket science, but is not. At least the Data Engineering part is not.
Last time we talked about Feature Stores
and their critical role in the Machine Learning lifecycle. The next obvious step in the process would be to train a model of those features, at least that‚Äôs the typical workflow. At least that‚Äôs usually what those crazy Data Scientists do next.
Rocket science, I know, we have to create an ML Model before it can be used. Anyways, it might sound simple, and it sorta is. But, then again it sorta isn‚Äôt. There is a lot of proverbial creepy crawlies scurrying and bustling under that log when you look underneath it.
Well, let‚Äôs get to it.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Model Training Basics
Model Training. It‚Äôs the next thing that happens after the Feature Engineering step we talked about in a previous post. Once you have a
pile o‚Äô gold
, in the form of features, the next step is to do something with them.
Typically at this point, some Data Scientists will start the R&D work of training model
s
. Notice the plural in that last sentence. Model
s
.
This is an essential point for Data Engineers who are new to Machine Learning Ops, repetitive tasks, that is.
Think about it, it‚Äôs what we do.
We automate everything. In essence, that‚Äôs what we do with Data Pipelines every day. We get data, we transform, we store, rinse and repeat.
The model training ‚Äúpipeline‚Äù if you will, should meet a few basic requirements (much like any other pipeline).
Repeatable
.
Model training in the beginning and after production will be done
many times over
.
Tracking
Every
input
and
output
should be tracked meticulously.
Easy
Training models should be easy as changing parameters and kicking off a job via a
command line
or a
UI
(like Airflow).
What does this look like in real life?
I think the model training lifecycle of MLOps might make a bit more sense if we do some pseudo code, so we can talk concretely about the important aspects of the process, rather than in simple theory.
Let‚Äôs pretend we are tasked with taking a SparkML regression model into ‚Äúproduction.‚Äù This means some Data Scientist has come up with an idea, been doing POC work, we have features and are now at the point where they want to run many iterations on training a model. They are tired of manually changing data sources, tracking models, figuring how what the performance was, etc.
Basically, they need a system to easily train models in a reproducible and trackable manner!
Let‚Äôs look at some pseudo-PySpark code for training a model. Then we will go through it step by step, and talk about the important bits.
Don‚Äôt worry about the code itself, but what the code is doing, that‚Äôs what matters.
The Big Picture about Model Training.
I know it‚Äôs a lot of pseudo-code, but let‚Äôs break down the big picture. Think about running this process a few times over and being able to answer questions about how it was run. What jumps out at you as important?
I‚Äôm just going to make a giant list, I‚Äôm assuming you‚Äôre intelligent engineers, since you are reading this after all.
There are a bunch of input parameters to model training that should be configurable.
Training and test datasets and/or uris.
Where to save a model that is produced.
Where to save model results.
hyperparameters.
specific features.
There are a bunch of parameters to control the output and track the output.
Where the model is saved.
Where to log all information for the job run.
train, test, models, results, and parameters used in the run.
Not shown, but how to actually run or automate the job. (Airflow for example).
This can all widely vary based on use cases of course, and our example is simplistic and contrived, but, I assure you, the basics of Model Training are mostly what we‚Äôve talked about.
It‚Äôs imperative Model Training is configurable, to allow flexibility when re-training. It‚Äôs also of upmost important that all information related to the meta-data of Model Training be saved and logged, so they can be related and analyzed later.
Maybe you store all the data in Postgres, Delta Lake, or write it to a TXT file for crying out loud. Anything is better than someone later asking the question ‚Ä¶
‚ÄúHow was this model trained? What were the parameters, and what dataset was used to train it? What did the performance look like for the model when it was built? Where is the model stored?‚Äù
These questions are critical, and mostly the reason why ML projects fail to make it to production. Product, Marketing, Engineering, everyone starts to ask questions about a model being used or developed.
Typically the above workflow we‚Äôve talked about is haphazard or simply doesn‚Äôt exist. It might not seem like a big deal, but these small things being missing lead to confusion, lost of trust, errors, and the general death of a project.
Getting the simple stuff right.
Making things configurable.
Make training easy to run.
Tracking every input and output.
These are just general engineering tasks that should be applied outside of MLOps as well, so it really isn‚Äôt a big deal. It‚Äôs just about knowing them Machine Learning context, what parameters and outputs are important in the context of training a ML model.
Did I miss something? Tell me about your MLOps experiences training models. Got horror stories? Share!"
129114638.everyones-talking-about-vector-databases,Everyone's Talking about Vector Databases. I tried one.,Introduction to Vector Databases.,2023-09-11 09:21:10.341-05,"You knew I could not resist, just couldn‚Äôt do it, like a fly to light and bee to its honey. As soon as the first murmurings and mutterings started to rise and fall about Vector databases, I took note. Something new, something shiny, the promise of a challenge? Count me in.
It‚Äôs hard to so no to something new in tech isn‚Äôt it? The human spirit is eternally drawn to the novel and the new. We need it. As a Data Engineering ‚Ä¶ I need it.
I‚Äôm starting out at ground zero. I know next to nothing about
Vector databases
, other than I‚Äôve heard on the street that it‚Äôs being
used for feature stores and possibly LLM training data, which would explain the sudden popularity.
Either way, I‚Äôm going to kick the tires and find out what‚Äôs going on.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What‚Äôs up with Vector Databases?
Since I‚Äôm not that smart, we are going to let the interwebs tell us the scholarly definitions.
‚Äú
A vector database is a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. Each vector has a certain number of dimensions, which can range from tens to thousands, depending on the complexity and granularity of the data.
‚Äù
So what?
‚ÄúThe main advantage of a vector database is that it allows for fast and accurate similarity search and retrieval of data based on their vector distance or similarity.
This means that instead of using traditional methods of querying databases based on exact matches or predefined criteria, you can use a vector database to find the most similar or relevant data based on their semantic or contextual meaning.‚Äù
Interesting. Makes sense why the rise of LLMs led to the rise of Vector databases. Storing all those tokens used in the Large Language Models probably.
Understanding Vector Databases by using one.
When all else fails we should revert to trying something out, and see how it feels and smells, simply using something teaches more fundamentals than all the theories in the world.
What is Chroma?
‚Äú
Chroma is the open-source embedding database
.
Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs
‚Äù
‚Äú
the AI-native open-source embedding database
‚Äù
What are embeddings?
‚Äú‚Ä¶
text embeddings measure the relatedness of text strings.
-
OpenAI
‚Äù
‚Äú
Embedding something turns it from image/text/audio into a list of numbers ‚Ä¶ This process makes documents ""understandable"" to a machine learning model
-
chroma
‚Äù
Okay, got it.
So most people probably use Vector DB‚Äôs for ‚Ä¶
text ‚Üí embedding ‚Üí vector db ‚Üí search
This makes sense
in the LLM world we live in
. If you have tons of text, or whatever else, computers can easily understand that information when it‚Äôs converted into numerical form.
It also makes sense for ‚Äúsimilar‚Äù searches ‚Ä¶ aka find stuff that is close to ‚Äúthis‚Äù or ‚Äúthat,‚Äù that storing those pieces of text for example, as embeddings inside a vector database suited just for that purpose ‚Ä¶ well that would give great usability and performance benefits, especially in complex and expensive LLM pipelines.
Using chroma Vector DB.
Of course, like everything else in this world, there is a Python package for that. We can simply pip install chromadb.
It‚Äôs usage is very straightforward.
Create a Client
Create a collection
Add documents or whatever to that collection
query the collection and find things that are ‚Äúclose‚Äù to what you‚Äôre searching for.
Here is a simple example, I mean who doesn‚Äôt believe in Bigfoot?
You can see in the above example results are returned, which also includes the distances, or how close these stored documents are to my query string.
It does a good job and gives the closest answer that ‚Äú
Bigfoot lives in the woods
‚Äù to my question ‚Äú
Where does Bigfoot live?
‚Äù
The Obvious
It‚Äôs clear to see how useful Vector DB‚Äôs could be, especially in the LLM world that is eating everything. It does seem to be a specific use case, probably not very common to a lot of people, but yet very powerful indeed.
It made me wonder what the use cases for Vector Databases in a Data Engineering context would be, outside of LLMs.
I couldn‚Äôt think of that many. I guess in my experience Data Engineers mostly work with tabular type data. Working with audio/video/raw text that would need to be put into a Vector Database for similarity search seems like a small subset of us.
Other Vector Database options?
So what other open-source Vector Database options are out there?
Milvus
Weavite
Pinecone
deeplake
Just to name a few, honestly, the list just keeps going. I‚Äôm sure in the future as LLMs slowly make their tangled way into our Data Engineering world, we will have to use and learn them more, but right now it just seems like the hot new toy.
Little use to everyday Data Engineering (someone will probably make me eat my words), but probably something we will have to use more in the future."
125938792.mlops-101-feature-stores,MLOps 101 - Feature Stores,The Unsung Hero of Machine Learning,2023-06-12 07:28:23.559-05,"Previously, we discussed the basics of
MLOps
, things that are core tenants of Machine pipelines, but yet are apparently not cool enough to be given much attention by the talking heads.
But, never fear, I have come to rescue you.
You've been living in George Orwell's 1984, oblivious to what's going on around you. Dutifully swallowing the drivel that Machine Learning is too complicated for the mindless masses, and far out of reach. The truth is far from that, especially for Data Engineers.
Today, we will be paying special homage to one of the unsung heroes of ML, that is,
Feature Stores
.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
We will delve into
What are feature stores?
A preview of open source and other options.
How to use them.
How to build them.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What is a Feature Store?
Ok, so first I‚Äôm going to give you the boring textbook talk about a Feature Store in the context of Machine Learning, and once we‚Äôve finished with that, I will give you to
real-life
download.
A Feature Store is a piece of Machine Learning architecture designed to manage and serve machine learning features across different projects and models.
A Feature Store serves two primary functions:
Feature Storage:
It stores, organizes, and manages features ‚Äî individual measurable properties or characteristics of a phenomenon being observed. Features are often stored in a database or other storage systems.
Feature Serving:
It serves features in a consistent and efficient manner to machine learning models in both training and production environments.
Feature Stores act as a bridge between feature engineering (transforming raw data into a format that is compatible with machine learning algorithms) and model training, ensuring that the same feature transformation logic is applied consistently.
Here are some of the key benefits of using a Feature Store:
Consistency:
A Feature Store helps in maintaining consistency across the training and serving environments, which is crucial for reliable model performance. It ensures that the same features are used during both the model training phase and the model serving/inference phase.
Reusability:
With a Feature Store, machine learning teams can reuse features across different models, eliminating the need to recreate or recompute the same features again and again.
Efficiency:
It saves time and resources by reducing redundant computations and data processing.
Discoverability:
It allows teams to search and discover existing features that can be useful for their current projects.
Monitoring:
It provides monitoring and management capabilities for features, such as tracking feature distributions over time and alerting on data drift.
While Feature Stores are becoming increasingly important in machine learning operations (MLOps), they also pose new challenges, such as managing the life cycle of features, ensuring data quality.
Real-Life Machine Learning Feature Stores.
Now that we have the boring stuff out of the way, I want to give some high-level comments and learnings from my years in the ML world. Also, keep in mind that the flow chart I showed above, half the chart is just normal Data Engineering, the ingestion, and transformation of data.
I have a lot to say and limited space, so I‚Äôm just going to make a giant list of things you should know.
Use cases for ML Feature Store vary widely and depend on the context.
One of the main ‚Äúbenefits‚Äù of a feature store is simply it‚Äôs a centralized location of trusted feature data.
It‚Äôs common to have feature stores that range from s3, Postgres, DynamoDB, Delta Lake, and everything in between.
Trusted Feature Stores increase the ability to develop and serve models 20x vs some data mess.
Feature Stores give a window into why models are acting/predicting a certain way.
Feature Stores are often backed up by complicated and complex data pipelines.
How, why, and where a feature came from in a Feature Store end up being almost as important as the features themselves.
I could keep going, but you get the idea.
Preview of Feature Store Options.
This is where you will find the world of MLOps, especially with Feature Stores, starts to show its ugly little cracks that have been hiding from you. Googling your options will pretty much get you nowhere, but more confused.
Depending on the use case and type of data the actual technology selected for the Feature Store is going to vary widely, and contrary to popular belief, the MLOps world is still very much in flux, the tools are sparse, inadequate, and limited.
Feature Stores
I‚Äôm not sure what else to do besides just run through the list of options on the table, and say a little blurb about each.
Databricks Feature Stores
Feast (open-source)
Postgres
DynamoDB
s3 (cloud storage)
Have others you use? Tell about them in the comments.
Databricks Feature Store
Probably one of the newest feature stores and one with great promise are
Databricks Feature Stores
.
Databricks Feature Stores
are probably one the, if not the most scalable option for a Feature Store that needs to deal with big data.
Databricks Feature Stores are built on top of
Delta Lake
, and therefore able to deal with TBs of feature data with ease. A Databricks Feature Store probably makes the most sense if you are using Big Data, and Machine Learning with Spark, or other distributed systems that require scale.
It would be excessively hard, although not impossible, to use something like Postgres when you‚Äôre churning out 50 million features+ a day. Besides scale, what else can you expect from Databricks Feature Stures?
Access via a UI, for exploration, etc.
Lineage in the form of models etc that are using those features (more important than you think at scale).
Tight integration with model serving and scoring.
Feast (open source).
Next, one of the other popular options that will appear in your feature store quest will be
feast
. It seems to be only of the only reasonable open-source options.
It‚Äôs available on
GitHub
and has about 4.5K stars. That makes it a serious contender, you know you‚Äôre not getting a half-baked product at that level.
What‚Äôs interesting about Feast is they have a paid-for version, of course, just like anyone else, so it makes you wonder at the long-term commitment by the money mongers to the actual open-source project, say what you will, but it‚Äôs always a worry.
What is there to know about Feast in a quick download? It‚Äôs a Python project.
pip install feast
You can create a feature repository, register features, and kick off a UI. It would obviously require some CI/CD and configuration for production, but I‚Äôm sure you are smart and could overcome that obstacle.
There is a wide range of data sources and offline sources, enough to keep anyone happy.
Also impressive are the options for deployments, AWS and Kubernetes. And the Feature Serving client is available of course in Python, with even a Golang option on the way! Impressive.
Interested in learning more,
read through some nice documentation
.
DIY Feature Stores
For a lot of my career in the ML space, it‚Äôs been the DIY option. Why? Mostly because 5 years ago there just weren‚Äôt that many well-known options, and the options available were less than ideal.
Honestly, sometimes the biggest challenge is that ML pipelines and requirements vary widely from business to business and use case to use case. It‚Äôs just hard to write MLOps pieces that can be applied across all projects in real-life ML.
I‚Äôve used both Postgres, Delta Lake, and s3 as a feature store, each with its own challenges.
Designing your own feature store isn‚Äôt hard, it just requires some foresight and planning.
Postgres and Delta Lake work great for tabular features.
Delta Lake for scale.
Postgres for medium-small data.
s3 or other cloud storage for non-traditional features.
can be combined with Postgres to extend metadata and management.
Closing Thoughts.
Honestly, Feature Stores aren‚Äôt as hard or complex as you think. The very act of creating a central store for prepared features brings enormous value and efficiency.
Instead of Data Scientists spending hours and days searching, building, and depositing features into a single place themselves, before even doing the hard work, pointing to a single location of prepared features is a game changer for R&D and other model development.
Most Data Teams could probably just get away with coming up with a special schema in Postgres, if the data is at scale, with Delta Lake. The more serious use cases will be something like Feast or Databricks.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
124189713.dealing-with-stress-anxiety-and-hardship,"Dealing with Stress, Anxiety, and Hardship in the Workplace",Data Teams can be rough. How to overcome.,2023-06-05 08:15:54.459-05,"Photo by
nikko macaspac
on
Unsplash
It‚Äôs probably been happening since some caveman was assigned the task of smashing rocks or gathering firewood. Workplace stress, anxiety, and general hardship. These problems do seem to be commonplace and even exaggerated on Data Teams, although everyone probably says that about their work.
With data comes ambiguity, unknowns, high expectations, alerts, on-call, complexity, disagreements and the like. All these problems devour Data Team members with an unrelenting fevor on a daily basis with no end in sight.
This a common in tech, who knows why? There is probably a myriad of reasons as varied and colorful as the people upon which these misfortunes fall.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I would like to think that over the last decade+ of working on Data Teams myself, I've learned a few things, in every situation from khaki pants in a glass highrise with a financial institution to a startup with beer on tap.
Surprisingly, it doesn‚Äôt really matter where you work on a Data Team, corporate, startup, and in between, the problems are the same, with only slight differences.
Setting Some Ground Rules.
We need to set the playing field a little bit. I‚Äôm not anti-work, far from it. I actually think most people probably don‚Äôt have a good enough and more positive view and relationship with their work. In our case Data Work. We should be proud of what we do, it‚Äôs unique and important, and
data makes the world go round
. We are part of that.
Humans are human.
We all get stressed.
We all overwork.
We all make mistakes and mess up.
Stress is very bad for us.
Mentally stress destroys us.
Physically stress destroys our bodies.
You're not special or immune to this.
Work can be good.
It can be fulfilling.
It can give us a sense of accomplishment and purpose.
It can be fun and exciting.
Stress and Anxiety in Data Teams.
I think stress, anxiety, and general workplace discomfort can come from multiple sources and some common patterns can be found in them. Some of these sources come from inside us, others from without.
Much like life, there are good times and bad times, times you are walking on clouds and feeling invincible, and other times want to be a hermit locked away in some cabin deep in the woods, nary to be seen again.
Also, some of these sources of frustration are because of actions, behaviors from ourselves, and others from other persons and cultures. It‚Äôs a big topic, and a hard one to solve, so let‚Äôs just jump in and find out the truth of the matter.
Let‚Äôs define some of the major sources of stress and anxiety in data teams first, then talk about how to overcome them.
We need some sort of framework within which to have this conversation and bucket these ideas. So here is an imperfect start that will help us cover the wide range of topics we need to overview.
We will start at the 20,000-foot level, way up high, and work our way down to your very soul, and everything in between. Let‚Äôs get some of the obvious stuff out of the way.
Workplace Culture
Photo by
Redd F
on
Unsplash
It‚Äôs worth mentioning that no matter who you are and what kind of Data Team you work on, no matter how positive you can be, some things cannot be overcome.
Workplace culture is one of those classic ideas and environments, that most likely, you have very little to no control over.
‚ÄúThis may not be a popular statement, but I have experienced this myself, there are some places and circumstances that you just need to leave.‚Äù
If you‚Äôre expected to work
60+ hours a week indefinitely
and frowned on when you
leave or clock out at 5 pm
, or when
don‚Äôt respond to Slack messages at 10 pm
, but pay for it later ‚Ä¶
you simply need to leave.
You as a human have inherent worth and dignity.
Don‚Äôt trade your life and happiness for money, not a good trade.
You are going to die someday, is your misery now worth it from that point of view?
Culture is a big deal. If you‚Äôre in a bad culture, go find a different one.
Work Issues and Personal Issues.
As we start to chip away at the rock and delve our way down into the human soul, we come across a fork in the cavern. T
hat is, workplace issues, and personal issues.
When talking about stress and anxiety in our work as Data folk, we have to be cognizant that these issues bubble up from the ground and spring forth from two different sources.
Workplace
Personal Life.
We can‚Äôt escape it. Often it‚Äôs the very intermingling of these two topics that boil and roil inside us like some very real supernatural Kraken brought up from the depths. We are human, we work, and the two are going to collide at some point.
Money and financial issues.
Health issues.
Family and kids.
School
Friends
Culture
These issues swirl about us and spill over into our workplace, and that workplace is filled with others who are dealing with the same issues.
In fact, the workplace is like a microcosm of the greater world around us.
Politics, dysfunctional family, raises, promotions, demotions, deadlines, failures, successes, culture. Work is life and life is work. We take it from both sides. Like a castle set on a hill, we don‚Äôt know from which direction the next attack will come.
Photo by
K. Mitch Hodge
on
Unsplash
How to stabilize the Personal side of the equation.
The truth is, we are going to be less stressed and have less anxiety at work if we can learn to address and deal with our personal life a little better.
When we look, feel, and function like a healthy human beings, it‚Äôs a fact that most likely our life in the workplace will probably blossom like the spring wildflowers, still blown and buffets by storms, but always springing back up into all our glory when the clouds pass.
I would like, if I may be so bold, to give some advice to you about things you can do at the personal level to ensure health and happiness that will by all means reduce stress and anxiety at work.
Also, I‚Äôm telling you things that I‚Äôm willing to do myself, and things I practice, so these aren‚Äôt empty ideas. (
I work full-time, have multiple side hustles, hobbies, raise a family, and have many close friends I spend a lot of time with. How? Read on.
)
Get your
finances
and
money
under control.
Check out
Dave Ramsey
for paying off debt and setting a healthy budget.
Get physical
exercise
every single day.
Check out the
Centr
app. It‚Äôs cheap and makes exercise fun and easy.
Breathe the
fresh air
, and get
outside
every day.
Go for a walk outside you hobbit.
Control
your news and
social media
intake.
Delete social media apps from your phone. You can use a web browser.
Make time for
personal development
.
Take classes and read books. (Udemy, FrontEnd Masters, Coursera, Educative.io, Kindle Store, blah, blah, blah)
Eat
better.
Cook
your own
food
.
Stop going out to eat, you know what healthy food is, eat it.
No nothing of this is rocket science. Yes, these are probably the hardest things you could ever decide to do with your life. They won't be easy at first, but once they've found their place in your life, you will be a totally different human, a much more happy and resilient human.
The truth is, these simple things like exercise, fresh air, and eating better, give us a goal, a purpose, and meaning, that is separate from our vocation. Acting upon these instincts, say running a mile, gives us a sort of freedom, a freedom of will to do as we please, and gives us a sense of ownership and control over a slice of our personal kingdom.
Being a measly peon chained to the overlord of Facebook, Instagram, the News, and their incessant pinging and beeping, glued to the dull glow of a screen, mindlessly for hours of precious time that will never return, well that is a sad fate indeed my friend.
Be different, turn the world into your oyster, and grasp the bull by the horns, even if it pokes you in return.
You will assuredly turn into a different person, and by default, a different and more resilient person at work.
How to stabilize the Work side of the equation.
Now that we've seen what we can do on the personal side of the equation to keep work and life stress at a minimum, let's talk about the elephant in the room, The
Work demons
that keep us enslaved to stress and anxiety.
Unfortunately, I have no secret sauce to sell you, no witch's brew of potions to *poof* your work life into some new bliss. That isn't going to happen, and that leads to the first piece of advice I have for those dealing with stress and anxiety at work.
Face the beast.
Seeing the Work Beast
Photo by
Geran de Klerk
on
Unsplash
You must face your fears, that standup, the meeting, that person, that boss, the deadlines, whatever. ‚ÄúNothing has overtaken you but such that is common to man.‚Äù It is true.
There is comfort and ease to be found in the fact that many unknown millions have come before you and will come after, have also faced and struggled against the same work beast.
The work beast comes in several common forms.
Workload
Communication
Growth
If you can wrestle and master the work beast in these three forms, you will truly be like conquers of old, a knight in shining armor. These three are the crux of the matter and drive towards the heart of what gives us
stress
and
anxiety
.
Dealing with Workload
These topics are really as simple as the personal lifestyle choices we talked about before. But they are tricky, that's what makes them stress and anxiety inducers. How can you deal with overwhelming workloads that never seem to end?
Know that the work never ends.
Accept you are human with limits.
Speak up when overloaded.
Be better at task planning and estimation.
Do your best and let go of the rest.
Have laser focus when working.
Avoid the ‚Äúside‚Äù pulls.
Say ‚Äúno"" more often.
It's not always the case, but many times our work-related stress and anxiety can be at least partially, if not wholly, due to ourselves and our human nature.
That nature is to please. We want to make others happy, we want to portray an image of smartness and genius. But, that comes at a price. You have to be able to say no. Understand there will always be the next urgent request, since when has there not been?
Practically speaking there are loads of things you can do to reduce your workload so you can do better and more productive things.
It's simple. Don't underestimate your work and its complexity. Know your own limits and stay within them. So no more and avoid the noise. Have a laser focus on the task at hand.
The reason so many people are stressed is that their work life is chaotic at best, a nightmare at worst. No direction or focus always fires to put out, and nothing gets done on time. Break the cycle, slow down, and get more done.
Communication
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
This one is short and sweet, you've probably become tired of hearing people talk about it. I'm going to take a different angle though. I'm not talking about conflict resolution. What I'm talking about is simply learning to speak your mind, within reason of course.
You will feel better when give your opinion.
People aren‚Äôt mind readers.
Your work relationships will improve the more you talk.
People will learn from you, and respect you.
You will become more comfortable with yourself.
In short, you are human, you were built for communication and relationship, you can‚Äôt be at your best with all your ideas and thoughts rattling around inside and never coming out. Speak!
Do you have a strong opinion about some technical direction? A concern about something going wrong? Then bring it up, and talk about it. Speaking something into existence is better than leaving that ‚Äúthing‚Äù to grow and become something its not in silence.
You simply need to talk more to those around you at work, and your stress and anxiety will lessen. Once you realize you're all in the same boat and rowing in the same direction.
Professional Growth and Development
The last piece of the work puzzle that causes stress and anxiety is many times well founded or can be. At the very least some problems can be solved with a little elbow grease and work.
People have stress and anxiety about things they don‚Äôt understand, aren‚Äôt good at, and are new.
The hard truth is that working in Data Engineering or tech, in general, can be hard because there is always some new piece of software, idea, or whatever, there is always something popular that you don‚Äôt know about. But it doesn‚Äôt need to stay that way.
You have to learn. Grow. Develop yourself and your skills.
Take a class online.
Read a book.
Watch some YouTube.
Go to a conference or attend a virtual one.
Keep tabs on your industry.
It really isn‚Äôt that hard or rocket science at all. If you stagnate and don‚Äôt learn or grow, you will of course feel some stress and anxiety. As you well should.
Some people think it takes hours or all this time they don‚Äôt have. No, it doesn‚Äôt. I‚Äôm pretty sure you‚Äôre wasting hours on a phone or in front of a TV. I‚Äôm also certain you can read a blog post or two on your lunch break. Watch a video after work while you‚Äôre relaxing on the couch. Pick up a book.
Sometimes I think it takes more work to not care and not grow than it does to learn something new. There are a few benefits to learning and growing in your space.
You will feel better about yourself.
You will perform better at work.
You will be smarter.
It‚Äôs not like you have to be the smartest person around, that‚Äôs not the idea. It‚Äôs the idea of being ‚Äúwell-versed‚Äù in your profession. You know what‚Äôs going on in the industry, what tech is being talked about, how people are approaching and solving problems.
It will make your life easier in general, that is usually the case with learning more.
In Conclusion
That was probably a little long-winded, I think all in all it‚Äôs an important topic to cover though. I‚Äôve met way too many stressed-out and therefore perpetually unhappy people in my work life.
A little levity can go a long way.
Be happy, slow down, be healthy, and be wise in your personal life, have balance. Then apply those same principles to your work life."
123102646.new-mojo-lang-for-data-engineering,New Mojo Lang for Data Engineering,What's all the hype? Is it real for the average Data Engineering workload.,2023-05-29 09:31:09.287-05,"I‚Äôm not sure the new
Mojo lang
has what it takes. It‚Äôs hard to say. I haven‚Äôt even used it yet. They sure are trying hard and have been doing lots of marketing and hype-pushing.
If you live in the forest under moss and rocks like a hermit, don‚Äôt we all wish, and haven‚Äôt heard of the new Mojo language, well good on you. But the rest of us are putting up with it.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Could it possibly be true? Someone has replaced Python with a fast version of Python that includes static typing and immutability ‚Ä¶ but yet it is still Pythonic and interoperable with Python as we know it?
Seems like a tall order.
‚ÄúMojo is a new programming language that bridges the gap between research and production by combining Python syntax and ecosystem with systems programming and metaprogramming features. Mojo is still young, but it is designed to become a superset of Python over time.‚Äù - Mojo
Still, you think about how popular Rust has become, but it has been a slow burner, rising up through the ranks and finally breaking through in all its glory.
Summarizing Mojo so you don‚Äôt have to.
Photo by
Saurav Mahto
on
Unsplash
I'm not really interested in doing some ‚Äú
hello world
‚Äù crud with Mojo. There is plenty of content starting to come out. What I‚Äôm going to do, if I can, is try to see what Mojo means for the
Average Data Engineer
.
I will give a little time to the approach of Mojo, and what it
feels like
as a language, but I want to focus on its application for general Data Engineering, and if we should care about it.
What‚Äôs the deal with Mojo?
Here is some stuff about Mojo that you should know about.
You can import and write normal Python modules/packages and code.
It looks and feels like Python.
Mojo wants to bring ‚Äúsystems programming‚Äù to Python.
Mojo adds `
let
` and `
var
` function declarations to give immutability, or not.
Mojo adds those famous `
structs
`.
The ability to have
static typing/checks
at compile is available.
Addition of `
fn
`s instead of `
def
`s for more immutable, static, and generally stricter methods.
Mojo brings the concepts of
borrowing
and
ownership
to immutable `
fn
`s, in a Rustacian sort of twist into Python.
Here is a quick and dirty of the above.
let a = 123 # <- this immutable ... can't be changed later.
var b = 'ABC' # <- this is mutable, can change to anything at any time.

from PythonInterface import Python
let pd = Python.import_module(""pandas"") <- wala, Python as you know it.

# structs  ... aka Classes.
struct Bing:
    var ding: Int
    var dong: Int
    
    fn __init__(inout self, ding: Int, dong: Int):
        self.ding = ding
        self.dong = dong
    
    def hey(inout self) -> None:
        print(self.ding+self.dong) 
        
b = Bing(1,2)
b.hey()
>> 3
Initial Thoughts.
My initial thoughts are a little all over the place. One part of me says if you‚Äôre going to be something, be something ‚Ä¶ but Mojo seems to take the middle ground. It‚Äôs neither Python nor Rust, it‚Äôs both, they had a baby and called in Mojo. Which is surprisingly confusing.
Honestly, I found it harder to write Mojo the first time than when learning Rust. Some of the concepts are too muddied together, like the `Struct` which is basically a Python Class, except not, more immutability and definition has to be complete at compile time.
The introduction of immutability via the `
let
`s was fine and welcome, but the strange implementation of Structs that act more like classes, combined with the unclear imagination of static, immutable, borrowed, and mutable concepts requires strangeness like `
inout
` you see above.
In essence, Mojo has managed to neither be Python nor Rust, but a strange mix, that appears to be more confusing than anything.
I have a funny feeling Python developers are going to shy away from the ideas and syntax, mostly because they are using Python for a reason ‚Ä¶ that reason being flexibility and ease of use. If they wanted compile time type checking ‚Ä¶ they would be using Golang, Rust, or Scala already.
I sort of get it.
Look, we all get it. Python is its own worst enemy. It‚Äôs beautiful and expressive, allowing rapid prototyping and the like. Hence the entire ML world is carried on its shoulders.
But it‚Äôs also slow, and error and bug-prone. It appears Mojo is trying to solve those problems. Give speed and the ability to immutable and static typing along with all those benefits.
Annoying.
The most annoying part about trying to learn Mojo is the horrible documentation. I mean, as someone who‚Äôs been writing Rust I understand a lot of concepts and how in general they would be beneficial and improve the performance and generally reduce bugs etc,but trying to sus out even how to use the Structs, or scroll through the built-in modules specific to Mojo (not Python), was a terrible experience.
For example, we Python folks and Data Engineers use lists quite often. Apparently, Mojo gives us a customer
ListLiteral
‚Ä¶ what I would use it, how I would use it, and why I should use it are a complete mystery.
Apparently, the only thing Mojo wants you to care about is that it is ‚Ä¶
faster than Python (like that takes a lot).
immutability and static typing available.
I guess everything else is just a thing you might not care about.
Again, I was scrolling through the modules trying to figure out file reading for example, maybe something in the IO module where we could read a CSV file and then be able to do something with it?
Yeah, no, apparently you can only print from this module.
Mojo for Data Engineering.
Well, I‚Äôm not really sure where to go from here. I will try to stay positive and not be Gandalf Storm Crow about the whole thing. There are some parts of Mojo that of course will be helpful to Data Engineering, as they would to programming in general.
immutable vs mutable.
static typing options.
structs.
borrowing and references and the like.
it‚Äôs fast ‚Ä¶?
I don‚Äôt know, those are the reasons I use Rust, and it‚Äôs a joy to write. Mojo is not, it‚Äôs a confusing amalgamation. If I want Python flexibility I will use Python, if I want fast, immutability, etc I will use Rust. Because those tools focus on ‚Äútheir thing‚Äù and do it well.
Mojo doesn‚Äôt do either well.
‚ÄúWe expect that folks who are used to C++ and already use MyPy-style type annotations in Python to prefer the use of
fn
s, but higher level programmers and ML researchers to continue to use
def
.‚Äù - Mojo
So what‚Äôs the point, to force both people to use the same language?
Let‚Äôs try a Data Engineering Task in Mojo.
Ok, let‚Äôs not. I wrote enough Mojo to decide against it. It just feels strangely wrong writing it. Is it because I write both Python and Rust, so this feels like heresy? Probably.
Go
read the docs
for Mojo yourself.
I do try to do some normal data engineering for you.
tried reading some files from s3 to do some stuff, no go. Mojo couldn‚Äôt handle boto3.
uploaded a CSV file and tried to read it with Mojo, no go, Mojo crashed.
After reading the rest of the documentation, I‚Äôve come to the conclusion if you would like to build your own Machine Learning models and algorithms from scratch, Mojo might be your thing, otherwise, steer clear. Maybe they will fix some stuff before the final release.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
122530116.celebrating-3k-subscribers,CELEBRATING 3K+ SUBSCRIBERS!,50% discount on subscriptions.,2023-05-19 13:51:18.044-05,"Thanks to everyone for following my content and reading my data ramblings. We passed 3K subscribers and are growing fast. Thanks!
Always remember, enjoy life, data is only a means to a happy end!
Take a break from the grind and get outside.
As a thank you, here is a link for a 50% of discount for life.
Get 50% off forever
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
122061710.data-modeling-at-scale,Data Modeling at Scale.,Lessons from the trenches.,2023-05-22 09:35:16.338-05,"Photo by
Tzenik
on
Unsplash
The age-old battles, I carry the scars. Times have changed, and technology has changed, but challenges and bitter hand-to-hand combat have not.
I found, and have the wounds to prove it, that there are few things in the life of a Data Soldier as contentious as Data Modeling.
There are memories I have buried deep in the dusty recesses of my mind, memories of conference rooms, whiteboards, angry and red-faced zealots scribbling furiously, diagraming their way to The Promised Land.
Such is Data Modeling. There is no room to take prisoners, it's all or nothing, my way or the highway.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Yet, with the rise of Big Data and tools like
Delta Lake
instead of SQL Server, things are no better than they were before. If anything, Data Modeling finds itself in a more confused state than ever.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Big Data ‚Ä¶ Data Modeling.
Photo by
Drew Hays
on
Unsplash
Now we come to the land mine at the center of the noman's land, who dares to step on it?
It's clear that Data Modeling confusion reigns supreme in the world of Big Data. Life was much simpler when we all used Postgres, MySQL, or SQL Server. The data models were set, OLAP vs OLTP, and Kimball reigned supreme over the Data Warehouse. Life was easy, the battles were mostly fought and won.
Should Big Data Modeling be different than RDBMS modeling?
Is there a ‚Äústandard"" for data modeling at scale?
What aspects of Big Data affect the data model?
These are questions that are never answered or talked about much. Everyone is supposed to have a Lake House, Data Lake, or Data Warehouse using these new technologies like Snowflake, Delta Lake, or Iceberg ‚Ä¶ yet at the same time, the data world is silent on Data Modeling.
The world is still SQL based.
The other caveat about the whole mess is that the world, all these decades later, is still driven by SQL. SQL still overruns Data Engineer by enlarge.
Every tool worth its weight has a SQL API, mostly because people won't use it otherwise.
(
You Need To Break Up With SQL
) ‚Üê
Read This
So what?
So who cares? Well, because both the underlying technology behind Big Data storage tools, the computing tools, and the How of interacting with these tools, combined are going to press us towards certain types of Data Models.
The New Data Model ‚Ä¶ that is Old.
So, have Data Models changed, is there a new type of Data Model you should be using?
Yes, and No.
Yes, there are things that have changed, affected by the new underlying technologies that require us to approach Data Modeling differently. No, in the sense that specific logical Data Models are so effective that their concepts cross technological boundaries.
This or That.
Let‚Äôs forget about OLTP for now, most Data Engineers work on OLAP models. Things for Data Warehouses, Data Lakes, Lake Houses, and many sundary and various other analytical and machine learning needs.
Most all the material (not put out by vendors) is generally either star schema or snowflake, is around 15-20 years old, and made with Postgres and SQL Server in mind.
Sure, we can still apply many basic principals about facts and dimensions to our new world. But does the rest work that well for 300TBs of data stored in Delta Lake?
Classic Star and Snowflake schemas were built with other techs in mind.
You know, the problem with most technologies and design principals is that the lines are not clear. Where one thing wanders off into another, and then back again, it‚Äôs so obvious.
The question real is ‚Ä¶
At what point does a star or snowflake schema designed for Relational Databases break down for the Data Lake at scale?
This is the million dollar question.
The star schema from something like the Data Warehouse Toolkit, Ralph Kimball style, is a great and wonderful idea that teaches us many things about data modeling, and lots of concepts from this ideology can be applied in something like Delta Lake.
But, there is always a but. How far do you take it?
The star schema works well in a relational database where indexes reign supreme and joins run like a hot knife through butter, blazingly fast. You can normalize your data into eternity and things usually work out well.
But what about with 300TBs of data in cloud storage?
Data Modeling at Scale.
This is where the lines get a little blurry.
Here are some general thoughts and experiences I‚Äôve had with data modeling outside of classic relational databases, when storing hundreds of TB‚Äôs of data in could storage with a Delta Lake or Iceberg type technology over the top.
You will pay dearly for large joins.
This means normalizing like a relational database isn‚Äôt always wise.
Data types start to matter more at scale. Joins on strings take longer than joins on numbers.
You can‚Äôt rely in indexes to make things fast, because they don‚Äôt exist, like you could in Postgres for example.
Fact tables are still extremely important.
But they should hold more dimension type information where possible to avoid later costly and compute heavy joins.
Lots of small dimension tables is probably not wise.
Simply put, don‚Äôt normalize into oblivion like you normally would.
It may sound a little strange but you start to regress at scale, in the data model, from what most people have been taught their whole life.
Suprisingly, data modeling at scale, moves back towards the OLTP model from a purely OLAP and thrid-fourth normal form model.
To summarize, most likely you should find your data models at scale to fall somewhere in the middle.
It‚Äôs unwise to normazlie to data model at scale like you would in Postgres, for the reasons we listed above. Of course you don‚Äôt want to go all the way OLTP for obvious reasons, like updates or changes, but it‚Äôs hard to know where to draw the line.
I‚Äôm curious to hear from my readers, those who work on data sets at scale outside of a relational database ‚Ä¶ what do you do? Do you do what you‚Äôve always done in Postgres/MySQL/SQL Server, do you do something else?"
119944583.async-rust-for-data-engineers,Async Rust for Data Engineers,It's easier than Python.,2023-06-19 07:41:44.184-05,"Rust, like most things in my life, has been a series of short bursts, starts, and stops, ups, and downs. I‚Äôve tried many different languages throughout my life, pretty much the gambet. In the last few years I‚Äôve always kept an eye on what the popular choice is, just seems like a smart thing to do.
At one point it was
Scala
, at another point it was
Golang
.
I found
Scala
to be mainly boring and unattractive, *gasp*, I know. It just didn‚Äôt
feel
that nice, I had a feeling that popularity wouldn‚Äôt last. I feel somewhat vindicated these days. The drop in popularity of Scala has peaked according to surveys, Scala3 made everyone mad and broke stuff, Akka going commercial, and other general mayhem.
What‚Äôs the chance NEW tech is going to be written with Scala? Probably not much.
Golang
. It seemed better than Scala, well, it is better than Scala. Error handling was strange, very popular, good documentation, and seems to be be spreading across the tech world like a weed. Slowly and quietly it has conquered. Still, it felt good to write Golang, but something was missing.
Than came
Rust
. Like a midnight juggernaut, it‚Äôs clearly taking no prisoners.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Async in General.
So, let‚Äôs get too it. Python asycn sucks, it‚Äôs complicated and impossible to use. Not so with asycn in Rust. Why? Probably because Rust doesn‚Äôt let you mess stuff up very easily. It‚Äôs hard to do things wrong. Async with Rust is no expectation to that rule.
What is async?
‚ÄúAsynchrony, in computer programming, refers to
the occurrence of events independent of the main program flow and ways to deal with such events
.‚Äù - wiki
Asynchronous (async) programming is a design pattern that has several advantages, particularly in environments that require heavy I/O operations or those that involve significant user interaction.
Where is async useful?
Typically, async is used for I/O bound processing, as opposed to CPU bound tasks (with might benefits from multi-processing). For example, think about downloading lots of files over HTTP. Probably, you‚Äôre really waiting for the CPU to do any computation, you‚Äôre just
waiting
for another outside system to respond, like return requested content.
That‚Äôs how I think about it in my head. If I have a list or array of files I want to download of HTTP, I could do it by one by one, or I could send a request for a file, and, while I‚Äôm waiting, ask for another one, then switch back to the one I asked for ‚Ä¶ etc. You get the idea.
Rust Async with Tokio
Let‚Äôs work through a very simple example of using async in Tokio with Rust, to show how easy it is. You Pythonistas will be pleasantsly surprised.
Code available on GitHub.
First, we will import the needed crates and setup the URL‚Äôs we want to download.
Divvy Bike Trip‚Äôs open data set
.
The main flow the Rust async problem is very straight forward. We have an vector of URLs we need to download. We can simply
map
those url‚Äôs and apply a
download
function to each.
From out futures crate we call
join_all ‚Ä¶
Creates a future which represents a collection of the outputs of the futures given.
The returned future will drive execution for all of its underlying futures, collecting the results into a destination
Vec<T>
in the same order as they were provided.
Of course we are calling our
async
download_file
function, as defined above. This is what returns the future we used in the
join_all
.
Not exactly complicated is it. I dare say it‚Äôs a little simpler than Python. Async is Rust is straight forward. We can define functions as asyncronis by simply putting
async
in front of a
fn
definition.
Really, the trickest part of async with Rust, which isn‚Äôt that bad, is just figuring out where we have to
.await?
something, but that‚Äôs usually pretty obvious.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Musings
The main goal of my musings on Rust
async
was to open your mind to Rust as a great programming language, which you are probably aware of, and how easy it is super charge already fast code with async, which has been historically a nightmare with tools like Python (which I use everyday by the way).
I feel like part of my job at this point in my life is to talk about, and push people towards hard things, that actually aren‚Äôt that hard, but make us grow as people and engineers.
Always keep learning something new, even something you think is hard, it will pay off dividends in the future."
119275717.become-a-terrible-data-engineer,Become a Terrible Data Engineer.,Simple steps anyone can follow to become the worst you.,2023-05-08 09:41:54.646-05,"Photo by
Sander Sammy
on
Unsplash
Bored? Looking for a little spice in life? Have the desire to destroy some things? Well, look no further, I have the answer for you. Become a terrible Data Engineer, be the worst you can be.  That just gets the blood pumping a little bit, be a nice change of pace.
I mean who doesn‚Äôt want to be a destroyer of teams? Things in life move too fast, want to grind all progress to a halt? No, maybe you just want more bugs and errors to fix, especially on the weekend and at night, forgetting the smooth-running processes that never fail, so boring.
Today I‚Äôm going to teach you how to be a terrible Data Engineering, so listen up, put down your phone, and let‚Äôs be as bad as we can be.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
To be a Bad Data Engineer
Do you need a roadmap to become a bad Data Engineering? Well, you‚Äôre welcome. Of course, if you are of the mindset you want to do the opposite, for some reason, you could in theory become a good Data Engineer.
Do this to be bad.
It‚Äôs really quite simple, after all, being a bad Data Engineer. It takes very little effort and will probably cater to your leanings. Being bad is always easier than being good, being good requires real effort. Who has time?
Do these things and be a bad Data Engineer.
Get distracted, and work as many tasks/tickets as possible ‚Ä¶ AT THE SAME TIME!
extra credit if you can context switch multiple times per hour.
Take OOP to the next level. Multiple layers of inheritance, obfuscate, and shuffle things around. Make it impossible to find or debug anything.
Make your functions and methods as large as possible.
Make your function do as many things as possible. (side-effects)
NO tests are allowed at all. NO unit tests that run locally.
Delete your development environment, or don‚Äôt make one.
Delete every README you can find. No documentation about anything.
Be a Jerk. Clutter PRs with nit-picky comments.
That should get you started anyways.
Photo by
Icons8 Team
on
Unsplash
Boiling it down. What Bad Engineers are Made of.
Ok Ok, maybe that list of stuff was a little too specific for you. I get it, you want a more boiled-down version that gets to the essence of what it really means to be a one-of-a-kind bad Data Engineer.
Again, I think I can help you.
We could probably come up with a few high-level groupings that can help you fit all your bad actions into a few specific ideals.
Subscribe now
Hard to work with.
Spurn best Engineering practices
I told you I would make it easy.
Hard to Work With.
Being hard to work with is an art that must be practiced and can be refined and perfected. You can approach this problem from many angles, and the best hard person to work with is hard to work with in every area of life.
It can be PR reviews, new ideas, meetings, or code reviews, to be the worst you have to destroy them all. And, this will lead to the destruction of the Engineering culture, which is the pinnacle of achievement for Bad Data Engineers.
Spurn Engineering Best Practices.
Closely related to being hard to work with is the concept of Engineering Best Practices, which must be avoided at all costs. Not only are they to be avoided, but the opposite must take place. This is a critical step in being a Bad Data Engineer. If you allow Engineering Best Practices to take place or get a foothold, bad things will happen. Morale would approve, the culture would start to change, and many other tragedies.
It is utterly important that from a
technical perspective
, you do everything possible to break down structure, order, and things that are generally known to increase reliability and reduce problems.
You must turn your nose up at all things automation, clean coding styles, and practices, or any kind of tests whatsoever. Again, all these terrible things tend to bring peace and calmness to otherwise chaotic environments. Avoid them.
Conclusion of being a Bad Data Engineer.
It‚Äôs hard to list every single thing it requires to be a Bad Data Engineer, the list would never end. But, this should give us all a good start. What‚Äôs important to note is that being a Bad Data Engineer
encompasses both technical and non-technical skills.
It‚Äôs important to realize that bad technical descisions impact the Engineering culture, and a single Bad Engineer can deploy an amazing amount of damage to both technical systems and persons.
Go against the grain, be bad. Never focus, hate all things new, grind PRs to a halt, destroy meetings, and avoid commonly held best practices at all costs. If you can do these things, you will be all means enable a culture of chaos, breakages, crisis, bitter and angry people, and many other wonderful things that are hard to describe.
Go forth and destroy."
118895478.mlops-basics-for-data-engineers,MLOps Basics - For Data Engineers.,MLOps 101 - What most people won't tell you.,2023-05-15 15:41:06.826-05,"ml life ‚Ä¶
I still remember quite clearly when I had my experience as Data Engineer working in the Machine Learning space. It was the wild west all those years ago. To say I was nervous was an understatement. My innards quaked and quailed like the trees and grass when that first bit of a storm brushes across the landscape. Trying something new, especially something that seems hard on the face is always hard.
I didn't know what to expect. Like most engineers, I had dabbled around myself in the ML world on my own time, and my thoughts on what Machine Learning actually was, were very skewed. But I didn't know this.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
If what you‚Äôre understanding of Machine Learning is what you‚Äôve read on Medium, than you don‚Äôt know ML very well.
I was a nervous little blighter, one minute convinced I could work my way through any problem, as I had in the past, and the next, convinced this would be my first spectacular failure. I would stumble, my brain would stop, and I would be ridiculed and driven from among the human race, doomed to wander the woods like Radagast the Brown, eating mushrooms and ruminating on my failures.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
But, like most imagined nightmares, the reality was much different. So let me spin a tale of Machine Learning for you, the truth of the matter. The one no one seems to talk about.
Being a Data Engineer and working on Machine Learning problems is way easier than you think.
The truth of the matter.
First, as with most things in life, there is always a sliding scale of difficulty. The same goes for Machine Learning. But, the difficulty isn't what you think.
The above picture is my take on where the ‚Äú
difficulty level
‚Äù of Machine Learning falls in the strata of Data Engineering work. It isn‚Äôt the most difficult work. By far working on Distributed Systems and Big Data is much more challenging. Sure, ML work can be harder sometimes than the general ETL and Pipeline work of day-to-day Engineers.
But in the end, ML work falls somewhere in the middle of it all. Maybe slightly to the right of the center.
What Machine Learning actually is.
Sure, there are some Machine Learning Engineers who are expected to be both Data Scientists and Data Engineer sevant.
But this is the exception, not the rule.
Most Data Engineers working on ML systems are just normal Data Engineers, the only difference being they understand the lifecycle of ML and have specific MLOps experience,
an experience which anyone can learn.
I'm sure you've heard that 90% of ML is data work, while that is true, it does gloss over some very important MLOps work that Data Engineers take on to enable ML to happen at scale, and in production.
This is what
most
ML work done by Data Engineers consists of. No magic here.
Ok, so this figure might not encompass absolutely everything, but it does cover 90% plus of what Data Engineers working on
normal
ML systems, at
normal
companies are going to do.
What you see above really encompasses the most important parts of MLOps, with of course some detail about each one. Here‚Äôs the truth, I‚Äôve worked with putting ML pipelines into production for years now ‚Ä¶
I can count on one had the number of times I‚Äôve had to hyper-parameter tune a model, and do other ‚Äúdifficult‚Äù tasks.
Those are left to the Data Scientist, sure I learn at the surface level what they are doing and why, but the rest of the time is spent ‚Äúhardening‚Äù and automating what some scientist is doing by hand.
So, you want to know what kind of ML model to pick for what use case, want to learn about over-fitting, parameter tuning, and other such things? Then be a Data Scientist or if you‚Äôre a unicorn, be an ML Engineer.
Otherwise, just be a Data Engineer and learn about ML operations. Let me give a quick overview.
Features
Features. They are like the golden nugget of Machine Learning that no one ever talks about. I have always found it strange that the peddlers of Machine Learning greatness on social media rarely, if ever, even mention the word features or feature store.
I mean what good is a model without features to feed to it? How can you train a model without features?
‚Äú
In the context of machine learning, features are individual measurable properties or characteristics of a phenomenon being observed. They are also known as predictors, input variables, or attributes.
Features are used to represent the data in a form that the algorithms can understand and process. They form the ""input"" part of a machine learning system, while the ""output"" is the prediction or decision made by the model.‚Äù
In other words, feature stores are the beating heart of most ML systems and pipelines. There are two option for feature stores, build your own or uses someone else‚Äôs. Some options are ‚Ä¶
Databricks Feature Stores
Feast
(open-source)
Or you could do as I‚Äôve done in the past and simply build your own on top of something like
Postgres
. Because what is a feature store after all?
Simply a single-stop shop for ML features that are ready for production. As long as you catalog how the features were produced, what data produced them, when they were produced, and any other relevant meta information ‚Ä¶ you my friend, have yourself a feature store.
How would a feature store be used in real life?
In the world of Data Science, especially with multiple folks working on many models, a feature store enables you to say ‚Äú
Here are reliable and ready-to-use features.
‚Äù
Also, feature stores can help support fast model building and training by allowing some Scientists to say ‚Äú
Give me x, y, and z features produced between this and that date.
‚Äù
This is the basis and the underlying idea behind feature stores and how they provide easy, quick, and centralized access to a vast time series of features to support quick mix-and-match model training, as well as production workload predictions.
Model Prediction and Training.
Alas, we could pontificate upon features ad-infinitum, but at some point, we must move on. The place to which we now journey is another very basic, seemingly simple, but often ignored topic, simply
the training and exercise of models for prediction, as well as storage for those predictions
.
I know it sounds boring, but honestly, if you can‚Äôt train, predict on a model, or do analysis on what was predicted ‚Ä¶ you‚Äôre in for some angry and grumpy Scientists.
There is a very high probability during the R&D period of model development some Scientists will write a script of some sort ‚Ä¶ to train a model, and then predict that model.
They will also start to run, re-run, and re-run a million times over those same scripts with slight changes. Maybe this or that model, or this or that feature(s). They will predict, analyze, and start all over again.
It‚Äôs a lot of honestly, repetitive tasks. The needs are simple. One or two commands or button clicks with some passable configs to train and predict on models. Also, easy access to metrics from training and prediction runs, with plenty of metadata.
Sound like feature stores? Not that much different, just some engineering excellence around automation, tooling, and tracking of what has actually happened. It‚Äôs just the application is Machine Learning.
Again, build your own or find some open-source stuff. Just depends on your use cases, the size of the company, and the number of models you work with.
mlflow
Automation and MLOps.
So this is pretty much the summary of the first two items, but it‚Äôs so important it‚Äôs worth a little side note. What it really boils down to as a Data Engineer working on ML Pipelines is the automation of everything.
automated training.
automated feature production.
automated prediction.
automated R&D tasks.
automated model deployment.
automated everything.
Really it‚Äôs just about picking the correct tooling or building your own tools, or maybe it is just a mix and match of both. What is important about the MLOps is that all those Operations are automated and configurable, and therefore the friction is reduced to build, test, and deploy Machine Learning models and pipelines.
You want to make it easy. Easy and hard to mess up. This brings value and enables quick iteration and model creation ‚Ä¶ it provides real business value.
What is probably one of the most popular ways to automate some of these tasks? I personally have used Apache Airflow twice to build production-grade ML pipelines and Ops. Why are people surprised Airflow is used? Beyond me. Airflow is used all across Data Engineering, and MLOps is not really any different in the end. Of course, Airflow is used.
Meta Data Storage.
The last piece that has already been mentioned, but bears the worth of being brought up again, is all the metadata storage that comes along with MLOps.
This is a somewhat unique part of the Machine Learning lifecycle that isn‚Äôt talked about much. Probably because it just isn‚Äôt cool or all that interesting.
An astute observer would have noticed in the above discussion all the references to storing data, what happened, how it happened, and relationships between data.
When a model was trained.
What data a model was trained with?
When and how features were produced.
What model produced some predictions?
What features were used to make a prediction?
Metrics around training.
The list goes on. Meta-data storage about what has happened, what is happening, and how a model was produced, its predictions, etc, are at the core of being able to run a reasonably stable ML environment.
What most people won‚Äôt tell you.
That‚Äôs what most people working in ML won‚Äôt tell you. Sure, there are complicated parts, just like with any Engineering task. But, why don‚Äôt you ever read about these basic tenets of good ML on Medium or Linkedin?
Probably because they are mostly boring and fairly straightforward. No magic is to be found. Just simple automation, best practices, tracking, and storage. Things that Data Engineers do on a daily basis anyways just applied to a different class of problems.
Hopefully, if you‚Äôre new to ML I gave you a taste of the real world, a little shove towards something you might have thought out of your reach because it‚Äôs not.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
115955999.the-brittleness-problem-in-data-pipelines,"The ""Brittleness"" Problem in Data Pipelines.",How to identify and solve brittleness problems in Data Engineering.,2023-04-24 09:35:49.309-05,"Photo by
Dominik Scythe
on
Unsplash
Brittleness
. What an interesting topic. What is it? What‚Äôs it like to live in it? How can it be contained and quelled? Data Engineering is hard enough in its own right. The landscape is riddled with potholes and chasms spewing forth putrid bugs and data ready to strike your beautiful pipeline to the ground.
You probably don‚Äôt notice the problem of brittleness in Data Pipelines when starting out your career. The more you‚Äôve been up and down the mountain, like a forlorn pilgrim on a long and weary journey, where every Data Pipeline and Data Platform are the same thing, you start to notice such things.
Brittleness. What is it?
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Defining Brittle Data Pipelines and Architectures.
You might think this definition of brittleness is a little nebulous and hard to define. I don‚Äôt really think that‚Äôs the case. I think you know what you see it, when you experience it. In fact, I think it‚Äôs likely that probably 90% of the Data Pipelines running today and that you and I are working on are brittle.
‚Äú
Brittle data pipelines and architecture are those pipelines that never run consisently because of ‚Ä¶
-
Data Quality issues.
- Many errors and problems go un-noticed for days or weeks.
- Are hard/impossible/overly burdensom to debug and fix.
- Are slow and un-performant, don‚Äôt scale.
- Feature changes and imrovements are hard to make and test.
Bad Stuff
You might have a brittle pipeline if ‚Ä¶
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
You might be thinking, ‚ÄúWhy, I deal with these problems every day.‚Äù I dare say most of us do. But, should we be? There are sundry and various reasons that Data Platforms and Pipelines end up in this sad state of tattered affairs.
Not least of those reasons being ‚Ä¶
Moving too quickly.
Not tackling technical debt on a regular basis.
Not enough project planning and foresight.
Bad engineering culture.
A bad business culture that drives results, without regard to cost.
But again, we could just say that‚Äôs life. It‚Äôs the life some people live without change, like a story that never ends.
How can a regular old Data Engineer, a little drop in the ocean overcome such obstacles?
Overcoming Brittle Pipelines.
Photo by
Michael Dziedzic
on
Unsplash
Courage my friend. Courage, foresight, battle-hardened nerves of steel, sweat, blood, and tears. It is possible to change the overwhelming tide of dark foes that try to overpower your pipelines, your work, and your stack on a daily basis.
It‚Äôs time to put up some bulwarks, man the battlements.  Here are some common and approachable ideas to help you combat these evil brittle pipelines, things that are within reach of the
Average Engineer
.
Data Quality Issues.
Dang, it‚Äôs always the data, isn‚Äôt it? It‚Äôs hard to get control of the data, and what to expect when knowing when things change. Sure, if you‚Äôre working on GBs of data, maybe you can, but in high volume and multiple TB+ systems, the simple task of getting a handle on the data can be hard.
What are some easy approaches to fixing brittle Data Pipelines when it comes to DQ?
Volume monitoring and alerting.
Basic sanity checks of data volume.
Basic sanity checks of important data values.
Using constraints inside data stores.
There are two options when doing such things. You can do it yourself, or you can use some tool like
Great Expectations
or
Soda Core
for example. I‚Äôve written about both, see those previous links.
Just remember, even simple checks will probably get you a long way down the path. Think about how much simple checks could change your pipelines.
// counts
assert df.count() >= 1
// nulls
assert df.filter(""x IS NULL"").count() == 0
// sanity checks
assert df.filter(""length(x) < 23"").count() == 0
assert x in df.columns()
The list goes on. The point I‚Äôm trying to make is that taking little steps can help be a stopgap for brittle breakages due to having no control over the data. Or if you‚Äôre using something like Soda Core or Great Expectations, you can get fine-grained control over your data.
table_name: tripdata
metrics:
    - row_count
tests:
    - row_count > 0
columns:
    ride_id:
    valid_format: string
sql_metrics:
- sql: |
        SELECT COUNT(DISTINCT rideable_type) as rideable_types
        FROM tripdata
  tests:
      - rideable_types = 2
Let‚Äôs move on to some other things that make data pipelines brittle, and how to overcome those issues.
Development Issues.
Photo by
Luis Villasmil
on
Unsplash
Probably one of the single greatest indicators of brittle data pipelines is problems with the ‚ÄúDevelopment Lifecycle.‚Äù What do I mean by development issues? I mean it‚Äôs a nightmare to be a Data Engineering working on those pipelines, you know what I‚Äôm talking about.
Inability/absence of running tests on the pipeline.
Extreme difficulty in debugging.
Extreme difficulty in adding new features.
These are all symptoms of a bigger problem. A piece of junk data pipeline(s). These issues usually point to a stinking pile of code that has been hastily thrown together over the years, and become a tangled web of traps and intricacies. Bad architecture on top of bad code, now that is a recipe for a bubbling cauldron of brittleness that eats Data Engineers for lunch.
What are some of the first things I look for to tell if a data pipeline is going to be brittle?
There is no Docker or containerization in the repo.
There are no unit tests.
The code can‚Äôt differentiate between ‚ÄúProduction‚Äù and ‚ÄúDevelopment.‚Äù
There is no Development environment.
All the code is 50+ lines per method/function.
Zero documentation or README‚Äôs.
No CI/CD exists.
Deployments are hard.
What does it all boil down to? Simply put, there is a complete lack of an attempt at Engineering best practices. Test, containers, CI/CD, clean reusable code, and documentation. It‚Äôs all missing, and it all indicates one single sad fact. The pipeline has brittleness oozing out of every pore.
What I‚Äôm not saying.
I‚Äôm saying that every pipeline has to be a perfect shining and gleaming pile of treasure, as if developed by some savant squirreled away at Netflix or Facebook. What I am saying is that it‚Äôs clear which pipelines are cared for and which are not.
You can read the Engineering Culture by its repos.
If all the problems indicated above exist where you work, then it‚Äôs most likely you work with people who want the world when it comes to data, but refuse to recognize the fact that good things do take some time and effort.
It doesn‚Äôt take much to change direction for the better.
Create a Dockerfile for your repo.
Write a few unit tests.
Write some README‚Äôs.
Make a development environment.
Run `
ruff
` and/or `black` on your code.
Clean up the code, and break it down into small functions.
Nothing listed above is earth-shattering or above your abilities. They will set up a data team and pipeline on the road to redemption. You will fall in love with good practices, and you will reap the benefits of your hard work.
Resources.
Here are some resources to help you on your journey to brittle-free Data Pipelines.
free Data Engineering GitHub template repo.
8 Data Engineering Best Practices
Introduction to Unit Testing with PySpark.
You Only Need 2 Data Validations, That‚Äôs It.
5 Basic and Undervalued Data Engineering Skills
Designing BETTER Data Pipelines.
Share
Leave a comment"
115256206.dsa-for-the-rest-of-us-part-4,DSA For The Rest Of Us - Part 4,Introduction to Graphs and Breadth First Search,2023-05-01 09:20:22.536-05,"Photo by
Edward Howell
on
Unsplash
Well, like a fly to the light, I‚Äôm finally returning to our series on DSA (
Data Structures and Algorithms
). I‚Äôve been slowly, in my much cherished free time, working through the book
Grokking Algorithms
. Highly recommend the book as a great refresher or introduction.
This series is more for me than anything and serves a few important purposes.
Future-proof myself against the rise of AI by ensuring my grasp of fundamentals.
Implement each topic in
Rust, and thereby learn more Rust
Python this time.
Keep me from boredom.
What have we done so far?
Part 1 -
Linked Lists.
Part 2 -
Binary Search.
Part 3 -
Quick Sort.
Now we turn the page of this well-worn book to
Graphs
(Data Structure) ],
Breadth
Search First
(Algorithm), and
Queues (Data Structure)
.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Introduction to Graphs.
Graphs are probably one of the most useful data structures we have, and you can solve all sorts of useful problems. Contrary to popular belief they are really not that difficult to understand as well. At their most basic level graphs typically represent a simple relationship.
Probably the easiest and most familiar way to understand a graph is to think about your friend network. If you have any friends? I hope you do.
There are two fundamental concepts of a Graph.
Node - the item(s) that are in a relationship somehow.
Edge - the relationship between two Nodes.
I think the above image of a network relationship makes it pretty straightforward what a graph is. I suppose you‚Äôre thinking, like me, that could get really complicated, real fast. This is what makes graph algorithms so powerful and useful.
Breadth Search First
While we still have the picture of a friend network in our mind, let‚Äôs talk about BSF (Breadth Search First), the idea of searching through a graph while looking for ‚Äúsomething‚Äù, in a manner where we do it by ‚Äúlevels.‚Äù
In the above image say we are looking for the first person in Bob‚Äôs network whose name starts with a ‚ÄúT‚Äù. BSF means we would check the level 1 nodes first, before moving on to level 2‚Äôs etc.
This probably makes sense in a company hierarchy graph.  Let‚Äôs take a look at the below example.
If you were doing a BSF (Breadth Search First) of this graph looking for a particular employee, you would start searching each Level ‚Ä¶ all the way through, before moving on to the next
lower
level. Hence, Breadth Search First.
Queues.
Photo by
Hal Gatewood
on
Unsplash
This brings up the last topic, queues. You might be wondering what a queue has to do with graphs and BSF, but if you think about it, how are we going to store these data structures and work through them in a logical and ordered manner?
This is where queues come in. We will be using a
deque in Python, you can read more about them here
. A deque is a double-ended queue that gives us quick and easy access to either end of the queue.
This is a great way to track the ‚Äúpeople‚Äù/nodes we are working through when implementing a
BSF
algorithm.
Implementing Breadth Search First with Python.
Without further ado, let‚Äôs jump into implementing
BSF
with Python, mostly because it will be easy. I‚Äôm just going to mind dump the whole thing first, then we will step through it bit by bit.
So, first things first. Since we are using Python this is going to be extra easy. I import a deque and make a
Person
class
to model our human, with a
name
,
age
, and
friend
list.
Next, and logically, we make a
queue
we can use to work through our graph, and a
visited
set, so we know who we‚Äôve checked while working through our graph.
Next, we make a bunch of
Persons
and give them
friends
. Aka we make our graph or network of
Persons
to
friends
.
In our graph case, we want to find the first
connection/friend
to John who is at least 50 years old. How do we do this?
It‚Äôs simpler than one might think. We simply add our first
Person
, John, to our
queue
, and then add him to our visited
set
as well. Then we start to do our
Breadth First Search
of our
graph
.
We do this by simply starting a loop of our queue, and will do this loop either until the queue is empty (we found no one 50 years old or older), or until we find someone.
Here comes the core of the BFS through our graph.
We pop the first person off our queue, then we start to loop through that person‚Äôs friends.
If we have not visited that person before, we put them onto the queue, add them to the visited set, and check if that person is 50 years or old not, returning if they are.
Here are the results from our little run.
If you remember from before when we added friends to our John, who were Jane and Bob, neither of who were 50. But Bob does have a friend who is 50, Alice.
All code is available on
GitHub
.
Closing Thoughts.
Graphs and Breadth Search First are interesting topics because there can be a lot of fear and misunderstanding around them. Sure they can be very complicated topics, but when you approach it as a simple, find a friend in my network, it doesn‚Äôt seem so daunting.
The idea of queues is an important one, as well as creating relationships between objects in code. Even more important is starting to think about how to traverse relationships, in our case with a very simple BSF algo.
Having these sorts of basic data structures and algorithms somewhere raddling around in our minds is good for us, as Data Engineers. These topics should not just be relegated to stale old Software Engineers in some dark basement."
114982334.join-me-on-notes,Join me on Notes,A new place to connect,2023-04-15 07:15:52.179-05,"I just published my first note on
Substack Notes
, and would love for you to join me there!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Notes
is a new space on Substack for us to share links, short posts, quotes, photos, and more. I plan to use it for things that don‚Äôt fit in the newsletter, like work-in-progress or quick questions.
Go to Notes
How to join
Head to
substack.com/notes
or find the ‚ÄúNotes‚Äù tab in the
Substack app
. As a subscriber to Data Engineering Central, you‚Äôll automatically see my notes. Feel free to like, reply, or share them around!
You can also share notes of your own. I hope this becomes a space where every reader of Data Engineering Central can share thoughts, ideas, and interesting quotes from the things we're reading on Substack and beyond.
If you encounter any issues, you can always refer to the
Notes FAQ
for assistance. Looking forward to seeing you there!
Go to Notes
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
112104962.the-truth-about-prefect-mage-and,"The Truth about Prefect, Mage, and Airflow.",The Battle for the Orchestration Future.,2023-04-10 08:16:45.489-05,"Photo by
Chris Tweten
on
Unsplash
Well, even though this is going to be the truth about
Prefect
,
Mage
, and
Airflow
, I do wonder, in 2023, if there is any truth left to be had. The data world is changing, and the pace at which the new tools are being released is mind-numbing. Every week it‚Äôs a new library like
Polars
, or the next hot SQL thing like
duckDB
. It can be hard to sort out fact from fiction, and marketing goup from the real deal.
What about arguably one of the most important parts of a data stack? The orchestration and dependency tooling?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Airflow has long reigned supreme in the Data Orchestration space for what seems like an eternity. With AWS adopting it as MWAA and GCP as Composer, it seemed like the flag had been planted firmly on top of the hill. The Airflow‚Äôens are entrenched and blasting their weapons down upon anyone or thing that dares to climb that same hill in search of glory.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
But, if one thing is sure, it‚Äôs that as soon as something takes the crown of glory and sets it upon its head, somewhere behind the throne the swords and plots are being sharpened.
If Rome fell, does Airflow have a chance?
The new Age of Orchestration is here.
In today's fast-paced business world, data has become one of the most valuable assets. With the ever-increasing volume and complexity of data, it has become essential to have efficient data orchestration tools that can manage data workflows. Three popular data orchestration tools are
Prefect
,
Mage
, and
Airflow
. In this article, we will compare these three tools at a high level, explore the concepts, and help you choose the right one for your data stack.
Honestly, I don‚Äôt care how shiny the UIs are, I‚Äôm sure both Prefect and Mage have better ones than Airflow. That‚Äôs ok. We should probably have other reasons for picking a tool than it‚Äôs like a shiny quarter you found on the ground.
‚ÄúWhat is the true difference between Airflow, Prefect, and Mage? Is it how they approach data flows? Is the integrations? What exactly is it that makes them different, and does it matter?
That‚Äôs what I want to start to answer today. I don‚Äôt care about performance, what I want to get at is how these tools are designed, how they are used, and how
they affect Data Engineers in the way they write and manage data pipelines.
What‚Äôs the difference between Airflow, Prefect, and Mage?
How do these tools approach data pipelines and orchestration?
How do these tools affect the way Data Engineers solve problems?
So, whether they like it or not, I‚Äôm going to lump all these tools into the ‚Äú
data orchestration and pipeline management
‚Äù group. I don‚Äôt care what the marketing hype is, that is what they are used for. Also, I‚Äôm not an expert, I‚Äôm just
an average engineer
trying to do average things.
First, let‚Äôs review what I would call the ‚Äú
core concepts
‚Äù of each tool. Then explore along the way, which tool is the best.
Prefect
Prefect's core concepts revolve around building, managing, and monitoring data workflows. These concepts are fundamental to understanding and working with Prefect effectively.
Prefect core concepts:
Many moons ago when
Prefect
was new I did a review and comparison to Airflow.
Check that out here.
If you go to the Prefect website and start reading ‚Ä¶ nothing big jumps out. It talks about ‚Äú
Building
‚Äù, ‚Äú
Running
‚Äù, and ‚Äú
Monitoring
‚Äù workflows. There is actually a quote from the Prefect website that sums up Prefect very well and is basically the conclusion I came to.
‚Äú
Prefect enables you to build and observe resilient data workflows so that you can understand, react to, and recover from unexpected changes. It's the easiest way to transform any Python function into a unit of work that can be observed and orchestrated. Just bring your Python code, sprinkle in a few decorators, and go!
‚Äù
What I found telling about this quote is that I have not found anything fundamentally different about Prefect, say than Airflow. There isn‚Äôt anything earth-shattering or special. It‚Äôs simply just trying to be Airflow, except slightly better.
It didn‚Äôt excite me. I need a good reason to abandon Airflow‚Äôs massive community.
Let‚Äôs talk about the
core concepts
of Prefect. How do they approach Data Engineering problems?
Tasks
:
Tasks are the basic building blocks of Prefect. They represent individual units of work or operations, such as fetching data from an API, transforming data, or training a machine learning model. Tasks can be created using the
Task
class, and custom tasks can be defined by subclassing them.
Flows
:
Flows are collections of tasks that have a defined order of execution. They represent a complete data workflow or pipeline. In Prefect, you create flows by linking tasks together and defining their dependencies and execution order. Flows are constructed using the
Flow
class.
State
:
State represents the status of a task or flows at a particular point in time. Prefect uses state to track the progress of tasks and flows during execution.
Common states
include Pending, Running, Success, Failed, and many others. State objects are instances of the
State
class or its subclasses.
Results
:
Results represent the data returned by a flow or a task defined above.
Task Runners
:
Task runners facilitate the use of specific executors for Prefect tasks, allowing tasks to be executed concurrently, in parallel, or distributed across multiple resources. Task runners are not mandatory for task execution. When you invoke a task function directly, it operates as a standard Python function, without utilizing a task runner.
Of course, Prefect offers a hosted cloud version of their tool. Remember what Prefect said about writing Python and then putting a decorator on top? This is in essence ‚Ä¶ Prefect. Look at this simple example.
What can we surmise about Prefect based on what we‚Äôve seen? It‚Äôs a fancy and nice GUI, focused on making native Python the tool used to write data pipelines. But what about if you want to run Databricks or Snowflake stuff? It can do that of course. For example ‚Ä¶
from prefect.tasks.databricks.databricks_submitjob import (
DatabricksRunNow,
DatabricksSubmitRun,
)
Remind you of anything? It should, it‚Äôs pretty much a spitting image of Airflow. There is a
big list of ‚ÄúIntegrations‚Äù on their website
, although it is not as large as I would have thought, and some seem pointless ‚Ä¶ like Twitter.
How does Prefect differentiate itself from Airflow?
You might be asking yourself, like me, why would someone choose to use Prefect over Airflow with its vast community, I mean you sorta need a compelling reason to break from the mold.
I combed the website back and forth and the docs. Just a lot of talk about building high-performance data pipelines, but there isn‚Äôt actually anything ‚Äú
solid
‚Äù behind those words. I can‚Äôt point to x or y feature, or design concept and say ‚Äú
This is what makes Prefect different or better than Airflow.
‚Äù
Prefect
does offer some interesting features like
Projects
and
Deployments
, a better way to organize, deploy, and run your pipelines. Is it enough?
I‚Äôm not sure. I personally fail to see how Prefect blows away Airflow in any category.
Of course, the UI is better and has more features, although the more complex something becomes, the less easy it is to use and the learning curve steepens. Maybe it‚Äôs better running at scale? Again, it just isn‚Äôt enough for me to want to invest time or resources into it. Maybe that‚Äôs just me.
I feel like it‚Äôs reliance on simply using native Python for defining tasks and workflows, along with decorators actually hurts it. It tightly couples data pipeline orchestration to the actual code you are writing to transform data sets. Anymore, data is too large to be processed on an Airflow ‚Äúworker‚Äù or on Prefect for that matter, the world is moving towards connectors for tools like Snowflake, Databricks, and the rest. We don‚Äôt actually need or want our orchestration tools to
DO
the data processing.
I feel this is the only, and main difference between Prefect and Airflow. Prefect does away with the DAG abstraction and uses Python and decorators to do your work.
Think I‚Äôm being too harsh on Prefect?
Maybe, but as an Airflow user, I don‚Äôt see any compelling reason I would switch to Prefect. It doesn‚Äôt have any earth-shattering features or breakthroughs that would make me want to switch to it. If I was starting clean would I pick it? Probably not. I want something that has a large community behind it.
Any Data Engineer knows a tool works well for a while, but when it doesn‚Äôt, you need somewhere to turn. Do you have some experience with Prefect? Do you love it? Drop a comment and let us all know.
We disappointment in my heart, onto the next tool.
Mage
Mage
is a data orchestration tool designed for modern data infrastructure. It provides a simple interface for building complex workflows, it‚Äôs also highly scalable and resilient. Mage's interface is intuitive, and it allows you to build and monitor your workflows easily. That‚Äôs what the marketing says anyways.
‚ÄúOpen-source data pipeline tool for transforming and integrating data. The modern replacement for Airflow.‚Äù - mage.ai
Well, I do like honesty. At least they are being upfront about what they are trying to do, replace Airflow.
Before you tell me I‚Äôm an Airflow lover and will never find anything to replace it, hold your horses. Airflow does drive me crazy sometimes, its backend architecture leaves a lot to be desired, can be clunky, and frequently pukes to the point it‚Äôs easier to spin up a new environment, rather than fix it.
I digress. Back to Mage.
Mage has several unique features that make it stand out from other data orchestration tools. For example, it has a built-in data catalog that allows you to track your data lineage, and it has a powerful retry mechanism that can automatically retry failed tasks. Also, it offers a Notebook UI to develop with, offering quick iteration and feedback. Very nice!!
Now, these are features that are substantially different from what we are used to with
Airflow
. It‚Äôs a different approach, and that‚Äôs what matters.
How does Mage approach data pipelines?
Mage Core Concepts.
Project and Pipeline
:
Project like you would think of a GitHub repo, Pipeline contains all the code blocks you want to run, represented by a YAML file.
Block
:
A set of code in a file that can be executed at will. Blocks can have dependencies. The types of Blocks that you would recognize ‚Ä¶
Transformer
Data Loader
Data Exporter
Sensor
Chart
Engineering Best Practices
is a core feature.
Bock reusability.
Automatic Testing of Blocks.
Data Validation is pre-built into Blocks.
Data is core, Blocks produce Data.
Pipeline and Data Versioning, along with Backfilling.
Scalable via Spark.
Develop via Notebook UI.
What does Mage code look like?
What sets Mage apart and makes it different?
One thing is immediately apparent that sets Mage apart from both Airflow and Prefect. While
it appears similar to Prefect in the way it uses simple Python and decorators
to define the bulk of processing, the approach to
Engineering best practices
is where it shines and is completely different from the other two.
Instead of Mage just being another ETL and Orchestration tool with a slightly different take from Airflow (Prefect), Mage tries to fundamentally change the way data pipelines are developed and used, focusing on the developer and Engineering aspects to set itself apart.
What are some things that jump out at me about Mage, without diving into too much detail?
Easy to use Docker setup
, going to make life easy when developing.
‚Äú
Each step in your pipeline is a standalone file containing modular code that‚Äôs reusable and testable with data validations.
‚Äù ‚Üê this is a big deal.
‚Äú
Immediately see results from your code‚Äôs output with an interactive notebook UI
.‚Äù ‚Üê This is a big deal.
‚Äú
Deploy Mage to AWS, GCP, or Azure with only 2 commands using maintained Terraform templates.
‚Äù ‚Üê this is a big deal.
Honestly, if Mage was just another nice UI, with a different take on developing pipelines than Airflow ‚Ä¶ I would be skeptical. But it appears they took a fundamentally different approach, providing probably not only a better UI, logging, and monitoring ‚Ä¶
but actually focusing on the Developer experience and pushing Engineering Best Practices in a way that provides clear value that Airflow and Prefect do not.
Apache Airflow.
I‚Äôm not going to devote time to this tool. There are reams and volumes of video and text content produced on the topic. You can google it. Airflow is boring and popular.
Comparison
Before you accuse me of nepotism I don‚Äôt have any skin in the game of Airflow, Prefect, or Mage. I‚Äôve never used Prefect or Mage in anything but a play-around experience. I‚Äôve used Airflow in a standalone deployment, via Composer, and via MWAA.
As I‚Äôve scaled up with Airflow, even with MWAA, I‚Äôve started to see the crack in the Airflow armour that may not be obvious to everyone. Also, the development experience, the way code is written and deployed, with Airflow doesn‚Äôt condon best practices and leaves a lot to be desired.
As I‚Äôve grown and added years to my Data Engineering career I‚Äôve found that certain tools are ‚Ä¶
Scalable.
Push you towards best practices.
Reliable.
Don‚Äôt require vast amounts of management and a deep understanding of the system architecture simply to use them at scale.
As someone who loves and has used Airflow for years, it isn‚Äôt any of those things. It works. It‚Äôs enough. It does the job tolerably well. Hence people use it.
When you have fans of all tools, how can you get past the arguments about which features are better, what‚Äôs faster, which one does this better, or that better? It can be hard to compare tools and boil down the essence of the matter.
Sure, we could install each tool, build a pipeline, and run it. But, what would that really tell us? We know that each tool, Airflow, Prefect, and Mage are capable of building and running pipelines for probably the majority of situations and circumstances.
What we have to ask ourselves is more of a big picture question, ‚Äú
which tool at a high level is doing something new and better, is approaching the problem of Data Engineering and orchestration and management of pipelines in a new and unique way that would provide REAL benefit, other than just a shiny new tool?
‚Äú
The answer is clearly
Mage.ai.
No, I have not been paid to say that, and I‚Äôm saying that as someone who‚Äôs been an active Airflow user for years, via both self-hosted, GCP Composer, and MWAA on AWS.
I‚Äôve nothing against Prefect, other than that it just isn‚Äôt providing enough differentiation from Airflow. Maybe it‚Äôs just their bad marketing, who knows? I could be missing something amazing.
Airflow is here to stay for a long time, but cracks start showing at scale.
I will always love Airflow, it‚Äôs a unique tool that does a decent job with an amazing community, and it‚Äôs simple to learn and use. But, anyone who has used Airflow at scale and doesn‚Äôt have a dedicated team to support it (why should you need one?), knows that you can easily shoot yourself in the foot, and what was once easy to use becomes a bear to manage and massage at scale to keep it performant.
It‚Äôs also inevitable that newcomers show up and start to pick and scratch at the corners of Airflow, putting best practices in place and solving pain points for next-level Airflow users.
UI is clunky, even the latest versions.
At scale, Airflow takes serious management and understanding to keep performant.
Good development experience is non-existent.
Sure, there is plenty of use cases for Airflow at a small scale where it works like charm. Should those organizations migrate away? Probably wouldn‚Äôt hurt them too, but it most likely doesn‚Äôt make a ton of sense.
How do you know you should switch from Airflow to either Mage (probably), or Prefect?
I‚Äôm going to ask you a few questions about your Data Engineering culture and use cases.
Is your Airflow setup starting to run over 50+ DAGs and growing?
Are your DAGs and pipelines wandering from simple to complex?
Is your Data Engineering culture embracing best practices?
Unit tested code.
Automated deployments (CI/CD).
Data Quality is important.
Do you aim for a seamless code experience?
If the above items are where you want to go, and you care about excellence in Engineering Culture, then I would suggest checking out a tool like Mage.ai over Airflow if you get the choice.
At the end of the day when running a bunch of pipelines at scale would most likely thank yourself. Why? Because your codebase will probably be clear, more reliable, and follow best practices better. Your deployments will probably be better, your Data Quality will be, and testing will be.
The Development of your Engineers working on the codebase to maintain and write new pipelines will be a much smoother and more efficient process. If I got to pick between the three for a new project, which one would I pick? Mage.
Have a different opinion? Share.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
111854964.the-best-kept-secret-to-ml-success,The best kept secret to ML success.,Databricks + Delta Lake.,2023-04-17 09:37:56.585-05,"image courtesy of
DalL-E
Note: Yes, Delta Lake supports this Substack. However, the opinions presented here are from a decade of working as a Data Engineering on large ML projects. I‚Äôve seen it all. I speak the truth.
So I‚Äôm just going to state the truth up front. There is a never-ending stream of Machine Learning content being poured down your throat in a never-ending stream of drivel about as good as late-night as-seen-on-TV adds. Seems all shiny and nice at 3 a.m., but the reality is somewhat different.
Today, I want to talk about the best-kept secret of Machine Learning, which is Databricks + Delta Lake. I will spend the rest of the time explaining that.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
ML definition as experts want you to see it.
So here is a typical set of ideas that you will see poured like honey out on your sticky and fair-weather ears.
Machine learning is a complex process that requires expertise in mathematics, statistics, and computer science. It has become an essential tool for businesses and researchers looking to gain insights from large datasets. Blah, blah, blah, blah.
For some reason, most of the content I see pushed around ML focuses on the model aspect, maybe the newest and hottest framework that just came out. Like we need another one. Really.
And yet, what is it they say? 90%+ of ML projects never make it to production? There is a reason for that.
The reality of most ML projects.
The truth is that most of the complexity revolves around the ancillary patterns that surround an Machine Learning Project.
I‚Äôve seen things in my time. I‚Äôve seen a dozen data scientists unable to bring a project to production after two years.  Do you know what it took to make it happen? A single Data Engineer, me, for about 6 months.
Here is how classically Machine Learning projects are approached.
Some Data Scientists play around with a model on their laptops.
Data gets too big and they move it to some large machine on AWS or GCP.
At this point, the code and data are spaghetti scripts and cobbled together crap not even they can reproduce it or make it work anymore.
With actual results people start getting excited in Product or other areas, a giant push comes to get more results and data, and more people get added to work on it.
Things start to fall apart because the size of the spaghetti code and cobbled together process and model training starts to fail.
The errors and inconsistencies start to appear because of poor data quality and other issues, trust is lost, and the excitement turns to anger and frustration.
Yet a more Excellent Way.
All this is so easily avoided. Most people don‚Äôt like the simple, looking down from their ivory tower of wisdom, it‚Äôs easy to miss the obvious. Machine Learning projects with relative simplicity can be brought like a golden pot of goodness to the leering eyes of those thirsty Product and Marketing wizards.
Simplicity is often the answer.
Start with a solid foundation.
Keep code in a repo with unit tests and development best practices.
linters and formatters.
testing framework
Nothing is done in a vacuum.
Every step in the process must be designed with repeatability and automation in mind.
Data preparation, cleaning, and storage are of the highest value.
Forget the ‚Äúframework,‚Äù and the question ‚Äúhow does this scale‚Äù must be answered upfront.
Logging and metrics capture or top-tier, not afterthoughts.
What model was trained when, with what data, and the metrics and predictions were this or that.
I know some of the above may seem like ‚Äú
this has nothing to do with Machine Learning.
‚Äù And you would be correct. It doesn‚Äôt. Its simply the realization that ML projects are just complex software projects.
Why do ML projects fail? Because they weren‚Äôt treated as Software Projects that they are. ML is code. Running of code with dependencies. Code that has to scale. Outputs that need context. Inputs that need to be clean. A system of moves and iterations that needs to be explainable and actionable.
Databricks + Delta Lake quietly solved this age-old problem.
When I first started working on automation around ML, what the cool folks call MLOps, it was the wild west. Python frameworks here and there and everywhere like some overgrown shrubbery in your bad neighbor‚Äôs yard.
Tools were immature, scalability was an issue, meta-tracking was hard, and everyone was building their own cobbled-together end-to-end systems. There was more work going into the Ops part than the actual ML part. Many times this is still the case.
Many technologies like Dask and others were incredibly unreliable. The open-source MLOps world was not well developed and took your sweat, blood, and tears to just get working. No more.
These problems have been solved, and people have yet to realize it.
The Solution.
In recent years, Databricks has emerged as a game-changer in the field of machine learning, making it easier and more accessible for everyone.
Databricks has changed machine learning, by solving and taking away the complexity from the following areas.
Data collection, preparation, and storage:
The first step in the machine learning process is to collect data. This data can come from a variety of sources, such as sensors, databases, and social media platforms.
Once the data is collected, it needs to be cleaned and prepared for analysis. This involves removing any outliers or errors, transforming the data into a suitable format, and selecting relevant features.
Delta Lake
and
Databricks Feature
Stores solved this problem and more, at scale. Constraints, data quality, usability, dang, it‚Äôs amazing.
Model training:
After the data is prepared, the next step is to train a machine-learning model. This involves selecting an appropriate algorithm, tuning its parameters, and testing its performance.
Databricks Notebooks
,
Job Clusters
, and other tools make this a trivial matter. It‚Äôs now simply an afterthought.
They even integrate with Repos
! No excuses for bad code.
Model deployment: Once the model is trained, it needs to be deployed in a production environment. This involves integrating the model into a larger system, monitoring its performance, and updating it as needed.
Tracking of metrics, performance, features, and the like.
Managed MLFlow
is here for your use, with no overhead and additional architecture management.
The problems faced in the traditional machine learning process are now gone for the most part. It‚Äôs a Data Engineers‚Äô dream now to automate the process on Databricks withe their wonderful APIs and Worflows, it has never been easier than it is right now to do ML at scale.
Conclusion:
I‚Äôm not sure why I don‚Äôt hear about it more. It‚Äôs game-changing.
Databricks has changed the ease of machine learning by providing a unified platform that simplifies the entire machine learning process. It provides tools for data preparation, model training, and model deployment, as well as collaboration tools that make it easy for teams to work together. With Databricks, machine learning is no longer limited to a small group of experts but is accessible to anyone with a basic understanding of data science and engineering."
111286895.primer-on-data-quality,Primer on Data Quality,The Hardest Problem in Data to Solve.,2023-07-31 07:38:00.375-05,"Mmmmm. This is one topic we hear a lot about, sure maybe not as much as Polars or DuckDB, but it‚Äôs like a continual dripping of water, the humdrum of ‚Äúget better with your Data Quality,‚Äù never ends.
Yet, it‚Äôs safe to say that probably the 80/20 rule applies, at most 20% of Data Teams are practicing any sort of ‚Äúreal‚Äù Data quality. It‚Äôs something we all aspire to but never achieve.
One of the reasons that the Data Quality nut is so hard to crack is that it‚Äôs hard to eat an elephant. Where do you start? How do you convince leadership to spend money on some solution when the bottom line has now become our Lord, to be worshiped and cherished?
That‚Äôs what we are going to try to tackle today. A Primer on Data Quality. What is it, how do we do it, what tools can we use? Here is the outline of our Primer.
What is Data Quality?
Why is it hard to implement Data Quality?
What tools are available for Data Quality?
Home-Grown Data Quality.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What exactly is Data Quality?
You would think this would be the easiest question to answer, but I think it‚Äôs one of the reasons Data Quality is so non-existent, lacking in tooling, and generally related to the ‚Äú
oh we will get to that
,‚Äù bucket.
It‚Äôs simply hard.
There are a million other competing priorities, and Data Quality, in its true form, is simply just not seen as ‚Äúimportant enough‚Äù by the powers that be. Heck, even in the Data Engineering community at large you don‚Äôt hear much about it, just some little mention here and there.
So is all hope lost? Probably. But that being said what can we say definitively about Data Quality?
Data Quality forces the
definition of data
.
Data Quality forces capturing the
business requirements of data
.
Data Quality must somehow
screen both incoming and outgoing data.
Data Quality must
alert us that something is wrong
.
And, I think this helps us know why the definition of Data Quality is so hard, and why it isn‚Äôt widely adopted. Because every business will answer these questions differently for the most part.
Unlike say, unit tests, everyone has code that they can unit test. But at the core of data quality is knowing what data is and what it‚Äôs supposed to look like. That is different for everyone. Therefore it‚Äôs not that trivial to have a one size fits all.
What can Data Quality look like?
We of course could get into some high and mighty discussion about how to implement Data Quality, but what about if you‚Äôre just wanting to dip your toe in the water? Like most Data Teams you want a good DQ program, but have no time or resources to do something ‚Äúserious.‚Äù
Well, I do think based on our above definitions there are ways to start driving the car in the right direction before fully committing to some major Data Quality project.
Use
constraints
inside your Data Stores.
NULL
vs
NOT NULL
Uniqueness
(primary keys etc)
CONTAINS
or
IN
(control what‚Äôs allowed)
Constraints have been around since dinosaurs walked the earth. From MySQL to Postgres, now to Delta Lake. The problem is people think of them as an afterthought. Probably because of weak Data Modeling.
The truth is if a Data Team pays very close attention to constraints, they will absolutely increase Data Quality in a serious way without even incurring new technology or costs! It just sounds boring so people don‚Äôt do it.
Let‚Äôs take this example above. What jumps out at you? Does someone need to have a last name? If so, then why is it allowed to be
NULL
? Do you expect everyone to have a birthdate? Then why can it be
NULL
?
It‚Äôs simple and straightforward decisions like this that lead to bad Data Quality. If you want something to be a certain way, then make it that way.
Again, if you don‚Äôt think people are vampires and live forever, shouldn‚Äôt you enforce that in the birth date by putting some constraint around the date that is used?
My guess is that in most Data Teams there are a plethora of ‚Äúideas‚Äù around every single table and data point that are simply not enforced. Some of them sneak by, and others probably lead to serious Data Quality issues downstream when someone starts to do calculations and analytics.
Refer a friend
The above type of Data Quality is what I would call the late-night TV starter pack. Not that cool, cheap, will probably serve a short term purposes and maybe a good place to start.
Rolling your own Data Quality.
Of course, there is always the tried and true method of ‚Äúrolling your own‚Äù when it comes to data quality. I mean, in theory, no one knows more about the data than the Data Engineers, as much as we work with the data and the business.
For most Data Teams this is probably the best way to dip the toes in the bath, without getting too wet.
If we think about what we really want from Data Quality, it probably boils down to a few simple things.
We want
alerts
when something ‚Äúis
wrong
or
not normal
.‚Äù
We want to stop bad data from happening.
We want a
holistic view
of what our
data looks like over time
.
I mean in the end, that solves most of our problems right?
Knowing that something is out of normal or going bad. Stopping super bad things from happening. Also being able to get a holistic view of our data over time, trends, volumes, things of that nature.
I would like to propose that most Data Quality is really about understanding and having a grasp of what data looks like, and what is happening to data over time.
What does this look like at a high level? It looks like two things.
Dashboards to show us trends and give us insight into Data Quality and what our data is doing.
Coded data quality sanity checks and assertions.
One is a very high level (dashboard) that gives us a view of our universe, which of course can have alerts attached to them. The other is very granular and specific to each data set, actual data assertions, and checks.
Both of these things are relatively easy for any Data Team worth their salt to produce. It‚Äôs just a matter of having the time and resources to devote to it.
Every Data Engineer could easily look at each data pipeline and come up with obvious and critical Data Quality checks that should be happening.
For example, customers are never supposed to have NULL ids.
Or maybe it‚Äôs more complex like some threshold on the number of orders we should be expecting from our customers every day.
Who knows. The point is nothing that has been shown is really rocket science, at all, it‚Äôs actually easy than most of the day-to-day tasks taken on by Data Teams.
But this is the type of Data Quality work that would pay dividends in the long run. Assurance and knowledge that things are going as expected. That is the key to Data Quality.
What tools are available for Data Quality?
There really isn‚Äôt much available for tooling the Data Quality landscape, at least compared to data processing tooling, governance, etc. It‚Äôs a veritable no-man land when it comes to Data Quality tooling in both the commercial and open-source landscape.
Basically, the only two realistic options are ‚Ä¶
Great Expectations - open source
Soda Core - open source core and commercial.
Both tools are well-rounded and perfect for getting started in real Data Quality.
I have written elsewhere extensively about both tools. Check out the links below for a more in-depth analysis of using these tools. They are all generally very similar to each other, with configurable and out-of-the-box configurations to validate everything from DataFrames to Postgres tables.
Make sure to read the links above. So, Data Quality? It‚Äôs the wild west and hopefully, a lot more work is done in this space in the future, by both Data Engineers and companies bringing more offerings to the market.
Probably you should just start with rolling your own Data Quality, simply because it‚Äôs good practice and will help you understand your data as a baseline. Once you‚Äôve done the simple steps we mentioned above, then you are ready for the next level. Soda or Great Expectations.
Happy Data Quality Hunting!"
111286843.immutability-for-data-engineers,Immutability for Data Engineers,Nothing ever changes ...,2023-11-22 07:20:41.906-06,"What‚Äôs the old saying ‚Ä¶ ‚Äú
Nothing ever changes?
‚Äù Same old same old. I guess sometimes that could be a good thing.
Anyone who‚Äôs played around in a giant Python codebase before, a literal morass of code spewed out over years into a purifying tangle of variables that takes a wizard to divine ‚Ä¶ would agree that having something not change would be nice ‚Ä¶ for a change.
Immutability. A concept not much used in most Data Engineering circles, but one that could be the savior of us all. Take us away into the promised land o‚Äô thou glorious immutability.
You should check out
Prefect
, the sponsor of the newsletter this week!
Prefect
is a workflow orchestration tool that gives you observability across all of your data pipelines. Deploy your Python code in minutes with
Prefect Cloud
.
Why immutability matters in Data Engineering.
This is a most interesting topic related to Data Engineering, data being immutable. It‚Äôs safe to assume a fair number of my readers have never thought about data as being ‚Äú
immutable
‚Äù when writing their most recent Data Pipeline.
First, we should probably try to define immutability in a Data Engineering context.
‚Äú
Immutability in the context of data engineering refers to the concept that once data has been written (into memory or disk), it cannot be changed or deleted.
‚Äù
Immutability applies to data at all points during the data processing lifecycle.
It isn‚Äôt limited to data just written to disk.
Immutability can mean different things to different people, each in their context. If you‚Äôre a Rust or Scala developer you might be thinking about variables. This is correct, at least when thinking about a Python codebase, these other languages would have different approaches.
This is from a programming and code perspective.
But there is also a pure Data perspective
, regardless of the language of choice. How do you treat your data in memory or at rest?
Do you touch it, mutate it, how do you track through your system and code a piece of data?
Everything is harder in a Data Context.
Let‚Äôs say you write your data pipelines in
Python
. Complex data pipelines. We know we can‚Äôt really have immutable data as such, not like Rust, when coding in Python. It‚Äôs more of a theoretical approach to how you should write your Python pipelines.
Let‚Äôs take a Polar‚Äôs Python approach. Ok, so we all know production pipelines are very complex and are twice as big and complicated.
What do you notice about this code? Well, it cares nothing for even the concept of immutability. It just embraces Python and cares about nothing else besides shoving the data through the pipeline.
Everything is the same dataframe (df)
. T
hat‚Äôs simply a dangerous way to approach life and problems.
In a real-life use-case, things would be much more complicated with many different calculations and this and that going on.
I would argue code that doesn‚Äôt think in an immutable way becomes exponentially harder to reason about, work on, and debug.
With no philosophical or real immutability, it‚Äôs hard to find what you're looking for.
In a complex codebase with no immutability, debugging is a major task. It‚Äôs hard to know the ‚Äústate‚Äù of things.
Let‚Äôs be honest, saying what we mean and treating each data point as ‚Äú
immutable
‚Äù even in Python makes things more clear. Easier to reason about, easier to debug, and the like.
True immutability in code.
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
With Python, we can‚Äôt have immutability in code. We can dream about, think about, and act like it, and gain some benefits when approaching our data throughout our codebase as immutable, but it‚Äôs still a pipedream.
This is why some teams and folks chose to dive in head first. Static languages solve this problem, think Rust and Scala. With these tools, immutability can move from an idea to a reality.
Let‚Äôs take an example of some Rust code that
downloads a CSV file and does some processing
. Here is a snippet of some code.
In Rust we have two types of variables we can define with the `
let
` keyword. By default these values are immutable. They cannot change.
If we want to mutate something, we have to be explicit ‚Ä¶ we must `
let mut
`. This has a major impact on how we write and reason about code in a Data Engineering context.
In Rust, unlike Python, we can‚Äôt simply just make variables or ‚Äúobjects‚Äù to hold data, pass them around willy-nilly, do stuff to them, pass them along again, do more things.
I mean maybe we could if we tried, but it goes against the grain of how most folk write Rust. For example, some time ago
I was writing some Rust code to process a graph
.
When dealing with Dataframes in Rust that are a mix of mutable and immutable, the code is much more verbose.
But, it has the benefit of being obvious when reading it. It‚Äôs not confusing what is what, it‚Äôs hard to ‚Äúmess it up.‚Äù Everything isn‚Äôt ‚Äú
the same df variable.
‚Äù Simply because it can‚Äôt be without extra work.
Code that treats all data as immutable has lots of benefits.
simplicity and predictability.
no need to reason about data structures changing state.
fits well into modern architecture.
concurrency and parallelism.
data that doesn‚Äôt change can easily be thread-safe.
cache-friendly.
There is also another piece of data immutability that has a place in Data Engineering, namely data at rest (outside our code).
Immutability in data at rest.
Another aspect of immutability in Data Engineering that is usually overlooked, or at least not thought about, is our data at rest. Think about it, most of our data is sitting somewhere, in an s3 bucket, in a Lake House, or in a database.
How do we treat this data?
I‚Äôm sure you work on data platforms where data in its various forms and states is written to various and sundry sinks. What can you reason about this data? I‚Äôm sure questions come up. Is it immutable? Had it changed? Can it change?
These are serious questions with real-life implications.
Data sinks that are immutable in nature can provide clear benefits, much like code that treats data as immutable.
historical accuracy and audibility
you can track things correctly that aren‚Äôt in flux.
data consistency in large distributed systems.
recovery and failover plans.
integrity.
less likely to be messed with and broken.
This can be a complicated topic, intermixed with data models, but think about it. What if you ingest raw data and assign some composite primary key made up of a few columns to help identify data as it flows through downstream systems?
What happens if someone comes along and updates a customer_id or name that is part of that primary key ‚Ä¶ aka something that should be
immutable
in theory?
Do you really want that data to be updated? Probably not, maybe you need to model the data as needing a new record that replaces another.
Data that is mostly immutable at rest is much easier to work on. Anyone who‚Äôs worked around data long enough knows that the classic UPDATE statement ‚Ä¶ something becoming mutable, is probably the core of some of the biggest bugs and data warehousing fiascos to ever strike unsuspecting victims.
The complexity of cascading changes. Akkk.
Not everything fits perfectly into an immutable world, data is complex, but, Data Engineers who approach designs and models with an immutable mindset first ‚Ä¶ will build far more hardened and useful systems in the long run.
What have your experiences with immutable vs mutable data been like?
Do you have horror stories? Do you fall into one camp or the other? Do you even think about data as being immutable or not? Please share your thoughts below!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
109104608.dsa-for-the-rest-of-us-part-3,DSA For The Rest Of Us - Part 3,Introduction to QuickSort.,2023-04-03 08:51:52.683-05,"I knew we couldn‚Äôt avoid it forever, the complicated stuff that is. Our days were always numbered, that‚Äôs just life. Our doom is upon us, alas, great mourning and suffering are about to stretch their cruel hand upon us.
Ok, I might have been a little dramatic. QuickSort is probably slightly more complicated than our last two DSA topics, Linked Lists, and Binary Search. But, if you remember our Binary Search probably, which required looking for a particular item in a sorted list/array, this topic of QuickSort is a natural next step.
Remember in our Binary Search discussion, it required a
sorted
list/array to work with? Someone might have asked, but where did this pre-sorted list/array come from? Well, that is the point of a QuickSort algorithm.
If we have a jumbled mess of a list or array, how can we efficiently sort it? Again, I‚Äôm going to keep this at a high level, so some things I‚Äôm going to skim over.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The Big Idea
One of the concepts behind Quick Sort is the ‚Äú
divide and conquer
‚Äù approach. I encourage you to read more about this topic, I think it will be illuminating for you in how to understand complicated problems, even in your everyday Data Engineering life.
Break down a problem into it‚Äôs the smallest possible unit, then repeat, recursively.
The idea of a function calling itself, being recursive is a powerful one when it comes to writing algorithms, and QuickSort is probably one of the simplest DSA problems that use this idea of ‚Äúdivide and conquer‚Äù combined with recursive function calls to solve a problem.
{smallest unit of work} + (recursive) = answer
QuickSort Algorithm
With all that mumbo-jumbo out of the way, let‚Äôs dive into QuickSort. This is a great introduction to sorting algorithms, the complexity of that topic, divide and conquer, and finally recursive functions.
sorting complexity
divide and conquer
recursion
The basis of Quick sort, the problem, and the solution can be seen in the below diagram. The idea is you have a list/array of unsorted items, how do you efficiently sort them into the right order?
Here is my 10,000-foot view of how to do that.
First, pick a ‚Äúpivot‚Äù point out the array.
typically the first, or last item.
Iterate through the rest of the items.
if the item is greater > than your pivot push to another array
if the item is less > than your pivot, push it to another array.
Of course, there are slight variations in the implementations, depending on the language. In Python, for example, many examples will simply shuffle the items in the array, but I felt using the concept of the brand new array to capture what happening was helpful.
Let‚Äôs walk through a QuickSort example in Rust.
First things first, let‚Äôs make an unordered list of ints in a Vector.
The next step would be actually writing our QuickSort function, it will of course take a Vec and return a Vec. If you‚Äôre not familiar with Rust, just
think
of a Vector like it‚Äôs a List or Array.
You will also not in the above function that we check for an empty Vec, and probably should actually change that code to be
< 2
, so if there are 1 or 0 elements, we simply return as there is no sorting to do.
The next few steps in my QuickSort implementation are fairly simple, I will pick a pivot, the first time, and create two new Vectors, to hold values above and below my pivot.
The Meat and Taters.
Ok, so now that the easy stuff is behind us we are getting to the meat n‚Äô taters portion of the QuickSort, where all the magic happens. Let‚Äôs review quickly what we need to happen before we dive in.
We picked a pivot, or number to hold, in our case the first item in the Vector.
We want to iter through the remaining items in the Vector checking if they are greater than or less than our pivot.
We want to push items into one of our two Vectors, the left, less than or equal to side, or right, higher than side.
Then we must assemble these pieces back together into a single Vector, [left] + pivot + [right']
We must then call the function on itself and apply this same logic over and over again in a recursive manner.
Let‚Äôs get cracking.
The next most obvious thing to do is iterate the input Vector, pushing the items after our pivot into the correct new Vector.
Again, this part isn‚Äôt all that complicated. You can see we start to iterate the Vector starting from position 1, notice it‚Äôs not 0, that‚Äôs our pivot. We then simply check it against our pivot and based on if it‚Äôs less than or greater or equal to the pivot, it will go into either the left or right Vector.
Now for the finishing touches.
Let‚Äôs see if you notice the special sauce in the mix. See it? We take our left, right, and pivot Vectors and combine them into a single result. With one catch.
We insert a recursive call to the function we are writing, we call itself on the
left
and
right
vectors while appending them to the result. This might look a little strange at first if you‚Äôre not used to recursive calls, a function calling itself, but it is indeed a powerful idea.
What‚Äôs the result of running this code?
On the top, you can see the unsorted Vector printed out, and on the bottom the sorted one. It‚Äôs like magic.
You can find the entire code here
.
Closing Thoughts.
Subscribe now
I hope that was a decent introduction to QuickSort, and a great building block upon the Linked Lists and Binary Search we have already covered in this introduction to DSA for the rest of us.
I think that QuickSort definitely ups the ante a notch, but mostly because of two important concepts.
Divide and conquer.
Recursion.
I think both these topics are well worth your time to spend a little energy researching and reading up on the two, if not to become an expert, just to ensure you understand them, and can at least talk with some form of confidence on the topic."
108895794.dsa-for-the-rest-of-us-part-2,DSA For The Rest Of Us - Part 2,Introduction to Binary Search.,2023-03-27 09:19:50.876-05,"Photo by
Markus Winkler
on
Unsplash
In Part 1
of this most illustrious and illuminating series on DSA (Data Structure and Algorithms),  we converted the basics of
Linked Lists
. In Part 2 of this series, we going to cover the much-maligned Binary Search.
I don‚Äôt know if it‚Äôs from LeetCode hatred, which is justified, or what, but it seems like
Binary Search
has always taken the beating in some online rant about how out of touch the interview process is with real life. While this might be true, we are taking a different approach in this series on DSA.
Sure, some things might be pointless on the face value that you won‚Äôt use them explicitly per se in your day job, but to think that becoming smarter, and understanding fundamentally how many popular DSA topics work, well, that‚Äôs just short-sighted to your future self.
Pushing ourselves as Data Engineers and code writers is the only way to get better. Like it or not, DSA comes up in the interview life. But, it‚Äôs also true that a fundamental understanding of what even a Linked List is, or how basic Binary Search would make you worse off, well that just isn‚Äôt true. In fact, the opposite. You will be more well-versed, accomplished, and confident.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Introduction to Binary Search. What is it?
Code available on GitHub.
Binary search, I‚Äôm sure you‚Äôve heard about it at least once or twice in passing. Sounds scary right? Well, it‚Äôs not the evil monster you think it is. We are going to take the 10,000-foot view of binary search, let‚Äôs make it easy.
What
is
binary search?
‚ÄúAn algorithm that can quickly (logarithmic time) search through a sorted list/array of elements to find what it‚Äôs looking for.‚Äù
Let‚Äôs think about the problem at a high level.
You have a ‚Äúlist‚Äù, ‚Äúarray‚Äù or whatever of
sorted
items.
You want a function that can perform the
best Big O notation
search to find the location of a
random
element.
Ok, so you really only have two options to solve this problem.
Starting going through the items one by one until you find what you‚Äôre looking for.
Jump around in some sort of ‚Äú
smart
‚Äù manner to find the item more quickly.
The answer that makes the binary search a great option and teaching tool for algorithms is that the solution is straightforward, and makes you say ‚Äú
well duh
.‚Äù
The idea is to cut the search area in half every time
, you can do this by saying is the
x
I‚Äôm looking for greater or less than where I am currently at. Then jumping to the middle of the next slice and repeating the process.
Example
I‚Äôm sure it will make more sense with an example, first, we will walk through a simple example using pictures then we will switch to code.
Let‚Äôs say we have an array of numbers, 1-12, like in our picture above. For argument‚Äôs sake let‚Äôs say we want to write a function that can take any number,
x
, and find its location.
The first most obvious thing to do, in binary search, is to go directly to the middle of the array and ask if the x is either equal to, greater than, or less than that number. If it‚Äôs greater, then we can throw away the bottom half of the array, if it‚Äôs less than that, we can throw away the top portion.
Let‚Äôs say x came in as 4.
If x was equal to 4, and we started in the middle, at 6, and 4 is less than 6, we can throw away half the array.
Obviously, we could throw away 6 as well, but you get the point.
And guess what, if we do that again, we can do the same thing, cut the search area in half!
And so it goes. Each time we can throw away half the list/array area we are searching.
This is the infamous binary search.
Look at that, all your fears about running into binary search in some interview or now swiftly fading into the distant background! Well, maybe.
Let‚Äôs try to write some code first before we declare victory.
Binary Search with Rust.
Ok, since we are gluttons for punishment, and of course, we could use Python like everyone else, we will do no such thing!
Rust
it is. So we have the theory, right? We need a function that can take any number
x
, search through an array of integers and return the location of that
x
if it exists.
First, the array.
Well, that was the easy part. Next our function.
Hmm‚Ä¶ so the first thing we would want to do is slice into the very middle of
numbers,
and check if that position is equal to, greater than, or less than our
target
number.
That should do the trick, the length of the Vector divided by 2. So here is the shell of our Rust function so far.
Ok, but now following our logic, we need to check if our
target
is above or below the index, and throw away what we don‚Äôt need. Let‚Äôs add some more
if-else
logic.
Ok, what would happen if we put this together and run it so far?
So we can see we are headed in the right direction! We passed in 42 as our target and the code picked the correct half of the array to work on!
But we have a problem, this function requires some loop or recursion. That should be obvious to smart engineers like yourself. Also, we either have to modify the Vector based on throwing out what we dont need, or look into a slice of the array we are interested in.
Here is the working code,
while
loop, and all. Although I think it would be a fun exercise to use
recursion
(function calling itself) rather than a
while
loop. Also, not modifying the
Vector
like I‚Äôm doing would be a better implementation as well.
Let me preface this code by saying I don‚Äôt know Rust very well, only a month or two in. Also, my solution is just that, my first solution that is not production quality, I pray you can write a better one! But, does it find what it‚Äôs looking for? Yes!
And here is the results of the
print statements
. The important thing to note is how to search area is sliced in half each time, which is the point of binary search, finding our needle in the haystack as quickly as possible.
A very interesting and fairly simple algorithm!
Not the best binary search, far from it, but I think it‚Äôs a simple enough example to show us the ideas behind binary search and possible implementations. It‚Äôs a great place to start, a fun project to work on (without
Googling
or
ChatGTP‚Äôing
answers), which can be tempting to do, but detrimental to learning.
After I finished this little fiasco, I went and asked ChatGTP to do it. It‚Äôs smarter than me and gave better code, here it is.
Using ChatGTP or CoPilot to write code is dangerous, sure it can write it, but if you‚Äôre learning a new topic, you should struggle through it yourself so you actually learn something.
I learned a lot about Rust when writing my version, I was especially excited to learn the
drain()
method on
Vectors
. If I had asked AI for the answer, I would have learned nothing, but now I have
binary search
embedded in my brain because I struggled through it.
I hope you‚Äôve enjoyed this simple series on DSA as much as I have. It‚Äôs good to do the basics, and learn them from scratch. It does a lot to make it stick in your head for the long term.
Code available on GitHub."
108648293.future-proof-yourself-against-ai,Future proof yourself against AI.,Data Engineering Excellence.,2023-03-20 08:55:40.711-05,"Photo by
Micaela Parente
on
Unsplash
Ignore
ChatGPT
and
CoPilot
at your own risk. Just like the old folk riding around horses bemoaning the iron horses, you are liable to throw the baby out with the bath water. It can be hard to tell sometimes if the hype is real if the headlines are true, and how much you should care.
I mean, I did write
a fairly decent PySpark pipeline using CoPilot
.
There are three types of folks when it comes to AI and programming I think.
One‚Äôs who‚Äôve never used AI to help with writing code, and the poopoo it.
Folks who‚Äôve used it to become more efficient and better at their job.
The lazy want it to do everything for them and become terrible engineers because of it.
I think personally the truth is somewhere in the middle.
Can AI write better code than more than most new and junior software engineers? Yes. Have the best programmers always been the ones who can do more than just write code? Yes.
Is the future of software engineering going to change if AI can write decent code for us? Yes, I don‚Äôt know exactly what that will look like, and change takes time, but things are probably going to change.
What are we going to cover today?
What will AI change about Data Engineering?
What makes an excellent Data Engineer?
Future-proof yourself against AI.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What will AI change about Data Engineering?
As an avid user of both CoPilot and ChatGTP, I‚Äôm very torn on this topic. I can see both the good and the bad of technology, just like anything else in life. I can see the potential for good, and for evil.
We can talk all day about how these AI technologies that can write code might change our jobs, or I can just talk about how I‚Äôve personally used them and maybe we can extrapolate the future. And that‚Äôs the other thing, the future that is, is hard to divine, who knows the timetables involved? It seems impossible to know.
So in what ways have I personally used ChatGTP and CoPilot coding?
I‚Äôve used ChatGTP to generate boilerplate Python code for documentation and technical writing. Aka, the code is not actually in production.
I‚Äôve used CoPilot to write an entire PySpark data pipeline, speeding up the process significantly.
I‚Äôve used CoPilot to unblock me when writing some new-to-me Rust code. Aka, taking its suggestions and modifying them.
None of this probably sounds earth-shattering at the moment to you, if you‚Äôve never used these technologies for anything, but I think the truth is a little different than that.
The truth is that on the surface, ChatGTP and CoPilot can speed up your efficiency at writing code by orders of magnitude.
I‚Äôm not commenting on the fallout from doing that ‚Ä¶ yet.
So what will AI change about the future of Data Engineering? Here is my best guess.
Over the coming years, it will cause employers to hire fewer Data Engineers and expect the ones they have to be more efficient.
Simple data transformations and connectors will become more and more abstracted away. Data Engineering will be less about push and pull.
It will be harder for junior engineers who lean too heavily on ChatGTP or CoPilot to become masters of their craft.
Bad code will simply just to be acceptable anymore. Because people will wonder why they hire someone to write bad code when a computer can do it better.
Data Engineering will become more about ML and architecture than just code.
Let‚Äôs forget about computers for a moment, and talk about humans. We are unique after all.
What Makes an Excellent Data Engineer?
I‚Äôm going to sum this up by saying that I think all humans are unique and creative, and despite many of our unwanted qualities, generally are incredible problem solvers and thinkers. I mean ‚Ä¶ we made AI after all didn‚Äôt we?
If you are a Data Engineer, or any kind of programmer for that matter, and are worried about this technology I would encourage you to stop for a moment and think hard about what computers are good at, and what you are good at. Here is a short and not exhaustive list of things that make a Data Engineer excellent.
Things AI will not be taking away from you anytime soon.
Big picture architecture and knowing the tradeoffs that come from experience, human experience.
Upskilling and mentoring young team members into productivity.
Planning and executing large-scale data projects and migrations.
Being good at JIRA is only half the battle.
Human communication of engineering problems, deadlines, and roadblocks.
Using your human brain to make tradeoffs around decisions based on code, architecture, tools, and people at the right time and place.
Let‚Äôs be honest. Anyone can write code. Sure, some are better than others, but style is different, OOP, TDD, Functional, and all the other isms. This is not where the greats are great, those that lead and execute successfully at scale. Sure, they understand good software and design principles.
But they are where they are because of the other related human skills.
Future-proof yourself against AI.
Photo by
drmakete lab
on
Unsplash
This will be a regurgitation of what I just said, but let‚Äôs cover it so no one misses the point. You can easily future-proof yourself against all your AI worries and anxieties. It‚Äôs easy.
Just be more of you. Be good at code, but be so much more than that. Be human, be yourself, and recognize that much of who you are is why you were hired and people pay you. Focus on these things.
Be good at code, but work harder at design patterns and architecture.
Learn to answer the ‚Äúwhy‚Äù of doing x, not just the how.
Be kind to other humans you work with, people like nice people, and you will be the last to go probably.
Capitalize on the above and learn to upskill other people. Be known as a teacher and the person people go to for help and answers.
Learn more non-engineering skills like project planning and execution.
Cultivate a passion for what you do, and let it come through in your work.
Nothing earth-shattering or groundbreaking, but that‚Äôs probably what makes them all so hard to do. It was easy in the past to just reach for LeetCode or the Algo grind, just memorize and know all the best ways to write code. But honestly, that was only going to get you so far anyways, the path to happiness never was built down that road.
Happiness was always going to be found in human relationships and human skills. Skills like communication, project planning, and using past experiences to influence and shape the future.
What would I do in your shoes? What I‚Äôm doing now.
Embrace ChatGTP and CoPilot, use them as your friend, and don‚Äôt put all your eggs in that basket, but also don‚Äôt ignore or throw the basket away. Let them help you become a better you, why not after all?
Subscribe now"
108498959.when-a-good-engineer-goes-bad,When a good engineer goes bad.,A tale of genius and heartbreak.,2023-04-12 09:38:50.749-05,"Photo by
Ashley Jurius
on
Unsplash
I'm going to tell a tale. Maybe it's one that will make you uncomfortable, maybe you will relate, who knows. It's a timeless tale, told many times over since the beginning of time. It's a story about humans, the human in us all, and how some people forget they are just that, human.
There is probably nothing worse, nothing sadder, and more destructive than a good engineer gone bad. It seems like when they go bad, they go all the way.
What will we cover today?
Good engineers gone bad.
How to deal with smart bad engineers.
Closing thoughts.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Good Engineers gone bad.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
If you haven‚Äôt run across one in the wild, consider yourself lucky. If you have, I‚Äôm sorry. If your living with one right now, time to move on. Drastic you say? I so no. There is probably nothing more detrimental to your career than a good engineer gone bad. Especially one with power. That‚Äôs the worst kind of all.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What exactly is a good Engineer gone bad?
‚ÄúThat software engineer who sucks all life from a room. They dominate all decisions and discussion. They are extraordinarily unhelpful, and the bane of all junior engineers. The are a major roadblock and everyone tries to work around them.‚Äù
It‚Äôs like Sasquatch, you will know one when you see one.
They are typically the smartest person in the room. That‚Äôs how they started out ‚Ä¶ good. But someone along the line they lost their humanity. Now they are black holes sucking away all that is good.
That‚Äôs probably the saddest part of the whole thing, is that these good engineers gone bad are savants, the smartest of the smart, and they
COULD
be so useful. They are expensive as well.
Expensive how? The funny thing is if the good engineer goes bad over time, most of the time it gets lost in organizational churn and burn. By that time a new manager comes along that actually sees the problem, this person is so entrenched and senior, it‚Äôs like digging that rock out of your bike tire. It ain‚Äôt going to happen.
Good engineers gone bad are expensive in the following way.
They lower the tenure of anyone that works with or around them.
They stifle creativity and progress.
They destroy morale, and software productivity.
The long-term viability and growth of engineers junior to them are gone.
The entire system of things they touch is turned to coal.
They are hard to get rid of.
And that‚Äôs putting it nicely.
How to deal with smart bad engineers.
Photo by
Intricate Explorer
on
Unsplash
Well, if you can‚Äôt fire them, I guess you can work around them? Here is my advice on the subject.
If you work directly with one, change teams, or get a new job.
You‚Äôre being held back in more ways than you can count.
Find other mentors in your organization who are the opposite of the bad smart engineer.
Learn from their mistakes, aka don‚Äôt be like them.
Be kind. Smile and nod.
Try to learn from their smartness, because they do have that.
Learn the signs and avoid them in future jobs.
The best thing you can do is just learn from the situation. Learn what not to be like, learn what to avoid and not be like. If you find yourself in direct contact a lot, just try to take the good and leave the bad. Try to learn from their smartness and technical prowess and discard the rest.
But in the long run, if you can‚Äôt avoid them you should probably just change jobs, internal or external. If you‚Äôre blocked from learning because of a single person, can‚Äôt be creative, can‚Äôt grow professionally, and thrive on a team ‚Ä¶ then what‚Äôs the point? Leave.
Closing Thoughts.
I think this is an important topic in today‚Äôs environment. Many times people have misguided reasons for putting up with such tomfoolery, and most of it is in their minds. Even with all the layoffs in the news, being able to write code makes you a very valuable person. Being a nice person who writes code makes you even more so.
If you‚Äôre working at a big company or a small startup, and they knowingly uphold and leave good engineers gone bad in their place of power, to wreak havoc on everyone around them, you should look yourself in the eye and make up your mind to move on.
Life is too short for that.
Sometimes it‚Äôs not as easy as leaving tomorrow, but you can make a plan, and that plan will put you in a place of mental power and fortitude. What you must remember is that in such a toxic environment, the detrimental effects on your health and career are probably far greater than you imagine.
Good engineers gone bad are smart, but they are bad. They can be both. Avoid such ones at all costs."
108167342.dsa-for-the-rest-of-us-part-1,DSA For The Rest Of Us - Part 1,Introduction to Linked Lists.,2023-03-13 09:18:57.233-05,"Photo by
Olav Ahrens R√∏tne
on
Unsplash
Maybe you‚Äôre like me, or maybe not. You‚Äôve been plodding along through Data Engineering life like those oxen of old who pulled those wagons across the prairie, over the never-ending rolling hills of life and software. And, truth be told, you‚Äôve never come across the fabled DSA (
Data Structure and Algorithms
) so lauded by the rest of your compatriots.
Now you‚Äôre too afraid to ask.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
No matter what those too smart for the rest of people on Stackoverflow and Reddit trolls might say, the obvious fact that most of us know but few will say out loud ‚Ä¶
Unless you are a Data Engineer building the underlying complex distributed systems, the reality is you will most likely never run across the need to use DSA.
Does that mean we should not care? Of course not. If you want to live in this world and pass any interview, understand how software works, and generally become a better version of yourself, of course, you should probably learn at least
something
about DSA.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
What‚Äôs Ahead.
This is Part 1 of what I hope is a semi-regular set of sessions where I bring you along for the ride while I teach myself some of these DSA topics. But, I want to do it somewhat differently than most of what you will find on Google.
What I mean by that is, I‚Äôm going to try to teach myself DSA like I‚Äôm 12. I don‚Äôt want a bunch of junk that muddies the water, that stuff can come later.
I‚Äôm going to attempt to cover introductory DSA topics like we are sitting on the moon looking back at earth.
With that being said, I‚Äôm going to tackle this DSA stuff in no particular order, just whatever strikes my fancy. And these will not be long-winded explanations. They will be short, to the point, and hopefully make you think ‚Äú
Oh, that‚Äôs what that looks like, maybe I will go Google that and learn some more about it.
‚Äù
What‚Äôs a Linked List?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Photo by
Kelly Sikkema
on
Unsplash
Since you are an astute Data Engineer and writer of much software, you‚Äôve probably at least heard of a
Linked List
.
What is it, let‚Äôs ask the internet.
‚ÄúA linked list is a linear data structure, in which the elements are not stored at contiguous memory locations. The elements in a linked list are linked using pointers ‚Ä¶‚Äù
This probably means these nodes are stored in
heap
memory. But, let‚Äôs keep it high level. I mean it sounds fairly simple, right? We just use
lists
everywhere, especially all of you Python folks, probably one of the most common data structures.
So a linked list, theoretically, would be similar (maybe not under the hood), in the sense that each item in the list would point to the next item in the line.
But, you say, what‚Äôs the practical real-world idea behind this
linked list
? Something that is common to anyone in manufacturing or shipping would know this as
FIFO
. First In, First Out.
Linked List in Rust.
So we kept that brief, now we know sorta what a Linked List is. Probably not enough to make all those Stackoverflow and Reddit trolls happy, but we are learning something! The next course of action is going to be to make a simple
Linked List
in Rust.
Why Rust? Because of static typing, I think writing a new DSA concept in a language you are not familiar with can help solidify the idea in your mind.
First, we are going to go back to the picture we drew above, what would we make first? How about a Node. This Node can hold a value, but more importantly, it needs to have something that points to the next Node in the list.
Simple enough, uh? Don‚Äôt worry if you don‚Äôt know Rust well, neither do I. `
Option
` lets me have a
null, nill, none
for
next
, and the `
Box
` allows me to have a recursive struct (aka the `
next
` will reference itself, a `
Node
`.
And now of course we need a
LinkedList
to hold all our
Nodes
.
Our
LinkedList
will hold a `
head
` that is, our first Node, which in turn will hold a pointer to the next Node, and so on.
So how can we string this together into a really simple Linked List?
We create 3 nodes above and give them each a value. The `
next
` is set to None out of the box (
next
is what would point to the following
Node
, in whatever order the list will be in.)
After that, we set, for example,
node_2.next
equal to
node_3
. The same with
node_1.next
, we point it to
node_2
. Make sense?
And print the resulting Linked List?
LinkedList { head: Some(Node { value: 1, next: Some(Node { value: 2, next: Some(Node { value: 3, next: None }) }) }) }
That my friend is my very very very simple introduction to Linked Lists.
Conclusion.
Well, we probably aren‚Äôt going to pass any FAANG or MAANG interviews with that uh? But I guess that‚Äôs not the point. The point of this DSA series is going to give us a very high level of most of the popular DSA topics, giving us at least a solid foundation to start understanding some of the concepts.
Even if we don‚Äôt use them in our daily Data Engineering life, I‚Äôm convinced at least having some idea of these concepts will make us better engineers.
Now for Spring Break, see you next week!!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
107048442.rayon-in-rust-vs-python-process-and,Rayon in Rust vs Python Process and Thread Pools.,Data-parallelism in the real world.,2023-03-09 13:14:19.143-06,"Hmmm. This is one of those topics that drives folks crazy. I love it. When it comes to coding and programming there are few things in this world that will make people light torches and rise pitchforks than parallel data processing. The only thing that makes people madder is probably
async
code.
Many moons ago I used to have to worry a lot about
ProcessPools
and
ThreadPools
in Python all day long. For various long-winded and interesting reasons, I was stuck in the grips of a Python-only world, doomed to put TheadPools and MultiProcessing to work on a regular basis to make things run in reasonable amounts of time.
Today I want to dip my toe in the cool, clean, and clear waters of parallel data processing in
Rust
. Yikes. Enter the
Rayon
crate, which is apparently easy enough for even me to work with. Out of pure curiosity, we will compare it, performance-wise, to ThreadPools and ProcessPools in Python.
I know they are not one-to-one relationships. I‚Äôm more interested at a high level what options are for parallel data processing in Rust with Rayon vs Python with ProcessPools and ThreadPools.
Let‚Äôs get to the good stuff.
Introduce Rayon in Rust.
Introduce ThreadPools and ProcessPools in Python.
Write some CSV processing code.
Compare Results.
Wait for the insults to come.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Rayon - parallel data processing with Rust.
Code available on GitHub.
So, perchance one day I was scrolling through my interesting Twitter feed, and a little tasty morsel popped up. Rayon. It caught my eye.
‚ÄúRayon is a data-parallelism library for Rust. It is extremely lightweight and makes it easy to convert a sequential computation into a parallel one. It also guarantees data-race freedom.‚Äù
The part that got my attention was ‚Äúeasy.‚Äù Sold. Why, because it mentioned in the
README
that all you have to do to take advantage of Rayon was to turn any
.iter() to a .par_iter()
, and the world would be at your feet.
I had recently written a
much-maligned post about Rust vs Python in an AWS lambda.
It was a great way to learn some more Rust, and I remembered using `iter()` to process the lines of the flat-file/CSV file.
Me thinks to myself, what a perfect little test for Rayon. Mix in a little Python and compare the performance. Now there‚Äôs a recipe for making people mad again. Can‚Äôt resist, I‚Äôm a creature of habit.
Python - TheadPools and ProcessPools.
The devil‚Äôs work in Python
started with
ThreadPools
and
ProcessPools
and ended with a strange demon child called
async
. Anyways, I digress. Because of the infamous GIL in Python, our most beloved language has always languished far behind pretty much every other language since the beginning of time.
But, there are many instances where
TheadPools and ProcessPools
in Python can come in very handy when hobbled by the ability to use another language for speed.
Now before you get all up in arms and cry foul, I‚Äôm staying that Rust‚Äôs Rayon is the same thing as ThreadPools or ProcessPools in Python.
What I‚Äôm saying is that all of these little wonders are made with the idea to increase data processing speeds and are fairly easy to use.
That puts them in the same fighting ring in my book. Put the gloves on, it‚Äôs time to fight. What I‚Äôm most curious about is, we have all these tools that are ‚Äúeasy‚Äù to use and implement, how easy are they? How does the performance compare?
Our simple data processing example.
Photo by
Matthias Speicher
on
Unsplash
So we need a simple data processing ‚Äúproblem‚Äù to use for our example and work. What I did was combine about 10 months‚Äô worth of
Divvy Bike trip data
into a single large CSV file.
How did I combine the files?
Spark
of course. It‚Äôs good for lots of things. Data munging is one of them.
Spark context available as 'sc' (master = local[*], app id = local-1678228180729).
SparkSession available as 'spark'.
>>> df = spark.read.csv('*divvy-tripdata.csv')
>>> df.repartition(1).write.csv('out')
This turns out to be enough records for our needs probably, 5 million-ish.
>>> df.count() ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†¬† 5,354,607
for our pretend probably let‚Äôs just turn this CSV file into a tab-delimited file. Aka, we will have to iterate each row and exchange the commas for tabs. This should suffice.
Rust with Rayon - flat-file processing.
So it‚Äôs going to be a pretty simple Rust program, we will write it normally, see the performance, then swap out any
iter()
we have for Rayons
par_iter()
and see what happens. Sounds simple, doesn‚Äôt it?
Here is the first, non-parallel code written in Rust.
The time,
8.14 seconds
.
(Strange, slower than Python out of the box.)
Probably my terrible Rust catching up with me.
I bet you‚Äôre as curious as I am if we swap out every instance of an
iter()
with
par_iter(),
of which the astute reader will have counted exactly 1 of them.
Converting each line from, to tab-delimited, easy spot to parallelize. It must
iter()
the records, so we can easily
par_iter()
Here is the same code with the new
par_iter()
for the comma-to-tab replacement.
And the runtime,
4.86 seconds
. Nice! 41% percent performance improvement over the non-Rayon Rust code.
So what does Rust look like with and without Rayon on 5 million+ records? I guess you shouldn‚Äôt shake your finger at a
41% increase
, especially when doing real-life production workloads.
What do I love most about Rayon + Rust, it‚Äôs super easy. I mean you just have to think slightly differently about your code and add some
par_
swaps to your code, and that‚Äôs it. No worries about memory corruption, races of threads, nothing.
Python ThreadPools and ProcessPools.
You knew it was coming. I can‚Äôt wait for all the Python fanatics to come at me with anger and vitriol. But, that‚Äôs the spice of life after all. With the Python GIL it‚Äôs not that easy to do data parallelization for real. I mean there is always the cursed async nightmare.
But, let‚Äôs start by writing the plain old Python script.
Very quick.
6.04 seconds
. What‚Äôs this devilry, Python is way faster than Rust?! My rust ignorance must be catching up.
TheadPools in Python.
Ok, so now what can we do to try TheadPools out with our Python code? They are actually a little more painful to use, but not out of hand. We have to break up our lines to separate chunks, spin up a ThreadPool, write a function that can do some of our tab conversion work, collect the futures, and write them back out. Ok, maybe it‚Äôs a big pain.
Funny, it actually takes longer than vanilla Python. Why? Because our work is not IO bound, it‚Äôs CPU bound, and therefore doesn‚Äôt benefit from our ThreadPool. Now if we were waiting on some network call.
There‚Äôs the performance, 6.17 seconds.
ProcessPools in Python.
Maybe since we are CPU bound with our string work, a ProcessPool will be kinder. The code is almost the same.
Yikes! Even slower.
Ouch.
22 seconds.
Dang, those Python ProcessPools are slow!
Code available on GitHub.
Now, before you send me angry comments and emails, the whole point of this exercise isn‚Äôt really what‚Äôs faster than what, whats better performance wise. It‚Äôs really about stepping back and doing two things.
Starting to understand parallelization in Rust at a high level.
How easy is it to use Rayon in Rust.
Remind ourselves about Process and Thread Pools in Python.
See how easy or not easy it is to implement all of the above.
I found that surprisingly, for whatever reason, using Rayon in Rust is extremely easy, and using ThreadPools and ProcessPools in Python is somewhat of a pain.
This isn‚Äôt really a surprise after all though, when we think about why and how Rust was written, and what Python is good at and used for on a daily basis.
I hope you found something interesting, I caused you to think, yell at my code, and otherwise exercised your curiosity and passion for Data Engineering and programming in general. Till next time.
Share Data Engineering Central"
106903914.contributing-to-open-source-software,Contributing to Open-Source Software.,The ins and outs.,2023-03-06 22:06:44.268-06,"I've always slept with one eye open when it comes to open-source software. I'm an avid user of too many open-source projects to count. But, I've never had the time or inclination to contribute in any way. I've seen my role as more of one out in the community creating content for others to consume.
It's hard to put into written word the effect that those who contribute their time and labor to open-source projects have upon the rest of us. Where would we be without them? Up a creek.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I decided to try my hand.
Well, I did it. I decided to take the plunge and contribute to a open-source project. I'm going to do two things.
Explain the benefits you will gain from contributing.
Let you watch me stumble through a simple, small, code contribution.
First, here is why you should find a project that you can relate too, and contribute.
It forces you out of your comfort zone.
It forces you to ask questions and be humble.
It makes your appreciate the hard work of others.
You will probably run into and learn new styles of coding and design.
It makes you think more about documentation and testing.
It teaches you to slow down and think.
You get to read lots of code and learn from others.
It gives you a sense of purpose and accomplishment that's hard to find elsewhere.
That's just to name a few. It isn't that hard to find projects that need help. If you write code you probably use 20 of them a day that have a long list of needs ranging from documentation to more complex tasks.
Go on GitHub, find some projects who's work you use, look at their list of open issues and find the ones marked ‚ÄúGood first issue.‚Äù Contact someone on the issue and go to town.
Still unsure of what it takes to contribute? Watch this video of me opening a small PR into an open-source project.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
106275571.goodbye-spark-hello-polars-delta,Goodbye Spark. Hello Polars + Delta Lake.,Freeing Delta Lake from Spark.,2023-10-02 09:00:48.573-05,"Photo by
Junseong Lee
on
Unsplash
Sometimes it‚Äôs the small things in life, we get so used to the same old same old, that we don‚Äôt know what we are missing. Such has been the case with
Delta Lake
and Spark. They‚Äôve been one and the same, nearly impossible to distinguish between the two.
This inseparable tie between
Delta Lake
and
Spark
has had many benefits and is probably responsible for the meteoric rise of
Delta Lake
as the defacto Big Data storage API. But, this has had a somewhat unfortunate side effect. Not all data is created equal. Also, as a user of Spark for many years ‚Ä¶ I appreciate it, but I also want to be set free.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Delta Lake has grown in popularity and is used by organizations large and small, with data large and small. This presents a problem for Spark. Spark is for big data and comes with overhead and complexity. Many times Spark isn‚Äôt necessarily required for some Delta Lake operations. Spark was the only real option for easy interaction with Delta Lake for a long time, but no more.
Spark is too heavy for some use cases with Delta.
There haven‚Äôt been many production-level non-Spark Delta options.
Polars provide production-level light and easy Delta Lake access.
It‚Äôs time for us to break from Spark a little bit, our old friend Spark will be fine, always ready to support us when we need it. But we‚Äôve got a new kid in town ‚Ä¶
Polars
, and the new kid has a lot to offer.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Imagine a world where you can simply `
pip
` install on dependency and suddenly have access to
Delta Lake
on anything from a small
lambda
, Airflow
worker
, or a tiny Docker
container
. That‚Äôs powerful, that can save money, compute, and complexity. It‚Äôs a game-changer.
Enter Polars + Delta Lake.
This is what we‚Äôve all been waiting for. A cheap, fast, and reliable way to do Dataframe and Analytics work with a Rust-based tool on top of a Python API. The potential to upset the throne of Spark is here.
Trying it out.
It‚Äôs one thing to talk about replacing something like Spark with Polars, especially when you are using a storage layer like Delta Lake. You can‚Äôt just be doing the classic ‚Äú
hello-world
‚Äù examples, where you just read some local disk with local data.
It has to be real. So, in the spirit of realness, we are going to work on a sample problem with files based in s3, ingesting and transforming that data, depositing the results in a Delta Lake stored in s3.
A very simple, yet common data pipeline that is, in pretty much every case, built with Spark. I, like you, don‚Äôt know if this is going to work yet, some of it, or all of it. Who knows?
Setting up the Project
To get started, let‚Äôs build a custom Docker image that will contain all the tools and packages we need to execute the code we will be writing.
Good DevOps practices are at the core of next-level Data Engineering
.
In our case, we should need a fairly small set of tools. Since we‚Äôve actually started yet, let‚Äôs just take a guess at what we will need.
Ubuntu Dockerfile
docker-compose
polars
delta-lake
pyarrow (because Polars sucks at s3 files)
some test data.
Maybe we will use them all, maybe not, we shall see. All code is available on GitHub here for your use or to ignore as you please.
First, our `
requirements.txt
` file.
Next, our simple `Dockerfile`
Of course, we will need some sample data, probably CSV.
Divvy Bike trips is a free open-source data set
, wonderful for little projects like this.
I downloaded 7 CSV files from 2023 into my personal s3 bucket.
This is what the data looks like.
The Polars data pipeline.
Let‚Äôs do a very simple pipeline in Polars. Something that is common and basic to every Data Engineering at some point in their life. Let‚Äôs
pull
some data,
transform
and
cast
data into the correct format,
calculate
some metrics, and deposit it into a data source, then read it back. In this case, our data source will be
Delta Lake
.
There are probably more Spark pipelines running through these steps than you can imagine, running just now, at this moment, burning money like there is no tomorrow.
Read CSV data
Transform/Analytics
Save to Delta Table.
Read back Delta Table.
What does this look like with Polars?
Code available on GitHub.
Heck, as someone who‚Äôs been writing
Spark pipelines for years ‚Ä¶ this Polars one
really isn‚Äôt that much different. I would so the only major difference is that we have to mix in
pyarrow
into the mix with Polars is the only gotcha.
Polars doesn‚Äôt have the best support for reading remote directories of files on cloud storage, like s3 for example. In the future, I would like to see Polars make this functionally more seamless. This would only increase the usage of the tool.
This little extra work with pyarrow requires us to ‚Ä¶
make a connection to s3 via S3FileSystem support in pyarrow.
scan the bucket and files creating a
pyarrow dataset
.
It really isn‚Äôt that much extra work, and once we are done at that point we can switch over to Polars, pointing it at the pyarrow dataset.
At that point, we can then use
Polars SQL Context
to do our dirty work, just like SparkSQL, very nice. Polars even has the ability to write back the results to a remote Delta Lake in s3. Not much to complain about there.
The pipeline is very straightforward and easy, and that‚Äôs the way it should be. Here you can see the metrics, as well as the Delta Lake files in my s3 folder.
and the actual Delta Lake files.
The future is now.
I‚Äôve been working on Spark and Data Lakes for years, and I can honestly say this is a game-changing tech stack. There is serious potential for cost reduction and savings with Polars +
Delta Lake
.
Just recently at work, I put in a PR that was merged to do this exact thing. Read a Delta Lake outside of Spark ‚Ä¶ inside Apache Airflow as a matter of fact.
Spark is for Big Data and a lot of pipelines need Big Data tools. But, there are lots of workloads that don‚Äôt necessarily need a whole Spark Cluster, and all the added costs of Databricks to solve the problem.
We just use Spark because it‚Äôs convenient and easy to use ‚Ä¶ we‚Äôve never really had another real option. I mean Pandas is junk. Well, that has all changed now. Polars is here, and paired with pyarrow not even s3 buckets are safe from us now. No excuses.
Polars has been all the rage and talk for 2023, but now is the time for actual execution. Let‚Äôs put Polars into production."
105836392.should-you-use-githubs-copilot-to,Should you use GitHub's CoPilot to write your code?,"CoPilot wrote a Data Pipeline for me. Good, bad?",2023-03-02 18:56:47.28-06,"Since everyone else is talking about AI these days I have no choice but to get on the bandwagon. Like jump on it. Hard.
Let‚Äôs be honest, as people who write code all day long it‚Äôs hard not to be a little skeptical about AI taking all our programming jobs. We know code too well to fall for that line.
‚ÄúIn our haste to wave our hands in the air about AI and shove it under the rug, are we missing out? I think so.‚Äù
The problem is you don‚Äôt want to be the person riding around on your horse complaining about those pesky automobiles and how they will never catch on. Let‚Äôs not be those people. Is there anything good about ChatGPT and GitHub‚Äôs CoPilot?
Yes. Is there something bad about it? Yes.
What's the catch with CoPilot?
It's actually pretty simple. There are only a few things you need to know about AI taking over programming.
CoPilot can write code better than a new junior developer.
CoPilot can make it more efficient and effective.
If you are new to programming, don't touch it. It's a drug.
The world is changing, you should come along.
These are my takeaways. Best listen up."
105088310.data-what,Data What???,"Data Warehouse, Data Lake, Lake House ... will it ever end?",2023-02-25 13:59:41.624-06,"What‚Äôs the difference between a Data Warehouse, Data Lake, and Lake House? Asking for a friend.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
103887859.data-engineering-and-not-loosing,Data Engineering and not loosing your mind.,How I balance an aggressive professional and personal life.,2023-02-27 09:30:11.722-06,"A reader recently asked a question about
work-life balance
. It brought up a lot of mixed feelings in me, I‚Äôve been through times of burnout and times of perfect balance and relaxation.
Life‚Äôs hard, and so is work
, both have highs and lows. The highs are high and boy, are the lows low.
Photo by
Tengyart
on
Unsplash
I can relate to burnout and all the emotions that come along with it, especially in the
early
part of my Data Engineering career. There are a number of influences that affect how you can balance work and life, and be successful at both, or not. The funny thing is, there is an overlap between ‚Äústuff‚Äù that works for life and work, they are not mutually exclusive.
Today we are going to dive into this fraught subject, work-life balance in Data Engineering.
What is work-life balance?
How to manage and identify stress and underlying issues.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Work-Life Balance.
Hasn‚Äôt this topic been done and overdone a million times over? Yes. Then why does the question come up, why is someone still asking me how to balance mental wellness while being a Data Engineer? Probably because we are all human and are born to trouble from beginning to end.
It seems like there is something underneath all these questions about work-life balance and handling the mental strains. Here‚Äôs my take.
What‚Äôs underneath the questions about work-life balance?
I‚Äôm going to side-step the question of ‚Äú
How to do this role when you have mental health concerns?
‚Äù I want to look past this question and give you my personal worldview on why this question comes up in the first place.
‚ÄúPeople, us, we, all are looking for meaning and happiness, to feel safe and secure. Most all of us are required to work in some form to provide for ourselves to obtain that end.‚Äù
‚ÄúWhen working we trade part of ourselves for ‚Ä¶. well, basically money. We in turn use that money to put a roof over our head, feed ourselves, and the like.‚Äù
Photo by
nikko macaspac
on
Unsplash
What is it about work-life balance that causes problems?
I‚Äôm going to now rattle off a bunch of reasons why I think work-life balance can cause us so much heartache. It‚Äôs because there are a lot of very complicated ‚Äúthings‚Äù happening in our personal lives, and work lives, and when these two things intertwine together ‚Ä¶ the result is many times ‚Äú
imbalance
‚Äù, in the sense the ship you‚Äôre own starts to rock back and forth and make you feel funny.
What kind of complicated ‚Äúthings‚Äù intertwine?
Workplace culture.
Home life and interpersonal relationships in the moment (
immediate and extended
).
Physical well-being (
sleep patterns, food, exercise, etc
).
Amount of ‚Äúwork‚Äù at work to do (
pressure
).
Self-identity and meaning as a person.
Co-worker and boss relations.
Finanacial obligations.
World and cultural happenings.
You have to admit this is a list of life ‚Äúthings‚Äù we all experience and that each of them as an individual can become extremely complicated, let alone throwing them all into some big pot and mixing them together while adding some fireworks.
At any given time, maybe half of the things on the list might be going well, and the other half not so much.
This is what I think leads to burnout, work-life balance, stress, call it what you want, a very real personal armageddon.
How to manage and identify stress and underlying issues.
So how can we unwind from this mess and get some clarity and relief from stress, work-life balance, or whatever? I‚Äôm going to give you a few baseline measures to put in place, that by no means will solve all your problems, but surely will lead to more happiness on your part.
Let me know if I missed any that have been helpful to you.
Photo by
Andre Hunter
on
Unsplash
Regular uninterrupted sleep.
No one functions well over an extended period without adequate rest.
Exercise (hard physical exertion)
Like actually running or going to the gym.
Being in nature, wind in the hair, sun in the face.
Go outside silly.
Have a hobby that makes you happy.
Find something that isn‚Äôt work (coding) that you find joy in.
Find a new job if you‚Äôre in a toxic work environment.
Work isn‚Äôt worth your health.
Find identity/meaning outside of, or in addition to your vocation.
Probably related to the hobby.
Don‚Äôt work overtime for extended periods.
It‚Äôs ok in short stints, not for months+.
Find ways to overcome hardship.
Life and work are hard, learn to persevere, it‚Äôs a learned skill (like running a few miles).
Practice ways to disconnect.
Read a book, go for a walk, whatever, train yourself to disconnect.
Eat well.
Don‚Äôt eat junk all the time, cook for yourself healthy meals.
Learn to manage your time better.
Become the best you‚Äôve ever been at time management.
Take a class on conflict management.
Get on YouTube, and learn how to have difficult conversations. You‚Äôll feel better.
Make a budget and live within your means.
Be a wise manager of your financial resources.
Stop consuming so much TV and social media.
FYI. They make money on getting your emotions boiling.
Big list uh? What did I miss? I haven‚Äôt gotten to the hard part yet.
I‚Äôve got good news and bad news.
Photo by
Roman Kraft
on
Unsplash
So after all that, if you‚Äôre not even more overwhelmed, just wait. Ha! So that was a big list but I would call most of it common sense, probably stuff your Grandma did every day of her life.
What‚Äôs the good news?
‚ÄúThe good news is that even though you might deny it, pretty much everything on the list and related to your work-life balance is under your control. No excuses.‚Äù
Bad job? Step up to the plate and make a plan to get a new one. Not exercising, getting outside, and eating well? Seriously,
it‚Äôs not rocket science
. Sure I know not everything is under your control, but more than you think is.
You‚Äôre your own master, act like it. Take control. In all seriousness, implement some of the stuff on the list, take charge and action, movement forward, that is how you get out of a bad situation and be on the wrong side of work-life balance.
What‚Äôs the bad news?
That you‚Äôre in control. Good luck."
103706699.hey-you-yes-you-come-here,Hey. You. Yes you. Come here.,,2023-02-18 10:34:27.568-06,"We have opinions, what's yours? Tell me about what topics you want me to write about more. Do you want more indepth stuff with code? You want more high level stuff? No wait, maybe you want both? No, I know, you probably like it when I do data product reviews and tear into some tool and make everyone mad.
I must know. Tell me what you want."
103134369.failure-the-best-teacher,Failure. The Best Teacher.,Putting yourself in the uncomfortable zone leads to mastery.,2023-03-06 08:55:19.791-06,"Photo by
Yaopey Yong
on
Unsplash
Maybe I'm just a glutton for punishment, maybe not. Could it be some offscouring from my childhood? Who knows. It's clear to me that most of my success has come through suffering and failure.
Why? Not sure. Maybe it has something to do with learning your lessons, finding your limits, or learning about overcoming obstacles.
This is the hill where I'm going to plant my flag and stick to it. Want to grow? Want to be a better Data Engineer? Then failure is your teacher, your schoolmaster, and the key to your future.
What will we cover today?
Data Engineering and being uncomfortable.
The dangers of not failing.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering and being uncomfortable.
I noticed something very early on in my career in tech, and it was this ‚Ä¶ I saw sitting next to me the stagnant developer. Festering like a ripe tomato in the sun, becoming an object of derision and ridicule.
I decided not me. I learned an important lesson back then that stuck with me. To be comfortable is not to grow, to grow is to learn and fail, rinse and repeat.
Putting yourself in uncomfortable technical positions is worth the price.
If you are comfortable, you won't fail, and you won't grow or learn.
The easy path is always the familiar path, but it has no adventure and is well-trodden.
Become uncomfortable and work your way out of that spot, all of a sudden you've become better.
It seems unpopular these days to accept hardship, I get it, life and work are difficult the way it is. Data Engineering moves fast, Databricks, Rust, Polars, Prefect, Dagster, data contracts, data mesh, data quality, get the point? Data Engineering never stops.
To become comfortable is to become that SSIS developer. Don't do that, or don't do it forever. Be curious, try tools, and new things, break things, and fail.
When you are a Data Engineer sitting in front of a new project, maybe it's a new cloud provider, a new tool you've never used before ‚Ä¶ and you are so lost you don't know where to start. Stop, and thank the stars. Why? Because you are uncomfortable, you are standing on the precipice of the unknown, of greatness, of learning, of struggle, and of overcoming.
Raise your glass to being uncomfortable.
Example of (weekly) failure in my life.
Let me share an insight from my personal life about this. This happens on a regular basis. So I write and develop content, hence your reading this. Do you know how much failure that involves? A ton!
I've written a lot about Rust lately
. That means I've had to learn Rust in my free time. Is that easy? No, it's extremely hard.
For example, I'm about 3 weeks into a
simple article about writing AWS lambda in Rust vs Python
. I shot through the Python part, been there, and done that.
The Rust part? Weeks of failure. I've never written or deployed a lambda with Rust. I work on it for an hour at night, fail, and leave it. Come back another night, fail, errors, and leave it. Come back again, success.
That feeling of frustration and uncomfortableness. I wanted to give up on the article, quit, and do something easier. Bad. But I didn't. I embraced the suck. Guess what? I learned a lot, lessons I won‚Äôt forget.
The dangers of not failing.
Photo by
Ra√∫l N√°jera
on
Unsplash
I know this is strange, probably not what you‚Äôve been taught your whole life. I‚Äôve been writing code for ‚Ä¶ let‚Äôs see ‚Ä¶ 17 years now. Most of it self taught. I‚Äôve come to a conclusion on the matter ‚Ä¶
‚ÄúTo fail is to learn, the learn is to fail‚Äù
If you aren‚Äôt failing you probably aren‚Äôt learning hard enough, or new enough things. And to sit around in Data Engineering and not learn for 2 years ‚Ä¶ is pretty much a death sentence, our world changes too quickly for that kind of stagnation.
If you aren‚Äôt trying something new and failing at it at least once a month, time to re-evaluate.
How can I apply this in my Data Engineering journey?
Be active on Linkedin, follow ‚Äúinfluencers,‚Äù and see where ‚Äúthings are going.‚Äù
Pick a new tool, no matter how small, once a month. Try it.
Think of DuckDB, Polars, and the like.
Read good technical content. There are plenty of classics, reading is learning.
Find some good YouTube channels.
Learn a new programming language (Rust, Golang, Scala).
At the very least you might not become an expert on everything, but you will know enough to see through marketing material, understand the value of tools and technologies, and generally be able to make better decisions as a Data Engineer.
Have you played around with Golang, Rust, Scala? Why not? Just because you don‚Äôt use them in your day job doesn‚Äôt mean they aren‚Äôt helpful to you.
Also, if you haven‚Äôt seen the
free 7 Data Engineering Exercises I put on GitHub
, try it out, you never know, might find something new.
When you find yourself in a comfortable spot.
It could be a job, maybe it‚Äôs a technology. Whatever it is, if you find yourself slightly bored, not learning or challenged ‚Ä¶ it‚Äôs time to make for the hills my friend. The reality is that you can‚Äôt always rely on your day job to provide those challenges and learnings.
At some point your going to have to break the mold and push yourself, but for better or worse, you know you better than anyone else. You know where your weaknesses are. Exploit them.
You‚Äôve been taught your whole life via your human nature to protect and bury your weaknesses, you should try the opposite sometime. Bad at DSA? Give it a try. Never played with Spark much, go for it. Mostly a batch person and never had a chance to mess with Streaming. What are you waiting for?
There is a Dockerfile for everything these days my friend.
Everyone breathes the same air. Death and taxes. We are all the same. You think people are much smarter than you, but that‚Äôs just social media junk. The 99% of us are just wandering around looking for something interesting to work on. Don‚Äôt wait.
Do something uncomfortable and new this week."
100070614.debugging-data-pipelines,Debugging Data Pipelines.,Is it a pipeline issue ... or a data issue? Or both? Argg.,2023-02-20 09:21:08.188-06,"Photo by
David Clode
on
Unsplash
Some things never change, and you would think as the years pass by, debugging software would become easier. I have found that data pipelines seem to have a ‚Äúspecial‚Äù kind of hard,
insidious
bugs that reap a harvest of sorrow upon Data Engineers.
Debugging data pipelines is probably one of
Dante's seven rings of hell
. Sure, you can do it over and over again, but complex data pipelines have one big problem.
Data
. Data is messy, data isn‚Äôt what you think, data changes, and data adds an extra layer of complexity. Data is hard.
The lethal combination of code and data will bring the Senior Engineer with decades of experience to their knees. But, there is light at the end of the tunnel. Fortunately, there are things you can do to lessen the pain. That‚Äôs what we are going to cover today.
What makes debugging data pipelines hard?
How to prevent data pipeline bugs.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Let‚Äôs dive into our topic for the day.
What makes debugging data pipelines hard?
So, you‚Äôre plodding along life as a Data Engineer, writing pipelines and taking Jira tickets like there is no tomorrow. Maybe you‚Äôre working on a big migration project, with lots of code, and complex and in-depth transformations a mile long.
Then things go wrong.
Failures, late-night Slack notifications. Pressure builds, it needs to be fixed and it‚Äôs on you. You struggle, it‚Äôs hard to tell what exactly is leading to the error.
Is it a particular function? A recent change? Did something in the data change? Was it a dependency?
You end up spending hours digging into stuff, not sure where to begin or look. So what is it about data pipelines that make them hard to debug?
Data pipelines are typically written poorly, simply put, with bad code quality.
No unit test or integration test coverage.
No Data Quality systems exist to catch or monitor bad data.
Testing environments are usually lacking.
The intersection of code and data can amplify complexity.
It‚Äôs worth noting that only one of those items listed above is out of your control. The rest can be solved or improved.
How to Prevent data bugs (and make debugging easier).
You know, if it wasn‚Äôt for the data and the code, all our pipelines would run perfectly at all times. Such is life. If we think about a data pipeline as our little child, carefully and wonderfully woven, we pawn over it and protect it with our life. How do we protect it?
We have two little evils that are always trying to corrupt our pipelines.
Bad and Changing
Data
Bad and Changing
Code
All these issues make it harder to debug poor data pipelines. It may seem backward, but your only hope of getting better at debugging data pipelines is understanding how to prevent problems from happening in the first place.
Protect your Data Pipelines.
We have to deal with these two bad friends, they are trying to lead our pipeline astray. Most of the time when a Data Engineer starts to build a new pipeline they start out with a cheery and happy attitude, glad to be working on something new and fun. Rainbows and sunshine.
‚ÄúUnfortunately, that attitude, while good in life, is a bad one for a Data Engineer when dealing with the creation of their hands and minds, the pipeline. Nothing is your friend when bringing your slithering and twisting pipeline from the bowls of the earth into the light.‚Äù
You must protect it. Be like
Gollum
and his precious. Your pipeline is precious. Everyone and everything is trying to take it, and break it.
Sorry you nice person, time to be mean.
How can we protect our Data Pipelines?
As I mentioned previously that there are two little blighters that are always coming from your pipelines,
Data
and
Code
. These are what we need to protect ourselves against. But, you say, ‚Äú
those are what pipelines are made for!
‚Äù I know, stop whining. All the same, we must protect.
Protection is against Data.
When it comes to data pipelines and protecting against data, all kinds of data, bad data, changing data, new data, and no data, it‚Äôs all the same ‚Ä¶ the data will eat your lunch. How can we protect our data pipelines from evil data?
When you write a bit of code, some function or method, maybe even some nasty SQL. Do these things.
Never, ever, ever, ever, ever, ever, e‚Ä¶
ASSUME
data is what you think.
Think about the absence of data.
Don‚Äôt assume data types.
Always assert your data before working on it.
Understand the uniqueness of your data.
Understand the data relationships.
Monitor and alert on your data.
That should keep you busy for a while. Ok, you say, but practically HOW do I do these things? Well, the
idea
of them is the more important part, rather than the
implementation
.
But for starts how about some Soda core or Great Expectations in your pipelines?
Soda core
Great Expectations
Honestly, even learning about these tools will make you think about your data and write code differently, let alone implement them in some way.
How else can you practice data protection .. and make debugging data pipelines easier?
Harden your data transformations, functions, and methods.
What do I mean? Well, harden your logic AGAINST data.
def some_thing(df_2: DataFrame, df_1: DataFrame) -> DataFrame:
    if df_2 and df_1:
        df_3 = df_2.union(df_1)
        return df_3
Ok, silly example, but my point is that this type of thinking and approach to code will not only solve a ton of a problems ‚Ä¶
but when it comes to debugging, this is one less thing you have to wonder about!!
When you approach your code with this sort of ‚Ä¶ ‚ÄúI don‚Äôt trust your data‚Äù mindset, life becomes easier.
Check data types, don‚Äôt assume duplicates don‚Äôt exist. Don‚Äôt trust anyone.
def some_thing(df_2: DataFrame, df_1: DataFrame) -> DataFrame:
    if df_2 and df_1:
        df_2 = df2.dropDuplicates(['id', 'name'])
        df_1 = df1.dropDuplicates(['id', 'name'])
        df_3 = df_2.join(df_1, on=['id', 'name'])
        return df_3
Your smart, you get the point. Think about it, if you are debugging a data pipeline that is having some issue, especially a data issue, these are the types of things you do. Check to see if data is getting to a point, what the data looks like, if there are duplicates, blah, blah, blah.
Instead, design and protect your data pipelines from the start against these intrusions. Don‚Äôt assume, assert and protect.
Protection against Code.
Code. If it wasn‚Äôt for all the code you wrote, coming back to haunt you. Think about it, we solved the last problem by writing more code, but what if we right bad code? Out of the frying pan into the fire like the old folk say.
How can we survive our own code? Protection against data is important, but debugging data pipelines means diving deep into the code. Yikes. Where do we even start? I know, with the basics.
Unit test everything.
Build integration tests.
Learn about functional programming.
Use statically typed languages, or type hint your Python etc.
Implement CI/CD builds, deploys, linters, etc.
The honest truth is that Data Engineers and their pipelines need to catch up with the rest of the Software Engineering world. Data Teams are typically embarrassingly lacking in the most basic protections.
Anyone can start writing unit tests, all it takes is a little push. There is no excuse. Writing unit tests push you to write better and different code, they start to catch errors and bugs.
Having solid development environments that are actually simple to deploy to and test changes seems like a superpower even today, which is silly. How can you expect good-quality code if you can‚Äôt test it in a production-like environment? You have no idea how many bugs you can catch in a development.
Also, I know you all love Python, as do I, but admittedly when you can do anything with anything, pass it around, mutate it, delete it, whatever ‚Ä¶ things are going to happen. The very least you can do is start to type hint, at least give people and the idea of what you are doing or what the intention was.
Protection is the best debugger.
In the end, you have to get ahead of problems. When you go to debug pipelines you will always start to look in two different places.
Data
Code
One of the two is coming for you. I have an idea, while you are trying to figure out what the problem is, duplicates, no data, bad join, data type mismatch ‚Ä¶ whatever, why don‚Äôt you put some protective measures in place?!?
Go ahead, save future you some time and frustration.
Tell your boss that a better development environment with production-like data is needed. Tell your boss you want to start unit testing, using type hints, and implementing a data quality tool. Explain your desire to raise the bar, and make development cycles faster and debugging easier. Be brave, be better."
98985559.how-to-deal-with-ambiguity,How to deal with ambiguity. ,"If you have no idea what to do, what do you do?",2023-02-06 08:50:36.793-06,"Photo by
Vadim Bogulov
on
Unsplash
Nothing strikes fear in the heart of some freshly minted developer than sitting in a room full of apparently intelligent people, who jibber and chatter like chimps in the zoo at feeding time. Everyone is talking fast, throwing terms, knowing nods, and mumbles and grumbles with insider looks and agreements. Everything seems important, and obvious to the crowd.
Finally, everyone turns their burning and perceiving gazes in your direction, asking you, ‚ÄúSo, what's your estimate, how long is this going to take?‚Äù
Fumbling for words like you just swallowed a lump of sawdust, you wave your hands back in forth and mumble something about a week or two. Exhausted, you slump back in your chair, sweating and feeling like you just ran a marathon. You want to crawl into the nearest hole and already wonder what you‚Äôre going to share at tomorrow‚Äôs standup.
Ambiguity. The destroyer of confidence, projects, and software engineers.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
When you don't know what you don‚Äôt know.
When you are new to your career, or job, it‚Äôs a pretty normal feeling, being
lost
that is. Pretty much nothing makes sense, at all. It‚Äôs strange, you go from being a confident human being to feeling like a helpless baby. You probably don‚Äôt like it, your body doesn‚Äôt like it. It‚Äôs not fun for anyone, although some people probably deal with it better than others.
Maybe you‚Äôve been around awhile, but something new gets dropped in your lap from Product. There‚Äôs a good chance that you can tell that Product Leader doesn‚Äôt even know what they want, shapeshifting between one meeting and another like some dark elven magic.
Doesn‚Äôt matter. It‚Äôs all the same problem, ambiguity.
This situation seems to be especially true for software engineers and developers, as per the nature of our chosen profession, tools, tech, and approaches change as quickly, and are never stable or last.
It can be quite difficult to tackle a project when you don‚Äôt know ‚Ä¶
The business context.
The toolset or architecture.
Who anyone is (relationships).
Where to look for answers.
What the expectations are.
Ambiguity, ambiguity, ambiguity.
That‚Äôs just life in software, but how can we overcome such things, is it possible to thrive in an ambiguous environment? I think so. I believe there are a few simple steps anyone can take to ferret out enough information to become a valuable member, the unicorn who doesn‚Äôt need a lot of hand-holding and is able to produce results and iterate as needed.
It starts with a question. ‚ÄúAsk and ye shall receive.‚Äù
Photo by
Ana Municio
on
Unsplash
Questions, it all starts with questions. Instead of stewing in the unknown, and thinking about all things you don‚Äôt know, start by asking a few questions. Part of what separates a Senior Engineer from one of them there ‚ÄúEngineers‚Äù is the ability to ask questions, and the right questions.
Subscribe now
It seems kinda obvious, but when you have ambiguity, some of that is by design, the product may be in flux and there might be no solid answers, but some of the ambiguity is most likely because something is locked inside someone‚Äôs head, tribal knowledge, and needs to be let loose. Let loose via questions.
I suppose it‚Äôs hard to generalize, but when it comes to ambiguity in Data Engineering projects, maybe we can come up with a list of questions to ask. Starting with the basics and diving deep.
To make sense of what you don‚Äôt know, and find out about what you don‚Äôt know, ask questions.
Where does this data come from, and what is its source?
How often does this data come, and what is the frequency?
What person knows most about this data, can I talk to them?
What is the exact desired state?
Which business person (Product?) is in charge of this product, and has a vested interest in this project?
What is the timeline for this project?
Who knows the most about this or that tool?
I guess the list will never end, but you have to start somewhere. Start with a lot of questions, the answers or the ‚ÄúI don‚Äôt knows‚Äù will give you hints and snippets of what‚Äôs going on, this can help you map out the big picture. Once you start sketching out the big picture, you will feel like slowly your world is coming into shape and form.
Start with what you do know.
You may not know everything, but you probably know something more than you think. Surprise, Surprise, the older and wiser you become, hopefully, the more you realize that no one knows everything. Part of being successful in Software Engineering is realizing it‚Äôs a team sport, the best products built are because a lot of good people work together to achieve a desired end.
Sometimes taking small steps can give you a feeling of confidence, help you feel like progress is being made, and actually is progress. Not moving forward for fear of not knowing can be a disease that slowly eats and rots the apple from the inside. From a Data Engineering project perspective, what can you do to start moving forward?
If you have a lot of ambiquity in your project, start working on what you know, as you move along some things will become more clear, some less, you will end up with a clearer picture of where the gaps of knowledge are.
Others will see your hardwork, clarity, and desire to get things done, inspiring them to step in to help, and answer questions.
Closing Thoughts.
I‚Äôm a big fan of action over inaction. If you don‚Äôt know something I think the best course of action is to attack, and move in a direction. Getting caught up in the not-knowing and the endless cycle of not getting anything done, relying on others to solve your problems ‚Ä¶ well that is a problem.
Of course, we have to rely on others for help. But we can still seize the day.
It‚Äôs typically better to do something than nothing, and honestly, all we have to do is apply a little rigor and logic to moving toward and we can slay that dragon of ambiquity."
95088173.whats-the-big-deal-with-immutability,What's the big deal with Immutability?,"To care, or not to care, that is the question.",2023-01-23 09:01:14.488-06,"Photo by
Zoltan Tasi
on
Unsplash
I'm sure at least one of those times you were doom scrolling Reddit or LinkedIn when you should have been learning, you ran across some vague argument about
immutability
. Am I right? I'm right. At least about the doom-scrolling part.
I would wager you might even see the topic come up more in the data engineering community sometime in the near future. Many times this topic of
immutability
comes up when some Scala, Java, or Rust savant is bashing a poor old Python bloke about being a little weasel, a dreg from the bottom of the proverbial barrel.
I‚Äôm guessing the majority of my readers may well be Pythonistas, happily churning out 3x the code of those
Scala
and
Rustaceans
, but I dare see we can learn a little something from them clever folk.
What shall we cover on this wonderful day?
Overview of immutability.
Benefits of immutability for Data Engineering.
Examples.
Closing Thoughts.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Immutability. Friend or foe?
In case anyone of you fine readers is from a county with only a single working stoplight, like yours truly, let's talk about what exactly is
immutability
.
‚ÄúSomething that doesn't change.‚Äù - me.
You would never know I'm from a small town, would you? Maybe the collective wisdom of the internet over on Wikipedia can shed some light for us.
‚ÄúIn object-oriented and functional programming, an immutable object (unchangeable object) is
an object whose state cannot be modified after it is created.‚Äù -
Wiki
So what do we Data Engineers think? Is immutability a friend or foe?
Photo by
Monika Simeonova
on
Unsplash
I would say immutability is our
friend
, and I hope to convince you of this. At the very least I think the underlying concepts and ideas of why to implement immutability in software can serve you well, even as you write your precious Python.
When you are approaching a piece of software, for us data pipelines, it doesn‚Äôt matter if you‚Äôre using OOP, more functional style, or something in between, you have to make choice, even if your choice is just a default because that‚Äôs all you know.
Every variable, object, and the results that you play with, must fall into one of two categories.
Immutable.
Mutable.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
If you‚Äôve not thought about immutability that much, it may seem like such a small difference. But, I assure you, it has important implications for you how you reason and design your software patterns, data pipelines, or otherwise.
Let‚Äôs dive into some more real, rubber-meets-the-road thoughts on immutability and data engineering design patterns.
Benefits of immutability for Data Engineering.
Well of course there are probably reams of internet paper written on many varied and sundry reasons for the benefits of choosing immutable objects when programming. I will relegate that to other smart folks, and I will try to stick to the simple and obvious benefits of immutability.
Here‚Äôs my list.
Objects will be more thread safe inside a program.
Easier to reason about the flow of a program.
Easier to debug and troubleshoot problems.
Easier to test the codebase.
As Data Engineers we should always strive to reduce complexity, keep bugs from reaching production, and generally put our best foot forward whenever possible.
Part of that journey is learning and growing, even say, if we are Python users, we can understand maybe how Scala and Rust use immutability, and how it benefits those codebases, and then try our best to pull those best practices and ideas into our life!
I think it might be best to look at some of these benefits using some
pseudo-code
examples, as it‚Äôs a little easier to paint the picture we‚Äôre looking for.
Examples.
Let‚Äôs take a look at some of the benefits from a Data Engineering perspective. First, let‚Äôs talk about
thread safety
. This probably is only relevant to those Engineers who are building platforms, but it‚Äôs good for all of us to understand some of these topics a little bit.
It‚Äôs kinda obvious, but immutable objects are going to be safe, in the sense, that different processes or threads on a CPU reading the same object is going to the same and with no surprises.
‚ÄúThe next three points we mentioned above can be lumped into a single statement for Data Engineers. Immutability makes life easier.‚Äù
Let‚Äôs look at this example. Something like this code as follows would not be uncommon to run across in a lot of data pipelines. But, this approach has its downfalls and ends up being how most mutable code is written.
It just becomes confusing, even in a simple example what is exactly happening and what‚Äôs happening during the dataflow. When most pipelines are many times more complicated than this example it can become a nightmare to debug, add functionality, or troubleshoot that codebase.
With all objects being mutable it becomes extremely important to know what‚Äôs been changed and when.
Code can‚Äôt be debugged in a modular way, you must start from the beginning to ensure an object‚Äôs
state
or
expected
value is what you think it is at some distance point.
Mistakes are much more likely to happen, mistakes that won‚Äôt nessesarily be caught by unit tests.
Writing code where objects are immutable by default leads to a different kind of experience.
Trying
to write code in Python that you pretend is immutable, while not the real deal, will probably make you write in a different manner. Most likely your code will be more functional, testable, and overall more trustable.
Closing Thoughts.
I hope to open up this topic of
immutability
and
ownership
again in the future, with some more in-depth examples using Rust. For Data Engineers there can be big payoff when we switch from Python‚Äôs world of ‚Äúanything goes,‚Äù to a mindset where we are more reasoned and careful about mutating objects‚Äô
state
and
values
.
We don‚Äôt always have to drink the kool-aid we are handed, but a good sip here and there can help color the way we see the world and solve problems.
If you‚Äôre new to the whole mutability discussion, check out an article or two to get a taste,
here is one for Scala
,
and one for Rust
."
94511809.reducing-cloud-costs-in-2023,Reducing Cloud Costs in 2023,Save some money instead of spending it for once you Hobbit.,2023-01-09 09:02:32.821-06,"Photo by
Mathieu Stern
on
Unsplash
I know you all saw it coming. Probably in the back of your head for the last few years while you were burning queries on Snowflake and spinning up Databricks clusters like there was no tomorrow ‚Ä¶ something in your mind was thinking ‚Ä¶ ‚Äú
This is too good to be true.
‚Äù
The chickens have come home to roost.
‚ÄúEvery data team is going to be pressured in 2023 to start caring deeply about Cloud spend. The Modern Data Stack reckoning is here. No more easy money.‚Äù
Today I want to talk about a few things.
Why the Modern Data Stack costs are getting out of control.
4 Easy Steps to reduce costs.
Conclusion.
Let's make you the cost-cutting overlord, destroying all bad practices and laying waste to the complex architecture laid down by those bloated SaaS companies and ‚Äúarchitects‚Äù hawking their wares on every street corner.
Modern Data Stack costs spiraling out of control.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
It‚Äôs no surprise things are getting out of hand in pretty much every data team, great or small. I mean, it starts with the big three.
AWS
GCP
Azure
These titans of industry are like the great
Kings of old
, marching around and sitting in their lofty castles imposing their burdens and taxes upon us all. In the diagram above, with AWS, GCP, and Azure at the bottom of the pyramid, those behemoths gobble up vast sums of our time, energy, and resources.
There are a few reasons why everyone is talking about costs, and rightly. Here‚Äôs my take on it.
Pretty much every data team sees the shiny new tools (
Databricks, Snowflake, etc.
) and thinks they aren‚Äôt keeping up without using them. This leads to overly complex and expensive architecture.
There has been a lot of add-on SaaS companies that provide everything from GUI ETL, Connectors/Adaptors/ managed compute, storage, and various other solutions. This all adds to the cost.
Hiring a plethora of engineers to support all this tooling and the work involved around implementation and patience.
Migration projects to these ‚Äúnew‚Äù tools can drag on and cost a lot of money (dual pipelines running, extra work, etc.)
All things together can create the perfect storm of costs that don‚Äôt seem to stop, and in fact, keep on rising.
‚ÄúExcitment around new tools like Databricks and Snowflake can lead to over-engineering and using a sledge hammer when in fact you need a small hammer. It ends up at the bottom line, costing big money.‚Äù
4 Easy Steps to Reduce Cloud Costs.
Most of these five easy steps to reduce cloud costs are focused more on data teams and data engineers. On the surface of course they look easy, but don‚Äôt be fooled, they will take a little time and focus to walk through and resolve. But, the payoff, in the end, will be worth it.
You are sure to save noticeable amounts of money if you approach each step with vigor.
‚ÄúTo be honest, most of these cost saving steps don‚Äôt get done on most data teams. Why? Because they always fall into the techincal debt category, that is until everyone is concerned about spend.‚Äù
I used these 4 steps because they are attainable and doable for any data team, and they can be done ‚Äúon the side,‚Äù while other important work continues.
Identify the long-running processes.
Look into your cloud storage usage.
Choose the correct tool for the job.
Deep dive into your compute usage.
See? Not that bad eh? Ok, well maybe they are a little involved.
Identify the long-running processes.
This is probably one of the easiest steps anyone can take on a data team to reduce costs, and has the benefit of being a ‚Äúfun‚Äù task for data engineers to walk on. If you‚Äôre one of the teaming masses using Airflow, for example, finding long-running tasks is a trivial matter.
This sort of identify the long running data pipelines or tasks, and find out what the problem is, gives data engineers something hard and fast to work on,
that gives a certain mental payback ...
aka it‚Äôs fun and rewarding work.
The 80/20 rule applies here. 20% of your data tasks are eating 80% of the costs.
It‚Äôs easy to fix, throw engineers at those problem spots, and they will find a solution.
Doing this will identify where the money is being spent, or a good chunk of it.
Optimize, optimize, optimize, that is the name of the game. Look for those problem children and send them to the principal‚Äôs office, give them the old whack with the ruler. Chances are you‚Äôre spending money, a decent chunk, on some long-running and poorly designed processes that could have runtimes cut by 3/4. Get to it.
Look into your cloud storage usage.
Buckets, buckets, and more buckets, tired of hearing about buckets yet? Cloud storage costs are one of those sneaky little buggers
that comes in the middle of the night and steals all your quarters out of the change jar
.
I know, I know, everyone always says ‚Äústorage is cheap,‚Äù and to a certain point it is. But, things can add up once you hit a few hundred terabytes and growing. But luckily there is usually plenty of low-hanging fruit to be had on those storage trees, delicious fruit full of money-saving nectar to satisfy your soul.
Make sure all your data is compressed.
Find all your unused and un-accessed data, in all environments!!
Put some of your data into
cold storage
, after a certain amount of time.
Nothing is easier than working with cloud storage, start with ensuring all your data is compressed ‚Ä¶ store .CSV files in the cloud? Think about
.gz
‚Äôn
them, it will save you money.
Go look into that awful development environment you have, you will be surprised how fast data piles up in there after a few years.
Nothing is quite as satisfying as deleting vast amounts of data.
The last money-saving tool that is not often used, is simply putting old data in cold storage, all the crud that is years old and you hardly ever use ‚Ä¶ easy peasy lemon squeezy.
Choose the correct tool for the job.
What‚Äôs that saying? ‚ÄúBeggers can‚Äôt be choosers‚Äù, or can they be? This might be one of those times when you should be picky and choosy, like when your mom yelled at you growing up to eat all your food and stop picking at things.
‚ÄúOne of the most common and expensive pitfalls that data teams make is running after shiny new tools, or being enamored with a certain tool, to the exclusion of all others, like that poor horse plodding along the city parade with those blinders on, oblivious to all those screaming kids pelting it with candy.‚Äù
You need to be critical with yourself and your team about what you are using to process what data.
Don‚Äôt use Big Data tools like Spark to process Pandas-sized data.
Move processes around between tools to match requirements,
Evaluate each tool and cut where you can.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Think about it, your paying to spin up some Databricks cluster with Spark, so you‚Äôre being charged some AWS or GCP instance price, plus some Databricks cost on top, all for a few GBs of data that could be processed on an Airflow worker with Pandas,
Datafusion
, or
Polars
.
Just use common sense and save yourself a bunch of money.
If you‚Äôre lucky you might even identify some tool that simply isn‚Äôt giving enough bang for your buck. When times are easy it defaults to just picking stuff willy-nilly with no thought to ROI, or if there is a different way to solve the problem.
Don‚Äôt be scared to be creative and find a way to solve problems with a smaller subset of tools. For example, Airflow is a tried and true technology that offers a massive range of
Operators
and features that might be able to replace certain data tasks found elsewhere.
Deep dive into your compute usage.
I saved the hardest one for last. Compute. If every data team was to look at their bill at the end of the month ‚Ä¶ where are most of the money going? Compute. Compute. Compute.
‚ÄúCompute is the start and the end of data engineering, it‚Äôs the work-horse that does all our transformation and acts upon most of what we create as data engineers. Therefore it‚Äôs going to eat a large portion of our money.‚Äù
There is no easy way to solve the compute problem, but there are a few simple steps that anyone can take.
Identify under-utilized compute.
Identify over-utilized (bottleneck) compute.
Identify whether you are using the
correct type
of compute.
Take a gander at all your data transformation and workloads, and inspect what type of resources they are consuming, most tools allow you to see some sort of memory and CPU utilization metrics. Dive in, and find where you are wasting compute and money.
On the other end find where your compute is maxed out and causing bottlenecks. This can have some overlap with the optimizations we talked about earlier, but you are sure to find a few spots where you can dial down some resources. That‚Äôs just life.
Also, never forget to inspect the type of compute you are using, what I mean by this is the instance types you are using in the cloud. There are massive differences in how, when, and why you are using them. It‚Äôs a bit of black magic and luck, but you can reduce costs by simply switching to SPOT, different regions, or an instance type that maybe has slightly different CPU and memory stats.
So a little research, save some money.
Conclusion.
Well, there you have it. I hope you weren‚Äôt looking for some late-night TV fix-it duct tape or goop. For 9.99 you can reduce all your cloud costs in half! Life doesn‚Äôt work like that, neither does data engineering. Be wary of anyone who tells you otherwise.
But, you can start with the basics.
Identify the long-running processes.
Look into your cloud storage usage.
Choose the correct tool for the job.
Deep dive into your compute usage.
I‚Äôm confident if you tackle a few things on this list, and give it a few days you can save a little money. Your boss will pat you on the head and throw you a bone maybe. Have other suggestions or stories of saving big money?! Please share!"
93982409.unit-testing-for-data-engineers,Unit Testing for Data Engineers.,"I know you don't want to, but if you don't I will call your grandma.",2023-02-13 09:31:04.499-06,"Photo by
RepentAnd SeekChristJesus
on
Unsplash
Don't make me call your grandma and tell her what you've been doing. Writing all that code with no tests, just busting through life like you got no worries. Letting tomorrow worry about itself. How dare you.
Get off the couch, put the potato chips down, and
write some unit tests
.
What we are going to cover.
Why don't data folk unit tests more?
Why data folk should unit test.
The anatomy of testable code ‚Ä¶ aka the How.
The end.
Why are Data Engineers so bad at performing simple tasks like unit tests that can solve so many problems? The million-dollar question that one.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Why data folk don't unit tests?
It's funny that something as foundational as unit testing, which has been accepted in the SWE world for decades, is such a struggle for data teams to adopt. It‚Äôs rarely argued that unit tests are for suckers, it‚Äôs pretty much generally accepted as a key way to maintain software quality control and ensure the squashing of bugs, but yet it remains un-implemented in the vast majority of data teams.
I think there are a few reasons this is true.
Too much SQL.
Focus on moving fast.
Data is seen as a quasi-business function and less engineering in the past.
More folks from a less traditional SWE background.
‚ÄúWe want to get there, but just haven‚Äôt had the bandwidth yet.‚Äù
Below is the pyramid of death for unit testing.
It‚Äôs my not-so-humble opinion that these sorts of situations formed a perfect storm, they all mixed together like some
witch‚Äôs brew boiling and bubbling in the corner
, and producing a poison of indifference and carefreeness in the teaming masses.
Too much SQL.
Photo by
Caspar Camille Rubin
on
Unsplash
I know. Sorry, I'm not sorry. I'm messing with your precious SQL again,
I didn't learn my lesson the last time around apparently
.
Speaking as someone who once wrote SQL for years on end, it is a problem. I'm convinced for the most part that those data teams that are 80% SQL based, probably have very few, if any, unit tests.
Send me the hate mail, I'm ready. Sure,
dbt
has changed a lot of things and some people are getting the memo, but most are not.
Why is this the case with SQL? Mostly because be design, or over time, all the SQL queries become large, complex, and cumbersome, and end up unwieldy and not reusable. Basically, an giant blob that has swallowed everything in its bloated path.
Such things do not lend themselves to unit testing. Oh, and don‚Äôt forget history, isn‚Äôt that what they are always telling us? History matters, and historically SQL is rarely tested, and things take time to change.
Focus on moving fast and business functions.
Another thorn in the flesh of many data teams is that fact that they are driven to rust fast by hard task-masters. It‚Äôs hard not to notice the difference, the data teams always sit closely to the business, which makes sense, the business claim to be ‚Äúdata-driven.‚Äù
This can have some unfortunate side effects.
Close to the business means high expectations and moving quickly.
Moving fast means something gets left behind (testing).
No surprises here. The truth can hurt sometimes, but honesty is refreshing. It‚Äôs better to understand why something is the way it is, then we can see clearly to deal with it.
If there is one takeaway from data teams I would give, it‚Äôs to slow down, take your time, and write tests. This sort of approach, while it may anger your over-lords who want stuff right now, but they don‚Äôt care about the bugs and breakages that will keep up in the middle of the night a month from now. Push back. Slow down. write tests.
The anatomy of testable code ‚Ä¶ aka the How.
Honestly, how to write code that is unit testable on a data team isn‚Äôt really earth shattering, but is surprisingly hard to find.
‚ÄúMostly because there is a correlation between the
IF
unit tests are written or not, and how
clean
the code is.‚Äù
The hurdle to having testable code is ‚Ä¶
Are you walking into a dirty codebase with massive functions that aren‚Äôt testable?
Do you have to
refactor
the code before you can unit test it?
Do you have infrastructure and knowledge to allow unit testing (Docker, etc.)
To write unit testable code as Data Engineer you should approach your code as follows.
Functions or method should be as small as possible (fewest lines of code).
Functions or methods should have as few side-effects as possible.
Functions or methods should be generalized and reusable.
For example, say we are working with PySpark and need to apply a window function, a very common task to filter out data and get the latest record.
Is this function unit testable? Why yes, in fact it is. It‚Äôs simply applying some logic to a Dataframe, which we could easily mock up with a unit test.
What makes a function not very unit testable? Let‚Äôs look at this function in way it could have been written. Something not uncommon in the DE world.
Now all the sudden we have a mess, and this is probably nice compared to a lot of DE code floating around. We have a few side effects and complexities, all wrapped into a single unit of work.
reading some remote s3 bucket for data.
reading a second data source, and the joining.
along with our original logic of filtering.
Does anyone want to raise their hand and write a unit test for this one? Not me. Why? because it‚Äôs just simply messy and not broken up into logic units of work. What are we really testing? Read the first data set? Reading the second one? Filtering? Joining? You get the point.
When starting down the long and winding path for writing code that is testable, one fraught with peril and tears ‚Ä¶ just remember a few simple steps.
Keep it simple and clean.
Break up your logic into units.
Reduce the lines of code inside a function.
Reduce the amount of logic and ‚Äúthings‚Äù aka side effects that happen.
These are simple and straight forward steps that
anyone
can implement in their next code project.
The End.
What did we talk about today? Unit testing. It‚Äôs lack in most data teams for various reasons, most of them obvious and easily fixable. I can‚Äôt tell you how many times I‚Äôve heard from people ‚Äúwe want to, but we haven‚Äôt gotten there yet.‚Äù
Usually there is a big story behind the ‚Äúwe haven‚Äôt gotten there yet.‚Äù There is a lot of technical debt, bad decisions, the business pushing for things. At some point you have to pay the piper, bite the bullet, look towards your future.
There is no easier way to take your data team to the next level than to simply start writing unit tests, all sorts of good things will follow."
93276139.data-modeling-101-part-1,Data Modeling 101 - Part 1.,"What is data modeling, and how to do it.",2023-01-05 11:24:38.921-06,"I‚Äôm not sure if there is a topic more fraught with gypsy black magic, obscured by mist, whispered about in the back halls, and practiced like black magic in the dark night under the old oak tree than ‚Ä¶ data modeling. I have no idea what that is.
‚ÄúMaybe it was the mad rush for the data is the new gold modern data stack craze. Whatever it was, it seems like nothing has happened since the Data Warehouse Toolkit came out all those decades ago.‚Äù
‚ÄúIt‚Äôs been a long silence on the data modeling front, even though it‚Äôs still a core tenant of good Data Engineering.‚Äù
If you look at the timeline of data events, it‚Äôs been a long dry spell for data modeling, compared to the rapid pace of data tool innovation.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Here is what we are going to tackle in this series of articles.
Data modeling overview and basics. (
Part 1
)
Data modeling for Relational Databases vs Data Lakes / Lake Houses. (
Part 2
)
Data modeling in the era of ‚Äúeasy‚Äù Big Data (
Snowflake, Databricks, etc
). (
Part 3
)
Without further ado, let‚Äôs get to it with
Part 1
.
Data modeling overview and basics.
What is it about data modeling that is so ‚Äúdifficult‚Äù, and why aren‚Äôt there more books and resources devoted to this topic? I‚Äôm not totally sure but I think it has to do with the fact that data modeling is half art and half science. When something is a little ambiguous it‚Äôs going to reduce the number of people willing to tackle the problem.
So what is data modeling?
‚ÄúUnderstanding the access patterns of the data set, or how the data will be queried, while applying knowledge of storage systems and relationships to identify the best model for the data.‚Äù
Access (query) patterns.
Type(s) of storage systems.
Relationships
.
Data modeling is tricky and not well defined because the data stack has changed so extensively since the RDBMS (relational database management systems) ruled the world.
The types of data sets have changed (more unstructured).
The size of the data sets has changed.
Storage systems for data have changed.
Compute and the advent of distributed data systems have changed.
More advanced business requirements (think ML).
All of these situations have worked in unison to make data modeling become more opaque and less clear,
mostly because each specific usage case changes ‚Ä¶ and so will the best data model.
Data Access (query) Patterns.
The very basics of any data modeling exercise start with asking the question, ‚Äú
What are the data access patterns for this data set
,
‚Äù also known as ‚Äú
how exactly is this data going to be used?
‚Äù This is one of the foremost problems a data engineer should try to understand when looking to model a new data set.
Choosing to model the data in such a way that supports how the data will be consumed is super important. If we don‚Äôt understand how the data will be pulled and used, there is a good chance the data model will not support it very well and
negatively impact the data platform as a whole
.
What is a practical example of data access patterns and how they would affect a data model?
Problem Statement(s):
Your company Acme LLC. manufactures widgets, and just installed a new custom WMS system, and the data is stored in an
OLTP
backend database. That data is available for export and the first two requests have come through.
The business and product group need the ability to retrieve and look up any random order from any customer at will.
The business and product group also separately need to know monthly gross sales.
Contrived examples, but you can notice right away simply by understanding what the ‚Äú
data access pattern(s)
‚Äù would be for each request.
Data will be accessed by scanning keys or IDs looking for a specific order from some customer, with a possible partition by the customer.
Data will be accessed via a time series with large aggregations, partitions would be by some part of
date-timestamp
.
What will that technical implementation look like? Well, it depends on what technology platform you‚Äôre working on. Postgres? Delta Lake? It all matters and will most diffienatly affect the data model implementation.
For example, with
Postgres
we will depends on indexes to help the data model perform, with
Delta Lake
we would rely heavily on partitions.
Let‚Äôs say we were using
Delta Lake
for the above example problem, and we want to model a table for each of the use cases above.
In the first case of looking for a specific order ID or key via a specific customer might look like this‚Ä¶
In the monthly gross sales for the time-series application.
This might be a simple and basic example, but it shows you the utter importance of divining correctly the data access patterns for your specific use cases. If you don‚Äôt pay attention to these details when modeling your data, and applying it to your design, the performance of the entire system will be affected.
‚ÄúBad or incorrect data models have major downstream impacts.‚Äù
Type(s) of Storage Systems.
I sort of alluded to it the importance of storage systems, and how they affect the data model in the example we used above, Postgres vs Delta Lake, etc. There is a larger discussion to be had about Relational Databases (
Postgres, MySQL, SQL Server
) vs File Storage Systems (
Delta Lake, LakeFS, Hudi, S3, etc.
)
You cannot design a data model in a vacuum, it would be nice to apply the same theory to every type of data model, but that simply isn‚Äôt possible.
The underlying technology is going to be better at some things than others, and act in a certain way, that makes it perform better with some data models vs others.
Today we live in a world we have two fundamentally different storage systems, each of which is going to require some different data model implementations. Of course, there are sets of ideas that can apply across all systems, but how you actually codify these data models will be different.
Big Data (file-based) Systems.
Relational Databases.
How do these two systems function in way different way that affects the data model?
‚ÄúBig Data systems like Delta Lake revolve around the concept of
partitions,
while relational databases revolve around
first through third-normal form
and
indexes
.‚Äù
This is a critical distinction that totally changes the way you data model for 300 TBs of data stored on
Delta Lake
with an
S3
backend, vs Postgres. What does that mean practically?
Big Data Systems with partitions at the core will have fewer ‚Äútables‚Äù, and probably fewer joins, and the data model will focus on co-locating related data over large data sets, without so much thought for third-normal form.
Relational database systems will probably have more ‚Äútables,‚Äù focusing on third-normal forms and indexes, as co-locating related data is not an ‚Äúissue.‚Äù
For large data sets used with Big Data tools like Spark, data engineers are designing data models that are centered around the
how
of the data access (
partitions
). Is the data set accessed via some
date
or
time
definition, does the data get consumed based on a particular
country
or
customer
?
‚ÄúBig Data engineers are going to try to reduce the number of scans of these large data sets, using a good data model, when queries are executed, simply because of the sheer size and compute required to complete each request.‚Äù
On the other end of the spectrum, designing data models in Postgres is much different. Often times for non-OLTP systems, engineers try to normalize the data, reduce redundancy, and identify indexes that will speed up joins.
Relationships.
Lastly, data modeling revolves around relationships. How does this data set relate to others, not only at a high level (
this ‚Äútable‚Äù relates to that ‚Äútable‚Äù
) but at a low level (
what data points relate these two data sets together
)? It‚Äôs also important to note that the uniqueness of each data set is important to identify, to avoid duplication problems during joins.
Data modeling a single data set isn‚Äôt done in isolation.
Data set(s) relationships are key to a good data model.
Being unable to relate datasets can turn into a disaster.
Uniqueness needs to be solved before relationship joins are done.
One of the most basic requirements of data modeling is the ability to relate data sets together. It‚Äôs rare that a single ‚Äútable‚Äù or data set is used in isolation, typically to expand upon and satisfy other more complex requirements, joins must be done.
‚ÄúFailing to understand what columns or keys (calculated or self-existent) in a data table not only make a record unique (to avoid data duplication), but relate the data easily to other data sets is critical.‚Äù
In relational databases, this typically consists of ‚Äúforeign keys‚Äù that can easily and quickly relate multiple tables together. For large file-based storage systems used in Big Data, many times these relationships are made up of multiple columns or
computed
‚Äú
primary or foreign key
‚Äù columns.
Closing Thoughts
In this Part 1 of our series on the basics of data modeling we mostly covered the theory and high-level ideas that encompass why and how we data model. Any good data model starts with these three basic components.
Data access (query) patterns.
Storage systems designs.
Relationships.
Without understanding the access (query) patterns of how our data sets will be used, it‚Äôs impossible to have a good data model that will perform under pressure and scale with the data.
It‚Äôs also important to understand that the type of storage system, relational database or file-based like Delta, will absolutely change how the data is modeled. Finally, relationships are the last key to a good data model.
It‚Äôs rare to have data sets used in isolation. Being able to identify uniqueness and define easy-to-use relationships between data sets is critical to provide value in a complex world of business requirements."
92728101.to-sql-or-not-to-sql-that-is-the,"To SQL or Not to SQL, that is the question.",,2022-12-25 20:36:23.237-06,"I've got a post scheduled to come out in the new year about ‚Äú
breaking up with SQL.
‚Äù
The idea is not to never use it, but to grow beyond it.
Do any of my readers have SQL horror stories, or where it saves your life? Please share!"
92093580.the-data-world-revolves-around-partitions,The Data World Revolves around Partitions.,Times are changing ... are you?,2023-01-16 09:01:06.466-06,"Photo by
Juliana Kozoski
on
Unsplash
Do you ever feel like something just slowly crept up on you, and then it was there, and you have no idea what happened or how it got there? Maybe like getting old. Or last year. Your mom. Whatever. That‚Äôs what I‚Äôve felt about
partitions
.
That topic no one is talking about much, but that is at the core of the new world of Data Engineering we live in. Some SaaS companies
talk about partitions
, others
sorta ignore it
and automate it, but in the end, it‚Äôs the blood pumping through the veins of many a data platform.
Partitions are found in many tools we use every day, are core to optimal data models, and performant Big Data pipelines. I still don‚Äôt know why they are not talked about more, but let‚Äôs change that shall we?
Today I want to cover.
What are data partitions?
Why we need them.
What tools use partitions, either forefront or behind the scenes?
Closing Thoughts.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What are data partitions?
So, if you‚Äôre sorta new to Big Data data partitioning might not be a familiar topic to you, it just all depends on your background and experience. This I assure you, with the rise of Databricks, Snowflake, BigQuery, and the like, data partitioning is going to start showing up more and more.
Sure, like anything in tech, some tools will always try to automate it or obfuscate partitions, but if you want to know what‚Äôs going on under the hood, and how Big Data can actually be processed in a performant manner, you need to understand partitions.
Let‚Äôs take a little walkabout of the internet, and see what all the ‚Äúsuper smart‚Äù people out on the web say about partitions.
‚Äú
A partition is composed of a subset of rows in a table that share the same value for a predefined subset of columns called the
partitioning columns
.
‚Äù -
Databricks
.
‚Äú
‚Ä¶ tables is automatically divided into micro-partitions, which are contiguous units of storage.
‚Äù -
Snowflake
.
‚Äú
A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data.
‚Äù -
BigQuery
Partitioning at its basic level is really about ‚Äú
breaking data up into smaller, smarter, units of work that allow efficient processing of large amounts of data.
‚Äù Maybe a picture?
Why do we need them (partitions)?
Say you are dumping massive amounts of data every day into an s3 bucket. Think about it. If you just randomly dump the files into the same place, how do you ever find what you need? Take a look at the figure above.
The best way to find that needle in the data haystack is to be able to use partitions to traverse all that data. In the example of dumping all our files into s3, and we know we are looking for a data point from January 2022, all of a sudden we know where to look.
Sure data partitions are implemented in a variety of manners by different tools, but in the end, they are key to providing a few features.
Improve query performance.
Reducing the number of bytes read.
Reduce costs.
Co-locate related data points.
This list should make you sit up straight. I don‚Äôt care how fast and great Spark is at crunching big amounts of data. I‚Äôve seen massive EMR clusters ground to a pulp and beat to the ground because of non-existent data partitions.
A good data model with data partitions that match the data access patterns will improve performance in a massive manner. The I/O of scanning an endless amount of files/storage can be a killer for any data processing system.
For example, many data stores use a time-series approach for analytics, grouping and crunching data by year, month, and day.
There is fine balance with data partitioning ‚Ä¶ you want to have enough partitions to ensure even data distribution, but no so much that you have the ‚Äúsmall file size or data size‚Äù problem.
We noted above in the figure that
improving query performance
is a key takeaway of good data partitions. This is for the reasons noted, reducing the amount of data that has to be read, and co-locating data together that matches data access patterns.
What would a real-world example look like with
Delta Lake
for example? Let‚Äôs say we are getting bike trip data, as per below.
We do some research on the data access patterns, and a bunch of reports about related time-series questions for this data set. How many bike rides per day, month, and year, and which stations are most popular at different times of the year?
In this case, we might want to partition the table by the
timestamp
of the trips,
started_at
.
Hopefully, this plethora of talk about data partitions is giving you a good baptism by fire and a basic understanding of why we need partitions on large data sets, and what function they supply.
What tools use partitions, either forefront or behind the scenes?
I also think it‚Äôs helpful to list and talk about a few of the Big Data tools, that many Data Engineers use, that make use of partitions. I think it helps us close the gap on why data partitions are important by exploring the tools that use them, it kinda helps to see where the rubber meets the road.
What tools use partitions?
Delta Lake
Google‚Äôs Big Query
Snowflake
Redshift Spectrum
I mean if that isn‚Äôt a list of the A-team I don‚Äôt know what is. That should give you a good idea of why understanding partitions are important. I would say, in general, you are going to see tools take two different approaches to partitions.
Hands-off - ‚Äúautomatic‚Äù data partitioning.‚Äù
Hands-on - ‚Äúmanual user-defined partitioning‚Äù
They are two very different approaches to working with partitions. In the Snowflake world for example ‚Äú
Snowflake micro-partitions are derived automatically; they don‚Äôt need to be explicitly defined up-front or maintained by users.
‚Äù
But when it comes to Delta Lake, ‚Äú
To use partitions, you define the set of partitioning column when you create a table by including the
PARTITIONED BY
clause.
‚Äù
It comes down to you vs. the machine, who‚Äôs smarter, Skynet or you? Hard to say. Different approaches rely on the same underlying concepts.
Closing Thoughts.
I hope this was a good introduction to data partitions if they were new to you. I understand that some folks haven‚Äôt used them yet, but I think the day is coming when you really need to be able to articulate and understand them to design efficient and performant data models.
In the future hopefully, we can dive into some specific use cases of data partitions, do some testing, and see actual performance numbers for things like Spark with and without good data partitions.
The future of Data Modeling revolves around partitions, whether your tool tries to hide them or not. Understanding how they are used to improve and allow the performance of Snowflake and Delta Lake is extremely helpful, especially when things go wrong."
91965410.designing-better-data-pipelines,Designing BETTER Data Pipelines.,Become the master of Data Pipelines with a few easy tricks.,2023-01-02 07:11:08.207-06,"‚ÄúIt's funny after all these years, millions of data pipelines plugging along in quiet benevolence for decades. Grab some data, load some data, transform some data, rinse and repeat.‚Äù
If it's so mundane then do we need Data Engineering at all, why are Data Engineers in such high demand, even in these uncertain economic times? Why hasn't true no code come to fruition yet? Why are so many data pipelines at this very minute, breaking and puking errors and alerts, waking up slumbering and sleepy eyes Data Engineers to look gloomily at glowing computer screens while punching the keyboard with a frenzied frustration that has built up over the years?
‚ÄúI mean haven't we done this enough by now to build every data pipeline to be invincible? Nope. Death, taxes, and data pipelines
.‚Äù
Today we will end the despair and recover as if from some ancient scroll, the fundamental truths to designing BETTER data pipelines.
Wizened and dusty, we are going to breathe life back into these old ideas, and see if they contain any magic and stardust that can save us from ourselves.
Hear ye, hear ye! Bow yourselves before these Truths! Humble yourselves, or be humbled!
Theoretical data flows come before code.
Data models for both source and sink come before the code.
Business requirements come before code.
Idempotency is the Ruler of all.
Data transformations must have tests.
Alerting and monitoring are not second-class citizens
.
Closing Thoughts.
Let us examine each of these timeless gems, peering if we dare into their depths, drinking up the fountains of wisdom that spill forth.
Theoretical data flows come before code.
Before embarking on any data pipeline journey, one must be introspective, like the philosophers of old, staring at your navel and asking ‚Äúwhy?‚Äù Without a view of the big picture, it's hard to get a handle on the puzzle pieces, and how those pieces should fit together.
Photo by
Hassan Pasha
on
Unsplash
‚ÄúMany times the ideas we have in our heads can be far removed from reality, and we make decisions way too early in the process, decisions that become hard to back out of.‚Äù
The simple act of drawing out or visualizing a data flow can do wonders at raising questions, showing where gaps may exist,  and allowing assumptions to be challenged, and feedback to be had. All before a single line of code is written.
‚ÄúIt's hard to underestimate the power of thinking before doing.‚Äù
Practically what does that mean? It means you should open a Google Presentation, Apple‚Äôs Keynote, or whatever, and draw and connect the pipeline together visually to understand what‚Äôs happening end-to-end and get input, before actually doing any work.
Data models for both source and sink come before the code.
Another woefully underappreciated and new skill today in the Data Engineering world is data modeling. Too often, the data at the core and center of our data pipelines, for some unknown reason,
is left un-modeled and not very well understood
.
Modeling data at both its genesis into a data pipeline, and its grave can have a major impact on the effectiveness of the system being developed.
Defining schema and data types (
DDL
).
Defining metadata and business meaning.
Defining storage types (
parquet, flat-file, RDBMS, NoSQL, JSON, etc
).
Defining constraints (
NULL, values IN, etc
).
Data velocity and size (
how much data comes and at what interval
).
Data uniqueness (
primary key
).
Data Modeling can make a pipeline fast, or very slow, just as correct data types can make joins fast or slow. The constraints we put on the data, NULL or not, can make you pull your hair out.
If you‚Äôre new to data modeling, start with Kimball‚Äôs
Data Warehouse Toolkit
, it will get you started.
Data models define how and understand what the data access patterns are (how is the data queried and used.)
Business requirements come before code.
You would think after hearing this topic preached ad infinitum that it would take root deep in the minds of Data Engineers, yet alas the first thing we want to do is just code.
Photo by
Jan Kah√°nek
on
Unsplash
I know as a bunch of know-it-all engineers, we think those silly old business folks are full of hot air and vinegar.
But, wouldn't you know it, their work pays our salary.
Might want to listen to ‚Äòem.
It is truly amazing to learn and see how much engineering and business intertwine.
If you think about many of the data transformations in pipelines that are taking place, 80% of them have to do with performing functions that will prepare and display the data for some business or analytics need. 20% is maybe actually cleaning the data and formatting it correctly for later use.
So, how can you know the best way to transform that data correctly without understanding what it is the business actually needs, what will provide
VALUE
?
Your career will be long and prosperous if you become an engineer that understands and listens to the business.
Idempotency is the Ruler of all
Yet another elusive topic that seems to escape most Data Engineers, especially at the beginning of their journey. The all-knowing and all-important topic of every data pipeline is
idempotency
.
Idemptoecny is probably the single most important topic, the ultimate core of what makes a data pipeline ‚Ä¶ truly great.
What is it? It‚Äôs the idea behind asking this all important question ‚Ä¶ what happens if I run my data pipeline twice ‚Ä¶ one run right after the other? Would it mess things up? Would it break? Would it cause duplicates?
Or would I have the same, correct answer, after both runs?
When think about a data pipeline, one must thing about what happens if I run this, and re-run it, and re-run it again. It should always produce the same result. Sorta like functional programming in a sense. It should be a functional pipeline. Only the inputs should change the output.
This is what makes a pipeline ‚Ä¶ intelligent, and smart.
I encourage you to do some internet searching and reading if this topic of
idemptoecnty is new to you
. This will take your Data Engineering skills to a whole new level, opening up new horizons for you to explore, and reduce bugs and serious data issues down the line.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Data transformations must have tests.
I threw this into the list because it‚Äôs obvious, and yet 80%+ of data teams I‚Äôve meet do not do this simple task. You would be amazed at the size of companies running complicated streaming and batch data pipelines, on the latest technology, yet when asked if their data pipelines are tested ‚Ä¶ they answer ‚Ä¶
‚ÄúNo, but we want to get to that soon.‚Äù
Data pipelines should not go into production with untested transformations.
Tests set the ‚Äúdirection‚Äù and overall ‚Äúfeel‚Äù of a Data Team (do we take this serious, or wing it).
Tests will catch 80% of simple human errors that creep into production.
Tests make you write better code in the beginning.
I won‚Äôt dwell here, but if you want to be taken as a serious Data Engineer, and make it to Senior+ levels, every single piece of code you write should have a test that goes a long with.
Do yourself and your team a favor, write tests and be more confident in the code you deploy.
Alerting and monitoring are not second-class citizens
.
Photo by
Sigmund
on
Unsplash
Last but not least is the often forgotten alerting and monitoring portion of Data Pipelines. Often relegated to the very last step of the design and development process, but hopefully not forgotten, alerts and monitoring can save your bacon in a pinch.
I‚Äôve seen pipelines without any alerting, but maybe just monitoring,
so manual intervention was required to ‚Äúknow‚Äù something was broken.
On the other end I‚Äôve seen pipelines with ‚Äúalerting‚Äù and no ‚Äúmonitoring‚Äù, so you might know something broke,
but find it impossible to track down what process is broke and where, and the downstream effects are known.
Alerts can tell you what is working, and what is not working.
Monitoring gives overall visibility and understanding of data pipelines.
The combination of monitoring and alerting can give confidence, understanding, and visibility into your data platform and pipelines.
Closing Thoughts.
In the end,
designing better data pipelines is no rocket science at all
, it‚Äôs really just the implementation of very basic and fundamental concepts, keeping them forefront in your thought and architecture, allowing them to guide you down the path to success.
Most of the common pitfalls around data pipelines stem from moving too fast and not taking time to think, or implement basics.
Theory before code. Data models and requirements before code. Idempotency and tests take it to the next level. Alerting and monitoring close the loop.
Let‚Äôs be Data Engineers who study and perfect the fundementals."
91964347.you-need-to-break-up-with-sql,You need to break up. With SQL.,SQL has engulfed the data world. Can we ever break free?,2022-12-28 15:47:30.527-06,"Photo by
Kelly Sikkema
on
Unsplash
Remember in high school when all your friends kept telling you to stop spending so much time with that person? To break up, get it over with, that you were no fun anymore and never around?
Well, I‚Äôm here to tell you the same thing again. Think of me like your mother, standing over you at the supper table, shaking my finger at you, ‚ÄúYou need to break up!!‚Äù
‚ÄúIt‚Äôs time. Time to break up with SQL.‚Äù
Heresy I know, I can see the angry mob lighting their torches and sharpening their pitchforks and pikes as I speak the words.
My days are numbered
. Here is what we will cover today.
Why you should break up with SQL.
How SQL is holding you back.
Broadening your horizons.
The importance of the right tool at the right time.
Closing Thoughts
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Why you should break up with SQL.
Things come and go, and sometimes they can back around again, that is the case with SQL. I remember a decade ago, well before the days of
Databricks
,
Redshift
, and
Snowflake
when the cool kids were running
Data Warehouses
on
SQL Server
,
Oracle
, and
SAP
.
Ahhh ‚Ä¶ the good ole‚Äô days ‚Ä¶ or not.
‚Äú
With the popularity of SparkSQL and Snowflake, pretty much every Data Engineering tool is following the bandwagon, offering a SQL API to allow for easier adoption and usage
.‚Äù
SQL here SQL there, SQL SQL everywhere. Ahhh! Look I know this is going to poke some people the wrong way, I get it. There was a time in my life when
80%+
of the work I did was SQL related.
But, we have to be honest with ourselves about the side effects of such SQL-heavy work
.
I want to be clear ‚Ä¶ SQL is awesome and lowers the barrier towards Data Engineering, it also makes iteration and development very fast. There is a reason everyone uses it.
But, you do need to have a break up with SQL, let me tell you why
.
SQL-only stacks stifle innovation.
SQL-only stacks block the growth and learning in DEs.
SQL-only stacks become inflexible over time.
SQL-only stacks don‚Äôt support Machine Learning and DS workflows well.
SQL-only stacks
tend
to have fewer or no tests (unless you‚Äôre using DBT).
SQL-only stacks create overly complex logic and slow down debugging.
SQL-only stacks tend NOT to favor functional reusable code.
‚Äú
These ideas of course are not blanket statements for every Data Team but generally hold true in the long run and in most cases. You need to break up with SQL and tell that old rascal you want to explore other ideas and code to solve problems.
‚Äù
Sure, there are teams here and there who are heavily invested in DBT and can overcome such issues as testing on a pure SQL stack, but those are the exceptions.
You should break up with SQL because it‚Äôs holding you back, let's explore this idea a little more
.
How SQL is holding you back.
It was a hard pillow for me to swallow at first, but I didn‚Äôt finally come to the realization that writing on SQL was going to hold me back, in the long run, it can only get you so far.
‚ÄúIf you‚Äôre writing 80%+ SQL all day long very day, with a smattering of Python here and there, you might be working yourself into a corner that‚Äôs hard to back out of.‚Äù
You don‚Äôt want to find yourself in the spot, whether by your own choice, or not, where you‚Äôre in the need of a new job, and you have to cut your prospects in half because you know all you are good for is some SQL.
The rise of Machine Learning and Data Science requires more programming experience.
The popularity of Databricks in Spark has increased the demand for PySpark and Scala.
The rising usage of Streaming pipelines requires more programming experience.
Most startups require broad tech stack experience, DevOps, CI/CD, and cloud services ‚Ä¶ via code.
The less you code, the rustier you‚Äôre going to be unless you're at the tail end of your career.
All I‚Äôm saying is that too much focus on anything in life can come back to bite you. Think about becoming a better programmer, with more than one language,
as an insurance policy, an investment in yourself.
Not breaking up with SQL, and not being honest with yourself that you‚Äôre in a comfortable spot, too comfortable, is a bad place to be.
Don‚Äôt let that monstrous blog of SQL slowly and quietly slide and slip over you, sucking you down into the depths, where you raise your head up 3 years later and wonder where the rest of the Data Engineering world went.
Broadening your horizons.
Photo by
Doug Kelley
on
Unsplash
It‚Äôs time to spread your wings a little, move past SQL, put on your resume, polish it proudly, and move on to the next thing for mastery. Ensuring you are good at more than just SQL is
key to your success and sticking out from the crowd
.
Let me give you a few practical ideas to help guide you on your path past SQL.
Learn
Docker
and
Docker-compose
.
Pick up
Golang
,
Scala
, or
Rust
.
Learn more about
Data Modeling
and
partitions
.
Learn about
functional programming
.
Learn about
idempotency in data pipelines
.
Learn your way around
Linux bash
and servers.
That should keep you busy for a little bit anyways. On top of learning some valuable new skills, think about how good those techs will look on your resume, much more marketable and will put you at the top of the stack of candidates.
Importance of the right tool at the right time.
Honestly, at this point, you might think I‚Äôm a downer on SQL, but I assure you that is not the case. I spent years and years of my life mastering SQL, and it comes into use nearly on weekly basis. SQL is everywhere, even with SparkSQL, being able to solve simple problems quickly and fiercely with SQL is a wonderful skill.
‚ÄúLean SQL, learn it well, but then move on.‚Äù
Where I get down on SQL is when the Data Stack has no variety and is pure SQL from end to end, I get a little skeptical.
Either it‚Äôs a boring job that I don‚Äôt want to do, or someone has no creativity and is trying to put a screw in the board with a hammer.
I‚Äôve seen the overuse of the following SQL and relational database functionality.
stored procedures
triggers
hundreds of lines of SQL for a single query
array, JSON, and other data parsing.
blob and text storage.
document storage
I‚Äôm not saying
you can never do these things with SQL
, I‚Äôm saying there is more to life than SQL, and a lot of good tooling has been developed to deal with certain issues more efficiently.
Part of being a good Data Engineer is touching up and down on the whole stack, from the DevOps to the data pipeline. That requires a broad set of skills around the entire architecture. Using the correct tools will more everything run more efficiently, Data Engineerings will be learning and happy, with increased tenures.
Closing Thoughts.
Put out those torches, put down the pitchforks your raving-mad peasants. Sure, I might have come off a little strong in the beginning, but you have to admit I‚Äôm right. Everyone is obsessed with SQL, as they well should be, but you need to plug your ears and not listen to the siren song.
Push yourself to learn more, and break away from SQL to expand your horizons. Don‚Äôt become that old, haggard, and grumpy Database Administrator sitting in the back room writing the same TSQL for the next 20 years. You can do better. You can do more."
91962474.supercharge-your-data-engineering,Supercharge your Data Engineering Career.,"Things your mom wouldn't tell you to do, but I will.",2023-01-30 09:18:06.865-06,"Photo by
jovin kallis
on
Unsplash
I have no problem telling you what other people won‚Äôt. They are trying to hold you back, coddle you, and turn you into a pile of mush. Not me. I think somewhere on my
About page
there is a line about ‚Äú
no holds barred
,‚Äù or something to that effect.
I know there are opinions all over the board about your career ‚Ä¶ work all night, don‚Äôt work all night (
don‚Äôt)
, get certified in this and that, go to the right meetings, you know, the usual stuff.
‚ÄúI suppose most of the career advice out there is situational to the person, culture, company, current mood in the culture, etc. ‚Äú
You know I‚Äôm going to do right by you. I‚Äôve got an opinion about everything, and career advice is no exception.
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
How to set yourself apart from the humdrum of everyone else.
Ok, that title might be a little misleading, let me be clear about something. In life and work, it will never do to step on other people to get to the top of whatever ladder you‚Äôre trying to climb. It‚Äôs bad karma at a minimum, it will leave you unhappy, empty, friendless, and probably familyless, etc.
Be kind, you will be a happier person, and if you want to be happy AND at the top of your craft, then do things and people right.
But, that being said, how can you excel and propel yourself forward as a Data Engineer, which by definition means doing things other people don‚Äôt? The good news is, you can do this very easily.
I‚Äôm going to give you 8 keys to becoming the best Data Engineer you can be. I hate to disappoint, but they are not all technical. In fact, I‚Äôm going to focus on mostly non-technical things, as those end up being more important.
I do want to say though, I‚Äôm assuming you care enough to be at the top of your craft and be good at programming because you try to be good.
A simple chart for a simple set of concepts.
Make decisions.
Spend time reading and learning.
Don‚Äôt stay in one job (position) too long.
Write tests and documentation.
Be kind and listen to others.
Share Your Knowledge.
Manage your time well.
We will take a quick look into each of these ideas, and I will give you my take on why they are important for your Data Engineering growth, and perhaps why the rest of the humdrum world fails to do these things, giving you a step up the proverbial ladder.
Make Decisions.
Photo by
Towfiqu barbhuiya
on
Unsplash
So this might be true more of startup culture than the lumbering behemoths, although it probably applied to both,
for some reason simply making a
well-informed decision
can be a superpower
.
We all know that software development for the most part tends to run fast, even if it doesn‚Äôt, the powers that be still always want features sooner,
and
development and feedback loops to cycle faster. You know the drill. Therefore it isn‚Äôt some sort of rocket science to show up on the scene and start making well-informed decisions with some measure of confidence.
‚ÄúYou want to set yourself apart from the crowd? Simply make well informed decisions and backup your decisions with some facts ( at least from your point of view). You will probably be amazed at what happens. People will start to look to you to make more decisions.‚Äù
I‚Äôve always tended to be a quiet follower myself, for most of my life, but that hasn‚Äôt stopped me from having well-informed opinions and decisions. Just learn to voice them kindly and confidently.
Spend time reading and learning.
Photo by
Kimberly Farmer
on
Unsplash
Books, books, books. Read, read, read. That‚Äôs probably what your mom told you growing up, taking you to the library to run around the isles. Sometimes I feel like true reading is a lost cause.
Don‚Äôt get me wrong, you can learn a lot from reading blogs,
like mine,
but it‚Äôs hard to replace a book that someone labored over for years with tears and sweat. And, their pain is your gain, all you have to do is pick it up here and there, open your brain and pour that knowledge down the hatch.
It‚Äôs not just technical, you can learn a lot of soft engineering skills. Here are some books on my list that I can recommend in no particular order.
Staff Engineer by Will Larson
Fundamentals of Data Engineering
The Pragmatic Programmer
Spark the Definitive Guide
That‚Äôs just a few that I have enjoyed, but the sky is the limit. Go in-depth, jump in, even if most of it doesn‚Äôt make sense. Growth comes from struggle, struggle comes from pushing yourself to learn. A great way to learn is to push yourself to understand.
Don‚Äôt stay in one job (position) too long.
Photo by
Victor Rodriguez
on
Unsplash
Now before you jump down my throat, I did put positions inside parenthesis. With that said, I would say jump. The reality is that we live in a different world. With all the layoffs in the news, from Google to Microsoft, people are starting to realize nothing is for sure, and that at some level you have to look out for yourself in your career.
I guess you can‚Äôt make blanket statements, you have super smart tenured engineers who‚Äôve been working for decades at companies, but I think that is very rare overall. For most people, it‚Äôs going to be a mistake to stay in the same job for years on end.
It‚Äôs harder to pick up new skills.
It‚Äôs harder to be pushed to grow and learn.
Your technical skills will probably lag over time.
Change forces us to adapt.
Sure, you might be in some company that makes you feel comfortable, but of course they do, they have a vested interest in not losing you and having to replace you. Ok, Ok, be smart, don‚Äôt just jump around for no reason, but ask yourself, are you bored? Are you learning? Are you growing? Is there a path forward for you?
If not ‚Ä¶ jump.
Write tests and documentation.
Photo by
Sigmund
on
Unsplash
You would be amazed at how much the easy stuff will set you apart and push you to the top of the pile. It‚Äôs the simple things in life, this applies to non-technical and technical topics. Focus on the simple, and become wise.
I don‚Äôt know what it is about Data Engineering, but it seems to me, data folks for the last decade have been dragged kicking and screaming into the SWE best practices.
Unit tests
and documentation my friend, keys to the kingdom.
If you‚Äôre writing code with unit tests, your code will be better. If you write documentation for your projects and code, those projects and code will be better. Why? Because they both require thought, foresight, problem-solving, and asking yourself if there is a better way, or did you do so and so correctly.
Start with a solid foundation, don‚Äôt rush it. Write tests and documentation.
Be kind and listen to others.
I‚Äôve had to work on this one myself, but it‚Äôs a good one. Sure, I know a lot of people are full of huff and bluff, but honestly, you can always learn
something
from pretty much everyone. Even if it‚Äôs how not to be or act.
We all live in our little boxes, our world is colored and shaped by our experiences, good and bad, and it effects how we do Data Engineering. Hating certain tools, loving others, liking certain methodologies, scoffing at the others.
To learn and grow we must let our assumptions be challenged, how we solve problems, we choose a certain path or design. One of the best ways to do this to listen to others, in meetings, and code reviews, and on that terrible internet. Just ask yourself a few questions.
Why do they think like that, am I missing something?
What has merit in the other persons idea.
How can my idea meet theirs in the middle.
Why did I pick that tool or that design?
What can I learn from their perspective, good or bad?
Listen before you talk. It‚Äôs good advice that will serve you well most of the time.
Share Your Knowledge.
Photo by
Patrick Tomasso
on
Unsplash
This one will be short and sweet. You need to learn to be less human in one sense. We are creatures of habit, ruled by emotion and a sense of self-preservation. This can happen in Data Engineering when someone starts hoarding their knowledge, holding back on explaining the why, all out of a false sense of protecting themselves.
As if sharing their knowledge and wisdom will harm them, make them less needed and useful. Very far from the truth. Some of the worst Principal Engineers were those who walked around with a chip on their shoulder. Everyone was afraid of them. Couldn‚Äôt ask them questions. They spoke is small bits and bytes, allouf and above the rest. Too wise to be approached.
They dragged the team down, made the culture horrible, and probably hurt more projects than they helped. How can you be different?
As you grow share your learnings.
Blog and write about your journey.
Focus on mentoring and upskilling younger and less experienced engineers.
Someone who can teach other people and upskill a team is worth a pretty penny.
Manage your time well.
Photo by
Aron Visuals
on
Unsplash
Last, but not least, and certainly more practical is the idea of managing your time well. A seemingly small and easy task, but very difficult to implement simply because of our human natures. We procrastinate, we dilly-dally, we get sidetracked, we have our pet projects, and we have life.
There is no point in trying to change those things, just accept it. Instead, learn to manage your time well.
Focused periods of intense work are worth a lot more than 8 hours of lazily floating around the internet and your phone, getting some work done. Break away from the bleary-eyed masses glued to their glowing screens of addiction. Focus, work, complete, move on. Start to finish, get it done.
A true superpower this one. The ability to focus on a single objective and push through till it is complete in some form. Better than a million little flies and ticks, picking and nagging their way into your brain, intent on distracting you. There is time for Facebook, Twitter, and Netflix later, they aren‚Äôt going anywhere. You are.
Manage your time well, and be productive."
91707838.review-of-mageai-data-pipelines-for,Review of Mage.ai (data pipelines) for Data Engineers.,"Can it be? Another pipeline tool? Why yes, yes it is.",2022-12-20 12:29:56.309-06,"I know you are all gasping and covering your mouths in astonishment and disbelief, hardly able to contain yourself. Another data pipeline tool has arrived at your doorstep, promising to solve every known problem you have, and ones you don‚Äôt.
‚ÄúPart of my job for your my dear readers is to poke at new tools for you, as I‚Äôm sure you have many more important things to do.‚Äù
On a more serious note, I think keeping on top of changes in the Data Engineering world is an important part of our job, for all of us. Part of our value comes from knowing the tooling landscape and making decisions about our Data Stack with a full understanding of what are the best options available.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Today we are going to review something new to me, and to you,
mage.ai
. To be clear I have no relationship with
mage.ai
, I speak the truth as I see it.
Why
mage.ai
?
Overview and concepts.
Installation and usage.
Closing Thoughts.
Why
mage.ai
?
Well, to be honest, I‚Äôve seen this name,
mage
, pop up a few times in my various feeds. Like most people, I simply roll my dreary eyes and move along to the next argument about Data Mesh. I thought to myself, just what we need, another Airflow replacement, as if
Prefect
,
Dagster
, and the rest weren‚Äôt enough.
Then something caught my eye, something different.
‚ÄúMage is an open-source data pipeline tool for transforming and integrating data.‚Äù
Well, I'll be, no mention of pipeline
orchestration
. The word that appears on the front page of both Dagster and Prefect‚Äôs websites ...
orchestration
, which I think was a mistake by them.
Airflow does orchestration fine
, that‚Äôs why it‚Äôs winning.
I‚Äôm glad to see someone thinking outside the box and talking about
data transformations and pipelines, which Data Engineers do all day long
.
Overview and Concepts of
mage.ai
Ok, so one of the most tricky parts of any new tool is really getting to the bottom of it and
figuring out what really is it
. I‚Äôm going to first just list what I think
mage.ai
, as well as simply list the features and functionality they list in their website and documentation.
We will poke at things later when installing and using it.
mage.ai appears to be a Python package (pip installable).
mage.ai appears to be a bunch of decorators on top of your normal Python code.
mage.ai appears to focus on Notebooks for easy development.
mage.ai appears to reference that it scales easily (whatever that means).
There are also a plethora of other ideas and concepts that
mage.ai
lists themselves.
‚ÄúEvery
block
run produces a
data product
(e.g. dataset, unstructured data, etc.)‚Äù
‚Äú
Data validation
is written into each block and tested every time a block is run.‚Äù
‚ÄúOperationalizing your data pipelines is easy with built-in
observability
,
data quality monitoring
, and
lineage
.‚Äù
‚ÄúEach
pipeline
and
data product
can be versioned.‚Äù
‚Äú
Backfilling
data products is a core function and operation.‚Äù
‚ÄúTransform very large datasets through a native integration with Spark.‚Äù
And that isn‚Äôt everything, kinda overwhelming a little bit. Well, at least it makes one thing clear.
Mage.ai is truly trying to be a data pipeline tool,
as it appears that‚Äôs what all the features are about.
Mage.ai Concepts.
Maybe we can get a better understanding of what we are in for by trying to understand the main concepts of mage.ai, as they present them.
These concepts are surprisingly straightforward and will make sense to anyone used to writing data pipelines.
‚ÄúThey don‚Äôt in themselves create some grand new idea that is transformational to data engineering pipelines. So mage.ai must be focusing on the ease of execution and development with their tool.‚Äù
Maybe
mage.ai
is just trying to do pipelines better generally? Not sure yet. Although, there were a few concepts tucked in there that did jump out at me as unique.
Blocks
produce
Data Products
that can be
versioned, partitioned
, or
backfilled
.
Blocks
produced trigger
validations
to run on the
Data Block
.
Data
Pipelines
being broken up into
Blocks
is nothing new, but capturing
versioning
,
partitioned
, or
backfilling
of the output automatically, and running
validation
on the
Product
is a new sort of implementation. Time will tell.
Subscribe now
Installation and Usage.
I won‚Äôt really spend much time on installation, you can simply use
pip to install it, or use Docker
, which is a nice touch.
Side note, when pip installing mage-ai I kept getting
errors
about
pandas
ERROR: Failed building wheel for pandas
Failed to build pandas
ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects
Although I could pip install Pandas successfully on its own in the same environment, mage refused to install.
Maybe it‚Äôs just my M1 Mac, who knows, you can always use the
Dockerfile
I guess.
Notes on Usage.
So if
mage.ai
is a data pipeline tool, we should probably write a pipeline eh? One thing becomes clear right from the start that I‚Äôm not the biggest fan of. Notebooks. I‚Äôve seen Notebooks too many times, and they are always abused, but I get it. Shortens the learning curve and all.
It does tell you something about who the target audience is though.
They are great from a development point of view, but inevitably they turn into something else,
an excuse to be messy
.
Mage.ai can be installed via pip (if it works for you).
Use the
provided Dockerfiles
.
Get ready to Notebook (or work your way around them like me).
If
mage.ai
is made for Data Engineers, they should focus on documentation and usage that is centered around the best practices they care so much about, for example, a good plugin with PyCharm or VSCode.
A Little Note Before We Start.
I want to make something clear, I don‚Äôt try to be unkind in my review of tools. I am simply an engineer, starting from zero, with no knowledge, and trying to ramp myself up onto a tool.
I comment about my experience along the way. Who doesn‚Äôt run into problems when trying out a new tool? The whole point of it is I struggle, so you don‚Äôt have to, and get a better understand of a new tool in the process.
It‚Äôs possible my struggles say more about me than a tool, but there is value there.
Example mage Pipeline.
I have 20GB worth of data from my previous post on DuckDB, we will write a mage pipeline to process this data. The data is from
Backblaze Harddrive failures
, Q1, Q2, and Q3.
Since mage supports Spark, we will attempt to write a
PySpark
pipeline with it, and see how things go. The first command I ran
init‚Äôd
a new project and started the Notebook.
mage start mage-test-spark
You will then end up with something like this in your browser. So it‚Äôs clear from the start
mage.ai
is
at least partly a visual Pipeline development tool
. That will become clear in the rest of the example walkthrough,
there are no CLI commands to run, it‚Äôs the GUI my friend.
Once you click to create a new
Batch
pipeline, you are met with this wizardry.
As you can see there is a number of
Blocks
, you can select from at the top. Rember these
Blocks
map to sections of code that produce a
Data Product.
I tried to select
PySpark
instead of
Python
at the very top and was met with a giant error. No surprise for a new tool. I figured maybe mage is having trouble finding my local Spark install.
I tried running the same command from the official mage Dockerfile as well and got a different error, it‚Äôs looking for
boto3
credentials just because I want to write a PySpark pipeline locally.
So strange. Then I
read the mentioned documentation
,
Lord save us all, it wants me to set up an EMR cluster
! By George, I will do no such thing!
Talk about expensive just to develop a PySpark pipeline locally!
Lest you think I lie ‚Ä¶ ‚Äú
steps required to use Mage locally with Spark
‚Ä¶‚Äù
They should have at minimum PySpark pip installed in their Docker at a minimum and Spark available inside the Docker image itself to help with local development
. I should be able to select PySpark and write a Spark pipeline locally without needing credentials to a remote cluster.
Below is my understanding of the data Pipeline workflow in
mage.ai
so far.
Trying to move forward.
At this point, we still have yet to write a Pipeline with mage, but let‚Äôs just go with a simple Python pipeline. It was quite annoying to be sure, I clicked
Data Loader
as my first
Block
from the UI and it generated a bunch of code to read a local file.
Not much use to me, someone who wants to write my own Python script to read multiple CSV files!
‚ÄúSomething is becoming clear to me, Mage is focused on being a cross between a GUI and pure code data transformation tool. Its trying to do all the mundane parts of Data Pipelines for you, leaving you to fill in the Blocks (code), while it takes care of the rest.‚Äù
I finally switch the the ‚Äú
Scratchpad
‚Äù to write what I hope is my own code to load the 20GBs of CSV files.
I wrote two
Blocks
in what mage calls a
Scratchpad
, I will try executing them next. Nothing happened. Then I read the docs again, oops.
‚ÄúUse these blocks to experiment and write throw-away code. Scratchpad blocks aren‚Äôt used when executing a pipeline.‚Äù
Dang it.
I then re-wrote my two Scratchpad blocks into ‚Ä¶
Generic Data Loader Block.
Generic Transformer Block.
I ran the
Pipeline
and got an error that
Polars
was not installed, of course, so I added it to a
requirements.txt
file and restarted the
kernel
. All that without knowing anything about the tool,
I suppose that says something about its intuitiveness
. Still no luck.
It appears mage keeps clearing out the requirements file every time I touch it, or restart the kernel. Not really sure what to do. Do you really have to specify all your requirements before you start a project? That seems probably not true, but I‚Äôm stumped on how to add a pip installable package while developing a pipeline.
I finally found the answer,
by accident, I can run the pip install from a ‚Äú
Scratchpad
.‚Äù Yikes.
The problem now is the
Scratchpad
defaults to
Python
it appears, and of course, errors when trying to run a pip install command. For the life of me, I can‚Äôt figure out how to switch it to a terminal.
Since I‚Äôm running this in Docker I simply run the suggested command to connect to the Container, and pip install it via the cmd line.
This time the
Pipeline
executes when told to do so in the UI, but I get another error. Something about a
Circular reference
? I know this code works, but it runs fine outside of mage.
At this point, I just want the pipeline to run so I can see it with my own eyes. I try combining both
Blockz
into a single
Block
that should just execute. This time it worked.
I think I‚Äôm starting to get the feel of mage. You‚Äôre probably not going to like what I‚Äôm going to say next, but hear me out. I‚Äôve had this sort of feeling before, I‚Äôve felt this sort of development before ‚Ä¶ can I put my finger on it? Why yes I can. It‚Äôs called
Microsoft‚Äôs SSIS
.
Let‚Äôs Pause.
Before everyone comes at me with pitchforks and torches, let me say my piece before you make a martyr of me.
‚ÄúSome of the most sucsefful and popular Data Pipeline tools every created and still used today are a marrige between visual GUI and code. This is every true to some extent for Airflow.‚Äù
I know we‚Äôve barely scratched at the surface of mage, but you can see it can‚Äôt you? Putting easy-to-understand abstractions on top of all the familiar concepts of Data Pipelines? This is classic.
After all, mage is a Data Pipeline tool, and it‚Äôs extremely clear from what little we have seen of it, that this is exactly what mage is trying to make easier.
Subscribe now
Closing Thoughts.
I‚Äôve been long-winded already, let me try to give some more closing commentary on
mage.ai
if you please. There are a few things that come to mind.
They talked a lot about Validation at each
Block
and
Data Product
produced. As for validation what they mean is that when you generate a new
Block
it auto-generates a test function for you.
While this is helpful and leads in the right direction, it seems a little misleading. Most of us are already writing unit tests for our code.
Also, after my problems trying to write a local PySpark pipeline, I‚Äôm a little suspicious of their integrations and how much of it has been tested in real life. Let‚Äôs be honest, the number of pure Python data pipelines is dropping by the day. Data gets bigger and more complex, which requires most pipelines to run on Redshift, Databricks, Snowflake, DBT cloud, and the like.
This is why Airflow is so popular, it connects to anything with ease.
But, there are some bright spots after trying out mage.
Mage is so much more than Airflow, it can do what Airflow does, but better and with more features.
Mage takes away a lot of the mundane parts of developing data pipelines.
Mage was able to but easy to understand and use concepts on top of pipeline components.
Mage is the perfect marriage of GUI and code, that much is clear.
‚ÄúI can say even after just trying it once, mage would help any Data Engineering team write uniform, clean, well tested Data Pipelines. This is NOT something found in Airflow, Prefect, or Dagster.‚Äù
If I was running a team of junior engineers, or just a very large team of data engineers writing all sorts of pipelines and I was looking for uniformity and a tool that will solve many of the common problems, it would be Mage without a doubt.
The last time I saw a tool like this was SSIS, no joke. Does mage have a bright future? I have no idea. Probably, but who knows? I‚Äôm a fan. I‚Äôm going to watch them closely in the future to see where the tool goes.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
91501152.a-gentle-introduction-to-data-quality,A Gentle Introduction to Data Quality.,Data Quality is a hot topic. From zero to hero in no time.,2022-12-19 13:07:36.218-06,"Data Quality came out of nowhere this year, you would think after decades of building Data Warehouses, Data Lakes, and Data Platforms that the data community would have committed itself to
Data Quality (DQ)
about 10 years ago.
But alas,
a prophet is never accepted in his homeland
.
DQ
has always been an afterthought at 99% of data organizations, and honestly, it still is. The difference is that it's being recognized and talked about in the open.
Will Data Quality tools be a hot topic in the near future?
I have my doubts
. The tooling lags far behind even now, and choices are limited.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
I'm going to save your bacon anyways and give you a crash course on DQ, making an expert out of you, without you so much as setting a finger to the keyboard. By the end of this article, you should have a good idea about what is DQ and your options for implementation. We will cover ‚Ä¶
What is Data Quality (DQ)?
The DQ tooling landscape.
Homegrown DQ for your team.
Closing thoughts.
Let's get to it, you rabble of data scoundrels.
What is Data Quality (DQ)?
I don‚Äôt think this is a particularly hard question to answer, but I think the devil is in the details of the answer. Of course, DQ is all about our data, what is it exactly about our data sets that need ‚Äúquality‚Äù applied to them?
‚ÄúI‚Äôm going to postulate that Data Quality can be boiled down into two main categories, firstly
data specific attributes
, and secondly,
business-related attributes.
‚Äù
All Data Quality related discussions and topics can be tossed into one of these two proverbial barrels.
Data specific attrbiutes.
Business-related attributes.
At a high level, we know that Data Quality (DQ) is about
understanding our data, and mostly getting in front of problems that may arise that will cause ‚Äúissues‚Äù downstream
. These data problems and issues can affect everything from incorrect or inconsistent customer experiences in a UI, to executives making decisions based on incorrect dashboards and everything in between.
Let‚Äôs dive into the two areas of DQ we mentioned earlier, data-specific attributes and business-related attributes.
Data Specific Attributes.
Sounds really fancy like right? That‚Äôs what I was going for. The truth is that by data-specific attributes I mean something very well-known to you smart folk.
The schema of the data (
think about the SQL create statement of a table for example
).
The data types (
string, int, decimal, boolean, etc
).
The data model (its relationships and uniqueness).
The over-abundance of data, or data going missing.
‚ÄúA good Data Quality system must
monitor
,
track,
and
alert
these key data-specific attributes.‚Äù
That‚Äôs your first lesson in DQ, and it isn‚Äôt rocket science, it‚Äôs common sense. At its most primal level, and I don‚Äôt care how shiny and new it is, if it misses this vital part, it‚Äôs a piece of junk. This is where the rubber meets the road, this is where most Data Quality issues arise, here at the core of the issue. Let‚Äôs talk about some examples.
NULL constraints.
Misplaced data.
Malformed data.
Mixed data types.
Duplicate data.
Missing data.
Get an idea of what I‚Äôm talking about? The first line of defense of any DQ system has to be
at the lowest level possible
. This is where most problems start, and the ones that cause serious fundamental issues.
Do you see how these above issues are data-related attributes? There is nothing more common than having NULL fields start to show up in your dataset, throwing off all sorts of metrics, aggregations, and poorly written code that cannot handle a NULL value.
Misplaced and malformed data are very common problems as well, like truncated data, and columns shifting by accident. Duplicated data, or data not being sent, cause cascading downstream Data Quality issues that your product team will not easily forgive.
Business Related Attributes.
So after all that, there‚Äôs still more to Data Quality? Why yes my friend, there is. No wonder everyone has been sweeping the DQ ideas under the carpet for all these years, it ain‚Äôt easy. But nothing in this life is easy, especially in Data Engineering.
So what is a business-related attribute in the context of Data Quality?
That can be sort of a hard one to answer, but I think you will catch on quickly.
‚ÄúBusiness-related Data Quality attributes give
meaning
and
context
to otherwise mundane data sets.‚Äù
Yes, it can seem a little ephemeral at first, but I assure you that is not the case, and ignoring the business-related attributes part of the Data Quality journey will only leave you beaten and battered, a shell of your former self.
Do the data attributes (values) meet the expectations of the business (do they make sense)?
What values are changing (on purpose) that we don‚Äôt know about?
Do the data transformations hold true to the business logic and rules?
At first, these business-related attributes of data can be hard to understand because
it requires more knowledge than simply ‚Äúcoding‚Äù or ‚Äútechnical‚Äù prowess to get ahead of these issues.
What are some examples of business-related DQ attributes that can go wrong, and very quickly cause major issues?
Customer status - ( active vs inactive, closed vs open, etc. )
Dimensional codes updating or changes ( say product code 55 and 56 are added without knowledge. )
Data transformation functions that encapsulate business logic grow stale over time.
As you can see, only the Data Quality battle is about the data-specific attributes, the other half, and arguably the more difficult half is the business-related context of the values attached to those datasets.
In DQ, it‚Äôs critically important to document, understand, and monitor those critical pieces of business logic that are found inside data pipelines and transformations, watching for unknown changes and problems, because these are typically the bugs that are extremely hard to catch.
The Data Quality Tooling Landscape.
This is supposed to be the fun part of the Data Quality discussion, but I think you will be surprised. The DQ tooling landscape is honestly in its infancy. There are few reliable tools to choose from that aren‚Äôt overwhelming, more than you bargained for, or just what you‚Äôre looking for.
I will introduce you to several of what I would call currently the ‚Äúbest-in-class‚Äù tools for Data Engineers to use as a Data Quality tool, that will work for most Modern Data Stacks.
The two tools I‚Äôm going to introduce to you are what I called in-situ DQ tools, placed and embedded into the core of your data infrastructure and data pipelines. Both of them are high-quality tools, although one is more complex than the other, capable of grasping a serious amount of your data quality.
Great Expectations
Great Expectations
is probably one of the most complex and all-around best Data Quality tools on the market, it simply can‚Äôt be beaten.
I‚Äôve written in-depth about it in the past
, but I will try to give you the 30,000-foot view right now.
In the GE world ‚Ä¶
‚Äú
Expectations are assertions for data.‚Äù
‚ÄùGreat Expectations is a shared, open standard for data quality. It helps data teams eliminate pipeline debt, through data testing, documentation, and profiling.‚Äù
-Great Expectations website.
Great Expectations can be pip installed for Python users, with as simple `init` command, GE will flood your repository with a plethora of files. It‚Äôs complex enough it should probably be stored in its own repository. Here are the concepts.
Data Context
‚Äì
Holds all configurations and below components for your project.
Datasource
‚Äì
Holds all configurations related to your data source(s) and interacts with them.
Expectation
‚Äì
The actual tests and ‚Äúexpectations‚Äù that describe your data.
Checkpoint
‚Äì
The validation that will run against your data, reporting metrics etc
.
Of course, the
Checkpoint
is the crux of this DQ tool. The DQ checks fall into several categories.
Interactive Expectation generation.
Profiler (auto-generated using data) Expectations.
Manually define Expectations.
Honestly, the sky is the limit when comes to writing DQ checks for your data with GE, you can profile data, check column order, check values in a range, and much more. The tool integrates with
Airflow
,
dbt
,
Prefect
,
Dagster
,
Kedro
for example.
You should take time to explore this tool, you will be amazed. But don‚Äôt be fooled, it‚Äôs a task to take on.
Soda CORE.
Soda-core
is provided by a company,
Soda
, which provides ‚ÄúData Observability‚Äù at scale for ‚Äúteams to find, analyze, and resolve data issues.‚Äù
Soda core is probably what I would call ‚Äúa step behind,‚Äù Great Expectations, but it‚Äôs still a wonderful tool in its own right, and in my opinion, is actually easier to use and has less of a learning curve.
I‚Äôve written in-depth about it before as well, feel free to check that out.
It doesn‚Äôt support and integrate as well as GE, but it has its fair share of plugins for tool extension.
soda-core-athena
soda-core-bigquery
soda-core-db2
soda-core-postgres
soda-core-redshift
soda-core-snowflake
soda-core-spark-df
soda-core-sqlserver
soda-core-mysql
soda-core-trino
Again,
soda-core can be pip installed
. It‚Äôs similar to the nature of GE but easier to use, you basically have two files you work with
configuration.yml
and
checks.yaml
. You then run what soda-core calls
scans
.
You can see examples of basic DQ checks
on the Soda website
. Here is an example.
# Checks for basic validations
checks for dim_customer:
  - row_count between 10 and 1000
  - missing_count(birth_date) = 0
  - invalid_percent(phone) < 1 %:
      valid format: phone number
  - invalid_count(number_cars_owned) = 0:
      valid min: 1
      valid max: 6
  - duplicate_count(phone) = 0
I suggest you dig a little into soda-core, I think you will be surprised about easy it would be to install and start using it as a basic data check in your own data pipelines. Is it all too much for you? Want something even easier?
Homegrown Data Quality for your team.
Don‚Äôt worry all is not lost. There is another way to gently dip your toes into the cool waters of Data Quality without diving into the deep end. Sadly, it‚Äôs the easiest way to get started down the path, but very few data teams even attempt it.
It‚Äôs called the ‚Äúdo-it-yourself‚Äù version of DQ.
Remember, your data team is already using some sort of data transformation and processing tool, you already have some sort of Data Warehouse or Data Lake, and you‚Äôre most likely using some sort of dashboarding or reporting tool.
‚ÄúThe only thing I‚Äôm suggesting is that you put some of those tools to work for yourself.‚Äù
And I mean a few specific things by that.
Build ‚Äúvolume‚Äù tables and dashboards.
Write your own data quality checks in all your data pipelines.
Start using constraints to their full potential.
Find and document all your business logic, and consolidate it into a single spot.
It isn‚Äôt hard but it does take a mindset shift. Simply writing ‚Äúcounts‚Äù or ‚Äúaggregate‚Äù numbers on the
data you‚Äôre already processing
is an easy task. Then magically you have all you need to build dashboards and notifications when data goes missing or leaves its normal range.
Constraints exist on everything from Postgres to Delta Lakes storing PBs of data, and the funny thing ‚Ä¶ no one uses them very much. Check for NULLs, and enforce values to be in a list or range. It isn‚Äôt rocket science.
Closing thoughts.
I hope you learned something about Data Quality today, I wanted to take all the mystery, smoke, and mirrors out of the topic. Data Quality is going to be a topic that continues to grow in the coming months and years. Having an idea of the landscape, and the types of problems a good DQ system can solve is critical for DEs.
I‚Äôm fairly certain most of the people I‚Äôm talking to don‚Äôt use any sort of Data Quality tool, but I hope you can see now that you have two easy choices you could start using tomorrow, Great Expectations and Soda Core.
Easy to install and get started, your manager and team members will be draping golden robes over your shoulders and shouting your name from the rooftops once you should them the power and payback of these tools when they start catching bugs.
I hope to write some more in-depth articles in the future about use cases with both Great Expectations and Soda Core so we can see them in action. Keep your eyes out for those articles coming soon!"
91299004.they-said-streaming-would-overtake,They said Streaming would overtake Batch.,The battle for the data pipeline.,2023-09-18 09:19:28.328-05,"In recent years, the
prophetic
stance in the data engineering landscape has been that streaming would eat batch ETL (Extract, Transform, Load) processes, fundamentally rearchitecting the way we handle big data.
The peddlers of new and shiny tools have been not shy in saying that batch ETL is dead, and you are
a proverbial mammoth wandering around waiting for your doom
if you‚Äôre not using streaming.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
As we find ourselves here towards the end of 2023, it‚Äôs high time to examine whether the proclamations have held true and to what extent the transition has impacted data operations.
Were the peddlers of ETL Doom correct in the destruction of everything we hold dear?
Thanks to
Delta
for sponsoring this newsletter!
I personally use Delta Lake on a daily basis, and I believe this technology represents the future of Data Engineering
. Check out their website below.
Streaming vs. Batch ETL: A Refresher
I know you are all very smart and wonderful Data Engineers, pumping through all day long. But, to set the stage for our discussion, let's revisit the basics of streaming and batch processing:
Batch ETL
Historically, data pipelines have relied on batch ETL processes, which involve collecting data in batches and processing it at scheduled intervals.
This makes up the vast majority of Data Engineering work done today, probably above the 80% mark.
Batch ETL is what I would call ‚Äúsequential‚Äù in nature. We pile up some work, then do the work, rinse and repeat.
It is less timely, in terms of delivery, but reduces the cognitive burden and development lifecycle in most cases. It‚Äôs considered ‚Äú
easier
,‚Äù although batch can be extremely complex depending on the context.
Streaming
Thank you for reading Data Engineering Central. This post is public so feel free to share it.
Share
Streaming, on the other hand, promises real-time data processing, handling data virtually as soon as it is generated. It‚Äôs about velocity when it comes to streaming. Fast, complex, fancy, and immediate results.
It‚Äôs seen as the North Star of Data Engineering, like Zeus on Mt. Olympus, such are those Data Engineers who are streaming savants.
In general, you deal in ‚Äúmessages‚Äù when it comes to streaming, although some micro-batch systems like
SparkStreaming
can ride in both worlds. Streaming is continuous in nature, batch is not.
Examining the Progress (Adoption Rate)
Streaming technology adoption has surged over the past years, propelled by the unprecedented growth in real-time analytics demands and IoT applications. Innovations in stream processing frameworks, like
Apache Flink
and
Kafka Streams
, have facilitated a smoother transition from batch to streaming architectures.
Tooling and Infrastructure
Over the years, the tooling ecosystem has evolved substantially to
support streaming processes
. The barrier to entry has been lowered, thanks to the advent of managed services and the consolidation of technologies that offer both batch and stream processing capabilities, simplifying the implementation of Lambda and Kappa architectures.
The Blurred Lines between Streaming and Batch
Why has streaming become easier? Because the lines are now more blurry as far as development around Batch vs ETL in some advanced systems. Take
Delta Live Tables
for example.
‚ÄúDelta Lake is deeply integrated with
Spark Structured Streaming
through
readStream
and
writeStream
. Delta Lake overcomes many of the limitations typically associated with streaming systems and files, including:
Maintaining ‚Äúexactly-once‚Äù processing with more than one stream (or concurrent batch jobs)
Efficiently discovering which files are new when using files as the source for a stream‚Äù
Databricks
Essentially what SaaS vendors like Databricks have done
is made the
use
and
development
of Streaming like capability much easier with less conginative and technical burden!
This is a far cry from setting up you‚Äôre own Kafka architecture on some Kubernetes cluster, messing with partitions, being a general genius, managing everything, and developing in that sort of environment.
In fact, it‚Äôs become such ‚Äúa thing‚Äù to simplify streaming for Data Engineers, companies like
Estuary
are popping up.
Tools like this take it a step past Databricks Delta Live tables and allow you to setup streaming pipelines in a simple UI. We‚Äôve come a long ways from SSIS haven‚Äôt we?
This type of work is a far cry from messing Producers and Consumers in a Python client for Kafka, not including the architecture work.
Challenges and Roadblocks
Despite its promising trajectory, the complete takeover of streaming is still hindered by a series of challenges including:
Complexity:
Streaming processes are inherently more complex to design, implement, and manage compared to batch processes.
Resource Intensity:
Streaming applications tend to be more resource-intensive, requiring robust infrastructural support.
Cost:
The transition to streaming architectures can be cost-prohibitive, especially for smaller organizations.
Hiring and Talent:
Honestly, it isn‚Äôt easy to hire Data Engineers with streaming experience. It just isn‚Äôt as common, it‚Äôs harder to hire for.
The reality of Batch ETL.
The reality of Streaming vs Batching in Data Engineerings is that most companies are working on small data that doesn‚Äôt not require real-time streaming systems.
It‚Äôs the classic 80/20 rule. Most Data Engineers in most companies don‚Äôt use streaming. Has that split grown over the last decade to include more streaming? Of course. Is the overwhelming vast majority of ETL still done in batch? Of course.
Future Directions
As we peer into the future, it is clear that batch processes won‚Äôt be rendered obsolete anytime soon.
Instead, we anticipate a more harmonized approach where organizations leverage both batch and streaming processing to meet diverse data workload requirements.
A promising trajectory is the growing interest
in serverless data architectures
which might change how we approach data processing, potentially altering the batch vs. streaming debate fundamentally.
Conclusion
Though streaming has not completely overtaken batch ETL, it has carved out a significant and growing niche in the data engineering landscape. The integration of streaming processes in data pipelines has indeed proven beneficial in harnessing real-time insights and fostering a data-driven decision-making culture.
However, batch ETL continues to hold ground, offering a reliable, cost-effective solution for a range of data processing needs. As we navigate the evolving data engineering landscape, it remains imperative to choose the right tool for the job, be it batch, streaming, or a harmonious blend of both.
Long live Batch ETL!
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber."
91298791.rust-will-never-break-into-mainstream,Rust will never break into mainstream Data Engineering.,"Rust is the new sliced bread, but it's future in DE is questionable.",2022-12-24 10:48:56.927-06,"Photo by
Saad Salim
on
Unsplash
It's true,
Rust is the future
, you can see it written in the sky, blowing by in the clouds, in the leaves blowing in the trees. Rust snuck in quietly while
Golang
,
Scala
, and
Java
were all playing games and frolicking in the fields as if there would never be a reckoning.
Python of course was just spending the nights at the club downtown.
But, like the first snow flurries darkening the winter sky, the summer days can't last, and it was inevitable that
Rust
came to be crowned.
Even so, Rust will never be accepted into mainstream Data Engineering for day-to-day use. I hope you stick around long enough for me to explain myself. Here's what we will cover today.
Why is Rust so popular?
Rust in the Data Engineering landscape today.
Why Rust won't be the Data Engineering language of choice.
Closing thoughts.
Oh,
Rust
, you are a special one, and much kinder overload than that dirty old
Scala
, that is for sure.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Why is Rust so popular?
Honestly, I kept asking myself that for a long time. I would quickly scroll past those annoying posts showing up saying ‚Äú
Look! Look! The End Is Near! Rust is here!!
‚Äù What could Rust possibly have to offer me? A lowly Data Engineering peasant toiling away in the dust and grime of data pipelines data after day, year after year.
That is, until like the grizzled old treasure hunter pawing through the filth, I one day decided to
actually try to use
Rust
, and discovered much to my amazement, a tool both terrible and wonderful to behold.
If you haven‚Äôt tried
Rust
, let me give you the quick and dirty of why you keep hearing about it.
Rust is
well known for its memory safety
.
Rust has a
better learning curve
than say C/C++ and other low-level languages.
Rust is
known to be fast
.
A flurry of Big Tech
announcements
that Rust is being adopted into the stack.
I mean, really, check out this example of working on a CSV with a Rust tool called
DataFusion
. Could it be any easier?
Rust is fast and known to be overwhelming to learn, but there are Rust crates that can make the transition easy.
That‚Äôs one of the main points I want to get across about Rust, and why you keep hearing about it. Yes, it‚Äôs a low-level language, and it can compete with C/C++, but it is an approachable language contrary to popular belief.
Rust in the Data Engineering Landscape today.
Now comes the interesting part, let‚Äôs take a view of the Data Engineering landscape and
see if we can find some ole‚Äô Rust hiding in there somewhere
, maybe we can divine something about the future from that.
Photo by
Ales Krivec
on
Unsplash
Rust has started showing up in a few spots in Data Engineering, places where it‚Äôs hard to ignore. Here are some of the spots.
delta-rs
Standalone API (no Spark) for Delta Lake.
DataFusion
for SQL, DataFrame, and Apache Arrow processing.
Polars
, blazing fast DataFrames.
Why is this interesting? Well because of the
type
of projects that Rust is showing up in. There is a common theme with the above projects (which are just a few), and this is
Big Data
.
Delta Lake
Delta Lake
has quickly taken over the Big Data world as the storage layer of choice for those using Apache Spark, and that‚Äôs a lot of people. It was made popular with the rise of
Databricks
, and has a massive active community behind it.
‚ÄúThe
delta-rs
Delta Lake standalone tool is going to be used for all sorts of Big Data related product innovation in the future.‚Äù
Apache Spark is going getting more popular and adoption is increasing, Databricks is riding the wave, and Delta Lake is riding the Databricks wave. It‚Äôs a vicious circle. As more folks adopt these technologies,
the demand for tooling and features will increase, delta-rs will probably be one of the main tools to provide the answer.
DataFusion and Polars
Then of course there is the ever-present rise of the Dataframe API. It‚Äôs pretty much Dataframes everywhere you look, every tool supports a Dataframe API or it isn‚Äôt considered best of class. With all that popularity, data users are clamoring for more than just Pandas, and sometimes Spark is overkill for them.
Enter
DataFusion
and
Polars
, both of which are Rust-based Dataframe processing libraries. In the case of Polars, a
Python package is provided
, which lowers the entry for usage by a lot, and the
speed of the tool is incredibl
e.
The adoption of Rust-based tools with Python packages will increase.
Rust will probably grow as the language of choice to replace Scala/Java data processing systems as the years go by.
You will probably see the adoption of Rust for normal DE tasks on the fringes and corners of the community.
But that does leave the question, why will Rust never break into the mainstream of Data Engineering?
Why Rust won't be the Data Engineering language of choice?
Photo by
arash payam
on
Unsplash
Considering I like Rust myself and it‚Äôs starting to appear in all sorts of interesting spots around Data Engineering, like Dataframes and Storage APIs,
how can it not be the future?
I mean if the likes of
Google
, Microsoft, and the rest are recommending it as the tool of the future, doesn‚Äôt it HAVE to take over Data Engineering? No way.
Rust is too heady (cognitive burden) for the majority of Data Engineers.
The development cycle won‚Äôt be quick enough for most DE tasks.
Python is too popular, and always will be.
Tools like Spark (which is going nowhere) are written in Scala and Java.
The fact is for Data Engineering, Python will not be unseated for decades probably.
Honestly, it takes time for the data community to change and move forward, just like most other Software Engineering disciplines.
‚ÄúThere is a reasonable chance that Rust will slowly start to replace Java and Scala as the language of choice for building new Data Engineering tools that the rest of the world use. It‚Äôs fast, memory safe, and not impossible to learn.‚Äù
But, because of those same reasons, it will never be mainstream. For the next many years, Rust will be relegated to the background, for those backend engineers building systems that everyone else will build on top of, with Python APIs for most DEs to use.
The data world is unique, it‚Äôs filled with Analysts, Data Scientists, and more self-taught coders than the rest of the tech industry. Rust really isn‚Äôt for anyone but true Software Engineers in every sense of the word.
Closing Thoughts.
I know Rust is here to stay, just another language in the long list that came before it in Data Engineering. I would be worried if I was Scala, but not Python. With all my doom and gloom, I can personally say I‚Äôve enjoyed learning a little bit of Rust, it‚Äôs refreshing.
I think you should take some time to read the
free online Rust book,
and write some yourself. Learning Rust will make you a better Data Engineering and programmer in general. You will be surprised how easy is to learn and dive into.
So next time you see a post about Rust taking over the world, go ahead and yawn, and give a big ole‚Äô stretch.
Tell yourself you will give it a try sometime, but then go back to your Python and code on."
91298485.new-years-resolutions-for-data-engineers,New Year's Resolutions for Data Engineers.,Start the new year off with a bang.,2022-12-26 16:02:43.993-06,"2023 is your year, it's the year of the Data Engineer.
Let it be one for the record books, a pivotal change for yourself
. Can you teach an old dog new tricks? I have my doubts, b they say hope springs eternal.
Maybe you're at the top of your game, or at the bottom. Either way setting 2023 New Year's resolutions for your Data Engineering life is a sure way to transform your digital self into the chiseled and fined tuned behemoth.
Let me give you 5 suggestions you can set as resolutions to improve your life.
I will unit-test all my code.
I will study the art of Data Modeling.
I will learn one new programming language.
I will read at least 2 new technical books in 2023.
Well, that should keep you busy this year eh?
Subscribe now
I‚Äôve found at different times in my Data Engineering I‚Äôm growing faster, than sometimes slower, and it usually has to do with me. Yes, life gets busy, and work gets crazy, but in the end, that‚Äôs the way it always is.
Finding your way through the crazy, being
intentional about your schedule
, and having focus time to learn and grow are essential for your career and
overall happiness.
I believe that growth and learning require sacrifice, but not as much as you think, it can be enjoyable and fun, paying dividends back to you for years. Let‚Äôs just jump into my 4 suggestions for New Year‚Äôs Resolutions, Data Engineering style.
Unit Testing
Your first task in the New Year is a big one, and it‚Äôs called unit testing my friend. Whatever the reason is, Data Engineering has always lagged behind in certain Software Engineering best practices,
and the number one item is unit testing
.
Photo by
Alvaro Reyes
on
Unsplash
Why unit testing? Because it‚Äôs the gateway to a better future, that‚Äôs why. It‚Äôs always the first step for every Data Team when working towards a better way to develop and do Data Engineering.
When you start down the road of writing unit tests for data pipelines, it starts to require other changes to your Data Stack, and processes that make can totally change the trajectory of Data Engineering.
Unit tests will drive you toward
functional code
... that tests easier.
When you
unit test your code
, you will write better code.
Unit-tested code is easier to reason about and debug.
If you‚Äôre already writing unit tests, then I‚Äôm preaching to the choir. If you‚Äôre not, it might sound like a small simple step, but it‚Äôs a true gamer change. A Data Team that unit tests all data pipeline transformations and code will be more forward-thinking, have fewer bugs and generally be more efficient.
Data Modeling
There have been a few hot topics in the DE community over the past year, and
data modeling
keeps coming up as one of those topics and will continue to do so for some time. Why data modeling?
‚ÄúWith the rise of Big Data and the popularity of Databricks, Snowflake, Redshift, and other storage systems like Delta Lake, LafeFS, and the like, it‚Äôs become clear that
no Data Platform is immune to a bad data model.‚Äù
It‚Äôs hard to find good resources to read up on data modeling, especially data modeling for Big Data applications. I would suggest if you are new to the Data Warehouse and Modeling landscape, to start with the
Data Warehouse Toolkit
, although it may be a little dated, most of what it teaches is foundational and timeless.
Do yourself a favor in 2023, and gobble up as much data modeling material as you can.
Learn a New Language
It‚Äôs a New Year, so we need to add some more new things to our repertoire of skills, a new bunny to pull from our hat so to say. I have found that there are very few things in my Data Engineering career to propel my skills forward than learning a new language.
‚ÄúThere is something other-wordly and exciting about it, starting from scratch, going through the struggles all over again. It keeps one humble.‚Äù
In 2022 I tried how not one, but two new languages,
Golang
and
Rust
, which I‚Äôve written about a few times. I can say for sure that it made me a better Data Engineer. Learning new languages forces a few things ‚Ä¶
Gaining a greater perspective of options when it comes to languages.
Remembering the struggle of learning something new (humility).
Learning how to approach a problem differently.
More tools in your box for future problem-solving and solutions.
You become a better programmer at what you‚Äôre already writing.
Learning new stuff keeps you from stagnating, turning into a stinky pile of rubbish the younger generation turns their nose up at. I suggest starting with
Rust online book
or
Golang
.
2 New Technical Books
You remember in school when you just played around in study hall when you should have been studying? Is that still you today? Time to change your wicked ways.
There are few things better for the soul than reading, it‚Äôs a great way to escape into another world, and learn something why you‚Äôre at it. Sure I know you can just read all the stuff you want for free online.
But, few things go in-depth like reading a technical book that someone has poured their life into. It‚Äôs almost impossible not to soak up some serious knowledge when reading such content.
Setting some time to read two new books in 2023 will be good for a few reasons. It forces you to set aside some time for growing and learning, it‚Äôs a good habit to develop. Also, it‚Äôs your chance to learn something about a topic you‚Äôve been wanting to in the back of your mind, but for whatever reason haven‚Äôt gotten around to.
Here are my recommendations in no particular order.
The Pragmatic Programmer
- basically about the theology of programming.
The Staff Engineer‚Äôs Path
- What it means, and how to become a Staff Engineer.
Staff Engineer
- interviews and talks about how to expect and how to become a force multiplier.
Fundamentals of Data Engineering
- general info.
Spark: The Definitive Guide
- become a Spark expert.
That‚Äôs just a few I‚Äôve read lately, I‚Äôm sure you can find others that suit you more if you poke around.
Closing Thoughts
I really hope 2023 dawns bright and new for you, it‚Äôs been kind of a downer end of the year this year in tech, layoffs, and the like. But, that‚Äôs life and we have to look forward with optimism to this new year.
We can‚Äôt control many of the crazy things happening around us, and to use. But, there is hope, we can control ourselves and how we spend our time.
I think we can all agree if it‚Äôs not one of the topics we just covered, it might be something else, there are always areas we know we need to grow in to become the best and most productive Data Engineers possible.
I will unit-test all my code.
I will study the art of Data Modeling.
I will learn one new programming language.
I will read at least 2 new technical books in 2023.
Pick one from the list and get started! Don‚Äôt forget to reach out and let me know how it goes!!"
90774584.join-my-new-subscriber-chat,Join my new subscriber chat,A private space for us to converse and connect,2022-12-14 22:08:13.892-06,"Today I‚Äôm announcing a brand new addition to my Substack publication: the Data Engineering Central subscriber chat.
This is a conversation space in the Substack app that I set up exclusively for my subscribers ‚Äî kind of like a group chat or live hangout. I‚Äôll post short prompts, thoughts, and updates that come my way, and you can jump into the discussion.
To join our chat, you‚Äôll need to download the
Substack app
, now available for both iOS and Android.
Chats are sent via the app, not email, so turn on push notifications so you don‚Äôt miss conversation as it happens.
How to get started
Download the app by clicking
this link
or the button below.
Substack Chat is now available on both iOS and Android.
Get app
Open the app and tap the Chat icon.
It looks like two bubbles in the bottom bar, and you‚Äôll see a row for my chat inside.
That‚Äôs it!
Jump into my thread to say hi, and if you have any issues, check out
Substack‚Äôs FAQ
.
Join chat"
89708749.whats-all-the-hype-with-duckdb,What's all the hype with DuckDB?,Introduction to DuckDB for Data Engineers. Performance and other thoughts.,2022-12-18 13:23:38.31-06,"Have you been as confused as me about
DuckDB
? My goal with this article is to cut through all the crud and get to the bottom of the issue. I keep seeing the name
DuckDB
keep popping up in my feeds here and there in my varied and disjoined internet travels.
There‚Äôs nothing for it but to answer a few questions.
What is DuckDb?
How could a Data Engineer use it?
How is being marketed and used today?
What is the performance like?
Overall thoughts on DuckDb.
It‚Äôs hard not to become jaded by the number of tools being released (even those supposedly open-source tools, which are still just a way for some SAAS tools to get more traction), it‚Äôs truly overwhelming and hard to keep up.
Is a tool worth it? Will it be here and gone in a year? Will others start using the tool? Could it help streamline my data pipelines?
I see that as part of my service to you, the reader.
‚ÄúI want to kick the tires on things for you, put in the time, research, and code so you don‚Äôt have to. Then give you my unfiltered opinion on that tool, in this case,
DuckDB
.‚Äù
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
Photo by
Timothy Dykes
on
Unsplash
What is DuckDB?
This answer turned out to be much easier to find than I thought it would be. Sometimes it‚Äôs tough to cut through all the layers of frosting before you get to the real cake at the core of the issue. I have to admit, DuckDB is positioning itself in an exciting place in the Data Stack, and fortunately, they are very straightforward about who they are, what they do well, and what they don‚Äôt do well.
‚ÄúDuckDB is the OLAP (analytical) version of SQLite.‚Äù
I feel like I‚Äôm giving away the conclusion from the beginning, but that‚Äôs fine. We can spend the rest of the time unpacking why I think DuckDB is the marriage of
SQLite
and Pandas.
What does DuckDB have to say about itself?
‚Äú
DuckDB is an in-process SQL OLAP database management system
‚Äù
-
duckdb
It‚Äôs funny, after all the DuckDB hype when digging into it, I wouldn‚Äôt say there isn‚Äôt a single thing that is ‚Äúearth-shattering,‚Äù it‚Äôs simply trying to do what SQLite does a little better. Maybe steal away some of those Pandas users.
What are the main points and features of DuckDB?
Simple, clean install, with very little overhead.
Feature-rich with SQL, plus CSV and Parquet support.
In-memory option, high-speed, parallel processing.
Open-source.
‚Äú
DuckDB
is a
transient
relational database
for in-process complex join, aggregation, and high-speed data transformations.‚Äù
How could a Data Engineer use DuckDB?
This is an interesting question to answer. I‚Äôve been doing data work for well over a decade and the use case for this sort of tool is most specialized and less common than you think.
There are a few things to remember before using a tool like DuckDB that are very important so you don‚Äôt shoot yourself in the old foot per se. You know that word they use to describe DuckDB called ‚Äú
in-process.
‚Äù Well, you should pay attention to that.
DuckDB isn‚Äôt meant to replace MySQL, Postgres, and the rest of those relational databases. In fact, they tell you DuckDB is very
BAD
at ‚Äú
High-volume transactional use cases,
‚Äù and ‚Äú
Writing to a single database from multiple concurrent processes.
‚Äù
What does that mean for Data Engineers?
‚ÄúNever use DuckDB for anything that needs to last longer than the duration of your data pipeline.‚Äù
You might be asking yourself,
then what good is DuckDB to me then
? I‚Äôve seen a few use cases that might benefit from this tool. Although I think they are more limited than a DuckDB fan might admit.
R&D data exploration and research.
Very specific in-pipeline data transformation and analytics (as an appendix to the main pipeline process.)
???
I‚Äôm grasping for straws here
. I‚Äôm not trying to be down on DuckDB, I‚Äôm just struggling to find real use cases that are more than just R&D data exploration. I can see its uses if you have a bunch of data locally in different forms and are doing serious research and data exploration that requires lots of joining and aggregations. At a certain point, DuckDB probably makes sense.
If you really stretch the imagination you might find some specific in-pipeline uses cases where maybe you‚Äôre crunching some ancillary data sets and you could use DuckDB to do that.
I know it‚Äôs supposed to be fast and easy to install, but is it really worth the cognitive overhead of adding yet another tool to your data pipeline? Not sure.
Maybe we can answer this question a little better by understanding how DuckDB has been marketed so far, and find some references to others who are using DuckDB, and getting a better idea of how it actually might be used for Data Engineering.
How is DuckDB being marketed and used today?
There are several sources I want to look at, including
DuckDB‚Äôs blog
, and their
Twitter wall
of testimonials. See what‚Äôs going on in the wild.
I reached out to my network on
Linkedin
and Mastodon. The response was tepid. Honestly, the stuff on the DuckDB website wasn‚Äôt all that specific, other than that it‚Äôs faster than SQLite, which I‚Äôm sure it is.
There were also a few comments about using it for quick analysis of data, which makes sense.
I did read some ridiculous blog post from DuckDB
about how it fits into the Modern Data Stack and could be run on a single large machine and replace a lot of data pipelines and Data Warehouses! That seems a very naive argument for sure. Especially since DuckDB by its own admission should not be used concurrently by multiple processes and is in-process and transient.
Most data folk know that using single large machines simply isn‚Äôt a scalable design and will absolutely come back to bite you and is a classic architecture mistake.
‚ÄúI have yet to find a definitive example of how DuckDB is being used by anyone, let alone Data Engineers for anything other than data exploration and research. Maybe it‚Äôs out there, but I‚Äôm having trouble finding it.‚Äù
With all that talk of performance, which is repeated over and over again, this should be interesting.
Quick DuckDB technical overview.
Honestly, there isn‚Äôt that much exciting to share about the technical use of DuckDB. One can simply pip install it with Python for example, and reading datasets is as simple as running something like
read_csv_auto('input.csv')
embedded in a SQL statement.
After that, you write DuckDB queries much like you would Postgres or MySQL. Join,
aggregate
, filter, and
select
, like you would any other SQL statement. I mean, that‚Äôs the whole point. You can create tables, copy data, and run queries, it‚Äôs simply another SQL manipulation tool.
Read the docs if you like
.
What is the performance of DuckDB like?
The movement we‚Äôve all been waiting for, a performance test. I‚Äôm not going to claim I run perfect and bulletproof performance tests, but I do my best. Install things, then run some normal tasks in a few tools and compare the results.
This code is available on GitHub
. We are going to use the
Backblaze Hard Drive failures dataset
. The dataset consists of three different sets of CSV files. Unpacked this is about
20.71 GB
of data.
The data itself consists of hard drive model information, along with dates and if the drive failed or not.
We will simply use DuckDB and Pandas to group the data by date and count the number of failures. So read the data, do a simple
groupby
and
count
.
The first thing I will note is that using DuckDB with Python is super easy, almost too easy.
import duckdb
from datetime import datetime

t1 = datetime.now()
conn = duckdb.connect()

conn.execute(""""""SELECT date, SUM(failure) as failures
                FROM read_csv('data/*/*.csv', delim=',', header=True,  IGNORE_ERRORS=1, 
                    columns={
                            'date': 'DATE',
                            'serial_number' : 'VARCHAR',
                            'model' : 'VARCHAR',
                            'capacity_bytes' : 'VARCHAR',
                            'failure' : 'INT'
                            })
             GROUP BY date;"""""")
print(conn.fetchall())
t2 = datetime.now()
print(""duckdb took {x}"".format(x=t2-t1))
The results are in.
duckdb took 0:01:09.602031
After posting preliminary results to a few social media networks, someone had a suggestion about using an experimental part of DuckDB, a parallel execution flag.
conn.execute(""SET experimental_parallel_csv=TRUE"")
This did indeed decrease the runtime, coming in at
0:00:49.865234
, a great performance improvement, but still much slower than Polars as you will see.
Pandas Performance
You guessed it. Pandas cannot handle it, pukes, and uses up all the memory on my M1 Macbook.
zsh: killed     python3 pandas_main.py
import pandas as pd
import glob
from datetime import datetime

t1 = datetime.now()

all_files = glob.glob(""data/*/*.csv"")
files = []
for filename in all_files:
    df = pd.read_csv(filename, index_col=None, header=0)
    files.append(df)

df = pd.concat(all_files, axis=0, ignore_index=True)
df = df.groupby([df['date'].dt.date]).sum()
print(df)
t2 = datetime.now()
print(""duckdb took {x}"".format(x=t2-t1))
This is no surprise as Pandas is well known to be a memory hog.
So if we can‚Äôt use Pandas to compare this to, what can we do? Let‚Äôs try
Polars
, a new tool in the Dataframe world.
Polars Performance
We are going to try the Rust-based Polars Dataframe tool, using the Python API.
import polars as pl
from datetime import datetime

d1 = datetime.now()
q = (
    pl.scan_csv(""data/*/*.csv"", parse_dates=True, dtypes={
                            'date': pl.Date,
                            'serial_number' : pl.Utf8,
                            'model' : pl.Utf8,
                            'capacity_bytes' : pl.Utf8,
                            'failure' : pl.Int32
                            })
)

df = q.lazy().groupby(pl.col(""date"")).agg(pl.col('failure').sum()).collect()
print(df)
d2 = datetime.now()
print(d2-d1)
I know DuckDB is claiming to be fast, but not fast enough.
Polars
blows DuckDB out of the water.
0:00:21.527071
Overall thoughts on DuckDB.
My thoughts on DuckDB are complicated. In some aspects, I was a little disappointed, mostly because I listened to the marketing fluff too much. I mean it‚Äôs just a nicer version of SQLite, which is helpful in some senses.
But, they talked so much about performance, but my tests were showing it to be at best half as fast as a tool like Polars, with which you can accomplish all the same tasks. I‚Äôm just a little disappointed.
Yes, it‚Äôs simple to install and use. But, if it‚Äôs mediocre for performance, is it really worth my time as a Data Engineer to ‚Äúfind‚Äù a spot for it in my data stack? No, it isn‚Äôt. In today‚Äôs crowded market space, with every vendor hawking and selling everything under the sun, you need to stand out in the crowd.
DuckDB does not stand out to me. I‚Äôm sure some folks find it useful for certain edge cases, and it might solve their pain.
But it appears you can solve those same pains with familiar Dataframe tools that will run twice as fast."
89523420.3-types-of-data-engineers,3 Types of Data Engineers,Which one of the Data Engineering archetypes are you? Who do you want to be?,2022-12-14 07:17:15.948-06,"What? You thought there was more? Or less? No, I tell you,
there are only three types of Data Engineers
, and it's important to know!
It's important to know because you might be just starting your Data Engineering journey, or well into it. Either way, it's important to understand the types of Data Engineering categories that exist so you can head in the correct direction, or change directions if needs be.
Trying to divine the future is difficult, but when it comes to choosing a Data Engineering future for yourself, you actually have control of your own destiny.
‚ÄúWhat, you don't like me putting everyone into three Data Engineering buckets? You may not like it, but it's the reality of the world you live in.‚Äù
By the end of this article,
I hope to convince you that I'm right about these Data Engineering archetypes.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
We will cover ‚Ä¶
What are the three archetypes
Why Data Engineering archetypes matter
What makes each archetype unique
Closing thoughts
Photo by
Bruno Martins
on
Unsplash
The three Data Engineering archetypes.
With all the build-up, it‚Äôs probably just time to unveil the three Data Engineering archetypes, let‚Äôs get to it.
These are the three major and all-encompassing Data Engineering archetypes, namely Big Data Engineering, SQL Engineer, and Backend Engineer. Now before you get all confused and huffy-puffy, let me explain myself, please.
Big Data Engineer
Backend Engineer
SQL Engineer
If you think about the job postings you see in the marketplace, the skills they require, and the technical stacks that different data organizations run, you will find these three archetypes for the most part exist for each position you look at or function in.
Let‚Äôs take a look at each one of the categories and see what functions they do, and the tools they use.
Big Data Engineer
Back in the day of Hadoop and Java MapReduce, you saw the title of Big Data Engineer floating around, you might see it every once in a while now. But what do I mean by Big Data Engineer?
‚ÄúI mean the type of Data Engineer who spends their day working on big TB and PB level datasets using things like Spark, Presto, EMR, Flink, Kafka, etc.
‚Äù
These are the Data Engineers who understand distributed systems, partitions, clusters, and the like.
There are lots of Data Engineers who have no experience with these large datasets and distributed systems like Spark, and that‚Äôs ok.
It does take some experience to understand and troubleshoot long-running queries at scale, along with topics like file compaction and partitions, and data modeling at scale.
These are skills that are very hard to pick up without practice. Here are some of thing things that are particular to Big Data Engineers.
Above-average programmers and software engineers.
Above average on DevOps and CI/CD skillsets.
Understand distributed systems and CPU/Memory.
Experts on most cloud storage and providers.
Some companies have tooling and architecture that are heavily reliant on big data technologies like Spark, and that will be reflected in those job descriptions.
Big Data (
Data
) Engineers usually command high compensation and have a bright future in the data landscape for years to come.
Some of the skills that Big Data Engineers have are
extremely hard to learn from a textbook
, and require a lot of real-life experience. Skills like working on distributed systems, understanding clusters, memory, CPU usage, and DevOps/CI/CD ideas that help one work effectively at scale.
This is why the Big Data Engineer requires above-average programming skills, working with tools like Spark at scale, and require expert-level programming skills to write efficient programs that work at scale.
SQL (Data) Engineer
I want to be clear about something when I say SQL (Data) Engineer I don‚Äôt mean this person can
only
write SQL, or that‚Äôs all they do. What I mean is that SQL is a large part of what these Data Engineers do, and reflects in the technologies they use on the job and what skills
‚ÄúSQL (Data) Engineers are experts at Relational Databases (Postgres, MySQL, etc.), analytics, Kimball Warehouses, data modeling, APIs, and everyday Python.‚Äù
I spent the first, and many years, of my career as this type of Data Engineer, and it‚Äôs the perfect entry point into the world of Data Enigneering.
You become an expert at the basics of many important and fundamental skills
.
Here are a few of the things that SQL (Data) Engineers work on and become experts with.
SQL expert, query tuning, optimization.
Relational database design/tables and indexes.
Fundamental Python coding skills.
API‚Äôs, data ingestion.
Data/transformation pipelines and ingestion.
Data warehousing and related concepts.
In my opinion, this is the best way to cut your teeth on Data Engineering and actually provides the greatest range of skills and different types of problems to work on.
You will not get bored in this job.
SQL (Data) Engineers have to write a reasonable amount of Python usually, pulling data from APIs and ingesting flat files.
They learn to be masters of the classic Data Warehouse, Kimball style, and analytics are their sweet spot.
If you‚Äôre trying to break into Data Engineering or want job stability, this is the type of Data Engineer you want to be.
The number of companies still using Postgres, MySQL, or SQL Server to store and process data is massive.
Backend (Data) Engineer
This is the last and the rarest of all Data Engineers, compromising a very small subset of the Data Engineering community, that is the Backend (Data) Engineer. These are the unicorns of the data world.
I‚Äôve personally interviewed for one or two of these jobs and never made the cut. (
I got lost, on purpose, when they asked me to write a state machine in Golang.
)
‚ÄúBackend (Data) Engineers are Senior level software engineers, with a hardcore software engineering background. Not only are they great Data Engineers (SQL and Big), they are excellent Software Engineers.‚Äù
What is it exactly that Backend (Data) Engineers do? Good question. From what I can tell based on my experience and job posting ‚Ä¶ they are capable of a few extraordinary feats. Most importantly, they walk the line between hardcore SE and DE.
Design high-performance API‚Äôs to serve data.
Expert in multiple languages, especially strongly typed ones (Golang, Scala, Rust, etc).
Expert in configuring and architecting complex distributed or streaming systems.
Work closer to the application layer.
Can also do normal Data Engineering tasks.
What makes the Backend (
Data
) Engineer special is that not only can they use a system like Kafka or Spark, they truly understand it at a core level, and could configure it or custom it if needed.
And what makes them even more valuable is that they can do what Big Data Engineers and SQL (
Data
) Engineers do as well. This makes them extremely capable of building the most scalable data systems in use today.
Why Data Engineering archetypes matter
I think it‚Äôs important to start this discussion with an overview of why we have to talk about Data Engineering archetypes at all, why are they important? There are a lot of misconceptions about what Data Engineers do, and if you are new to Data Engineering, or getting bored with being a Data Engineer all hope is not lost.
It can also be exceedingly confusing when searching Data Engineering job descriptions and requirements, they can range from SQL and Dashboards to Scala and Kafka. That is quite the dichotomy, my friend.
Understanding the Data Engineering archetypes can help solve a lot of heartaches and give a lot of direction.
You can know which jobs match your skills, and not waste your time.
You can switch to a different archetype if you're getting bored.
Understanding your archetype can help hone your skills and become an expert.
You can avoid imposter syndrome better.
Let‚Äôs dive into each one of these reasons that understanding Data Engineering archetypes will make your life easier and career better.
Understand how your skills map to the job market better.
This is probably more of an important topic for new engineers trying to land their first Data Engineering gig, or on the other end of the spectrum, those Data Engineers who‚Äôve been around for a while, maybe they are tired of SQL and are looking for something new to do.
Either way, as is now, and probably will be in the future, when looking for Data Engineering positions, there are a lot to choose from. You should inspect each job and make sure that your skills match what you‚Äôre applying for, or
what you actually want to do.
‚ÄúDon‚Äôt waste your time in applying for SQL (
Data
) Engineer roles when what you want is a Big Data Engineer role working on Spark, not Postgres.‚Äù
You can switch to a different archetype if you're getting bored.
It‚Äôs also critical to understand the Data Engineering archetypes so that when you get bored you can move on with your life, instead of thinking you need to give up your career in IT to start a YouTube channel.
Maybe if you‚Äôre a SQL (
Data
) Engineer you want to move on, try something new, work on a little Spark or Databricks, and add some spice to your life. Could be you‚Äôre the Big Data Engineer who‚Äôs written one too many Spark scripts, and just wants to move on to Golang and write some backend code.
What makes each archetype unique?
I think Data Engineering is a wonderful and unique space to work in, a true privilege. We have the luxury of being in high demand in the market, and probably some time into the future.
It‚Äôs also a boon to you as a Data Engineer that you have the opportunity to switch to different archetypes as the whim arises.
‚ÄúCan you imagine a Data Engineering team made up of each of the archetypes? Can you imagine what such a team could accomplish?‚Äù
The SQL (
Data
) Engineer hacks away on SQL, creating Data Lakes and Warehouses, analytics and dashboards, working the business to provide value. The Big Data Engineer as well, over in the corner, crunching masses of data with Spark, spitting out datasets that are oozing with insights and pipelines that run smoothly and wickedly at scale.
And there above them all, the Backend (
Data
) Engineer, architecting and scheming, building APIs and working on complicated microservices, walking the tightrope between two worlds, and pouring out code like water.
You can see that each one of these Data Engineers is important in their own right, providing value that is unique and needed in today‚Äôs world.
The End.
What type of Data Engineer are you today? Me, I‚Äôm a Big Data Engineer, and Spark is my tool of choice. I‚Äôve been a SQL (
Data
) Engineer, that‚Äôs how I cut my teeth. I learned lessons as a SQL (
Data
) Engineer that will stick with me for the rest of my life, it makes me a better Big Data Engineer.
Who do I want to be? A Backend (Data) Engineer maybe when I get bored with Big Data. I like Golang, I like Rust, and I can feel its call upon me.
‚ÄúWhat kind of Data Engineer do you want to be 5 years from now? I think you should answer that questions and have the plan to work towards that goal. Each type of Data Engineer is unique and has unique skills, and you won‚Äôt gain them by accident.‚Äù
It is a bright future for Data Engineers. You‚Äôre lucky to be one."
89300292.why-is-everyone-trying-to-kill-airflow,Why is everyone trying to kill Airflow?,"Apache Airflow has been the ruler of Data Engineering orchestration for years, is the end in sight?",2022-12-13 08:06:38.709-06,"The end is near, the end is near!! I feel like someone is constantly waving one of the signs about the end of the world coming and yelling to take cover, and that end of the world is the demise of our beloved
Apache Airflow
.
Let‚Äôs peer forward into the cloudy and murky future, divine the tea leaves, and look into the crystal ball. Is it really the end of the age of Apache Airflow?
Dagster
is here,
Prefect
is here,
Databricks Workflows
, everyone is trying to throw Airflow from its lofty throne.
Subscribe now
Photo by
Amy-Leigh Barnard
on
Unsplash
Is there something(s) wrong with Airflow?
I‚Äôm going to start with the conclusion, that Apache Airflow is far from dead, in fact, it probably has a bright future for a long time to come. I‚Äôm calling no-go on the doomsday proclaimers who are out there peddling the end of Airflow as we know it is near.
‚Äú
Anyone who thinks Airflows days are numbered has been reading too much marketing material and isn‚Äôt paying attention to the reality in the wider Modern Data Stack.
‚Äù
There is nothing fundamentally wrong with Apache Airflow, it‚Äôs a first-class Data Engineering tool that has been driving data pipelines forward for a long time. And the
adoption of this tool is
NOT
slowing down
.
Why then are people calling the end of Apache Airflow?
I‚Äôm going to make a proposition to you, the average Data Team or Data Engineer is not calling for the demise of Apache Airflow.
There are new SAAS companies and marketing teams calling for the end of Airflow, and there is a difference between the two.
‚ÄúIt isn‚Äôt really that Airflow is coming to an end, it‚Äôs that
Airflow has reached critical mass
and now competitors are chopping at the feet, taking market share.
The more people that use Airflow the more certain rocks come out of the water, because
Airflow is being pushed to its limit
.
And that‚Äôs a good thing.‚Äù
Airflow is taking head because there are two newcomers to the field of pipeline orchestration,
Prefect
and
Dagster
. This is only natural, it‚Äôs how the free and open market operates, even with open-source software.
When something, like Airflow, becomes extremely popular because it‚Äôs doing a task so well and filling a need, there is always someone or something that is going to come along and say, ‚ÄúI can do better than that.‚Äù
There is nothing better for Airflow than stiff competition.
It drives the community at large to improve and innovate.
How can we really know there is a solid future for Airflow still?
This is an important question, you don‚Äôt want to hitch your wagon to bum oxen if you‚Äôre in the market for a new orchestration and dependency management tool. I will give you two very compelling reasons to believe Airflow is here for the long haul.
MWAA (
managed Airflow by AWS
)
Composer (
managed Airflow GCP
)
It's a pretty safe bet that if AWS and GCP are willing to go to all the work to offer Airflow as a managed service, it will be around for a while. If only for the fact that they have so many customers and so much of the market share.
When cloud companies make it ridiculously easy to use a tool, you can bet that Data Teams are going to take the bait. Simplicity and keeping tooling on the same architecture is seen as more and more important.
What it really boils down to is that if AWS and GCP are supporting Airflow, it‚Äôs here to stay.
What are some things Airflow is bad at?
It's important to recognize that every tool is going to have ‚Äúareas that need improvement,‚Äù also known as areas that it sucks at.
Airflow is no exception to this rule,
its shortcomings are what have given rise to the next generation of tools
like Perfect and Dagster. Airflow has real problems that can cause serious problems for some data teams.
Doesn't scale well with big data.
UI isn't exactly something to cheer about.
Some folks complain about the verbosity of DAGs.
Running hundreds of thousands of DAGs can be a pain.
Integrating custom pipeline code into Airflow isn't very smooth.
Inter-task communication is a thorn in the flesh.
Some folks complain about the DAG learning curve.
To be honest, you can‚Äôt really blame all these things on Apache Airflow, the truth is that Airflow has become so ubiquitous and widely used in the Data Engineering community that it gets abused.
Trying to use Airflow with a bunch of workers to actually do data processing and transformations at scale ‚Ä¶ is probably not a good idea. You‚Äôre misusing Airflow. What you should instead do if you have data of any decent size is offload the compute using some
community-provided connector/packages
, leaving Airflow to do what it does best, monitor, orchestrate, manage dependencies, and schedule.
‚ÄúDon‚Äôt make the mistake of thinking Airflow is a great data processing tool, in itself. That is not where it shines.‚Äù
Sometimes the grass looks greener on the other side of the fence.
It‚Äôs hard not to get drawn away by the shiny new tools, we are all human after all. I mean when you‚Äôre looking at Airflow‚Äôs UI, compared to its direct competitors ‚Ä¶ it would make you start to wonder.
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
What is Airflow still good at?
Well, I‚Äôm glad you asked! With the number of people and engineers using Airflow around the world you can bet your bottom dollar that there are many spots where it excels and enables Data Teams to do amazing things.
Let‚Äôs sing the praises of Apache Airflow for a while, shall we?
Airflow is good at scheduling tasks.
Airflow is good at orchestration.
Airflow is good at dependency management.
Airflow has a massive and very active community.
Airflow has an amazing amount of third-party providers (think Snowflake, Databricks, etc.)
Airflow is very customizable and extensible.
Airflow is supported and managed by AWS and GCP.
Airflow has been around enough to be ‚Äúhardened‚Äù for production.
I will stop there but you get the point.
‚ÄúIf you‚Äôre looking for a basic, run-of-the-mill, bulletproof orchestration tool for your Data Engineering work, you would be foolish to overlook Airflow.‚Äù
Why is everyone trying to kill Airflow?
I think we found the answer to that. Not everyone is trying to kill Apache Airflow, throw it under the bus so to say. It‚Äôs just marketing crap mostly, but that‚Äôs ok. Look on the bright side of it all, Airflow is potent enough to bring to life Dagster and Prefect products that want to take bites off its heels.
Dagster and Prefect are wonderful tools, they are pushing the boundaries of what is possible in a data orchestration tool. It‚Äôs taking data pipelines to the next level and redefining what is popular and how we all solve problems."
89271506.we-need-to-talk-about-dbt,We need to talk about DBT.,How did DBT come out of nowhere to take over the Data Engineering and Analytics world.,2022-12-09 11:51:29.265-06,"Every once in a blue moon something comes out of nowhere and rises to prominence so quickly it makes your head spin. Is not
dbt
one of those few who have managed such an amazing feat? It feels like two years ago no one was talking or had heard about
dbt
, and now it‚Äôs everywhere and is embedded and supported by the Modern Data Stack.
In fact, if a Data Engineering tool doesn‚Äôt support in some way, shape, or form,
dbt
, then it‚Äôs seen as behind the times and on its way to the junk heap.
It‚Äôs a Product Managers dream come true, to create a tool that is so core and close to the hearts of its users, that they will fight for it, die for it, and market it for you.
DBT
has come to conquer the SQL world.
‚Äú
There has not been a game changer in the SQL world since the days of the
CTE
, that is until
dbt
came to town.‚Äù
Why did DBT take over the Data Engineering world?
That is actually a very simple question to answer. It‚Äôs an answer that most Data Gurus seem to have forgotten and ignored, to their own doom. What is the answer?
DBT has taken over the Data Engineering world because SQL, like it or not, makes up the vast majority of data pipelines and data stacks, even Modern ones.
It‚Äôs actually fairly obvious in retrospect, and it‚Äôs surprising the
dbt
rediscovered as it were, this very simple fact, that for decades SQL has been the lingua franca of Data Teams before there was anyone on that team called ‚ÄúData Engineer.‚Äù
This brings up a few funny dichotomies of Data Engineering.
The Modern Data Stack
is very modern
, so different from 10 years ago, but
SQL has not changed at all
.
Practically every new and popular tool puts out some sort of
ANSI-compliant SQL API
(think SparkSQL).
SQL is incredibly
easy to learn and use
, but was/is extremely
hard to apply Software Engineering best practices
to.
90% of the current Data Warehouses and Data Lakes
run off of SQL pipelines
.
A significant number of Data Teams are still trying to
migrate away from RDBMS classic Data Warehouse
systems.
Almost 100% of
Dashboards
and
Analytics
are run off of complex SQL.
‚Äú
All these SQL based Data Teams have been left feeling like they are second-class citizens in the Modern Data Stack.
‚Äù
Photo by
Atharva Tulsi
on
Unsplash
DBT is popular because SQL is ubiquitous.
So many tools have spent so much time trying to pry SQL from the cold dead hands of the Data kings and queens who‚Äôve been reigning over Data Lakes for years, and it just isn‚Äôt going to happen. SQL is here to stay, and with it
dbt
.
This should make any good Data Engineer who has or has not worked with
dbt
, to ask the question, what does
dbt
offer, like blood in the water, that has all the SQL sharks circling in their feeding frenzy?
What does DBT bring to the table?
Ok, ok, we all know what
dbt
says it does, transform, right? If you‚Äôre doing ETL or ELT,
dbt
was and is intended to take that
T
out of the mix. That seems a little strange uhh? Just another transform tool, one in a long line of products that take some data and transform it, spitting it out the back end. What is it about
dbt
that is different?
‚Äú
Replace boilerplate DDL/DML with simple SQL SELECT statements that infer dependencies, build tables and views, and run models in order
‚Äù - dbt website.
What you might not understand about the popularity of
dbt
, isn‚Äôt the fact it simply transforms data, but its
HOW
the data is transformed and the peripherals around that system that allow for innovation and expansion of SQL in a way that was never possible, or extremely difficult to obtain in the past.
dbt
brings a
concrete templating and abstraction layer
to SQL.
dbt
makes things like
unit testing SQL
, not only possible but easy.
dbt
brings the
project
concept and
YAML
files helping to integrate SQL into git
repositories
.
dbt
helps
document
your SQL projects and code.
I suggest jumping over to
dbt‚Äôs
documentation
if you‚Äôre not familiar with dbt, to see simple examples so you can understand better what I‚Äôm talking about. But these side effects of dbt listed above are things that made another simple transform tool explode in popularity.
It was incredibly insightful and solved a real and well-known problem on Data Engineering teams.
‚Äú
SQL pipelines have always been poorly documented, have little to no tests, hard to extend, and rarely scratch the surface of using well known software engineering best practices.‚Äù
DBT
said, ‚ÄúI will solve this problem‚Äù. And guess what, the overhead of dbt isn‚Äôt really that much. You can still write that familiar SQL.
If you write large amounts of SQL I strongly suggest you consider adding
dbt
to your Data Stack. DBT will force engineering rigor onto your projects and SQL which will pay massive dividends in the future. Sure, you can think of it as just another painful tool to manage, or you can be a glass-half-full person.
It‚Äôs well known the common headaches that plague all SQL-heavy teams. Overly complex spaghetti SQL, no reuse, no testing, no documentation. All these problems have very heavy hidden costs, ranging from wasted time and money to developer frustration and probably shorter tenures.
Is it all sunshine and roses? The hidden evil side of DBT.
Well of course there is, isn‚Äôt this true with every tool? Yes, but I think it‚Äôs important to remember there is a dark side to everything we do as Data Engineers, especially when we look at a single tool to be our savior as it were.
You need to be cognizant of the cost of running
dbt
.
Nothing is free in this life, so your Data Engineering tools, no matter how much you love them won‚Äôt be either. What you have to remember with DBT, as with any tool, this is the transformation were are talking about.
‚Äú
Transform is compute. Compute is money.‚Äù
If you aren‚Äôt careful with compute, for example, you decide to use dbt, then do way more Transform (models) and persist everything in-between, then yes, you might not be happy when you get your bill.
And of course, when you get popular like dbt, you are going to have detractors, some of them valid like
Ben Stancil on Substack talking about how dbt fails
.
Conclusion of dbt.
I still think what dbt did, and does, for the Data Engineering and SQL world is amazing, it solved a problem so primal and well-known it‚Äôs very hard to argue with. That makes its‚Äô concepts and implementation a worthy topic for any Data Engineer to dive into.
DBT
is popular because SQL is popular.
DBT
is popular because analytics are popular.
DBT
is popular because it makes SQL enter into the realm of Software Engineering.
Play around with
dbt
if you haven‚Äôt already, write your first model, write your first unit test of that model and be amazed. I plan to write more about dbt in the future, examine some of its features more in-depth, and how they can benefit us all as Data Engineers."
89014957.data-engineer-vs-senior-data-engineer,Data Engineer vs Senior Data Engineer,What is the difference between a Senior Data Engineering and a Data Engineer?,2022-12-07 11:36:57.753-06,"Engineers at every level and in any context are always asking some of the same questions, like what it takes to move to the next level. As humans alive on this planet, we are all cursed with a sort of self-deception, our own self-perception as it relates to our skillsets and skill level.
I have struggled with this often in my career. As a self-taught developer, it took me a long time to overcome imposter syndrome, fighting and striving to overcome that issue, and easily falling too far on the other side of the fence. Overestimating and seeing myself as superior in areas that I still need work in.
It‚Äôs like grading your own papers.
What do you see when you look in the mirror? How do you know when you are ready for the next level? What is the difference between a Senior Data Engineer and Data Engineer?
Data Engineering Central is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
The difference between Senior DE and DE isn‚Äôt what you think.
I can cleary remember a time in my life when I wanted the Senior Data Engineer title very badly. But that didn‚Äôt mean I was ready for it. Of course when you want something to happen, you will pretty much tell yourself and convince yourself of anything.
There are a few common misconceptions that more junior engineers have about becoming a Senior Data Engineer.
It‚Äôs purely technical skills that separate Senior from not-Senior level.
You have to be an expert on every new or popular tool released.
You have to be a star programmer, agile, TDD, OPP, functional genius.
You need to be able to code in Java, Scala, and Python.
Doing good work on time is enough to qualify you.
In my experience, although there is nothing particularly wrong with anything in that list, and they are all worthy goals to be pursued, in most cases it is not these things are are the separation between a Senior Data Engineer and a Data Engineer.
In fact, quite the opposite is true, and the skills and value add is at the other end of the spectrum.
What Senior Data Engineers bring to the table.
So if it isn‚Äôt a never ending list of amazing technical skills that make a Senior Data Engineer bring enough value to be given that title, what is it?
Here are a few of the skills and characteristics that I have seen Senior folks display consistently over time, and what makes them great team members and core drivers at the heart of Data Teams.
Able to
drive a project autonomously
without much need for guidance.
Able to
upskill
other team members.
Able to
delegate tasks
, not micro-manage and let others learn through trial and error.
Able to
learn new technical skills
and tools quickly when needed (
adaptability
).
Easy to work with,
kind
,
understanding
, and
empathic
.
Can make
hard decisions
and decide on a
direction
.
Ability to
admit mistakes
and course correct.
Breadth of
knowledge
on many topics (architecture, devops, ML, pipelines, SQL, etc.) ( I didn‚Äôt say absolute knowledge.)
Know what
corners to cut
, and which one‚Äôs should not be cut.
Aligned with the
business
and
product
, understanding what‚Äôs important for them.
Works hard
to write good,
clean code
.
‚ÄúSenior Data Engineers excel in many non-technical areas that help them provide value to the business and fellow team-members. Their technical skills in themselves are just icing on the cake.‚Äù
Let‚Äôs quickly dive into each one of these areas and see why they are such important skills that make the difference between a Senior Data Engineer and someone who is not.
First, I want to mention something. It‚Äôs not that technical skills are unimportant, it‚Äôs just that they are a given if you‚Äôre at a Senior level.
You should be writing unit tests, you should be writing concise clear code. Just being good at those things makes you a good individual contributor, if you want to multiple your impact, it takes something more.
Able to drive a project autonomously without much need for guidance.
This is probably one of the top non-technical skills any Data Engineer can gain, the ability to be given a task, project, epic, whatever, and manage that project well, defining units of work, understanding pain points, and drive that project through to the end without a lot of oversight.
So many skills are built into running a project, communication and understanding with business priorities, what‚Äôs important, what‚Äôs not, delegation, planning, and the list goes on.
Guidance takes time and energy, it‚Äôs great to give this type of direction to Engineers, but if you‚Äôre a Senior Data Engineer, you should need less of this. You should be able to run projects and make decisions yourself.
Able to upskill other team members.
Closely related to the last topic, the ability of a Senior Data Engineer to upskill other members of a Data Team is critical to the smooth function and efficient processing of projects and tasks.
Upskilled team-members
stay around longer
.
Upskilling engineers improves
relationships
and
communication
.
Upskilling engineers helps Seniors dig into
what they are teaching and why
.
As well, I‚Äôve found that upskilling other team members and lead to a lot of learning myself. Being challenged on some implementation, and looking at problems from a different perspective is the best way to learn and grow as a Senior as well.
Able to delegate tasks.
Nothing will slow down a data team to a painful crawl like someone who thinks they must work on everything themselves, or ‚Äúsaves‚Äù the most important and difficult tasks for themselves.
The belief that you are the only one smart enough to complete a task correctly is very harmful to others and yourself. It‚Äôs impossible to grow as a Data Engineer if you aren‚Äôt challenged with new problems to solve.
‚ÄúEvery Senior Data Engineer should actively seek to delgate difficult and important tasks to more junior members of a team, helping them and pushing them to grow and become confident in sovling problems.‚Äù
Senior Engineers who are able to delegate tasks free themselves up for more high level planning and work, paving the way for the next move in the career, maybe Staff and Team Lead level positions.
Adaptability
The Modern Data Stack and tech in general moves forward at a dizzy pace of constant change and innovation. It‚Äôs hard to be able to figure the marketing crazy from reality. Yet a Senior Data Engineer needs to be following what is happening in the industry, understanding what new tools and tech are truly game changing.
You can‚Äôt be stagnant and provide value for forward thinking Data Teams, trying to keep up on new releases, features being added to your current software, open source or otherwise, can
save time, money, and headaches
.
Kind
,
understanding
, and
empathic.
I can‚Äôt emphasize these character traits enough. You know what it really boils down too?
‚ÄúYou can‚Äôt lead people and processes very well if you aren‚Äôt simply a kind human being who is a joy to work with.‚Äù
Honestly, you have one life, you might as well be happy and have fun while doing it. You want your boss to be nice to you and promote you? You want more junior engineers to listen to you and follow you?
Here‚Äôs a novel idea. Be kind. Be pleasant. People inside and outside your Data Team  will enjoy working with you, and that builds a sort of organizational capital that is worth something.
Make hard decisions.
Of course life in the Data World isn‚Äôt always rainbows, unicorns, and sunshine. Many data projects are riddled with emotion, difficulties, and money, all leading to high tension and lots of decisions that can go many directions.
Picking a direction and sticking with it, making the best decision possible is a skill that should not be under-estimated.
Too many decisions get stuck in unhelpful arguments, folks picking sides based on what they like or their favorite tools. A Senior Data Engineer can cut through all the unnessesary fluff surrounding issues and just make the best decisions possible at the moment with what information is available.
Admit Mistakes.
Some of the hardest words to even leave your mouth, ever since you were a kid up until this very momement.
‚ÄúI made a mistake.‚Äù
There is something freeing about admitting you are a human, fraught with normal h human problems, like making a bad decisions. But, just because you made a bad decisions doesn‚Äôt mean things have to stay that way.
The best part about being open and honest is the ability to have people trust you more, and the option to fix the problem and move forward with a better solution!
Everyone wins.
Breadth of knowledge.
I‚Äôm not nessesarily talking about depth of knowledge here, although that is a good thing. The more the depth the better. But, it‚Äôs extremely important for Senior Data Engineers to have a breadth of knowledge across a wide range of topics.
This is one of the skills that make a Senior a Senior.
Knowing a little about most cloud platforms (AWS, GCP, Azure), Big Data Processing tools, SQL and databases, SQL vs NoSQL, DevOps, bash, server basics, security and permissions, make someone a very valuable asset.
I know it sounds like a lot, but having the basics across a wide range of topics will help the Senior Data Engineer make better and more informed decisions.
Know what
corners to cut
.
Maybe cutting corners has a bad connotation. Is there a better way to communicate this idea?
Knowing where to put more effort, and where to put less?
There you have it. This is something that most all junior engineers get caught up in.
‚ÄúSpending way too much time on un-important areas drags down productivity and has a major drag on a Data Team.‚Äù
Time isn‚Äôt infinite, you can‚Äôt just do what you want all the time. Projects have deadlines, some tasks are more important than others. In the end, being a productive solver of data problems requires that you understand where to put time, and where not to put time.
Aligned with the
business
and
product.
A related topic is being in sync with business and product in your organization. Like it or not the Product and other business teams are critical to the success of the company you work for. They have valid view points and needs.
Ignoring the needs of teams outside of Data, or not prioritizing them above your  own needs will have serious negative impact.
If you are not a team player it‚Äôs unlikely you will make to the Senior level, and have little chance of moving past that point. Being seen as someone who provides organizational value is a big deal for your career and your journey towards a Senior level roll and higher.
Works hard
to write good,
clean code
.
I do want to emphasize that writing good clean code, and being at the top of your game is almost a prerequisite for becoming a Senior Data Engineer, or at least should be. Data Teams will look to you, as a Senior, to model best practices at all times and in all the work you do.
Writing good
unit tests
.
Writing
DRY
code.
Keeping
KISS
at the front.
OOP
or
Functional
, doing it well.
Knowing and keeping
best practices
.
Not taking
short-cuts
.
Taking the time to write
readable
,
well documented
, and
maintainable
code.
I‚Äôm not saying you have to be perfect or the top of your craft. But you should be on your way and that should be the trajectory of the projects and code you work on. Not everything is going to be perfect all the time. We make mistakes and take shortcuts when we shouldn‚Äôt.
But, that isn‚Äôt who a Senior Data Engineer is.
In summary.
Of course I don‚Äôt think this list is all inclusive of what it takes to become a Senior Data Engineer, this is just my take based on my experience. There are all things I wish I knew in the beginning, of course I still would have had to learn them and work on them, but knowing this would have been helpful.
I was under the harmful impression in the beginning of my data life that all I needed to do was work on becoming the best at SQL and the best at programming. This just simply isn‚Äôt the case. In fact, if that‚Äôs all you do, it could be harmful to your long term career.
Learning to work with others and build no-technical skills is just as, if not more, important."
88471768.coming-soon,Coming soon,,2022-12-03 16:13:49.586-06,"This is Data Engineering Central
, a newsletter about Long Live the Data Engineer..
Subscribe now"
